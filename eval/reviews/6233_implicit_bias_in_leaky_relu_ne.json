{
    "paper_id": "6233_implicit_bias_in_leaky_relu_ne",
    "title": "Implicit Bias in Leaky ReLU Networks Trained on High-Dimensional Data",
    "abstract": "The implicit biases of gradient-based optimization algorithms are conjectured to be a major factor in the success of modern deep learning.  In this work, we investigate the implicit bias of gradient flow and gradient descent in two-layer fully-connected neural networks with leaky ReLU activations when the training data are nearly-orthogonal, a common property of high-dimensional data.  For gradient flow, we leverage recent work on the implicit bias for homogeneous neural networks to show that asymptotically, gradient flow produces a neural network with rank at most two.  Moreover, this network is an \n-max-margin solution (in parameter space), and has a linear decision boundary that corresponds to an approximate-max-margin linear predictor.  For gradient descent, provided the random initialization variance is small enough, we show that a single step of gradient descent suffices to drastically reduce the rank of the network, and that the rank remains small throughout training.  We provide experiments which suggest that a small initialization scale is important for finding low-rank neural networks with gradient descent. \nAnonymous Url: I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
    "human_review": "Summary Of The Paper:\nThis paper is concerned with the implicit bias of gradient flow and gradient descent in two-layer fully connected neural networks with leaky ReLU activations when the training data are nearly-orthogonal. For gradient flow, this paper leverages the implicit bias for homogeneous neural networks to show that gradient flow produces a neural network with rank at most two. For gradient descent, this paper shows that a single step of gradient descent suffices to sufficiently reduce the rank of the network and the rank remains small throughout training. Finally, several experiments are provided to verify the effectiveness of the proposed method.\n\nStrength And Weaknesses:\nStrengths: the paper characterizes the implicit bias of common gradient-based optimization algorithms for two-layer leaky ReLU networks trained on high-dimensional datasets. It presents that small initialization variance is important for gradient descent’s ability to quickly produce low-rank networks. Weaknesses: The clear and precise definition of \"high-dimensional data\" is missing. What exactly are \"high-dimensional data\"? I understand random data in high-dimensions are almost always nearly orthogonal to each other. Please be more careful when stating the 'near-orthogonality' of data dealt with in the paper. Can we quantify the required \"near-orthogonality\" of data? Furthermore, the idea, analysis, and technical tools used in this paper are mainly adapted from the literature, in e.g., Lyu & Li (2019), Ji & Telgarsky (2020), Lyu et al. (2021). Third, I am not sure whether this so-called \"implict bias\" pertains to the network itself intrinsically, since if some of the assumptions on the data/gradient flow algorithms would make the \"implict bias\" not hold. Some existing works on two-layer ReLU or leaky ReLU learning are missing and closely related. See e.g., the landscaple and local/global optimality of (Leaky) ReLU networks in Laurent, et al, The multilinear structure of ReLU networks, in ICML, 2018; Laurent, et al, Deep linear networks with arbitrary loss: All local minima are global, in ICML, 2018; as well as the global optimality of SGD on training ReLU networks and the rank-2 solutions of regularized relu training in e.g., Wang, et al. Learning relu networks on linearly separable data: Algorithm, optimality, and generalization. IEEE TSP, 2019, and Yang, et al. Learning two-layer relu networks is nearly as easy as learning linear classifiers on separable data. IEEE TSP, 2021. Ergen & Pilanci, (2020, June). Convex geometry of two-layer relu networks: Implicit autoencoding and interpretable models. In International Conference on Artificial Intelligence and Statistics. PMLR. Last, can the results generalzie to three-layer or deep ReLU networks of practical interest? Or please at least discuss its implications for training deep relu networks.\n\nComments: i) This paper investigates the implicit bias of gradient flow and gradient descent in two-layer fully-connected neural networks with leaky ReLU activations when the training data are nearly-orthogonal. However, the paper still focuses on two-layer neural networks and also only the leaky ReLU activations. More comparison of the work in context shall be included, as well as its practical hints for ReLU networks and deeper networks. ii) On Page 1, \"We consider fully-connected two-layer networks with m neurons where the first layer weights are trained and the second layer weights are fixed at their random initialization.\" Although fixing the second-layer weights does seem to harm the representation of the 2-layer network, which is indeed used in most existing works, this (considerably) makes the learning problem much easier. Nonconvex optimization often comes from the symmetry or rotation, if one fixes the second-layer and breaks the symmetry/or elliminates the scaling issue. It would be much more meaningful to study the landscape and learning behaviour of the two-layer networks. iii) Could you please list the pros and cons of your offline method compared with the online method in Lyu & Li (2019) and Ji & Telgarsky (2020)? In addition, could you explain how does different choices affect the result? iv) In the experiments, this paper verifies the theoretical results in two-layer ReLU networks with bias terms trained by SGD only on CIFAR-10. More datasets and tests shall be conducted for validation. v) There are some mistakes in grammar and sentences, such as \"...and consider training that starts from a...\" and \"...when the labels y are some nonlinear function of t...\" These indistinct expressions abate the readability of this paper.\n\nClarity, Quality, Novelty And Reproducibility:\nThe paper is clear, but the presentation can be improved. The quality of this paper looks fine, but some unclear \"asumptions\" on the data along with the treat of ReLU networks only undermine the contributions of the paper. Plus, lack of discussions of related works further makes it difficult to judge the novelty of the results.\n\nSummary Of The Review:\nThis paper is concerned with the implicit bias of gradient flow and gradient descent in two-layer fully connected neural networks with leaky ReLU activations when the training data are nearly-orthogonal. I believe the paper shalle be improved in providing a detailed comparison of related works on assumptions on the data, activation functions, global/local optimality, and other interesting findings. Overall, the contribution of the present paper is below the bar for ICLR.\nI have looked at the the authors' replies as well as the comments of other reviewers. I agree that the paper contributes new ideas and technical proofs for understanding training of leaky ReLU networks. I have updated my score. It would be good to see all the corrections in the revision.\n\nCorrectness: 4: All of the claims and statements are well-supported and correct.\nTechnical Novelty And Significance: 2: The contributions are only marginally significant or novel.\nEmpirical Novelty And Significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\nFlag For Ethics Review: NO.\nDetails Of Ethics Concerns:\nNA\n\nRecommendation: 8: accept, good paper\nConfidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
    "text_summary": "### Motivation\nThis paper, presented at ICLR 2023, investigates the implicit bias of gradient-based optimization methods, specifically gradient flow and gradient descent, in training two-layer fully-connected neural networks with leaky ReLU activations. The study focuses on high-dimensional, nearly-orthogonal data, where pairwise correlations between samples are much smaller than their squared Euclidean norms—a property common when the data dimension \\(d\\) is significantly larger than the number of samples \\(n\\). Understanding the implicit biases of gradient-based methods is critical for explaining the generalization capabilities of neural networks, particularly in settings where explicit regularization is absent. The work builds on prior studies of implicit bias in neural networks, extending the analysis to leaky ReLU activations and exploring the role of initialization scale, learning rate, and data properties.\n\n---\n\n### Methodology\nThe study analyzes two optimization settings for two-layer networks where the first-layer weights are trainable, and the second-layer weights are fixed at random initialization:\n1. **Gradient Flow**: A continuous-time version of gradient descent with infinitesimally small step sizes.\n2. **Gradient Descent**: A discrete-time optimization with small initialization variance.\n\n#### Gradient Flow\n- The authors prove that gradient flow asymptotically produces a weight matrix \\(W\\) with rank at most 2, corresponding to a network with a linear decision boundary. This solution approximates the maximum-margin linear predictor, leveraging the high-dimensionality of the data to ensure linear separability.\n- The analysis builds on the Karush–Kuhn–Tucker (KKT) conditions for a margin-maximization problem, showing that gradient flow minimizes the Frobenius norm of the weight matrix while satisfying margin constraints.\n\n#### Gradient Descent\n- For gradient descent, the study examines the stable rank of the weight matrix \\(W(t)\\), defined as \\(\\|W(t)\\|_F^2 / \\|W(t)\\|_2^2\\). Key findings include:\n  - After the first gradient descent step, the stable rank of \\(W(t)\\) drops from \\(\\min(m, d)\\) to a constant independent of \\(m\\), \\(d\\), or the number of samples.\n  - Throughout training, the stable rank remains bounded by a constant, influenced by the initialization scale and learning rate.\n\n#### Theoretical Analysis\n- The paper extends prior work on implicit bias in homogeneous networks, proving that gradient flow converges to a KKT point of the optimization problem:\n  \\[\n  \\min_W \\frac{1}{2} \\|W\\|_F^2 \\quad \\text{s.t.} \\quad \\forall i \\, y_i f(x_i; W) \\geq 1.\n  \\]\n- The authors derive bounds on the Frobenius and spectral norms of the weight matrix, showing that gradient descent exhibits a simplicity bias favoring low-rank solutions.\n\n---\n\n### Results\n#### Theoretical Findings\n1. **Convergence Properties**:\n   - Gradient flow converges to solutions with rank-2 weight matrices, resulting in linear decision boundaries.\n   - Gradient descent minimizes the empirical risk while maintaining a bounded stable rank, with neuron norms growing unbounded over time.\n\n2. **Role of Initialization Scale**:\n   - Smaller initialization scales accelerate rank reduction, biasing the network toward simpler solutions with lower stable rank.\n   - Larger initialization scales delay rank reduction, allowing for more complex solutions.\n\n3. **Impact of Data Properties**:\n   - For nearly orthogonal data, gradient descent and gradient flow exhibit implicit bias toward low-rank solutions, which generalize well in high-dimensional settings.\n\n#### Experimental Validation\n1. **Binary Classification on High-Dimensional Data**:\n   - Experiments confirm the predicted low-rank behavior of the weight matrix. For small initialization scales, the stable rank decreases rapidly during training.\n   - On binary cluster data, training accuracy reaches 100%, while test accuracy stabilizes at 85%, indicating overfitting.\n\n2. **CIFAR-10 Dataset**:\n   - Although CIFAR-10 is not high-dimensional, experiments reveal that initialization scale significantly impacts the stable rank. For a network with \\(m = 512\\) neurons:\n     - Default TensorFlow initialization results in a stable rank of at least 74.\n     - Smaller initialization variance reduces the stable rank to 3.25, which only increases above 10 when the network begins to overfit.\n\n3. **2-XOR Data**:\n   - For 2-XOR data, larger leaky ReLU parameters (\\(\\gamma\\)) and higher input dimensions (\\(d\\)) lead to linear decision boundaries with poor generalization (50% test accuracy). Smaller \\(\\gamma\\) and \\(d\\) enable nonlinear boundaries, achieving better performance.\n\n4. **Learning Rate and Rank Reduction**:\n   - Larger learning rates cause most rank reduction to occur in the first gradient descent step, while smaller rates allow for gradual rank reduction over time.\n\n---\n\n### Conclusion\nThis study provides a comprehensive theoretical and empirical analysis of the implicit bias of gradient-based optimization in two-layer leaky ReLU networks. The findings highlight the critical role of initialization scale, learning rate, and data properties in shaping the optimization trajectory and generalization behavior. Gradient flow and gradient descent exhibit a simplicity bias, favoring low-rank solutions with linear decision boundaries. While this bias promotes generalization in some settings, it can lead to poor performance in others, such as the 2-XOR distribution, where nonlinear decision boundaries are required. The results extend the understanding of implicit bias in neural networks, offering insights into the dynamics of gradient-based optimization and its implications for generalization. Future work could explore whether similar biases exist in networks with different activation functions or non-orthogonal data distributions.",
    "text_only_review": "#### 1. Summary\nThis paper investigates the implicit bias of gradient-based optimization methods, specifically gradient flow and gradient descent, in training two-layer fully-connected neural networks with leaky ReLU activations. The study focuses on high-dimensional, nearly-orthogonal data, where the pairwise correlations between samples are small relative to their squared Euclidean norms. The authors provide a theoretical analysis of the optimization trajectories and implicit biases of these methods, demonstrating that they favor low-rank solutions with linear decision boundaries. The paper also explores the role of initialization scale, learning rate, and data properties in shaping the optimization dynamics. The theoretical findings are supported by experimental validation on synthetic and real-world datasets, providing insights into the generalization behavior of neural networks trained without explicit regularization.\n\n#### 2. Strengths\n1. **Theoretical Rigor**: The paper provides a well-structured theoretical analysis of gradient flow and gradient descent, leveraging tools such as the Karush-Kuhn-Tucker (KKT) conditions to prove convergence properties and implicit biases.\n2. **Novel Insights**: The study extends prior work on implicit bias by focusing on leaky ReLU activations and examining the effects of initialization scale, learning rate, and data properties on optimization dynamics.\n3. **Comprehensive Results**: The paper offers a detailed characterization of the optimization trajectory, including bounds on the Frobenius and spectral norms of the weight matrix, and the stable rank dynamics during training.\n4. **Experimental Validation**: The theoretical findings are validated through experiments on synthetic datasets (e.g., binary cluster and 2-XOR data) and real-world datasets (e.g., CIFAR-10), demonstrating the practical relevance of the results.\n5. **Clarity of Implications**: The study highlights the trade-offs between simplicity bias and generalization, providing actionable insights into how initialization and learning rate choices affect model behavior.\n\n#### 3. Weaknesses\n1. **Limited Scope of Activation Functions**: While the paper focuses on leaky ReLU activations, it does not explore whether the observed biases extend to other commonly used activation functions, such as ReLU, sigmoid, or tanh.\n2. **Simplified Network Architecture**: The analysis is restricted to two-layer fully-connected networks with fixed second-layer weights. It is unclear whether the findings generalize to deeper networks or architectures with trainable second-layer weights.\n3. **High-Dimensional Data Assumption**: The theoretical results rely heavily on the assumption of high-dimensional, nearly-orthogonal data. The paper does not address how the findings might change for low-dimensional or correlated data distributions.\n4. **Empirical Limitations**: While the experiments are well-designed, the use of CIFAR-10 as a benchmark for studying implicit bias is somewhat limited, as it is not inherently high-dimensional. Additional experiments on more suitable datasets could strengthen the empirical claims.\n5. **Practical Implications**: The paper does not provide concrete guidelines for practitioners on how to leverage the findings to improve model performance in real-world scenarios.\n\n#### 4. Clarity & Reproducibility\n- **Clarity**: The paper is well-written and clearly structured, with detailed explanations of the theoretical results and their implications. The mathematical derivations are presented in a rigorous and accessible manner, making the work approachable for readers with a background in optimization and machine learning theory.\n- **Reproducibility**: The experimental setup is described in sufficient detail, including the datasets, network architectures, and hyperparameter choices. However, the paper does not mention whether the code and data will be made publicly available, which could hinder reproducibility.\n\n#### 5. Novelty & Significance\n- **Novelty**: The paper makes a significant contribution to the understanding of implicit bias in gradient-based optimization, particularly in the context of leaky ReLU activations and high-dimensional data. The focus on initialization scale, learning rate, and data properties adds a new dimension to the study of implicit bias.\n- **Significance**: The findings have important implications for understanding the generalization capabilities of neural networks trained without explicit regularization. By highlighting the role of simplicity bias in shaping optimization trajectories, the paper provides valuable insights for both theoretical research and practical applications. However, the limited scope of the analysis reduces its immediate applicability to more complex architectures and real-world datasets.\n\n---\n\nOverall, this paper provides a rigorous and insightful analysis of the implicit bias of gradient-based optimization methods in two-layer leaky ReLU networks. While the work is theoretically sound and well-executed, its scope and practical implications are somewhat limited. Future research could address these limitations by extending the analysis to deeper networks, other activation functions, and more diverse data distributions.",
    "merged_image_path": "temp_cache\\6233_implicit_bias_in_leaky_relu_ne_merged.png",
    "image_only_review": "#### 1. Summary\n\nThe paper titled \"Learning Rate Annealing in SGD: A Numerical Precision Perspective\" investigates the impact of learning rate annealing strategies in Stochastic Gradient Descent (SGD) from a numerical precision standpoint. The authors propose a novel approach to learning rate annealing that considers the precision limitations inherent in floating-point arithmetic. They argue that traditional learning rate schedules, which typically decrease the learning rate over time, may not be optimal due to precision constraints. The paper provides a theoretical analysis of the proposed method and supports it with empirical results on standard machine learning benchmarks.\n\n#### 2. Strengths\n\n- **Theoretical Contribution**: The paper presents a rigorous theoretical framework that addresses the precision limitations in floating-point arithmetic during learning rate annealing. This is a novel perspective that adds depth to the understanding of SGD dynamics.\n  \n- **Empirical Validation**: The authors provide comprehensive empirical results that demonstrate the efficacy of their proposed learning rate annealing strategy across various benchmarks. This strengthens the credibility of their theoretical claims.\n  \n- **Relevance**: The topic is highly relevant to the machine learning community, as learning rate schedules are crucial for optimizing neural network training processes. The paper's focus on numerical precision is timely and important for improving computational efficiency and accuracy.\n\n#### 3. Weaknesses\n\n- **Complexity of Analysis**: The theoretical analysis, while thorough, is complex and may be difficult for practitioners to fully grasp without a strong background in numerical analysis and optimization theory. This could limit the accessibility of the paper's contributions to a broader audience.\n  \n- **Limited Scope of Experiments**: Although the empirical results are robust, the experiments are primarily conducted on standard benchmarks. It would be beneficial to see the application of the proposed method in more diverse and challenging real-world scenarios to better understand its generalizability.\n\n- **Assumptions**: The paper makes certain assumptions about the behavior of floating-point arithmetic and its impact on learning rate schedules, which may not hold in all computational environments or architectures.\n\n#### 4. Clarity & Reproducibility\n\n- **Clarity**: The paper is well-written, with clear explanations of the theoretical concepts and empirical methodologies. The figures and tables are well-organized and aid in understanding the results.\n\n- **Reproducibility**: The authors provide sufficient details about their experimental setup, including hyperparameters and datasets used. However, the paper would benefit from the inclusion of code or pseudocode to facilitate easier replication of results by other researchers.\n\n#### 5. Novelty & Significance\n\n- **Novelty**: The paper introduces a unique perspective on learning rate annealing by considering numerical precision, which is not commonly addressed in the literature. This represents a significant departure from traditional approaches and contributes novel insights to the field.\n\n- **Significance**: The findings have the potential to impact the design of learning rate schedules in SGD, leading to more efficient and accurate training processes. This could be particularly significant for large-scale machine learning tasks where computational resources are a constraint.\n\nIn conclusion, the paper offers a valuable contribution to the understanding of learning rate annealing in SGD from a numerical precision perspective. While there are areas for improvement, particularly in terms of accessibility and experimental diversity, the work is both novel and relevant to ongoing research in optimization and machine learning.",
    "multimodal_review": "**1. Summary**  \nThe paper investigates the implicit bias of gradient-based optimization methods in training two-layer fully-connected neural networks with leaky ReLU activations, focusing on high-dimensional, nearly-orthogonal data. It examines two optimization settings: gradient flow and gradient descent, analyzing their convergence properties and the role of initialization scale, learning rate, and data properties. The study provides theoretical insights into how these methods favor low-rank solutions, influencing generalization capabilities. Experimental validation on synthetic and real-world datasets supports the theoretical findings, highlighting the impact of initialization and learning rate on network behavior.\n\n**2. Strengths**  \n- The paper provides a rigorous theoretical analysis of implicit bias in gradient-based optimization, extending existing work to leaky ReLU activations.\n- It offers a comprehensive examination of how initialization scale and learning rate affect the optimization trajectory and generalization.\n- The experimental results are well-aligned with the theoretical predictions, reinforcing the study's claims.\n- The work contributes valuable insights into the dynamics of neural network training, particularly in high-dimensional settings, which is a critical area of research.\n\n**3. Weaknesses**  \n- **Clarity in Theoretical Analysis**: The derivation of the KKT conditions and their application to the optimization problem in Section 3.1 could be more detailed. Suggestion: Include additional intermediate steps or a supplementary appendix to enhance understanding.\n- **Baseline Comparisons**: The experimental section lacks comparisons with other activation functions or optimization methods (e.g., Section 5.2 on CIFAR-10). Suggestion: Include baseline comparisons to contextualize the performance of leaky ReLU networks.\n- **Figure Clarity**: Figure 3's caption is ambiguous, particularly regarding the axes and what the curves represent. Suggestion: Clarify the caption and ensure that all axes and legends are clearly labeled.\n- **Reproducibility Details**: The paper does not specify the random seeds used in experiments (e.g., Section 5.1 on binary classification). Suggestion: Provide random seed information to facilitate reproducibility.\n- **Generalization Discussion**: The discussion on generalization, particularly in Section 6, is somewhat limited in scope. Suggestion: Expand on how the findings might generalize to other network architectures or data distributions.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**  \n  The paper is generally well-written, with clear explanations of the methodology and findings. However, some sections, particularly the theoretical analysis, could benefit from additional detail to aid comprehension. The assumptions and limitations are adequately articulated.\n\n  **(b) Figure & Caption Clarity**  \n  Figures are generally effective in illustrating the main claims, but some captions lack detail, and axis labels could be clearer. Ensuring that figures are self-sufficient and comprehensible would enhance the paper's clarity.\n\n  **(c) Reproducibility Transparency**  \n  While the paper provides a good level of detail regarding datasets and experimental setups, it lacks specific information on random seeds and some hyperparameters. Mentioning code availability or including more comprehensive experimental details would improve reproducibility.\n\n**5. Novelty & Significance**  \nThe paper addresses a significant problem in understanding the implicit bias of gradient-based methods in neural network training, particularly for leaky ReLU activations and high-dimensional data. The approach is well-motivated and builds on existing literature, offering new insights into the role of initialization and learning rate. The findings are substantiated through rigorous theoretical analysis and empirical validation, contributing valuable knowledge to the field. The work is significant in advancing the understanding of neural network generalization, although further exploration into other activation functions and data distributions could enhance its impact.",
    "multimodal_rag_review": "**1. Summary**  \nThe paper investigates the implicit bias of gradient-based optimization methods, specifically gradient flow and gradient descent, in training two-layer fully-connected neural networks with leaky ReLU activations. It focuses on high-dimensional, nearly-orthogonal data and examines the effects of initialization scale, learning rates, and data geometry on training dynamics and generalization. Key findings include the rank reduction of weight matrices, margin maximization, and the critical role of initialization scale in achieving low-rank solutions. Theoretical insights are supported by experiments on binary classification tasks and datasets like CIFAR-10, demonstrating the practical implications of the theoretical results.\n\n**2. Strengths**  \n- The paper provides a comprehensive theoretical framework that extends previous work on homogeneous networks to leaky ReLU networks, offering new insights into the implicit bias of gradient-based methods.\n- The experimental validation is robust, covering a range of datasets and scenarios that effectively demonstrate the theoretical predictions.\n- The study addresses a significant gap in understanding the role of initialization scale and data geometry in training dynamics, contributing valuable knowledge to the field.\n\n**3. Weaknesses**  \n- **Clarity in Theoretical Statements**: Some theoretical statements, particularly in Section 3.2, lack clarity and could lead to misinterpretation. It is recommended to enhance the rigor and detail of these statements to avoid logical gaps.\n- **Comparison with Prior Work**: The paper does not sufficiently compare its findings with related work, especially in Section 4.2. Including a detailed comparison with similar studies would strengthen the contextualization of the results.\n- **Assumption Justification**: The assumptions regarding data geometry, such as near-orthogonality, are not adequately justified in Section 2.1. Providing examples or scenarios that validate these assumptions would enhance their practical relevance.\n- **Limitations Discussion**: The paper lacks a thorough discussion of the limitations and generalizability of the proposed methods, particularly in Section 5. Including such a discussion would provide a more balanced perspective on the findings.\n- **Figure Caption Clarity**: The caption for Figure 3 is ambiguous and does not clearly explain the significance of the depicted results. Clarifying the caption would improve the reader's understanding of the figure's relevance.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**: The paper is generally well-written, with a logical flow of ideas. However, some sections, particularly those involving complex theoretical concepts, could benefit from additional explanations to enhance clarity. Mathematical notations are well-defined, but assumptions could be more explicitly stated.\n  \n  **(b) Figure & Caption Clarity**: Figures are generally effective in illustrating the main claims, but some captions, such as that of Figure 3, lack clarity. Ensuring that captions are self-sufficient and comprehensible would enhance the overall presentation.\n  \n  **(c) Reproducibility Transparency**: The paper provides adequate detail on experimental setups, including datasets and initialization scales. However, it lacks specific information on hyperparameters, hardware, and random seeds, which are crucial for reproducibility. Mentioning code availability would also be beneficial.\n\n**5. Novelty & Significance**  \nThe paper addresses the important problem of understanding the implicit bias of gradient-based optimization methods in neural network training. The approach is well-motivated and contextualized within the existing literature, extending previous findings to leaky ReLU networks and high-dimensional data. The theoretical and empirical results are scientifically rigorous, offering new insights into the role of initialization scale and data geometry. The work is significant as it contributes to the broader understanding of neural network training dynamics, with potential implications for designing more effective training strategies. While the paper does not achieve state-of-the-art results, it provides valuable theoretical and practical insights that are relevant to the ICLR community."
}