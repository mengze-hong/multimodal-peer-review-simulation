{
    "paper_id": "6357_learning_a_data_driven_policy_",
    "title": "Learning a Data-Driven Policy Network for Pre-Training Automated Feature Engineering",
    "abstract": "Feature engineering is widely acknowledged to be pivotal in tabular data analysis and prediction. Automated feature engineering (AutoFE) emerged to automate this process managed by experienced data scientists and engineers conventionally. In this area, most — if not all — prior work adopted an identical framework from the neural architecture search (NAS) method. While feasible, we posit that the NAS framework very much contradicts the way how human experts cope with the data since the inherent Markov decision process (MDP) setup differs. We point out that its data-unobserved setup consequentially results in an incapability to generalize across different datasets as well as also high computational cost. This paper proposes a novel AutoFE framework Feature Set Data-Driven Search (FETCH), a pipeline mainly for feature generation and selection. Notably, FETCH is built on a brand-new data-driven MDP setup using the tabular dataset as the state fed into the policy network. Further, we posit that the crucial merit of FETCH is its transferability where the yielded policy network trained on a variety of datasets is indeed capable to enact feature engineering on unseen data, without requiring additional exploration. To the best of our knowledge, this is a pioneer attempt to build a tabular data pre-training paradigm via AutoFE. Extensive experiments show that FETCH systematically surpasses the current state-of-the-art AutoFE methods and validates the transferability of AutoFE pre-training.",
    "human_review": "Summary Of The Paper:\nThis paper proposes a first of its kind architecture framework for automated feature engineering called Fetch, a system based on brand new data driven Markov decision process. The authors identify critical gaps by stating the current methods for AutoFe are insufficient when it comes resembling human effort in handling datasets as the underlying Markov decision setup is different i.e. more based on trial and error, which leads to poor generalization across datasets and higher computational costs. The new method also has a key element of transferability, which is basically the ability to enable feature engineering on new datasets using prior policy networks trained on previous datasets, without the need for additional data exploration. The authors present evidence that Fetch is superior to the existing state of the art automated feature engineering methods such as Random, DFS, AutoFeat, NFS, DIFER as well as AutoML methods such as AutoSklearn and AutoGluon. The method is also tested for transferability by application on several datasets.\n\nThe authors argue that the approach comes very close to mimicking human experts when it comes to handling new datasets when it comes to transferring experience.\n\nStrength And Weaknesses:\nStrengths:\n\nOne of the key strengths of this paper is its ability to provide transferability between datasets which previous methods have not been able to provide.\nThe method is highly flexible as it has been applied on different datasets as well different ML models.\nOverall there is sound discussion relevant work, gap analysis to find opportunity of development, strong mathematical rigour and experimental setup, evaluation and analysis.\nWeaknesses:\n\nThe authors declare that to the best of their knowledge there aren't any autoFE/autoML workarounds for managing tabular data to accomplish transferability. However this could be supported by a stronger statement that can more concretely say if such methods exist or not by clearly stating a comprehensive survey of analysis did not yield any methods.\nThis method is specifically geared towards tabular datasets. While tabular datasets prevail in several key applications, it would be interesting to know how this method applies/does not apply to any other form of datasets (eg. images, speech, unstructured datasets). At least there could be some discussion on consideration, if not a full scale evaluation.\n\nClarity, Quality, Novelty And Reproducibility:\nNovelty:\n\nPaper proposes a new form of markov decision process to generate features. Previous methods take number of features and only update using sequences of previous actions, iteratively. Whereas Fetch provides Feature engineering actions and constructions actions, instead of features, iteratively based on the newly generated datasets (features).\nAccording to this paper, Previous methods for AutoFE have never explored transferability between datasets, which has been done by this method.\nClarity & Quality: The paper has high clarity with explicit background provided and methods explained with sufficient depth both in terms of mathematical equations, datasets and analysis of results. There is further in-depth explanation in the appendix section around usage of material and datasets which is crystal clear.\n\nReproducibility: The method has been extensively tested on publicly available datasets which are linked. Algorithm and methods have been provided as well. The paper states that all experiments have been run with open source code provided.\n\nSummary Of The Review:\nI propose that we accept this paper on the basis of its merits around developing a new AutoFE method based on a novel markov decision process which not only is superior to the existing state of the art in terms of performance but also has a unique feature of transferability. Both strong points are well argued and evidenced in the paper throughout with sufficient gap analysis around opportunity areas, discussion of background and related work, strong mathematical rigor and sound experimental design setup, with thorough data analysis, discussion of evaluation and results. Although the work is limited to tabular data there is thoughtful coverage on several popular datasets including regression and classification examples and comparison to other AutoFE models and coverage across several ML models.\n\nCorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\nTechnical Novelty And Significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\nEmpirical Novelty And Significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\nFlag For Ethics Review: NO.\nDetails Of Ethics Concerns:\nNone.\n\nRecommendation: 8: accept, good paper\nConfidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
    "text_only_review": "#### 1. Summary\n\nThe paper introduces FETCH, a novel framework for automated feature engineering (AutoFE) tailored for tabular data. FETCH addresses key limitations of existing AutoFE methods by adopting a data-driven approach that directly observes datasets, incorporates transferability via pre-training, and evaluates feature sets holistically rather than individual features. The framework is based on a Markov Decision Process (MDP) and employs a Transformer-inspired policy network trained with Proximal Policy Optimization (PPO). FETCH supports a wide array of feature engineering operations and is pre-trained on multiple datasets to enhance generalizability. Extensive experiments demonstrate FETCH's superior performance, transferability, and efficiency compared to state-of-the-art AutoFE methods and AutoML frameworks. The paper highlights FETCH's scalability, flexibility across machine learning models, and its potential for real-world applications.\n\n---\n\n#### 2. Strengths\n\n1. **Innovative Framework**:\n   - The use of an MDP-based approach that directly observes tabular data is a novel and well-motivated contribution, addressing a critical gap in existing AutoFE methods.\n   - The Transformer-inspired policy network is well-suited for handling the permutation-invariant and variable-length nature of tabular data.\n\n2. **Transferability**:\n   - The pre-training paradigm is a significant contribution, enabling FETCH to generalize across datasets and reducing computational costs for new tasks. This is a pioneering step in AutoFE.\n\n3. **Comprehensive Evaluation**:\n   - The experiments are thorough, covering effectiveness, transferability, efficiency, scalability, and flexibility.\n   - FETCH's performance improvements over state-of-the-art methods and AutoML frameworks are well-documented and statistically significant.\n\n4. **Scalability and Efficiency**:\n   - FETCH demonstrates strong scalability to large datasets and high sample efficiency, making it practical for real-world applications.\n\n5. **Flexibility**:\n   - The framework's compatibility with various machine learning models and its support for diverse feature engineering operations enhance its applicability across domains.\n\n6. **Interpretability**:\n   - The naming convention for generated features (e.g., \"Weight/Height²\") improves interpretability, aligning FETCH's outputs with human expert practices.\n\n7. **Open Source**:\n   - The availability of the source code encourages reproducibility and further research, which is commendable.\n\n---\n\n#### 3. Weaknesses\n\n1. **Limited Discussion on Computational Overhead**:\n   - While FETCH demonstrates superior performance, the paper provides limited details on its computational requirements (e.g., runtime, memory usage) compared to other methods, particularly for large-scale datasets. This could be a concern for practitioners with limited resources.\n\n2. **Complexity of Implementation**:\n   - The Transformer-inspired policy network and reinforcement learning setup may pose challenges for practitioners unfamiliar with these techniques. A discussion on simplifying the implementation or providing pre-trained models for easier adoption would be beneficial.\n\n3. **Ablation Study Coverage**:\n   - Although the ablation study highlights the importance of higher-order features, it does not thoroughly explore the impact of other design choices, such as the architecture of the policy network or the choice of PPO for training.\n\n4. **Limited Exploration of Multimodal Data**:\n   - While the paper mentions future work on multimodal data, it does not provide insights into how FETCH could be extended to handle such data. A preliminary discussion or experiment in this direction would strengthen the paper.\n\n5. **Evaluation Metrics**:\n   - The paper primarily focuses on performance improvements in terms of accuracy or similar metrics. Additional metrics, such as interpretability scores or computational efficiency (e.g., FLOPs, wall-clock time), could provide a more holistic evaluation.\n\n---\n\n#### 4. Clarity & Reproducibility\n\n- **Clarity**:\n  - The paper is well-written and structured, with clear explanations of the methodology, experiments, and results. The use of diagrams or visualizations to illustrate the MDP framework and policy network architecture would further enhance clarity.\n  - The naming convention for generated features and the detailed description of feature engineering actions improve the interpretability of the framework.\n\n- **Reproducibility**:\n  - The availability of source code is a significant strength, enabling reproducibility.\n  - However, the paper lacks detailed hyperparameter settings, dataset preprocessing steps, and computational resource requirements, which are essential for full reproducibility.\n\n---\n\n#### 5. Novelty & Significance\n\n- **Novelty**:\n  - FETCH introduces several novel contributions, including the data-driven MDP formulation, the Transformer-inspired policy network for tabular data, and the pre-training paradigm for AutoFE. These innovations address critical limitations of existing methods and represent a significant advancement in the field.\n\n- **Significance**:\n  - The framework's ability to outperform state-of-the-art AutoFE methods and AutoML frameworks on diverse datasets demonstrates its practical significance.\n  - FETCH's scalability, efficiency, and transferability make it highly relevant for real-world applications, particularly in domains where feature engineering is a bottleneck.\n\n---\n\n#### Overall Recommendation\n\nThe paper presents a well-motivated, novel, and impactful contribution to automated feature engineering for tabular data. While there are minor weaknesses, such as limited discussion on computational overhead and ablation study coverage, these do not detract significantly from the overall quality of the work. The paper is suitable for acceptance at a top-tier AI conference, provided the authors address the identified weaknesses in a revised version.",
    "merged_image_path": "temp_cache\\6357_learning_a_data_driven_policy__merged.png",
    "image_only_review": "# Review\n\n## 1. Summary\n\nThe paper titled \"Learning a Data-Driven Policy Network for Pre-Training Automated Feature Engineering\" proposes a novel approach to automate feature engineering using a data-driven policy network. The authors introduce a framework that utilizes reinforcement learning to pre-train a policy network capable of generating effective feature transformations. The proposed method is evaluated on various datasets, demonstrating its ability to improve model performance by enhancing feature representation.\n\n## 2. Strengths\n\n- **Innovative Approach**: The paper presents a novel method that leverages reinforcement learning for automated feature engineering, which is a significant contribution to the field of machine learning and data preprocessing.\n- **Comprehensive Evaluation**: The authors conduct extensive experiments across multiple datasets, showcasing the effectiveness of the proposed approach in enhancing model performance.\n- **Clear Framework**: The structure and components of the proposed framework are well-defined, with a detailed explanation of the policy network and its training process.\n- **Relevance**: Automated feature engineering is a crucial step in the machine learning pipeline, and this work addresses a significant challenge in automating this process.\n\n## 3. Weaknesses\n\n- **Complexity**: The proposed method involves a complex setup with reinforcement learning, which might be challenging to implement and understand for practitioners not familiar with these techniques.\n- **Generalizability**: While the method shows promising results on the datasets used, the paper does not thoroughly discuss the limitations or potential challenges when applying this approach to other types of data or domains.\n- **Baseline Comparisons**: Although the paper includes comparisons with baseline methods, it could benefit from a more detailed analysis of why the proposed method outperforms these baselines, particularly in terms of specific feature transformations that lead to improved performance.\n\n## 4. Clarity & Reproducibility\n\n- **Clarity**: The paper is generally well-written and structured, with clear explanations of the methodology and experimental setup. However, certain sections, particularly those involving technical details of the reinforcement learning process, could be more accessible to a broader audience.\n- **Reproducibility**: The authors provide sufficient details about the experimental setup, datasets, and evaluation metrics, which should enable replication of the results. However, the inclusion of code or pseudocode for the policy network and training process would further enhance reproducibility.\n\n## 5. Novelty & Significance\n\n- **Novelty**: The integration of reinforcement learning into the feature engineering process is a novel approach that adds a significant contribution to the field. The idea of pre-training a policy network for feature transformation is innovative and opens up new avenues for research.\n- **Significance**: The work addresses a critical aspect of machine learning workflows—feature engineering—which is often labor-intensive and requires domain expertise. By automating this process, the proposed method has the potential to significantly streamline model development and deployment.\n- **Impact**: If widely adopted, this approach could reduce the time and expertise required for effective feature engineering, making advanced machine learning techniques more accessible to non-experts.\n\nIn conclusion, the paper presents a compelling approach to automated feature engineering with promising results. While there are areas for improvement, particularly in terms of clarity and generalizability, the work is a valuable contribution to the field and warrants consideration for acceptance.",
    "multimodal_review": "**1. Summary**  \nThe paper introduces FETCH, a novel automated feature engineering framework that addresses limitations in existing AutoFE methods by incorporating a data-driven approach. FETCH models feature engineering as a Markov Decision Process, using a Transformer-inspired policy network trained with Proximal Policy Optimization to iteratively generate and refine features based on dataset observations. The framework supports pre-training for transferability, allowing knowledge transfer across datasets. Experimental results demonstrate FETCH's superior performance, transferability, and efficiency compared to state-of-the-art methods across multiple datasets.\n\n**2. Strengths**  \n- **Innovative Approach**: FETCH introduces a novel data-driven framework that directly observes datasets, enhancing feature engineering effectiveness and interpretability.\n- **Transferability**: The pre-training paradigm allows FETCH to generalize across different datasets, reducing the need for dataset-specific exploration and improving efficiency.\n- **Comprehensive Evaluation**: The paper provides extensive experimental results across diverse datasets, demonstrating FETCH's superiority in effectiveness, efficiency, and scalability.\n- **Flexibility**: FETCH supports various machine learning models and feature engineering operations, making it applicable across different domains and tasks.\n\n**3. Weaknesses**  \n- **Clarity in Methodology Description**: The explanation of the policy network architecture in Section 3.2 could be more detailed. Including a diagram or pseudocode would enhance understanding of the network's operation.  \n  *Suggestion*: Add a detailed diagram or pseudocode to illustrate the policy network's architecture and operation.\n\n- **Baseline Comparisons**: The paper lacks a comparison with some recent AutoFE methods that might be relevant. For instance, in Table 2, comparisons with methods like FeatureTools or Deep Feature Synthesis are missing.  \n  *Suggestion*: Include comparisons with additional recent AutoFE methods to provide a more comprehensive evaluation.\n\n- **Ablation Study Details**: Section 6.6 mentions an ablation study, but the details are sparse. It would be beneficial to provide more insights into how each component of FETCH contributes to its overall performance.  \n  *Suggestion*: Expand the ablation study to include detailed analysis of each component's impact on performance.\n\n- **Figure Clarity**: Figure 3's caption is not sufficiently descriptive, and the figure itself is somewhat cluttered, making it difficult to interpret the results.  \n  *Suggestion*: Revise Figure 3's caption to be more descriptive and consider simplifying the figure for better clarity.\n\n- **Reproducibility Details**: While the paper mentions that the source code is publicly available, there is limited information on the experimental setup, such as hyperparameters, hardware specifications, and random seeds used.  \n  *Suggestion*: Include a detailed appendix with comprehensive information on experimental setups to enhance reproducibility.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**  \n  The paper is generally well-written, with logical flow and clear articulation of ideas. However, some sections, particularly the methodology, could benefit from additional detail and clarity. Mathematical notations are well-defined, but assumptions and limitations could be more explicitly stated.\n\n  **(b) Figure & Caption Clarity**  \n  Figures are generally informative but could be improved in terms of clarity and readability. Captions should be more descriptive, and figures should be simplified to avoid clutter. Consistency in labeling and legends is maintained.\n\n  **(c) Reproducibility Transparency**  \n  The paper provides a good overview of the experimental results but lacks detailed information on the experimental setup. While code availability is mentioned, specifics on datasets, hyperparameters, and hardware are not thoroughly covered, which could hinder reproducibility.\n\n**5. Novelty & Significance**  \nFETCH presents a significant advancement in the field of automated feature engineering by introducing a data-driven, MDP-based approach that observes datasets directly. The incorporation of a Transformer-inspired policy network and pre-training for transferability offers a novel solution to existing limitations in AutoFE methods. The work is well-motivated and contextualized within the literature, addressing critical gaps in current methodologies. The paper substantiates its claims through rigorous empirical evaluation, demonstrating the framework's effectiveness and efficiency. FETCH's contributions are valuable to the community, offering new insights and practical tools for tabular data analysis.",
    "multimodal_rag_review": "**1. Summary**  \nThe paper introduces FETCH, a novel framework for automated feature engineering (AutoFE) specifically designed for tabular data. FETCH leverages a data-driven Markov Decision Process (MDP) and reinforcement learning (RL) to iteratively generate and refine features, mimicking human experts' ability to analyze datasets and transfer knowledge across tasks. The framework is notable for its pre-training paradigm, enabling transferability across datasets, and demonstrates superior performance over existing AutoFE methods in extensive experiments. FETCH also emphasizes interpretability and flexibility, supporting variable-length and permutation-invariant data.\n\n**2. Strengths**  \n- **Innovative Approach**: FETCH introduces a novel data-driven MDP setup for AutoFE, which is a significant departure from traditional NAS-like methods that do not directly analyze datasets.\n- **Transferability**: The pre-training paradigm allows FETCH to generalize across datasets, reducing exploration time and computational costs, which is a substantial advancement in the field.\n- **Performance**: The framework consistently outperforms state-of-the-art methods in both classification and regression tasks, as evidenced by experiments on 27 datasets.\n- **Interpretability and Flexibility**: FETCH's ability to generate interpretable feature names and handle variable-length data makes it highly applicable to real-world scenarios.\n\n**3. Weaknesses**  \n- **Clarity in Methodology**: The explanation of the MDP setup and the role of the policy network in Section 3 could be more detailed to enhance understanding. Consider adding a step-by-step walkthrough of the process.\n- **Experimental Setup Details**: The specifics of the experimental setup, such as hyperparameters and hardware configurations, are insufficiently detailed in Section 5. Including these details would improve reproducibility.\n- **Ablation Studies**: Section 6 lacks ablation studies to isolate the impact of individual components, such as the multi-head self-attention module, on the overall performance. Adding these studies would clarify the contribution of each component.\n- **Broader Implications**: The discussion on the broader implications and potential applications of FETCH in Section 7 is limited. Expanding this discussion could enhance the paper's relevance to the community.\n- **Typographical Errors**: Minor grammatical errors are present throughout the paper, such as in the abstract and Section 4. A thorough proofreading would improve the overall presentation quality.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**  \n  The paper is generally well-structured, with clear section titles and logical progression of ideas. However, some sections, particularly the methodology, could benefit from more detailed explanations and examples to aid comprehension. Mathematical notations are well-defined, but assumptions and limitations are not explicitly stated.\n\n  **(b) Figure & Caption Clarity**  \n  Figures are generally clear and support the text well, but some captions, such as those in Figure 3, could be more descriptive to be self-sufficient. Axes and labels are consistent and readable, and diagrams correlate well with the textual descriptions.\n\n  **(c) Reproducibility Transparency**  \n  The paper provides a good overview of the experimental results but lacks detailed information on datasets, hyperparameters, and hardware used. There is no mention of code or data availability, which is crucial for reproducibility. Including these details, along with ablation studies, would significantly enhance transparency.\n\n**5. Novelty & Significance**  \nFETCH addresses a significant gap in automated feature engineering by introducing a data-driven approach that directly analyzes datasets, which is a novel contribution. The framework's ability to generalize across datasets through pre-training is a noteworthy advancement, offering substantial improvements in efficiency and performance. While the paper substantiates its claims with empirical evidence, the lack of detailed methodological explanations and broader implications limits its impact. Nonetheless, FETCH represents a valuable contribution to the field, with potential applications in various domains requiring efficient and interpretable feature engineering."
}