{
    "paper_id": "6592_pfedkt_personalized_federated_",
    "title": "pFedKT: Personalized Federated Learning via Knowledge Transfer",
    "abstract": "Federated learning (FL) has been widely studied as a new paradigm to achieve multi-party collaborative modelling on decentralized data with privacy protection. Unfortunately, traditional FL suffers from Non-IID data distribution, where clients' private models after FL are even inferior to models trained standalone. Existing approaches to tackle this challenge fall into two directions: a) pursuing a better global model through mitigating biases of private models, and b) improving personalized private models by personalized federated learning (PFL). Still, both of them have limited accuracy improvements in private models. To this end, we design pFedKT, a novel personalized federated learning framework with knowledge transfer, towards boosting the performances of personalized private models on Non-IID data. It involves two types of knowledge transfer: a) transferring historical private knowledge to new private models by local hypernetworks; b) transferring the global model's knowledge to private models through contrastive learning. After absorbing the historical private knowledge and the latest global knowledge, the personalization and generalization of private models are both enhanced. Besides, we derive pFedKT's generalization and prove its convergence theoretically. Extensive experiments verify that pFedKT presents accuracy improvements of private models than the state-of-the-art baseline.",
    "human_review": "Summary Of The Paper:\nThis paper aims to improve the performance of personalized federated learning, and, for which, the authors propose two knowledge transfer schemes. In particular, the historical knowledge learned in the local clients is transferred from the hypernetwork, which stores the knowledge of previous local models, to the current local model. Also, the global knowledge, obtained by the aggregation of local models, is transferred to the local clients based on the contrastive learning loss, where the similarity between the updated local model and the aggregated global model is maximized while the similarity between the updated local model and the previous local model is minimized. The authors validate the performance of their model, named as pFedKT, on two image (i.e., CIFAR-10/100) and one language (i.e., Stack Overflow) datasets, on which the pFedKT outperforms relevant personalized federated learning baselines.\n\nStrength And Weaknesses:\nStrengths:\n- The proposed historical knowledge transfer scheme with the hypernetwork brings the performance improvement for the personalized federated learning tasks.\n- The authors make effort to theoretically analyze the generalization and convergence bounds of the proposed pFedKT, while they are mostly inspired by the previous work [1] though.\n- The authors perform extensive analyses on the proposed pFedKT, by varying the hyperparameters, and by ablating the knowledge transfer mechanisms.\n\nWeaknesses:\n- The proposed global knowledge transfer scheme is not convincing enough in terms of both the motivation- and experiment-sides.\n- At first, the authors argue that, by maximizing the similarity between the locally updated model and the globally aggregated model while minimizing the similarity between the locally updated model and the previously updated local model, the proposed pFedKT improves the generalization performance. However, this design choice is not convincing, since the authors already transfer the historical knowledge in the previously updated local model with the hypernetworks, meanwhile, the historical knowledge is negatively considered (i.e., historical information is avoided) in the contrastive learning loss. Therefore, two objectives are in conflict in the federated learning.\n- In the experiment-side, the proposed global knowledge transfer scheme also does not bring meaningful performance improvement, i.e., not much helpful for personalized federated learning. For example, in Figure 7, the proposed global knowledge transfer scheme based on the contrastive loss does not bring performance improvement. Similarly, in Table 11, the results w/ and w/o contrastive losses are very similar.\n- Furthermore, it is unclear whether the proposed global knowledge transfer scheme can provide better generalization ability empirically. While the authors provide the theoretical result for the generalization bound, I suggest authors to include additional empirical results, if possible, which makes it more convincing.\n- In Section 3.4, the authors only compare the computational and storage efficiencies of the proposed pFedKT against the complex pFedHN model. It is meaningful to discuss the efficiencies of the most basic FedAvg model, as well as the other contrastive- and hypernetwork-based federated learning models, such as MOON and Fed-ROD.\n- Also, in Section 3.4, the authors argue that the proposed pFedKT has obvious strengths against the pFedHN model, since the pFedHN baseline has the larger hypernetwork in the server-side, while the proposed pFedKT has smaller hypernetworks in the client-side. However, this is not convincing, since if we have 1,000 clients, we have 1,000 individual hypernetworks distributed to 1,000 clients, and, in the global view, the size of 1,000 individual hypernetworks would be larger than the size of one hypernetwork in the server.\n- The analysis results in Section 5.3 may be problematic. The authors argue the proposed pFedKT can converge during 100 rounds, therefore, conduct analyses with 100 rounds; however, pFedKT does not converge until 100 rounds, as shown in Figure 4.\n- Since knowledge distillation-based federated learning methods share similar spirits to the proposed knowledge transfer-based federated learning model—the knowledge distillation allows the local/global models to transfer their knowledge effectively—I suggest authors compare such knowledge distillation-based federated learning baselines: FedPHP, FML and KT-pFL, discussed in the related work section.\n\n[1] Personalized Federated Learning Using Hypernetworks, ICML 2021.\n\nClarity, Quality, Novelty And Reproducibility:\nClarity:\n- The main idea of the proposed global knowledge transfer scheme with the contrastive loss is not clear enough (see Weaknesses above for details).\n- In regards to efficiency, the advantage of the proposed models compared to others is not sufficiently clear (see Weaknesses above for details).\n- In Figure 1(b), it is unclear how to measure the global and local model accuracies. Specifically, in main experiments, the authors report the test accuracy based on the local private data with the local model; then, how to measure the global model accuracy, and how to compare this global model accuracy to the local model accuracies?\n\nQuality:\n- The experimental quality of the analyses in Section 5.3 might be low, since the authors conduct the analyses with the unconverged model.\n- In Section 2.1, 'clients train the received GM on local datasets from scratch' should be toned down, since the client trains the local model from the globally aggregated model, which indeed contains the information for the local model; not training from scratch.\n\nNovelty:\n- The novelty is mild, since, for the knowledge transfer, the concepts of hypernetworks and contrastive learning are already proposed in the previous work, such as pFedHN, Fed-ROD, and MOON; however, the differences are faithfully and sufficiently discussed in the related work section as well as other sections.\n\nReproducibility:\n- The authors do not provide the source code, which lowers the reproducibility of this paper; however, the authors plan to release the source code after acceptance. Therefore, the reproducibility will probably be high.\n\nSummary Of The Review:\nThe main idea of the proposed global knowledge transfer scheme based on the contrastive loss is not convincing (see Weaknesses above), and there are some improvement points, such as efficiency analyses in Section 3.4 which are not sufficient, and model analyses which are perhaps conducted without the model convergence. Therefore, I cannot recommend the acceptance.\n\nCorrectness: 2: Several of the paper’s claims are incorrect or not well-supported.\nTechnical Novelty And Significance: 2: The contributions are only marginally significant or novel.\nEmpirical Novelty And Significance: 2: The contributions are only marginally significant or novel.\nFlag For Ethics Review: NO.\nRecommendation: 3: reject, not good enough.\nConfidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
    "text_summary": "### Summary of \"pFedKT: Personalized Federated Learning via Knowledge Transfer\"\n\n#### Motivation\nFederated Learning (FL) enables collaborative model training across decentralized data while preserving privacy. However, traditional FL methods, such as FedAvg, struggle with Non-IID (non-independent and identically distributed) data, where the global model (GM) often underperforms compared to standalone models. This is due to the aggregation of skewed local models, which deviates from the optimal model trained on all data. Personalized Federated Learning (PFL) methods aim to address this by improving the accuracy of personalized private models (PMs). However, existing approaches show limited improvements, particularly in handling Non-IID data. The study introduces **pFedKT**, a novel PFL framework, to enhance PM performance by leveraging historical private knowledge and global knowledge transfer.\n\n---\n\n#### Method\nThe proposed framework, **pFedKT**, combines two types of knowledge transfer to improve both personalization and generalization of private models:\n1. **Private Knowledge Transfer**:\n   - Each client uses a local hypernetwork (HN), a lightweight fully connected network, to transfer historical knowledge from previous private models to new ones.\n   - The HN generates parameters for the private model, avoiding the high storage and computational costs of server-side hypernetworks (e.g., pFedHN) and reducing communication overhead compared to methods like Fed-RoD.\n\n2. **Global Knowledge Transfer**:\n   - Contrastive learning is employed to align the private model (anchor) with the global model (positive) while distancing it from the previous round's private model (negative).\n   - The private model is initialized with parameters generated by the hypernetwork, ensuring effective fusion of global and private knowledge.\n\n**Workflow**:\n1. The server broadcasts the global model to clients.\n2. Each client uses its hypernetwork to generate an initial private model.\n3. Contrastive loss is computed between the private model, global model, and the previous private model, combined with supervised loss for training.\n4. The private model is trained locally, and its parameters are used to update the hypernetwork.\n5. The trained private model is sent to the server for aggregation into a new global model, which is then broadcast to clients.\n\n**Loss Function**:\nThe total loss combines contrastive loss (\\(\\ell_{con}\\)) and supervised loss (\\(\\ell_{sup}\\)):\n\\[\nf = \\mu \\cdot \\ell_{con} + (1 - \\mu) \\cdot \\ell_{sup},\n\\]\nwhere \\(\\mu\\) balances the two losses. Contrastive loss is computed using triplet loss with KL divergence as the distance metric.\n\n**Theoretical Analysis**:\n- The framework's generalization is proven, showing that the loss function satisfies:\n\\[\n|f(\\phi) - \\hat{f}_{D_k}(\\phi)| \\leq \\epsilon,\n\\]\nwith high probability, where \\(\\phi\\) represents hypernetwork parameters.\n- Convergence analysis demonstrates that pFedKT achieves the same \\(O(1/T)\\) convergence rate as FedAvg, even under Non-IID data distributions.\n\n---\n\n#### Results\n**Datasets and Experimental Setup**:\n- Evaluated on CIFAR-10, CIFAR-100, and Stack Overflow datasets with Non-IID distributions.\n- Models included CNNs for CIFAR datasets and LSTMs for Stack Overflow.\n- Hyperparameters were optimized, including embedding dimensions, hypernetwork size, contrastive loss weight (\\(\\mu\\)), and triplet loss margin (\\(\\alpha\\)).\n\n**Performance**:\n- On CIFAR-10, pFedKT achieved a mean PM accuracy of **98.12%**, outperforming the second-best method, pFedHN (97.01%).\n- On CIFAR-100, pFedKT achieved **69.38%**, 1.62% higher than FedPHP (67.76%).\n- On Stack Overflow, pFedKT achieved the highest PM accuracy (25.33%), outperforming FedPHP (25.19%) and Fed-RoD (24.98%).\n\n**Ablation Studies**:\n- Removing either private or global knowledge transfer significantly reduced accuracy, confirming the necessity of both mechanisms.\n- pFedKT converged faster and more stably than baselines, achieving the highest accuracy within fewer communication rounds.\n\n**Hyperparameter Sensitivity**:\n- Optimal embedding dimensions and hypernetwork sizes were identified for each dataset, balancing accuracy and computational complexity.\n- Contrastive loss weight (\\(\\mu\\)) and triplet loss margin (\\(\\alpha\\)) were critical for performance, with best results at \\(\\mu = 0.001\\) and \\(\\alpha = 0.1\\) (CIFAR-10) or \\(\\alpha = 5\\) (CIFAR-100).\n\n**Comparison with Baselines**:\n- pFedKT outperformed state-of-the-art methods, including MOON, Fed-RoD, and FedPHP, in both accuracy and personalization.\n- On CIFAR-10, pFedKT improved accuracy by 38.7% over FedAvg and 0.31% over Fed-RoD.\n- On CIFAR-100, pFedKT improved accuracy by 57.07% over FedAvg and 3.46% over Fed-RoD.\n\n**Computational and Storage Costs**:\n- pFedKT's local hypernetworks reduced server-side computational bottlenecks and storage overhead compared to pFedHN.\n- Storage costs scaled with the number of clients, making pFedKT more efficient in cross-silo FL scenarios.\n\n---\n\n#### Conclusion\npFedKT introduces a novel dual knowledge transfer mechanism, combining private knowledge transfer via local hypernetworks and global knowledge transfer via contrastive learning. This approach enhances both personalization and generalization of private models, achieving state-of-the-art accuracy on Non-IID datasets. The framework is computationally efficient, scalable, and robust to varying client participation rates. Theoretical guarantees and extensive experiments validate its effectiveness, making pFedKT a practical solution for personalized federated learning in decentralized environments.",
    "text_only_review": "#### 1. Summary\nThe paper presents **pFedKT**, a novel framework for Personalized Federated Learning (PFL) that addresses challenges posed by Non-IID data distributions. The framework introduces a dual knowledge transfer mechanism: (1) **Private Knowledge Transfer**, leveraging a local hypernetwork to incorporate historical knowledge into private models, and (2) **Global Knowledge Transfer**, employing contrastive learning to align private models with the global model while maintaining personalization. The authors provide theoretical guarantees for generalization and convergence, and extensive experiments demonstrate that pFedKT achieves state-of-the-art performance on multiple datasets with reduced computational and storage costs compared to existing methods.\n\n---\n\n#### 2. Strengths\n1. **Innovative Methodology**:\n   - The dual knowledge transfer mechanism is a novel contribution, effectively combining private and global knowledge to enhance both personalization and generalization.\n   - The use of local hypernetworks for private knowledge transfer is computationally efficient and avoids server-side bottlenecks.\n\n2. **Theoretical Rigor**:\n   - The paper provides a solid theoretical foundation, including generalization bounds and convergence analysis, which strengthen the validity of the proposed approach.\n\n3. **Comprehensive Experiments**:\n   - The framework is evaluated on diverse datasets (CIFAR-10, CIFAR-100, and Stack Overflow) with Non-IID distributions, demonstrating its robustness and applicability across different domains.\n   - Ablation studies and hyperparameter sensitivity analyses provide insights into the importance of key components and parameter settings.\n\n4. **Performance Gains**:\n   - pFedKT achieves state-of-the-art accuracy, outperforming strong baselines such as FedAvg, FedPHP, and Fed-RoD.\n   - The framework converges faster and achieves higher accuracy within fewer communication rounds, highlighting its efficiency.\n\n5. **Scalability**:\n   - By employing local hypernetworks, pFedKT reduces server-side computational and storage costs, making it suitable for large-scale federated learning scenarios.\n\n---\n\n#### 3. Weaknesses\n1. **Limited Real-World Validation**:\n   - While the framework is evaluated on standard benchmarks, the lack of experiments on real-world federated learning applications (e.g., healthcare, IoT) limits the demonstration of its practical utility.\n\n2. **Contrastive Learning Complexity**:\n   - Contrastive learning introduces additional computational overhead during local training, which may be challenging for resource-constrained clients. The paper does not provide a detailed analysis of this overhead.\n\n3. **Hyperparameter Sensitivity**:\n   - The performance of pFedKT is highly sensitive to hyperparameters such as the contrastive loss weight (\\(\\mu\\)) and triplet loss margin (\\(\\alpha\\)). While the authors conduct sensitivity analyses, the optimal settings may vary significantly across datasets, potentially complicating deployment.\n\n4. **Client Participation Assumptions**:\n   - The framework assumes consistent client participation across communication rounds. The impact of varying participation rates is not thoroughly explored, which is a critical aspect of real-world federated learning.\n\n5. **Scalability to Cross-Device FL**:\n   - Although pFedKT is efficient in cross-silo FL scenarios, its scalability to cross-device FL, where clients are highly heterogeneous and resource-constrained, is not explicitly addressed.\n\n---\n\n#### 4. Clarity & Reproducibility\n- **Clarity**:\n  - The paper is well-written and logically organized, with clear explanations of the methodology, theoretical analysis, and experimental results.\n  - The inclusion of mathematical formulations and pseudocode enhances understanding of the framework.\n  - Visualizations, such as convergence plots and ablation study results, effectively support the claims.\n\n- **Reproducibility**:\n  - The experimental setup is described in detail, including datasets, models, and hyperparameters, which facilitates reproducibility.\n  - However, the paper does not mention whether the code and datasets will be made publicly available, which is crucial for independent verification.\n\n---\n\n#### 5. Novelty & Significance\n- **Novelty**:\n  - The dual knowledge transfer mechanism, combining private knowledge transfer via local hypernetworks and global knowledge transfer via contrastive learning, is a novel and significant contribution to the field of personalized federated learning.\n  - The use of local hypernetworks to reduce server-side computational and storage costs is an innovative approach that addresses a key limitation of existing methods.\n\n- **Significance**:\n  - The framework addresses a critical challenge in federated learning: improving personalization and generalization under Non-IID data distributions.\n  - By achieving state-of-the-art performance with reduced computational and storage costs, pFedKT has the potential to advance the adoption of federated learning in practical applications.\n  - The theoretical guarantees further enhance the framework's significance, providing a strong foundation for future research.\n\n---\n\n#### Overall Recommendation\nThe paper presents a significant contribution to the field of personalized federated learning, with a novel methodology, strong theoretical backing, and state-of-the-art performance. While there are some limitations, such as the lack of real-world validation and sensitivity to hyperparameters, these do not undermine the overall quality and impact of the work. I recommend **acceptance** at a top-tier AI conference, provided the authors address the identified weaknesses, particularly by discussing real-world applicability and providing open-source code for reproducibility.",
    "merged_image_path": "temp_cache\\6592_pfedkt_personalized_federated__merged.png",
    "image_only_review": "#### 1. Summary\n\nThe paper introduces pFedKT, a novel approach to personalized federated learning (FL) that leverages knowledge transfer to enhance model performance across heterogeneous client data distributions. The proposed method aims to address the challenges of data heterogeneity and privacy in FL by utilizing a global knowledge distillation process. The authors present a comprehensive theoretical analysis of the method and demonstrate its effectiveness through extensive experiments on various benchmark datasets. The results indicate that pFedKT outperforms existing personalized FL methods in terms of accuracy and convergence speed.\n\n#### 2. Strengths\n\n- **Innovative Approach**: The paper presents a novel method that combines knowledge transfer with federated learning, addressing key challenges in the field such as data heterogeneity and privacy concerns.\n- **Theoretical Analysis**: The authors provide a thorough theoretical foundation for their approach, offering insights into the convergence properties and effectiveness of the proposed method.\n- **Experimental Validation**: Extensive experiments on multiple benchmark datasets demonstrate the superiority of pFedKT over existing methods, showcasing improvements in accuracy and convergence speed.\n- **Comprehensive Evaluation**: The paper includes a detailed comparison with several state-of-the-art personalized FL methods, highlighting the advantages of the proposed approach.\n\n#### 3. Weaknesses\n\n- **Complexity and Scalability**: The proposed method may introduce additional computational overhead due to the knowledge transfer process, which could impact scalability in large-scale deployments.\n- **Limited Real-World Applications**: While the experimental results are promising, the paper lacks a discussion on the applicability of pFedKT in real-world scenarios, particularly in terms of deployment challenges and potential limitations.\n- **Assumptions and Limitations**: The paper does not thoroughly discuss the assumptions underlying the theoretical analysis, nor does it explore the limitations of the proposed method in diverse settings.\n\n#### 4. Clarity & Reproducibility\n\nThe paper is generally well-written and structured, with clear explanations of the methodology and theoretical analysis. The inclusion of detailed experimental results and comparisons enhances the clarity of the findings. However, the reproducibility of the results could be improved by providing more detailed information on the experimental setup, including hyperparameter settings and implementation details. Additionally, sharing the code and datasets used in the experiments would further facilitate reproducibility.\n\n#### 5. Novelty & Significance\n\nThe proposed pFedKT method represents a significant advancement in the field of personalized federated learning by integrating knowledge transfer to address data heterogeneity and privacy concerns. The novelty of the approach lies in its ability to improve model performance across diverse client data distributions while maintaining privacy. The significance of the work is underscored by the comprehensive theoretical analysis and experimental validation, which demonstrate the method's potential to outperform existing personalized FL approaches. Overall, the paper makes a valuable contribution to the field and opens up new avenues for research in personalized federated learning.",
    "multimodal_review": "**1. Summary**  \nThe paper introduces pFedKT, a personalized federated learning framework designed to improve model performance on Non-IID data by leveraging both private and global knowledge transfer. The methodology employs local hypernetworks for private knowledge transfer and contrastive learning for global knowledge transfer. This dual approach aims to enhance the personalization and generalization of private models. Theoretical analysis supports the framework's generalization and convergence properties. Empirical results on CIFAR-10, CIFAR-100, and Stack Overflow datasets demonstrate that pFedKT outperforms existing methods in terms of accuracy and efficiency.\n\n**2. Strengths**  \n- The dual knowledge transfer mechanism is innovative, effectively addressing the challenges of Non-IID data in federated learning.\n- Theoretical analysis provides a solid foundation for the proposed method, ensuring both generalization and convergence.\n- Empirical results are comprehensive, showcasing significant improvements over state-of-the-art methods across multiple datasets.\n- The framework is computationally efficient, reducing server-side bottlenecks and storage overhead, which is crucial for scalability.\n\n**3. Weaknesses**  \n- **Clarity of Methodology**: The description of the hypernetwork's role in Section 4.1 is somewhat dense, which might hinder understanding for readers unfamiliar with hypernetworks.  \n  *Suggestion*: Simplify the explanation and provide a schematic diagram to illustrate the hypernetwork's operation.\n  \n- **Baseline Comparisons**: While the paper compares pFedKT with several baselines, it lacks a direct comparison with some recent PFL methods that might offer competitive performance (e.g., FedProx).  \n  *Location*: Section 6.3, \"Comparison with Baselines.\"  \n  *Suggestion*: Include additional baselines for a more comprehensive comparison.\n  \n- **Figure Clarity**: Figure 3's caption does not adequately explain the significance of the observed trends, which might confuse readers.  \n  *Location*: Figure 3.  \n  *Suggestion*: Expand the caption to clarify the trends and their implications for the proposed method.\n  \n- **Reproducibility Details**: The paper does not specify the random seeds used in experiments, which is crucial for reproducibility.  \n  *Location*: Section 6.1, \"Experimental Setup.\"  \n  *Suggestion*: Include details on random seed initialization to enhance reproducibility.\n  \n- **Hyperparameter Sensitivity**: The discussion on hyperparameter sensitivity (Section 6.4) lacks depth, particularly regarding how sensitive the method is to the choice of \\(\\mu\\) and \\(\\alpha\\).  \n  *Suggestion*: Provide more detailed analysis or visualizations to illustrate the impact of these hyperparameters on performance.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**  \n  The paper is generally well-written, with a logical flow and clear explanations of the main ideas. However, some sections, particularly those detailing the methodology, could benefit from simplification and additional diagrams to aid understanding. The assumptions and limitations are clearly stated, contributing to the paper's overall clarity.\n\n  **(b) Figure & Caption Clarity**  \n  Figures are generally well-designed, but some captions lack sufficient detail to be fully self-explanatory. For instance, Figure 3 could benefit from a more detailed caption that explains the trends and their relevance to the study's claims. Axes and labels are consistent and readable, aiding in the interpretation of results.\n\n  **(c) Reproducibility Transparency**  \n  The paper provides a good level of detail regarding datasets, model architectures, and hyperparameters. However, it lacks information on random seed initialization, which is crucial for reproducibility. The inclusion of ablation studies is a positive aspect, as it helps to understand the contribution of different components. Mentioning code availability would further enhance reproducibility.\n\n**5. Novelty & Significance**  \nThe paper addresses a significant challenge in federated learning—handling Non-IID data—by introducing a novel dual knowledge transfer mechanism. This approach is well-motivated and contextualized within the existing literature, offering a fresh perspective on personalized federated learning. Theoretical and empirical validations substantiate the claims, demonstrating the framework's effectiveness and robustness. The work contributes valuable insights and practical solutions to the field, with potential applications in various decentralized environments. The novelty and significance of the contributions make it a valuable addition to the federated learning community.",
    "multimodal_rag_review": "**1. Summary**  \nThe paper presents pFedKT, a novel framework for Personalized Federated Learning (PFL) aimed at improving model performance on Non-IID data distributions. The framework employs dual knowledge transfer mechanisms—private and global knowledge transfer—to enhance the personalization and generalization of private models (PMs). The authors provide theoretical guarantees for generalization and convergence and demonstrate state-of-the-art performance with significant accuracy improvements over existing PFL methods on Non-IID datasets.\n\n**2. Strengths**  \n- The introduction of dual knowledge transfer mechanisms is innovative, addressing the challenges of Non-IID data in federated learning effectively.\n- Theoretical analysis provides strong backing for the framework's generalization and convergence properties, which is crucial for validating the approach.\n- The experimental results are comprehensive, showing consistent improvements in PM accuracy across multiple datasets and baselines, highlighting the practical applicability of the framework.\n\n**3. Weaknesses**  \n- **Clarity in Methodology**: The explanation of the dual knowledge transfer mechanisms, particularly the contrastive learning component, is somewhat dense and could benefit from additional clarification. For instance, Section 4.2 could include more intuitive explanations or examples to aid understanding.\n  - *Suggestion*: Include a detailed example or diagram illustrating the contrastive learning process in Section 4.2 to enhance clarity.\n\n- **Baseline Comparisons**: While the paper compares pFedKT with several baselines, the selection of baselines could be expanded to include more recent methods beyond those listed. This would provide a more comprehensive evaluation of the framework's performance.\n  - *Suggestion*: Consider including comparisons with newer PFL methods that have emerged in the last year, if applicable, in Section 5.2.\n\n- **Ablation Study Details**: The ablation studies presented in Section 6.3 are informative but could be expanded to explore the sensitivity of the framework to different hyperparameters in more detail.\n  - *Suggestion*: Provide a more granular analysis of how varying key hyperparameters, such as the contrastive loss weight (\\(\\mu\\)), affects performance.\n\n- **Figure Clarity**: Some figures, such as Figure 3, are complex and could benefit from more detailed captions and annotations to make them self-explanatory.\n  - *Suggestion*: Enhance the captions of complex figures like Figure 3 to include a brief description of each component and its role in the framework.\n\n- **Reproducibility Details**: The paper lacks detailed information on the experimental setup, such as specific hyperparameters, hardware used, and random seeds, which are essential for reproducibility.\n  - *Suggestion*: Include a dedicated section or appendix detailing the experimental setup, including hyperparameters, hardware specifications, and random seeds.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**: The paper is generally well-written, with clear section titles and logical flow. However, some sections, particularly those describing complex methodologies, could benefit from additional explanations or examples to aid comprehension. Mathematical notations are mostly well-defined, but assumptions and limitations are not always clearly articulated.\n\n  **(b) Figure & Caption Clarity**: Figures are generally well-designed and relevant to the claims made in the text. However, some figures, such as Figure 3, are complex and could be improved with more detailed captions and annotations to ensure they are self-explanatory.\n\n  **(c) Reproducibility Transparency**: The paper provides a solid overview of the datasets and models used but lacks detailed information on the experimental setup, such as hyperparameters, hardware used, and random seeds. This information is crucial for ensuring the reproducibility of the results.\n\n**5. Novelty & Significance**  \nThe introduction of dual knowledge transfer mechanisms in pFedKT represents a novel approach to addressing the challenges of Non-IID data in federated learning. The framework is well-motivated and contextualized within the existing literature, addressing the limitations of current PFL methods. Theoretical guarantees and empirical results substantiate the claims made, demonstrating the framework's potential to significantly improve PM accuracy and generalization. The work contributes valuable insights to the field of federated learning, particularly in scenarios involving heterogeneous data distributions, and holds promise for practical applications in privacy-preserving collaborative learning environments."
}