{
    "paper_id": "5447_on_the_sensitivity_of_reward_i",
    "title": "On the Sensitivity of Reward Inference to Model Misspecification in Human Models",
    "abstract": "Inferring reward functions from human behavior is at the center of value alignment – aligning AI objectives with what we, humans, actually want. But doing so relies on models of how humans behave given their objectives. After decades of research in cognitive science, neuroscience, and behavioral economics, obtaining accurate human models remains an open research topic. This begs the question: how accurate do these models need to be in order for the reward inference to be accurate? On the one hand, if small errors in the model can lead to catastrophic error in inference, the entire framework of reward learning seems ill-fated, as we will never have perfect models of human behavior. On the other hand, if as our models improve, we can have a guarantee that reward accuracy also improves, this would show the benefit of more work on the modeling side. We study this question both theoretically and empirically. We do show that it is unfortunately possible to construct small adversarial biases in behavior that lead to arbitrarily large errors in the inferred reward. However, and arguably more importantly, we are also able to identify reasonable assumptions under which the reward inference error can be bounded linearly in the error in the human model. Finally, we verify our theoretical insights in discrete and continuous control tasks with simulated and human data.",
    "human_review": "Summary Of The Paper:\nThe paper addresses the question of whether misspecification of human behavior models for IRL can lead to detrimental effects on reward inference accuracy, and provide theoretical guarantees showing that while adversarial situations can be constructed such that small human behavior models lead to \"catastrophic\" inference errors, under a log-concavity assumption the effect is more stable. The authors then provide empirical results showing a correlation between human model accuracy and reward inference accuracy.\n\nStrength And Weaknesses:\nStrengths: the paper is very well-written, and I appreciate the attempt to explore different types of human irrationality (e.g. myopia), and the experiments include real human data (albeit w/ a very simplified form of misspecification via simple re-weighting)\n\nI am a bit confused why measuring reward misspecification by distance in parameter space works except for very restrictive, simple linear reward models. If the reward function is a NN, then it is well known that two networks that behavior similarly can have widely different parameters. Any necessary assumptions here should be clearly stated.\nI am likewise confused why KL diverge between demonstration policy and the human model is the right way to measure distance. Given the motivations discussed in the paper of modeling humans via noisy-rationality, systemic biases, etc., it seems like humans may deviate from an idealized model in ways much stronger and more state-specific than the KL measure assumes. While I like the idea of fitting certain types of irrationalities (myopia and illusion of control) into this framework, it feels a bit cherry-picked to only study irrationalities that satisfy the specific notion of distance the authors chose.\nClarity, Quality, Novelty And Reproducibility:\nClarity: The paper is very well written Novelty: There is no new methodology to evaluate novelty for, so it is mainly the question (how harmful misspecified human models are for reward inference) that is novel Reproducibility: Experiment details are included in the appendix.\n\nSummary Of The Review:\nMy primary concern and reason for rejection is the rather simplified assumptions the authors make in order to \"get the theory to work\". It is also unclear what the ultimate takeaway here is - if the goal of studying the sensitivity of reward inference is to make some kind of claim on whether reward learning is a valid direction, then only studying restricted forms of human model misspecification does not really lead to any meaningful insights. I think a more clear, isolated section stating the author's assumptions is necessary for acceptance.\n\nCorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\nTechnical Novelty And Significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\nEmpirical Novelty And Significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\nFlag For Ethics Review: NO.\nRecommendation: 3: reject, not good enough\nConfidence: 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
    "text_summary": "### Motivation\nThis paper, presented at ICLR 2023, addresses a critical challenge in aligning AI objectives with human values: the sensitivity of reward inference to inaccuracies in human behavior models. Reward learning, a cornerstone of inverse reinforcement learning (IRL), relies on interpreting human demonstrations to infer underlying objectives. However, the accuracy of this process depends on the fidelity of human behavior models, which are inherently imperfect due to natural biases, irrationalities, and cognitive limitations. Despite decades of research in cognitive science and behavioral economics, perfect human models remain unattainable. This raises a fundamental question: how accurate must these models be to ensure reliable reward inference?\n\n### Method\nThe study combines theoretical analysis and empirical experiments to investigate the relationship between errors in human behavior models and the accuracy of inferred rewards. The problem is framed as a Markov Decision Process (MDP) with unknown reward functions inferred from expert demonstrations. The reward function is parameterized by \\(\\theta\\), and the learner observes a dataset \\(D\\) of state-action pairs sampled from the expert's policy \\(\\pi^*\\). The learner assumes a model \\(\\hat{\\pi}\\), often a Boltzmann rational policy, while the true expert policy \\(\\pi^*\\) may include human biases. Reward inference is performed using maximum likelihood estimation (MLE), which scales well to high-dimensional environments.\n\nThe study introduces two metrics for policy divergence:\n1. **Worst-case policy divergence** (\\(d_{\\pi}^{wc}\\)): Measures the maximum KL-divergence across all states and reward parameters.\n2. **Weighted policy divergence** (\\(d_{\\pi}^w\\)): Averages the KL-divergence over states visited by the true policy \\(\\pi^*\\), focusing on states relevant to the true reward parameters.\n\nTheoretical analysis begins with a negative result, showing that even small errors in the assumed model (\\(d_{\\pi}^{wc} < \\epsilon\\)) can lead to large errors in inferred rewards (\\(d_\\theta(\\theta^*, \\hat{\\theta})\\)). However, under reasonable assumptions about human behavior, such as log-concavity of policies with respect to reward parameters, the authors demonstrate that the error in inferred rewards can be bounded linearly by the error in the human model. This stability result is further simplified for specific biases, such as false internal dynamics or myopia.\n\n### Results\n#### Theoretical Findings\n1. **Adversarial Sensitivity**: Small adversarial biases in human behavior models can lead to arbitrarily large errors in inferred rewards. However, such adversarial scenarios are unlikely in practice.\n2. **Stability Under Assumptions**: Under log-concavity and reasonable assumptions about human behavior, the expected reward inference error is bounded linearly by the weighted policy divergence. This result accounts for randomness in dataset sampling and uses a smaller divergence metric, making it more practical.\n\n#### Empirical Validation\nThe theoretical findings are supported by experiments in both tabular and continuous control environments:\n1. **Tabular Gridworld Tasks**: \n   - Three gridworld environments with structured biases (e.g., internal dynamics and myopia) were tested. Results showed that biases in human models lead to sub-linear increases in both policy divergence and reward inference error. For example, underestimating environmental stochasticity or discount factors caused demonstrators to favor suboptimal paths, but the errors remained bounded.\n2. **Continuous Control (Lunar Lander)**:\n   - The Lunar Lander game was used to analyze biases in continuous-action settings. Simulated biases, such as underestimating inertia, led to suboptimal trajectories. Results confirmed that policy divergence and reward inference error are strongly correlated, with errors decreasing as human models became more accurate.\n3. **Real Human Demonstrations**:\n   - Using real human data in the Lunar Lander game, the study observed that natural human biases, such as failing to account for gravity, led to crashes. However, interpolating between near-optimal and human policies showed that improving model accuracy consistently reduced reward inference errors.\n\n### Key Insights on Biases\n1. **Internal Dynamics Bias**: Humans may underestimate environmental stochasticity, leading to errors in transition dynamics. The study derived bounds showing that the weighted policy divergence is linearly proportional to the bias in transition probabilities.\n2. **Myopia Bias**: Humans may overvalue immediate rewards, modeled by a biased discount factor. The weighted policy divergence was shown to be linearly bounded by the absolute difference in discount factors.\n3. **Learning Bias**: Simulating a demonstrator still learning the environment revealed that as training progresses, both policy divergence and reward inference error decrease, reinforcing the importance of accurate human modeling.\n\n### Conclusion\nThe study concludes with an optimistic perspective on reward learning. While perfect human models are unattainable, refining these models can reliably improve the quality of inferred rewards. The findings emphasize the importance of continued efforts to enhance human behavior modeling in AI research. Theoretical results demonstrate that under reasonable assumptions, reward inference errors are bounded by model inaccuracies, and empirical results confirm that improving human models leads to better reward learning outcomes.\n\n### Future Directions\nThe paper highlights several avenues for future research:\n1. Developing more robust reward inference algorithms that are less sensitive to model misspecifications.\n2. Exploring alternative metrics for reward similarity that do not assume smoothness in reward parameters.\n3. Investigating the impact of reward identifiability issues and equivalence classes of reward functions.\n4. Extending the analysis to more complex environments and broader classes of human biases.\n\nBy addressing these challenges, the study aims to advance the field of reward learning and contribute to the broader goal of aligning AI systems with human values.",
    "text_only_review": "#### 1. Summary\nThis paper investigates the sensitivity of reward inference in inverse reinforcement learning (IRL) to inaccuracies in human behavior models. It addresses a critical challenge in aligning AI objectives with human values, particularly the dependency of reward learning on imperfect human models. The authors provide a theoretical analysis and empirical validation to explore the relationship between errors in human behavior models and the accuracy of inferred rewards. They introduce two metrics for policy divergence—worst-case and weighted policy divergence—and demonstrate that under reasonable assumptions, reward inference errors can be bounded linearly by model inaccuracies. The paper also empirically validates these findings in both tabular and continuous control environments, including real human demonstrations, and provides insights into specific human biases such as internal dynamics bias, myopia, and learning bias.\n\n#### 2. Strengths\n1. **Timely and Relevant Problem**: The paper addresses a fundamental issue in reward learning—how inaccuracies in human behavior models affect reward inference. This is a critical challenge for aligning AI systems with human values.\n2. **Theoretical Contributions**: The authors provide a rigorous theoretical framework, including both negative results (adversarial sensitivity) and stability results under reasonable assumptions. The analysis is well-grounded and mathematically robust.\n3. **Practical Metrics**: The introduction of weighted policy divergence as a more practical metric for evaluating model inaccuracies is a notable contribution, as it focuses on states relevant to the true reward parameters.\n4. **Empirical Validation**: The paper includes extensive experiments in both tabular and continuous control environments, as well as real human demonstrations, to validate the theoretical findings. The results consistently support the theoretical claims.\n5. **Insightful Analysis of Biases**: The study provides a detailed analysis of specific human biases (e.g., internal dynamics bias, myopia, and learning bias) and their impact on reward inference, offering actionable insights for improving human behavior models.\n6. **Clear Future Directions**: The paper outlines meaningful avenues for future research, such as developing more robust reward inference algorithms and exploring alternative metrics for reward similarity.\n\n#### 3. Weaknesses\n1. **Limited Scope of Biases**: While the paper analyzes specific biases like internal dynamics and myopia, it does not explore more complex or less structured biases that may arise in real-world scenarios. This limits the generalizability of the findings.\n2. **Assumptions on Human Behavior**: The stability results rely on assumptions like log-concavity of policies with respect to reward parameters. While reasonable, these assumptions may not hold in all settings, particularly for highly irrational or unpredictable human behaviors.\n3. **Scalability to Complex Environments**: The experiments are limited to relatively simple environments (e.g., gridworld and Lunar Lander). The applicability of the findings to more complex, high-dimensional environments remains unclear.\n4. **Real Human Demonstrations**: The use of real human demonstrations is a strength, but the analysis of these demonstrations is somewhat limited. For example, the paper does not deeply explore how individual differences among humans (e.g., expertise levels) affect reward inference.\n5. **Robustness of MLE**: The paper relies on maximum likelihood estimation (MLE) for reward inference, which may not be robust to significant model misspecifications. Alternative inference methods could have been explored to complement the findings.\n\n#### 4. Clarity & Reproducibility\n- **Clarity**: The paper is well-written and clearly structured. The theoretical analysis, experimental setup, and results are presented in a logical and coherent manner. The use of illustrative examples and detailed explanations enhances understanding.\n- **Reproducibility**: The empirical experiments are described in sufficient detail, including the environments, biases, and metrics used. However, the paper does not explicitly mention the availability of code or datasets, which could hinder reproducibility. Providing open-source implementations would significantly enhance the paper's impact.\n\n#### 5. Novelty & Significance\n- **Novelty**: The paper makes a novel contribution by rigorously analyzing the sensitivity of reward inference to human model inaccuracies and introducing practical metrics like weighted policy divergence. The combination of theoretical and empirical approaches is also commendable.\n- **Significance**: The findings are highly significant for the field of reward learning and AI alignment. By demonstrating that reward inference errors can be bounded under reasonable assumptions, the paper provides a foundation for improving human behavior models and developing more robust reward inference algorithms. The insights into specific biases further contribute to the broader understanding of human-AI interaction.\n\n---\n\nIn conclusion, this paper makes a valuable contribution to the field of reward learning by addressing a critical challenge in aligning AI systems with human values. While there are some limitations in the scope of biases analyzed and the scalability of the findings, the theoretical and empirical results are robust and impactful. The paper is well-suited for publication in a top-tier venue like ICLR, provided the authors address the concerns regarding reproducibility and explore additional biases or environments in future work.",
    "merged_image_path": "temp_cache\\5447_on_the_sensitivity_of_reward_i_merged.png",
    "image_only_review": "#### 1. Summary\nThe paper investigates the sensitivity of reward inference to model misspecification in human models. It explores how inaccuracies in modeling human behavior can lead to significant errors in inferred rewards, which are crucial for reinforcement learning systems that rely on human feedback. The authors provide a theoretical analysis of the impact of model misspecification, present stability results under certain conditions, and conduct empirical evaluations to demonstrate the practical implications of their findings.\n\n#### 2. Strengths\n- **Theoretical Analysis**: The paper provides a rigorous theoretical framework to analyze the sensitivity of reward inference to model misspecification. This is a significant contribution as it offers a formal understanding of the problem.\n- **Stability Results**: The authors present stability results that identify conditions under which reward inference can be robust to certain types of misspecification. This is valuable for designing more reliable systems.\n- **Empirical Validation**: The empirical analysis supports the theoretical findings, demonstrating the real-world applicability of the results. The experiments are well-designed and cover various scenarios.\n- **Relevance**: The topic is highly relevant to the field of reinforcement learning, particularly in applications involving human interaction, such as robotics and autonomous systems.\n\n#### 3. Weaknesses\n- **Complexity of Analysis**: The theoretical analysis, while rigorous, is quite complex and may be difficult for practitioners to apply directly in real-world scenarios without further simplification or guidance.\n- **Assumptions**: Some of the stability results rely on assumptions that may not hold in all practical situations. The paper could benefit from a more detailed discussion on the limitations of these assumptions.\n- **Limited Scope of Empirical Tests**: While the empirical tests are well-executed, they are limited in scope. Expanding the range of scenarios and models tested could strengthen the paper's conclusions.\n\n#### 4. Clarity & Reproducibility\n- **Clarity**: The paper is generally well-written, with a clear structure and logical flow. However, some sections, particularly those involving complex mathematical derivations, could benefit from additional explanations or visual aids to enhance understanding.\n- **Reproducibility**: The paper includes detailed descriptions of the experimental setup, which should facilitate reproducibility. However, providing code or supplementary materials would further enhance reproducibility efforts.\n\n#### 5. Novelty & Significance\n- **Novelty**: The paper addresses a critical gap in the literature regarding the sensitivity of reward inference to model misspecification. The combination of theoretical and empirical analysis is novel and adds depth to the understanding of this issue.\n- **Significance**: The findings have significant implications for the design of reinforcement learning systems that interact with humans. By highlighting the potential pitfalls of model misspecification, the paper provides valuable insights that can guide future research and development in this area.",
    "multimodal_review": "**1. Summary**  \nThe paper investigates the sensitivity of reward inference in inverse reinforcement learning (IRL) to inaccuracies in human behavior models. It combines theoretical analysis and empirical experiments to explore how errors in these models affect the accuracy of inferred rewards. The study frames the problem as a Markov Decision Process (MDP) and introduces two metrics for policy divergence: worst-case and weighted policy divergence. Theoretical results show that under certain assumptions, reward inference errors can be linearly bounded by model inaccuracies. Empirical validation in gridworld tasks and continuous control environments supports these findings, highlighting the importance of refining human behavior models to improve reward learning.\n\n**2. Strengths**  \n- The paper addresses a significant challenge in aligning AI objectives with human values, contributing to the field of reward learning.\n- It provides a comprehensive theoretical framework that rigorously analyzes the impact of model inaccuracies on reward inference.\n- Empirical experiments in both tabular and continuous control environments effectively validate the theoretical findings.\n- The introduction of two policy divergence metrics offers a nuanced approach to measuring model errors and their effects on reward inference.\n- The paper suggests practical future directions, emphasizing the need for robust reward inference algorithms and improved human behavior models.\n\n**3. Weaknesses**  \n- **Clarity of Theoretical Results**: The presentation of the theoretical results, particularly in Section 4, could be made clearer by providing more intuitive explanations alongside the formal proofs. Suggestion: Include a brief summary or example to illustrate the implications of the theoretical findings.\n- **Baseline Comparisons**: The empirical results in Section 4.2 lack comparisons with existing reward inference methods. Suggestion: Include baseline comparisons to contextualize the performance of the proposed approach.\n- **Figure Clarity**: Some figures, such as Figure 5, have captions that are not sufficiently descriptive, making it difficult to understand the results without referring back to the text. Suggestion: Enhance figure captions to be more self-explanatory.\n- **Assumption Justification**: The assumptions regarding human behavior, such as log-concavity, are not thoroughly justified in Section 5. Suggestion: Provide empirical or literature-based evidence supporting these assumptions.\n- **Reproducibility Details**: The paper lacks detailed information on the experimental setup, such as hyperparameters and hardware specifications, which are crucial for reproducibility. Suggestion: Include a supplementary section detailing these aspects.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity** – The paper is generally well-written, with a logical flow of ideas and clear explanations of methods and claims. However, some sections, particularly those involving complex theoretical results, could benefit from additional intuitive explanations. Section titles are informative, and mathematical notations are well-defined. Assumptions and limitations are mentioned but could be elaborated further.\n  \n  **(b) Figure & Caption Clarity** – Figures are relevant and support the paper's claims, but some captions are not self-sufficient. For instance, Figure 5's caption could be more descriptive to aid understanding without referring back to the main text. Axes and labels are generally consistent and readable.\n  \n  **(c) Reproducibility Transparency** – The paper provides a good overview of the experimental setups but lacks detailed information on datasets, hyperparameters, and hardware used. There is no mention of code or data availability, which is crucial for reproducibility. Ablation studies are not explicitly mentioned, and algorithmic steps could be more clearly delineated.\n\n**5. Novelty & Significance**  \nThe paper addresses a pertinent problem in the field of AI alignment, specifically the sensitivity of reward inference to human model inaccuracies. The approach is well-motivated and contextualized within the existing literature, providing both theoretical and empirical insights. The introduction of new metrics for policy divergence and the demonstration of bounded reward inference errors under certain assumptions are novel contributions. The work is significant as it highlights the importance of refining human behavior models to improve reward learning, offering valuable insights for future research in AI alignment and IRL.",
    "multimodal_rag_review": "**1. Summary**  \nThe paper investigates the sensitivity of reward inference to inaccuracies in human behavior models within the context of inverse reinforcement learning (IRL). It provides theoretical insights showing that small errors in human behavior models can lead to significant errors in inferred rewards under adversarial conditions, but are generally bounded under realistic biases. The authors empirically validate their findings using diagnostic gridworld tasks, the Lunar Lander game, and real human demonstration data. The results suggest that refining human behavior models can improve AI alignment, offering an optimistic outlook for reward learning.\n\n**2. Strengths**  \n- The paper addresses a critical and timely issue in AI alignment, focusing on the robustness of reward inference to model inaccuracies.\n- It provides a comprehensive theoretical analysis that is well-supported by empirical validation across different environments.\n- The connection to existing IRL frameworks, particularly MaxEnt IRL, is well-articulated, enhancing the paper's relevance to ongoing research in the field.\n- The study offers practical insights into improving human behavior models, which is valuable for advancing AI alignment.\n\n**3. Weaknesses**  \n- **Limited Experimental Diversity**: The empirical validation primarily focuses on gridworld tasks and the Lunar Lander game (Section 5). Expanding the range of environments could strengthen the generalizability of the findings. Consider incorporating more diverse and complex scenarios to test the robustness of the theoretical claims.\n- **Clarity in Theoretical Presentation**: Some theoretical results, particularly in Section 4.1, are dense and could benefit from clearer exposition. Simplifying the mathematical notations and providing more intuitive explanations would enhance understanding.\n- **Related Work Discussion**: The discussion of related work (Section 2) could be expanded to include more recent advancements in IRL and human behavior modeling. This would better position the research within the broader context of the field.\n- **Assumption Justification**: The assumptions regarding log-concavity and reward identifiability (Section 4.2) are critical to the theoretical results but are not thoroughly justified. Providing empirical evidence or literature support for these assumptions would strengthen the paper's claims.\n- **Figure Clarity**: Some figures, such as those in Section 5.2, lack detailed captions and axis labels, making it difficult to interpret the results. Enhancing figure clarity and ensuring that captions are self-sufficient would improve the paper's overall presentation.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**: The paper is generally well-structured, with a logical flow of ideas. However, some sections, particularly those involving complex theoretical results, could benefit from clearer explanations and simplification of mathematical notations. The assumptions and limitations are acknowledged but would benefit from more explicit articulation.\n\n  **(b) Figure & Caption Clarity**: While figures are relevant to the paper's claims, some lack sufficient detail in captions and axis labels, which can hinder comprehension. Ensuring that all figures are accompanied by comprehensive captions and clear labels would enhance their effectiveness.\n\n  **(c) Reproducibility Transparency**: The paper provides a good level of detail regarding experimental setups, including datasets and tasks used. However, information on hyperparameters, hardware specifications, and random seeds is limited. Including these details, along with a mention of code availability, would significantly enhance reproducibility.\n\n**5. Novelty & Significance**  \nThe paper addresses a novel and significant problem in AI alignment by exploring the sensitivity of reward inference to inaccuracies in human behavior models. The approach is well-motivated and contextualized within existing literature, particularly in relation to MaxEnt IRL. The theoretical and empirical findings are robust, offering valuable insights into the feasibility of reward learning under realistic human biases. The work contributes new knowledge to the field, emphasizing the importance of refining human behavior models to improve AI alignment. While the paper does not achieve state-of-the-art results, its contributions are impactful and relevant to the ICLR community."
}