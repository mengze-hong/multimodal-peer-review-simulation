{
    "paper_id": "5507_when_and_why_vision_language_m",
    "title": "When and Why Vision-Language Models Behave like Bags-Of-Words, and What to Do About It?",
    "abstract": "Despite the success of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode the compositional relationships between objects and attributes. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order information. ARO consists of \\emph{Visual Genome Attribution}, to test the understanding of objects' properties; \\emph{Visual Genome Relation}, to test for relational understanding; and \\emph{COCO-Order \\& Flickr30k-Order}, to test for order sensitivity in VLMs. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases. We present the settings where  state-of-the-art VLMs behave like bags-of-words---i.e. when they have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity. VLMs are predominantly trained and evaluated on large scale datasets with rich compositional structure in the images and captions. Yet, training on these datasets has not been enough to address the lack of compositional understanding, and evaluating on these datasets has failed to surface this deficiency. To understand why these limitations emerge and are not represented in the standard tests, we zoom into the evaluation and training procedures. We demonstrate that it is possible to perform well on image-text retrieval over existing datasets without using the composition and order information. This further motivates the value of using ARO to benchmark VLMs. Given that contrastive pretraining optimizes for retrieval on large datasets with similar shortcuts, we hypothesize that this can explain why the models do not need to learn to represent compositional information. This finding suggests a natural solution: composition-aware hard negative mining. We show that a simple-to-implement modification of contrastive learning significantly improves the performance on tasks requiring understanding of order and compositionality.",
    "human_review": "Summary Of The Paper:\nThis paper probes the compositional generalization ability of large pre-trained vision-and-language models, such as CLIP. They introduce a benchmark dataset of synthetically generated caption perturbations, called ARO. They show that out-of-the-box performance on ARO is poor, and that existing models do not seem to distinguish captions with ordering perturbation. They also propose a method for training these models to be more robust compositionally, by mining and generating hard negative examples to fine-tune with.\n\nStrength And Weaknesses:\nStrengths:\n\nThe discussion of the problem and presentation of the dataset and solution is extremely clear; the paper is very well-written.\nThe paper presents a clear problem with existing systems and the drawback of using retrieval as an evaluation method. The presented benchmark is offering a \"bare minimum\" sort of evaluation for such systems.\nWeaknesses:\n\nThe fonts in some of the figures is really small. I also strongly suggest putting numbers on the actual bar charts; it's difficult to interpret the results otherwise.\nThe numbers for the experiment in 2.3 (evaluating COCO/Flickr30k on perturbations of captions) should be in the main paper, not the appendix.\nIt is suggested that the reason these models ignore word ordering so much is that they really are trained as keyword identifiers, as required for image retrieval, and there's no reason to learn ordering. However, what happens if you incorporate better priors on the captions? I would imagine that a large language model would place very low probability on most of the perturbed captions (I could be wrong, though), and a VLM that uses features from a general large language model would be able to distinguish the obviously grammatically incorrect examples from the true caption.\nI was hoping the dataset would be a bit more of a scaled-up Winoground dataset, because Winoground directly tests all four settings (perturbation of relations in text and perturbation of relations in image). However, this evaluation set only seems to test perturbation of relations in text.\nQuestions:\n\nWhy are there so few relations and attribute pairs in ARO, as described in Section 2.1?\nPractically, how are some of the perturbations done? With operations on top of the parse tree?\nDoes the experiment in Section 4 backprop through both text and image features in CLIP?\nClarity, Quality, Novelty And Reproducibility:\nQuality:\n\nExperimental setup is thorough. My main concern would be that this becomes a new benchmark to achieve such that people interpret success on it to imply good compositional understanding. I think it works as a \"bare minimum\" evaluation, but performing well on it doesn't necessarily confirm a model is performing correct compositional reasoning (partially because I imagine using a LLM prior will downweight the grammatically incorrect or semantically implausible perturbations, for example).\nClarity:\n\nPaper is very clearly written, with a few small comments on readability of figures.\nOriginality:\n\nRelatively original; the experiments are new as far as I can tell, as is the proposed training method. However, in terms of a dataset, there are many that evaluate compositionality with synthetically generated data (GQA, CLEVR), or could be adapted to evaluate more semantically plausible alternatives for compositionality. Missing citation for the NLVR(2) corpora (Suhr et al. 2017/2019) which also evaluate compositionality with both true and false image pairs.\nSummary Of The Review:\nThis paper introduces a new benchmark and training method for evaluating compositionality of VLMs. The compositionality is mostly evaluated by perturbing captions of existing datasets in several ways. Experiments find that SOTA VLMs perform poorly (i.e., they cannot easily distinguish between perturbed and true captions). The paper is very well written. My main concern is that the kinds of perturbations in the evaluation set may be easy to reject simply by considering priors from an LLM, something which is not evaluated in this paper.\n\nCorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\nTechnical Novelty And Significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\nEmpirical Novelty And Significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\nFlag For Ethics Review: NO.\nRecommendation: 8: accept, good paper\nConfidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work",
    "text_summary": "### Motivation\nVision-language models (VLMs) have achieved remarkable success in tasks like image-text retrieval, but their ability to understand compositionality—relationships, attributes, and word order—remains limited. For example, models often fail to distinguish between captions like \"the horse is eating the grass\" and \"the grass is eating the horse.\" This deficiency arises because current training paradigms, such as contrastive pretraining, optimize for retrieval performance rather than deeper compositional understanding. Existing benchmarks, like Winoground, are too small to provide statistically robust evaluations of these limitations. This study aims to address these gaps by introducing a large-scale benchmark and proposing a novel training approach to improve compositional understanding in VLMs.\n\n### Methodology\n#### Benchmark: ARO (Attribution, Relation, and Order)\nThe authors introduce the ARO benchmark, a large-scale evaluation framework with over 50,000 test cases designed to assess compositional understanding in VLMs. ARO includes four tasks:\n1. **Visual Genome Attribution**: Tests models' ability to understand object properties (e.g., \"the crouched cat and the open door\" vs. \"the open cat and the crouched door\").\n2. **Visual Genome Relation**: Evaluates relational understanding, such as spatial relations (\"the dog is behind the tree\" vs. \"the tree is behind the dog\").\n3. **COCO Order** and **Flickr30k Order**: Assess sensitivity to word order in captions by comparing original captions to systematically perturbed versions.\n\n#### Dataset Creation\nUsing the Visual Genome (VG) dataset and GQA annotations, the authors generate:\n- **VG-Relation**: 23,937 test cases across 48 relations (e.g., \"sitting on,\" \"eating\").\n- **VG-Attribution**: 28,748 test cases across 117 attribute pairs (e.g., \"gray vs. wood\").\nFor order sensitivity, COCO and Flickr30k datasets are augmented with perturbed captions using part-of-speech tagging to shuffle linguistic elements systematically.\n\n#### Proposed Training Approach: Composition-Aware Hard Negative Mining\nTo address the limitations of contrastive pretraining, the authors propose a method called **NegCLIP**, which extends CLIP's contrastive objective by incorporating \"hard negatives\":\n1. **Negative Caption Generation**: Captions are perturbed by swapping linguistic elements (e.g., nouns, adjectives, verbs) to create challenging negative samples.\n2. **Negative Image Sampling**: Images similar to the target image are identified as hard negatives using pairwise similarity scores.\n3. **Training Objective**: The model is trained to distinguish between original and perturbed captions/images, forcing it to learn fine-grained compositional and order-based cues.\n\n#### Experimental Setup\nThe study evaluates four state-of-the-art VLMs—CLIP, BLIP, Flava, and X-VLM—on the ARO benchmark and standard downstream tasks. NegCLIP is fine-tuned on the COCO dataset using a batch size of 1024 and evaluated on both compositional and retrieval tasks.\n\n### Results\n#### Compositional Understanding\n1. **Relational Understanding (VG-Relation)**:\n   - NegCLIP achieves the highest accuracy (81%), outperforming CLIP (59%), CLIP-FT (63%), and BLIP (59%).\n   - Models struggle with spatial relations like \"to the right of,\" with most performing near chance level.\n2. **Attributive Understanding (VG-Attribution)**:\n   - NegCLIP scores 71%, higher than CLIP (62%) and CLIP-FT (65%).\n   - BLIP and X-VLM perform well (88% and 87%, respectively), while Flava lags behind (73%).\n3. **Order Sensitivity (COCO and Flickr30k Order)**:\n   - NegCLIP significantly improves performance, achieving 86% on COCO Order and 91% on Flickr30k Order, compared to CLIP's 46% and 59%, respectively.\n\n#### Retrieval and Downstream Tasks\n1. **Text and Image Retrieval**:\n   - NegCLIP maintains competitive performance on retrieval tasks, achieving Recall@1 scores of 41% (COCO Image) and 79% (Flickr30k Text), comparable to CLIP-FT.\n2. **Image Classification**:\n   - NegCLIP performs slightly below CLIP on CIFAR-10 (94% vs. 95%) and ImageNet (72% vs. 75%), indicating no significant trade-offs in generalization.\n\n#### Ablation Studies\nThe study confirms that composition-aware hard negatives are the key driver of performance improvements. Models trained without these negatives perform significantly worse on compositional tasks.\n\n### Analysis and Critique\nThe study critiques the dominant image-text contrastive pretraining paradigm, showing that VLMs can achieve high retrieval performance without understanding compositionality. For example, models perform well even when captions or images are perturbed, suggesting reliance on shortcuts rather than true compositional representations. The findings align with prior work showing that text-conditioned image generators using CLIP struggle with relational fidelity, while models with language model priors (e.g., T5) perform better on compositional tasks.\n\n### Ethical Considerations\nThe paper highlights ethical concerns with large-scale datasets, including privacy issues, stereotypical representations, and resource-intensive training. Failures in compositional understanding can reinforce stereotypes, as demonstrated by text-to-image systems like DALL-E misrepresenting demographic attributes. The authors advocate for exposing these biases to inform the development of more equitable models.\n\n### Conclusion\nThe ARO benchmark reveals significant deficiencies in VLMs' compositional understanding, particularly in relational reasoning and word order sensitivity. The proposed NegCLIP method demonstrates that targeted hard negatives can significantly improve performance on these tasks without compromising generalization. These findings underscore the need for richer evaluation frameworks and training objectives aligned with compositional understanding. Future work will explore scaling composition-aware contrastive pretraining and integrating language model priors to further enhance VLMs' capabilities. The study provides a robust foundation for advancing the compositional and relational reasoning abilities of vision-language models.",
    "text_only_review": "#### 1. Summary\nThis paper addresses the limitations of vision-language models (VLMs) in compositional understanding, specifically in distinguishing relationships, attributes, and word order in image-text pairs. The authors introduce the ARO benchmark, a large-scale evaluation framework with over 50,000 test cases, to assess compositional understanding across four tasks: attribution, relation, and order sensitivity. To improve VLM performance, the authors propose NegCLIP, a novel training approach that incorporates composition-aware hard negative mining into the contrastive learning paradigm. The study evaluates state-of-the-art VLMs on the ARO benchmark and standard downstream tasks, demonstrating that NegCLIP significantly enhances compositional understanding without sacrificing generalization in retrieval and classification tasks.\n\n#### 2. Strengths\n1. **Comprehensive Benchmark**: The ARO benchmark is a significant contribution, offering a large-scale, diverse, and systematic evaluation framework for compositional understanding in VLMs. Its inclusion of multiple tasks (attribution, relation, and order) provides a holistic assessment of model capabilities.\n2. **Novel Training Approach**: The proposed NegCLIP method effectively addresses the limitations of existing contrastive pretraining paradigms by incorporating hard negatives that explicitly target compositional understanding.\n3. **Empirical Results**: The study provides extensive experimental results, showing that NegCLIP significantly outperforms baseline models on compositional tasks while maintaining competitive performance on retrieval and classification benchmarks.\n4. **Ablation Studies**: The inclusion of ablation studies strengthens the paper by isolating the impact of composition-aware hard negatives, demonstrating their critical role in performance improvements.\n5. **Ethical Considerations**: The paper thoughtfully discusses ethical implications, including biases in datasets and the societal impact of compositional failures, highlighting the importance of addressing these issues in future work.\n\n#### 3. Weaknesses\n1. **Limited Analysis of Failure Cases**: While the paper identifies areas where models struggle (e.g., spatial relations), it lacks a detailed qualitative analysis of failure cases to provide deeper insights into why these challenges persist.\n2. **Generalization Trade-offs**: Although NegCLIP performs well on compositional tasks, its slight drop in classification performance (e.g., on ImageNet) raises questions about potential trade-offs. A more detailed discussion of these trade-offs would strengthen the paper.\n3. **Scalability of NegCLIP**: The paper does not explore the computational cost or scalability of NegCLIP, particularly for larger datasets or models. This omission is critical given the resource-intensive nature of training VLMs.\n4. **Limited Comparison to Language Model Priors**: While the paper mentions that models with language model priors (e.g., T5) perform better on compositional tasks, it does not provide a direct comparison to such approaches, which would contextualize the contributions of NegCLIP more effectively.\n5. **Benchmark Bias**: The ARO benchmark relies heavily on datasets like Visual Genome, COCO, and Flickr30k, which may themselves contain biases. The paper does not address whether these biases could influence the evaluation of compositional understanding.\n\n#### 4. Clarity & Reproducibility\n- **Clarity**: The paper is well-written and clearly structured, with detailed explanations of the benchmark, methodology, and experimental results. The use of examples (e.g., perturbed captions) effectively illustrates key concepts.\n- **Reproducibility**: The methodology for creating the ARO benchmark and training NegCLIP is described in sufficient detail, including dataset preprocessing, negative sample generation, and training objectives. However, the paper does not provide code or specific hyperparameters (beyond batch size), which may hinder full reproducibility.\n\n#### 5. Novelty & Significance\n- **Novelty**: The ARO benchmark and NegCLIP training approach represent novel contributions to the field. The benchmark addresses a critical gap in evaluating compositional understanding, while NegCLIP introduces a creative extension to contrastive learning by leveraging composition-aware hard negatives.\n- **Significance**: The findings have significant implications for advancing VLMs, particularly in applications requiring fine-grained compositional reasoning (e.g., visual question answering, captioning). The study also highlights the limitations of current training paradigms, paving the way for future research in this area.\n\n#### Overall Assessment\nThis paper makes a meaningful contribution to the field of vision-language understanding by addressing a critical limitation in compositional reasoning. The ARO benchmark and NegCLIP training approach are both valuable innovations, supported by rigorous empirical evaluation. While the paper could benefit from deeper analysis of failure cases, scalability considerations, and comparisons to language model priors, its strengths outweigh these weaknesses. The work is well-suited for a top-tier conference and has the potential to inspire further research in compositional understanding for VLMs.",
    "merged_image_path": "temp_cache\\5507_when_and_why_vision_language_m_merged.png",
    "image_only_review": "#### 1. Summary\nThe paper investigates the behavior of Vision-Language Models (VLMs) and their tendency to act like bag-of-words models, which disregard the order and structure of input data. The authors explore when and why this behavior occurs and propose methods to mitigate it. The study is motivated by the observation that VLMs sometimes fail to capture the compositional semantics of visual and textual inputs, leading to incorrect or suboptimal outputs. The authors conduct a series of experiments to analyze the conditions under which VLMs exhibit bag-of-words behavior and propose strategies to enhance their compositional understanding.\n\n#### 2. Strengths\n- **Comprehensive Analysis**: The paper provides a thorough examination of the conditions under which VLMs behave like bag-of-words models, offering valuable insights into their limitations.\n- **Methodological Rigor**: The experiments are well-designed and cover a range of scenarios, providing robust evidence for the claims made.\n- **Practical Solutions**: The authors propose actionable strategies to mitigate the identified issues, which could be beneficial for improving the performance of VLMs in practical applications.\n- **Relevance**: The study addresses a critical aspect of VLMs that is highly relevant to the field, given the increasing reliance on these models for various applications.\n\n#### 3. Weaknesses\n- **Limited Scope of Solutions**: While the proposed solutions are practical, they may not fully address the underlying issues of compositional understanding in VLMs. Further exploration of more fundamental changes to model architectures could be beneficial.\n- **Generalizability**: The experiments are conducted on specific datasets and models, which may limit the generalizability of the findings to other contexts or newer models.\n- **Depth of Analysis**: Although the paper provides a good overview of the problem, a deeper theoretical analysis of why VLMs exhibit bag-of-words behavior could strengthen the contribution.\n\n#### 4. Clarity & Reproducibility\n- **Clarity**: The paper is well-written and clearly structured, making it easy to follow the arguments and understand the experiments. The use of figures and tables effectively supports the text.\n- **Reproducibility**: The authors provide sufficient details about the experimental setup, including datasets, model configurations, and evaluation metrics, which should facilitate replication of the results. However, the inclusion of code or additional supplementary materials would further enhance reproducibility.\n\n#### 5. Novelty & Significance\n- **Novelty**: The paper addresses a relatively unexplored aspect of VLMs, contributing new insights into their limitations and potential improvements. The focus on bag-of-words behavior is novel and adds to the understanding of VLMs' compositional capabilities.\n- **Significance**: The findings have significant implications for the development and deployment of VLMs, particularly in applications where understanding the compositional semantics of inputs is crucial. By highlighting a key limitation and proposing solutions, the paper paves the way for future research and improvements in this area.\n\nIn conclusion, the paper makes a valuable contribution to the understanding of VLMs and their limitations. While there are areas for further exploration, the insights and solutions provided are a meaningful step towards enhancing the compositional capabilities of these models.",
    "multimodal_review": "**1. Summary**  \nThe paper introduces the ARO benchmark, a large-scale evaluation framework designed to assess compositional understanding in vision-language models (VLMs). The benchmark comprises over 50,000 test cases across tasks that test attribution, relation, and order understanding. Additionally, the authors propose a novel training approach, NegCLIP, which enhances compositional understanding by incorporating composition-aware hard negative mining into the contrastive learning framework. The results demonstrate that NegCLIP significantly improves performance on compositional tasks without compromising retrieval performance, highlighting the limitations of current VLMs and the potential of the proposed method.\n\n**2. Strengths**  \n- The introduction of the ARO benchmark addresses a critical gap in evaluating compositional understanding in VLMs, providing a comprehensive and statistically robust framework.\n- The proposed NegCLIP method effectively enhances compositional understanding, as evidenced by substantial improvements in benchmark performance.\n- The paper provides a thorough experimental evaluation, including comparisons with state-of-the-art models and ablation studies to validate the effectiveness of the proposed approach.\n- Ethical considerations are thoughtfully addressed, highlighting the potential biases and societal impacts of VLMs.\n\n**3. Weaknesses**  \n- **Clarity of Methodology**: The description of the negative caption generation process in Section 4.1 is somewhat vague. It would benefit from a more detailed explanation of the linguistic perturbation techniques used. Suggestion: Include specific examples or pseudocode to clarify the process.\n- **Baseline Comparisons**: In Table 2, there is no comparison with methods that specifically target compositional understanding, such as models incorporating language model priors. Suggestion: Include comparisons with such models to contextualize the performance of NegCLIP better.\n- **Figure Clarity**: Figure 3 lacks a detailed explanation of the axes and the meaning of the plotted metrics. Suggestion: Enhance the figure caption to include a comprehensive description of the axes and metrics.\n- **Reproducibility Details**: The paper does not specify the random seeds used for experiments, which is crucial for reproducibility. Suggestion: Include random seed information in the experimental setup section.\n- **Dataset Bias**: While the paper discusses ethical considerations, it does not address potential biases in the ARO benchmark itself. Suggestion: Conduct an analysis of the benchmark for potential biases and include this discussion in the paper.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**  \n  The paper is generally well-written, with clear explanations of the motivation and methodology. However, certain sections, such as the negative caption generation process, would benefit from additional detail. Mathematical notations are well-defined, and assumptions are clearly articulated.\n\n  **(b) Figure & Caption Clarity**  \n  Figures are generally effective in illustrating the main claims, but some, like Figure 3, require more detailed captions to be fully comprehensible. Axes and labels are consistent, but additional context in captions would enhance understanding.\n\n  **(c) Reproducibility Transparency**  \n  The experimental setup is described in detail, including datasets and hyperparameters. However, the omission of random seed information is a notable gap. Code and data availability are not explicitly mentioned, which could hinder reproducibility.\n\n**5. Novelty & Significance**  \nThe introduction of the ARO benchmark is a significant contribution, providing a much-needed tool for evaluating compositional understanding in VLMs. The NegCLIP method offers a novel approach to enhancing this understanding by leveraging hard negative mining, demonstrating substantial improvements over existing methods. The work is well-motivated and contextualized within the literature, addressing a critical limitation of current VLMs. The findings have significant implications for the development of more robust and equitable vision-language models, contributing valuable insights to the field.",
    "multimodal_rag_review": "**1. Summary**  \nThe paper introduces the Attribution, Relation, and Order (ARO) benchmark to evaluate the compositional understanding of Vision-Language Models (VLMs), addressing their limitations in encoding relationships, attributes, and order in visual and textual data. The benchmark includes four tasks that test models on visual genome attributions, relations, and word order sensitivity using COCO and Flickr30k datasets. The authors propose a novel training method, composition-aware hard negative mining, to improve VLMs' compositional understanding. The experimental results demonstrate that the proposed NegCLIP model significantly outperforms existing VLMs on compositional tasks while maintaining performance on standard tasks.\n\n**2. Strengths**  \n- The paper addresses a critical gap in the evaluation of VLMs by introducing a comprehensive benchmark, ARO, that focuses on compositional understanding.\n- The proposed method, composition-aware hard negative mining, is a practical and innovative approach to enhancing VLMs' ability to capture compositional structures without sacrificing general performance.\n- The experimental results are robust, showing significant improvements in compositional tasks and maintaining performance on standard benchmarks, demonstrating the effectiveness of the proposed approach.\n- The paper provides open-source code and detailed documentation, ensuring transparency and reproducibility of the experiments.\n\n**3. Weaknesses**  \n- **Clarity in Technical Explanations**: The explanation of the composition-aware hard negative mining method in Section 4 is somewhat dense and could benefit from additional examples or diagrams to enhance understanding. Consider providing a step-by-step illustration of the method.\n- **Scalability Discussion**: The paper lacks a detailed discussion on the scalability of the proposed methods, particularly in terms of computational and memory costs for larger datasets or more complex models (Section 5.3). It would be beneficial to include a scalability analysis or discussion.\n- **Failure Cases and Limitations**: The exploration of failure cases and limitations of the proposed method is limited (Section 6). Expanding this analysis could provide a deeper understanding of the model's shortcomings and guide future improvements.\n- **Presentation Issues**: Some figures, such as Figure 3, have captions that are not fully self-explanatory and could be clarified to better convey the information presented. Consider revising the captions to ensure they are comprehensive and informative.\n- **Broader Applicability**: While the paper focuses on VLMs, it does not discuss the potential applicability of the proposed methods to other domains or types of models (Section 7). Including a discussion on broader applicability could enhance the impact of the work.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**  \n  The paper is generally well-written, with ideas and methods explained logically. However, some sections, particularly those detailing the technical aspects of the proposed method, could benefit from additional clarity and examples. The mathematical notations are well-defined, but the assumptions and limitations could be more explicitly stated.\n\n  **(b) Figure & Caption Clarity**  \n  Figures are generally effective in illustrating the main claims, but some captions, such as those for Figure 3, could be more detailed to ensure they are self-sufficient. Axes, labels, and legends are consistent and readable, and diagrams correlate well with the textual descriptions.\n\n  **(c) Reproducibility Transparency**  \n  The paper provides adequate detail on the experimental setups, including datasets, hyperparameters, and evaluation metrics. The availability of open-source code and detailed documentation enhances reproducibility. However, the paper could benefit from more detailed descriptions of the algorithmic steps and any specific hardware or software requirements.\n\n**5. Novelty & Significance**  \nThe paper makes a significant contribution by addressing a critical gap in the evaluation of VLMs' compositional understanding. The introduction of the ARO benchmark and the novel composition-aware hard negative mining method represent important advancements in the field. The work is well-motivated and contextualized within the existing literature, and the experimental results substantiate the claims made. The significance of the work lies in its potential to improve VLMs' ability to capture compositional structures, which is crucial for their application in real-world scenarios. The paper's contributions are likely to inspire further research in improving the compositional understanding of VLMs and other types of models."
}