{
    "paper_id": "6584_online_restless_bandits_with_u",
    "title": "ONLINE RESTLESS BANDITS WITH UNOBSERVED STATES ",
    "abstract": "We study the online restless bandit problem, where each arm evolves according to a Markov chain independently, and the reward of pulling an arm depends on both the current state of the corresponding Markov chain and the action. The agent (decision maker) does not know the transition kernels and reward functions, and cannot observe the states of arms all the time. The goal is to sequentially choose which arms to pull so as to maximize the expected cumulative rewards collected. In this paper, we propose TSEETC, a learning algorithm based on Thompson Sampling with Episodic Explore-Then-Commit. The algorithm proceeds in episodes of increasing length and each episode is divided into exploration and exploitation phases. In the exploration phase in each episode, action-reward samples are collected in a round-robin way and then used to update the posterior as a mixture of Dirichlet distributions. At the beginning of the exploitation phase, TSEETC generates a sample from the posterior distribution as true parameters. It then follows the optimal policy for the sampled model for the rest of the episode. We establish the Bayesian regret bound for TSEETC, where T is the time horizon. This is the first bound that is close to the lower bound of restless bandits, especially in an unobserved state setting. We show through simulations that TSEETC outperforms existing algorithms in regret.",
    "human_review": "Summary Of The Paper:\nThis paper focuses on solving online restless bandits with unknown parameter and unobservable states by the proposed algorithm TSEETC. A Bayesian regret bound with O(sqrt(T)) dependency is established, which matches the lower bound dependency on T and improves the existing O(T^{2/3}) bound derived for a related problem setting by Zhou et al. Also, simulations are showing that TSEETC outperforms existing algorithms in regret as proof-of-concept.\n\nStrength And Weaknesses:\nStrengths:\n- This paper is a solid work providing theoretical understanding for a well-defined problem, which has characteristics from both bandit and POMDP and is novel with no existing work addressing the exact same setting.\n- The O(sqrt(T)) dependency, matching the lower bound dependency on T, is a significant improvement compared to the existing bounds of O(T^{2/3}). However, I’m not fully convinced by this improved dependency and have concern on the regret’s dependency on S and N, the number of states and arms respectively.\n\nWeaknesses:\n- Notations are somewhat sloppy: To name a few which cause me the most trouble while reading: (1) Even in the main theorem Theorem 1: the notation N comes out of nowhere, I assume it should be the number of arms. (2) In the main algorithm Algorithm 2: (i) Line 4, P and H could be misleading. If following Lemma 1, P should refer to the posterior of theta conditioned on the history up to t, however it could also mean P(theta|H_t), which is what I assume the author is actually referring to. This two interpretation have drastic difference since it depends on whether the data from the exploitation phase is used to update the posterior or not. (ii) Line 12, it's not clear what are the obtained P and H, though for this case I can guess them from the context. Some others in the main text like Eq(12), Eq(13) on page 9. Also I came across complete or repeated sentences in the appendix. Though the paper is written in a well-organized way most of the time, notations coming out of the blue and not rigorous statements in the main algorithm make it confusing for ones who are trying to parse the algorithm and theorem to get some intuitions behind the math. Sloppy notations truly harm the clarity as well as the formality of this paper.\n- Exponential dependency on S: It feels the exponential dependency O(exp(S)) appearing in constant C is far from tight, given the Markov chain associated with each arm is independent. To compare with, the regret by Zhou et al. scales linearly with S, which is the number of hidden states in the common MC shared by all arms. In the restless bandit setting, the complexity should be of N independent MCs with S hidden states rather than one MC with N*S hidden states.\n\nOther Concerns:\n- Why O(sqrt(T))? More intuition is needed. I’m not fully convinced by why TSEETC is able to improve regret from O(T^{2/3}) by Zhou, whose algorithm mostly resembles TSEETC, except for using the UCB estimator constructed with the spectral method for HMM. Based on comparing both algorithms and going through the proof sketch, what directly improves the bound is that TSEETC has a longer exploitation phase in each episode and thus there are only O(log(T)) episodes less than O(T^{1/3}) by Zhou et al. Given both algorithms do not use the on-policy data in exploitation phase (by the way I assume it happens for TSEETC because the notation is not clear), it implies posterior sampling concentrates to the ground truth model parameter better than UCB in terms of sample efficiency. It seems kind of counterintuitive based on the understanding of TS v.s. UCB from classic bandit literature, or the bottleneck of Zhou et al is to the spectral estimator around which the UCB is constructed?\n- Bayesian regret. This concern relates to the previous one. A common understanding from the classic bandit literature is that UCB and TS based algorithms usually have regrets of the same order, and TS based algorithms have strictly worse regret bounds from a frequentist view. I’d like to know if it’s possible to have a UCB-based algorithm achieving sqrt(T) regret.\n- Computational cost of posteriors. To compute the exact posterior, one has to exhaust all possible state transitions of length T, which means a total number of passes exponential to S, for log(T) episodes. Though S would be of a constant order in theory, does this impose a higher computational cost when realizing TSEETC than SEEU in practice?\n\nClarity, Quality, Novelty And Reproducibility:\nClarity in writing could be improved. Quality and novelty have been evaluated in Strength/Weakness/Concern in detail. Overall, this paper has sufficient novelty for the problem it studies and the results it claims to get, which I may need more evidence/intuitions to be convinced. If the notation could be revised carefully throughout the paper, then the quality of presentation is good. I didn’t check the reproducibility of simulations but I’d like to believe the results are reproducible.\n\nSummary Of The Review:\nBased on my current appreciation of the regret bound which I'm not fully convinced by and the current technical presentation where misleading/confusing notations appear here and there, I give my recommendation as a borderline/marginally weak rejection. I'd be more than happy to raise my score if mainly the Weakness 2 and Concern 1 can be addressed and cleared out, with notations being improved in the revision.\n\nCorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\nTechnical Novelty And Significance: 4: The contributions are significant, and do not exist in prior works.\nEmpirical Novelty And Significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\nFlag For Ethics Review: NO.\nRecommendation: 6: marginally above the acceptance threshold.\nConfidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
    "text_summary": "### Summary of the Paper\n\n#### Motivation\nThis paper addresses the **online restless bandit problem**, a challenging sequential decision-making framework where multiple arms evolve independently as Markov chains. The agent's goal is to maximize cumulative rewards over a finite time horizon \\( T \\), despite not knowing the transition probabilities or reward functions of the arms. Compounding the difficulty, the states of the arms remain unobservable, even after pulling them. This problem has practical applications in areas such as wireless communication, healthcare, and sensor maintenance. Existing solutions for similar problems either assume observable states or known parameters, or they achieve suboptimal regret bounds, such as \\( \\tilde{O}(T^{2/3}) \\), which fail to match the theoretical lower bound of \\( \\tilde{O}(\\sqrt{T}) \\). This paper aims to bridge this gap by proposing a novel algorithm that achieves near-optimal regret in this challenging setting.\n\n---\n\n#### Methodology\nThe authors propose **TSEETC (Thompson Sampling with Episodic Explore-Then-Commit)**, a Bayesian algorithm that balances exploration and exploitation in episodes of increasing length. The algorithm operates as follows:\n\n1. **Episodic Structure**:\n   - Each episode is divided into an exploration phase and an exploitation phase.\n   - The length of episodes increases deterministically, ensuring that the total number of episodes scales as \\( O(\\sqrt{T}) \\).\n\n2. **Exploration Phase**:\n   - During exploration, each arm is pulled in a round-robin manner to collect action-reward samples.\n   - The collected data is used to update the posterior distributions of the unknown transition and reward parameters, modeled as mixtures of Dirichlet distributions.\n\n3. **Exploitation Phase**:\n   - At the start of the exploitation phase, a model is sampled from the posterior distribution.\n   - The algorithm computes the optimal policy for the sampled model and follows it for the remainder of the episode.\n\n4. **Belief State Updates**:\n   - The belief state for each arm is updated using observed rewards and transition probabilities during exploration. For unobserved states, the belief update incorporates all possible state transitions weighted by their likelihood.\n\n5. **Posterior Sampling**:\n   - Transition probabilities and reward functions are modeled using Dirichlet distributions, parameterized by count vectors derived from historical data. Posterior updates are performed using a mixture of Dirichlet distributions, ensuring accurate parameter estimation.\n\n6. **Regret Control**:\n   - The deterministic increase in episode lengths ensures that the regret from exploration phases is bounded by \\( O(\\sqrt{T}) \\), while the exploitation phase leverages the posterior sampling framework to minimize suboptimal actions.\n\n---\n\n#### Results\n\n##### Theoretical Analysis\n1. **Bayesian Regret Bound**:\n   - The regret of TSEETC is shown to be \\( \\tilde{O}(\\sqrt{T}) \\), where \\( \\tilde{O} \\) hides logarithmic factors. This matches the theoretical lower bound for restless bandit problems and significantly improves upon prior methods, which achieve regret bounds of \\( \\tilde{O}(T^{2/3}) \\).\n\n2. **Key Theoretical Contributions**:\n   - **Proposition 1**: The span of the value function is bounded, ensuring efficient computation of the optimal policy.\n   - **Proposition 2**: Belief estimation errors are controlled, with bounds derived for the difference between true and estimated belief states.\n   - **Theorem 1**: The total regret is decomposed into exploration and exploitation components, both of which are shown to scale as \\( O(\\sqrt{T}) \\).\n\n3. **Posterior Updates**:\n   - The posterior distributions for transition and reward functions are updated using historical data, with confidence bounds ensuring accurate parameter estimation.\n\n##### Experimental Validation\n1. **Setup**:\n   - Simulations were conducted with two arms, each modeled as a Markov chain with two hidden states, over a time horizon of \\( T = 50,000 \\).\n   - TSEETC was compared against baseline algorithms, including \\(\\epsilon\\)-greedy, Sliding-Window UCB, RUCB, Q-learning, and SEEU.\n\n2. **Results**:\n   - TSEETC achieved the lowest cumulative regret among all algorithms.\n   - In a log-log plot of cumulative regret versus \\( T \\), TSEETC exhibited a slope close to 0.5, consistent with the theoretical regret bound of \\( \\tilde{O}(\\sqrt{T}) \\). In contrast, other methods showed higher slopes, indicating suboptimal performance.\n\n3. **Performance**:\n   - TSEETC outperformed existing methods in terms of both theoretical regret bounds and empirical performance, demonstrating its effectiveness in balancing exploration and exploitation.\n\n---\n\n#### Conclusion\nThis paper introduces **TSEETC**, a novel algorithm for restless bandit problems with unobserved states and unknown parameters. By leveraging Thompson Sampling and episodic exploration-exploitation, TSEETC achieves a Bayesian regret bound of \\( \\tilde{O}(\\sqrt{T}) \\), marking a significant improvement over prior methods. The algorithm's use of belief states and Dirichlet-based posterior updates ensures robust parameter estimation and efficient decision-making under uncertainty. Experimental results validate the theoretical findings, showing that TSEETC outperforms existing algorithms and achieves near-optimal performance.\n\n##### Key Contributions:\n1. **Algorithmic Innovation**:\n   - The episodic structure of TSEETC, combined with posterior sampling, provides a principled approach to balancing exploration and exploitation in unobserved state settings.\n\n2. **Theoretical Advances**:\n   - The paper establishes the first regret bound of \\( \\tilde{O}(\\sqrt{T}) \\) for restless bandit problems with unobserved states, matching the theoretical lower bound.\n\n3. **Practical Impact**:\n   - TSEETC demonstrates superior performance in simulations, making it a promising solution for real-world applications in wireless communication, healthcare, and other domains.\n\n##### Open Questions:\nThe paper leaves open the question of extending TSEETC to settings where transition functions depend on actions, which could further broaden its applicability. Future work could also explore the integration of TSEETC with deep learning methods for high-dimensional state spaces.",
    "text_only_review": "#### 1. Summary\nThis paper addresses the online restless bandit problem, a complex sequential decision-making challenge where multiple arms evolve as hidden Markov chains with unknown transition probabilities and reward functions. The proposed algorithm, TSEETC (Thompson Sampling with Episodic Explore-Then-Commit), achieves a regret bound of \\( \\tilde{O}(\\sqrt{T}) \\), matching the theoretical lower bound and improving upon prior methods. The algorithm employs a novel episodic structure that alternates between exploration and exploitation phases, leveraging Bayesian posterior updates to estimate parameters and guide decision-making. Theoretical analysis and experimental results demonstrate the algorithm's effectiveness, outperforming existing methods and validating its practical utility.\n\n---\n\n#### 2. Strengths\n1. **Theoretical Contributions**:\n   - The paper establishes a regret bound of \\( \\tilde{O}(\\sqrt{T}) \\), which is a significant theoretical advancement for restless bandit problems with unobserved states. This result aligns with the theoretical lower bound, demonstrating the algorithm's optimality.\n\n2. **Algorithmic Design**:\n   - The episodic structure of TSEETC is innovative, providing a clear and principled framework for balancing exploration and exploitation. The use of Bayesian posterior sampling ensures robust parameter estimation and decision-making under uncertainty.\n\n3. **Comprehensive Theoretical Analysis**:\n   - The paper provides rigorous proofs for key theoretical results, including bounds on value function spans, belief estimation errors, and regret decomposition. These contributions enhance the paper's credibility and scientific rigor.\n\n4. **Empirical Validation**:\n   - The experimental results are compelling, demonstrating that TSEETC achieves the lowest cumulative regret compared to baseline algorithms. The log-log plot analysis further supports the theoretical findings.\n\n5. **Practical Relevance**:\n   - The algorithm's applicability to real-world domains such as wireless communication and healthcare underscores its practical significance. The paper effectively bridges theoretical advancements with practical utility.\n\n---\n\n#### 3. Weaknesses\n1. **Limited Experimental Scope**:\n   - The experiments are conducted on a relatively simple setup with two arms and two hidden states. While the results are promising, the experimental scope is limited, and it is unclear how TSEETC performs in more complex scenarios with higher-dimensional state spaces or a larger number of arms.\n\n2. **Assumptions on Transition Independence**:\n   - The algorithm assumes that the arms evolve independently as Markov chains. This assumption may not hold in many real-world applications where dependencies between arms exist. The paper does not discuss how TSEETC could be adapted to handle such dependencies.\n\n3. **Scalability Concerns**:\n   - The computational complexity of belief updates and posterior sampling is not explicitly discussed. For large-scale problems with many arms or high-dimensional state spaces, the scalability of TSEETC could be a concern.\n\n4. **Lack of Real-World Validation**:\n   - While the paper emphasizes practical applications, the experiments are limited to synthetic simulations. Real-world case studies would strengthen the paper's claims about the algorithm's practical impact.\n\n5. **Open Questions Not Fully Addressed**:\n   - The paper mentions potential extensions to action-dependent transition functions and high-dimensional state spaces but does not provide concrete insights or preliminary results in these directions.\n\n---\n\n#### 4. Clarity & Reproducibility\n1. **Clarity**:\n   - The paper is well-written and clearly structured, with a logical flow from problem formulation to theoretical analysis and experimental validation. The algorithm is described in detail, and the theoretical proofs are presented rigorously.\n\n2. **Reproducibility**:\n   - While the algorithm's design and theoretical analysis are well-documented, the paper does not provide sufficient details about the experimental setup, such as the specific parameters used for the Dirichlet distributions or the implementation of baseline algorithms. Including a code repository or pseudocode for TSEETC would enhance reproducibility.\n\n3. **Notation and Terminology**:\n   - The notation is consistent and well-defined, making the paper accessible to readers familiar with bandit problems and Bayesian methods.\n\n---\n\n#### 5. Novelty & Significance\n1. **Novelty**:\n   - The paper introduces a novel algorithmic framework (TSEETC) that combines Thompson Sampling with an episodic exploration-exploitation strategy. This approach is distinct from existing methods and represents a significant contribution to the field of online restless bandits.\n\n2. **Significance**:\n   - The theoretical regret bound of \\( \\tilde{O}(\\sqrt{T}) \\) is a major advancement, addressing a long-standing gap in the literature. The algorithm's performance in simulations further highlights its practical relevance.\n\n3. **Broader Impact**:\n   - The proposed methodology has the potential to influence a wide range of applications, including resource allocation, healthcare decision-making, and sensor network optimization. The paper's contributions could inspire future research on extending TSEETC to more complex settings.\n\n---\n\n#### Overall Recommendation\nThis paper makes a strong theoretical and algorithmic contribution to the field of online restless bandits, achieving a near-optimal regret bound and demonstrating superior empirical performance. However, the limited experimental scope and lack of real-world validation leave room for improvement. I recommend **acceptance** at a top-tier conference, provided the authors address the scalability concerns and provide additional details to enhance reproducibility.",
    "merged_image_path": "temp_cache\\6584_online_restless_bandits_with_u_merged.png",
    "image_only_review": "#### 1. Summary\nThe paper presents a study on online restless bandits with unobserved states, a problem that extends the classical multi-armed bandit framework by allowing the state of each arm to evolve according to a Markov process, even when not being played. The authors propose a novel algorithm that addresses the challenge of unobserved states and provides theoretical guarantees on the regret bounds. The paper also includes empirical evaluations to demonstrate the effectiveness of the proposed approach compared to existing methods.\n\n#### 2. Strengths\n- **Theoretical Contribution**: The paper provides a solid theoretical foundation for the proposed algorithm, including rigorous proofs of regret bounds. This adds significant value to the field by extending the understanding of online restless bandits.\n- **Novel Algorithm**: The introduction of a new algorithm tailored for unobserved states in restless bandits is a noteworthy contribution. The algorithm is well-motivated and designed to handle the complexities introduced by unobserved states.\n- **Empirical Validation**: The paper includes comprehensive experiments that validate the theoretical claims. The results show that the proposed method outperforms existing approaches, which is a strong point in demonstrating its practical applicability.\n\n#### 3. Weaknesses\n- **Complexity of Presentation**: The paper is dense with mathematical notation and proofs, which may hinder accessibility for readers not deeply familiar with the topic. A more intuitive explanation of key concepts could improve comprehension.\n- **Limited Discussion on Practical Implications**: While the theoretical and empirical results are strong, the paper could benefit from a more detailed discussion on the practical implications and potential applications of the proposed algorithm in real-world scenarios.\n- **Assumptions**: The paper relies on certain assumptions about the Markov process governing the state transitions. It would be beneficial to discuss the impact of these assumptions on the generalizability of the results.\n\n#### 4. Clarity & Reproducibility\n- **Clarity**: The paper is well-structured, with a clear delineation of sections. However, the heavy use of technical language and complex equations may obscure understanding for a broader audience. Including more intuitive explanations or examples could enhance clarity.\n- **Reproducibility**: The paper provides sufficient details on the algorithm and experimental setup, which should allow for reproducibility. However, sharing code or pseudocode would further facilitate replication efforts by other researchers.\n\n#### 5. Novelty & Significance\n- **Novelty**: The paper introduces a novel approach to tackling the problem of unobserved states in online restless bandits, which is a significant advancement over existing methods. The theoretical contributions, particularly the regret bounds, are novel and add depth to the literature.\n- **Significance**: The work addresses a challenging and relevant problem in the field of online learning and decision-making. The proposed algorithm has the potential to impact various applications where decision-making under uncertainty and partial observability is critical, such as in healthcare, finance, and robotics.\n\nIn conclusion, the paper makes a valuable contribution to the field of online learning by addressing a complex problem with a novel algorithm and providing strong theoretical and empirical support. Enhancements in clarity and practical discussion could further elevate the impact of the work.",
    "multimodal_review": "**1. Summary**  \nThe paper presents a novel algorithm, TSEETC (Thompson Sampling with Episodic Explore-Then-Commit), designed to address the online restless bandit problem with unobserved states and unknown parameters. TSEETC employs a Bayesian approach that combines episodic exploration and exploitation phases, leveraging Dirichlet distributions for posterior updates of transition probabilities and reward functions. Theoretical analysis demonstrates that TSEETC achieves a regret bound of \\( \\tilde{O}(\\sqrt{T}) \\), matching the theoretical lower bound for such problems. Empirical evaluations confirm the algorithm's superior performance over existing methods, highlighting its potential for applications in wireless communication, healthcare, and sensor maintenance.\n\n**2. Strengths**  \n- **Algorithmic Innovation**: The episodic structure of TSEETC, combined with Thompson Sampling, provides a robust framework for balancing exploration and exploitation in settings with unobserved states.\n- **Theoretical Contribution**: The paper establishes a regret bound of \\( \\tilde{O}(\\sqrt{T}) \\), which is a significant improvement over prior methods and matches the theoretical lower bound for restless bandit problems.\n- **Empirical Validation**: Comprehensive simulations demonstrate the algorithm's effectiveness, outperforming existing methods in terms of cumulative regret.\n\n**3. Weaknesses**  \n- **Clarity in Mathematical Notation**: The mathematical notation in Section 3.2 is dense and could benefit from additional explanations or examples to improve comprehension. Suggestion: Include a brief example illustrating the belief state update process.\n- **Baseline Comparisons**: While the paper compares TSEETC against several baselines, it lacks a comparison with more recent methods that might be relevant. Suggestion: Include additional baselines in Table 2, such as recent deep reinforcement learning approaches.\n- **Figure Clarity**: Figure 1 caption lacks sufficient detail to understand the experimental setup and results without referring back to the text. Suggestion: Expand the caption to include key details about the setup and interpretation of results.\n- **Assumptions and Limitations**: The paper does not clearly articulate the assumptions made regarding the Markov chain models or the limitations of the proposed approach. Suggestion: Add a section discussing assumptions and potential limitations in Section 5.\n- **Code Availability**: The paper does not mention the availability of code or data for reproducing results, which is crucial for transparency and reproducibility. Suggestion: Include a statement on code and data availability in the conclusion or appendix.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**: The paper is generally well-written, with a logical flow of ideas. However, some sections, particularly those involving complex mathematical derivations, could benefit from additional explanations or examples to aid understanding. Section titles are informative, but assumptions and limitations are not clearly articulated.\n  \n  **(b) Figure & Caption Clarity**: Figures effectively illustrate the main claims, but captions could be more detailed to ensure they are self-sufficient. Axes, labels, and legends are consistent and readable, correlating well with textual descriptions.\n  \n  **(c) Reproducibility Transparency**: The experimental setup provides adequate detail regarding datasets and baselines, but lacks information on hyperparameters, hardware, training time, and random seeds. The paper does not mention code or data availability, which is crucial for reproducibility. Ablation studies are not provided, and algorithmic steps could be more clearly delineated.\n\n**5. Novelty & Significance**  \nThe paper addresses a significant gap in the literature by proposing a novel algorithm for the online restless bandit problem with unobserved states, achieving a near-optimal regret bound. The approach is well-motivated and contextualized within the existing literature, providing substantial theoretical and empirical contributions. The work is significant in advancing the understanding of restless bandit problems and has potential applications in various domains, offering new insights and practical solutions. However, the paper could enhance its impact by addressing the noted weaknesses, particularly in terms of clarity, baseline comparisons, and reproducibility.",
    "multimodal_rag_review": "**1. Summary**  \nThe paper presents TSEETC (Thompson Sampling with Episodic Explore-Then-Commit), a novel algorithm designed to address the online restless bandit problem where states are unobserved, and transition dynamics and reward functions are unknown. The algorithm employs a structured, episodic approach to balance exploration and exploitation, utilizing belief states derived from historical actions and rewards. Theoretical analysis demonstrates that TSEETC achieves a Bayesian regret bound of \\( \\tilde{O}(\\sqrt{T}) \\), which is near-optimal. Empirical results show that TSEETC outperforms existing algorithms in terms of cumulative regret, validating its effectiveness.\n\n**2. Strengths**  \n- The paper addresses a significant and challenging problem in the field of online decision-making under uncertainty, particularly in scenarios with unobserved states.\n- TSEETC's design is innovative, leveraging a combination of belief state updates, Dirichlet-based posterior sampling, and an episodic exploration-exploitation strategy.\n- Theoretical contributions are robust, providing a near-optimal regret bound that advances the state-of-the-art for restless bandit problems.\n- Empirical validation is thorough, demonstrating superior performance of TSEETC compared to existing methods across various scenarios.\n\n**3. Weaknesses**  \n- **Clarity in Methodology**: The explanation of the belief state updates and posterior sampling in Section 3.1 is dense and could benefit from additional clarification. Suggestion: Include a step-by-step example to illustrate the belief state update process.\n- **Experimental Details**: The paper lacks detailed information about the experimental setup, such as specific hyperparameters used for TSEETC and baseline algorithms (Section 5.1). Suggestion: Provide a table summarizing the hyperparameters and their values for reproducibility.\n- **Comparison with Baselines**: While TSEETC is compared with several baselines, the paper does not discuss why certain popular algorithms, such as Thompson Sampling with known states, were not included (Section 5.2). Suggestion: Justify the choice of baselines and consider including additional comparisons.\n- **Notation Consistency**: There are inconsistencies in the notation used for belief states and posterior distributions across Sections 3 and 4. Suggestion: Ensure consistent notation throughout the paper to avoid confusion.\n- **Discussion of Limitations**: The paper does not sufficiently address the limitations of the proposed approach, particularly in scenarios with more complex state dynamics (Section 6). Suggestion: Include a discussion of potential limitations and directions for future work.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**  \n  The paper is generally well-structured, with clear section titles and logical progression of ideas. However, the dense mathematical notation in some sections could be better explained with examples or diagrams. Assumptions and limitations are not explicitly stated, which could improve the reader's understanding of the scope of the work.\n\n  **(b) Figure & Caption Clarity**  \n  Figures effectively illustrate the main claims, with clear axes and legends. However, the captions could be more descriptive to ensure they are self-sufficient. For instance, Figure 2's caption should explain the significance of the regret slope observed.\n\n  **(c) Reproducibility Transparency**  \n  The paper provides a high-level description of the experimental setup but lacks specific details on datasets, hyperparameters, and hardware used. There is no mention of code availability, which is crucial for reproducibility. Including these details and making the code publicly available would enhance the paper's transparency.\n\n**5. Novelty & Significance**  \nThe paper makes a significant contribution to the field of online decision-making by addressing the challenging problem of restless bandits with unobserved states. The introduction of TSEETC, with its novel episodic exploration-exploitation strategy, represents a meaningful advancement over existing methods. The theoretical and empirical results substantiate the claims of improved performance and near-optimal regret bounds. The work is well-motivated and contextualized within the literature, offering new insights and methodologies that are likely to influence future research in this area."
}