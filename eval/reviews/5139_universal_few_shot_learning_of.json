{
    "paper_id": "5139_universal_few_shot_learning_of",
    "title": "Universal Few-shot Learning of Dense Prediction Tasks with Visual Token Matching",
    "abstract": "Dense prediction tasks are a fundamental class of problems in computer vision. As supervised methods suffer from high pixel-wise labeling cost, a few-shot learning solution that can learn any dense task from a few labeled images is desired. Yet, current few-shot learning methods target a restricted set of tasks such as semantic segmentation, presumably due to challenges in designing a general and unified model that is able to flexibly and efficiently adapt to arbitrary tasks of unseen semantics. We propose Visual Token Matching (VTM), a universal few-shot learner for arbitrary dense prediction tasks. It employs non-parametric matching on patch-level embedded tokens of images and labels that encapsulates all tasks. Also, VTM flexibly adapts to any task with a tiny amount of task-specific parameters that modulate the matching algorithm. We implement VTM as a powerful hierarchical encoder-decoder architecture involving ViT backbones where token matching is performed at multiple feature hierarchies. We experiment VTM on a challenging variant of Taskonomy dataset and observe that it robustly few-shot learns various unseen dense prediction tasks. Surprisingly, it is competitive with fully supervised baselines using only 10 labeled examples of novel tasks (of full supervision) and sometimes outperforms using of full supervision.",
    "human_review": "Summary Of The Paper:\nThe paper tackles few-shot dense prediction from image, i.e. tasks where the input is an image and the output is a per-pixel prediction. The authors propose a Transformer-based meta-learning approach: learning some dense prediction tasks in an episodic fashion helps the model generalize to novel tasks. Most of the parameters on the model are shared, so a small-scale finetuning is needed to learn a new task.\n\nThe architecture is simple: the target image and the support images are split into patches that are featurized using a Transformer. Then the target patches attend to the support patches. Based on their similarity, a target patch inherits the weighted label encoding of each support patch. The label encodings are summed over all patches into a single vector. Another network upsamples this vector to the full image resolution. There are two important details here. First, all weights are shared across tasks, except for the bias terms in the image encoder. This forces us to finetune for a new task, but the actual number of parameters is small. Second, supposing that the output has C channels, the proposed model predicts a dense map for each channel separately. This allows generalization to an arbitrary number of channels at test time.\n\nThe authors test their approach on Taskonomy, by splitting 10 dense prediction tasks into 8 for training and 2 for testing (5-fold validation). They compare against fully-supervised and few-shot baselines and significantly outperform the few-shot ones while sometimes performing on par with fully-supervised approaches. Their ablation indicates that finetuning for new tasks is necessary and that most of the times their network benefits from context (attending to the support set) to improve performance.\n\nStrength And Weaknesses:\nStrengths:\n\n(1) The problem of few-shot multi-tasking for dense prediction is important and unexplored.\n\n(2) The proposed approach is simple and intuitive, without task-specific inductive biases or extreme computational complexity. The experiments show that it works well.\n\n(3) The writing is clear and helps the understanding of the motivation, desiderata, solution.\n\nWeaknesses:\n\nThere are some discussions/comparisons that are missing.\n\n(1) Could you please explain the technical differences between the w/o Matching baseline and DPT? Equivalently, if we add a support set on DPT, how different is this from the proposed model?\n\n(2) The proposed architecture is not specifically designed for few-shot settings. In fact, if we compare the w/o Matching variant from Table 2 to the results of the baselines in Table 1, it does pretty well. This is good but also raises the question of how much the fast generalization is a result of the powerful architecture vs the multi-tasking meta-training. These additional experiments would shed light on this: (a) What is the performance of the proposed model if trained fully-supervised? Does it scale well when a lot of data is there? How does it compare to DPT? (b) How does out-of-the-box DPT work after few-shot finetuning?\n\n(3) From Table 2 it appears that fine-tuning is very important. This is fine as long as we care about the performance on the novel tasks only. But once finetuned, the model could forget the training tasks. This makes the approach less significant from the point of continual learning. Could you evaluate the forgetting, i.e. what is the performance on the training tasks after finetuning on the novel tasks?\n\n(4) Training for more tasks jointly is a harder job for the model but makes the dependency on the support set stronger. How important is that aspect for the proposed model? If you train on 5 tasks and test on 5 vs 8-2 vs 2-8, how does the performance change? It would be nice if you keep 2 novel tasks as fixed and then train the model on 2, 5 and 8 tasks and show how the performance on the two held-out tasks changes.\n\nQuestions:\n\n(a) Is the whole support set used in a forward pass? That's fine in 10-shot, but when you have 275 support images, do you still feed all of them at the same time? Would it make sense to finetune on 275 support images but use only a fixed set of them during inference? In that case this approach could scale to large datasets as well.\n\n(b) Why do you think that you need to finetune the model? Isn't the support set enough to provide the task description? Could somehow the support set predict the task-specific parameters to use?\n\nClarity, Quality, Novelty And Reproducibility:\nThe paper is well-written and clear.\n\nThe contributions are original and few-shot adaptation to general dense prediction tasks is indeed unexplored (although task-specific few-shot methods exist, e.g. for semantic segmentation). The proposed method needs to be finetuned for a novel task. I would like to see some discussion in the related work section about methods that do not need to finetune, e.g. some prompting approaches in language (e.g. GPT-3) and vision (e.g. the recent work \"Visual Prompting via Image Inpainting\" or other similar). On a technical level, the architecture combines existing modules in an elegant way.\n\nAn experienced reader would probably be able to reproduce parts of this work, as it is based on well-known components. If the authors release their code it would be useful to the community.\n\nSummary Of The Review:\nIn general I like the motivation and technical aspects of this paper. I feel the problem is important and the approach seems to make a step towards the right directions. However, I feel that some comparisons that could make the arguments stronger are missing. In addition, I am concerned about depending on finetuning, although I recognize that continual learning is not the focus of this work. I'm looking forward to the discussion with the authors and will happily vote for accepting the paper if they address my questions.\n\nAfter the rebuttal, all my concerns were addressed and I strongly recommend this paper for acceptance.\n\nCorrectness: 4: All of the claims and statements are well-supported and correct.\nTechnical Novelty And Significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\nEmpirical Novelty And Significance: 4: The contributions are significant, and do not exist in prior works.\nFlag For Ethics Review: NO.\nRecommendation: 10: strong accept, should be highlighted at the conference\nConfidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
    "text_summary": "### Motivation\nDense prediction tasks, such as semantic segmentation, depth estimation, and edge detection, require pixel-wise annotations, which are expensive and time-consuming to obtain. Few-shot learning offers a potential solution by enabling models to generalize from a small number of labeled examples. However, existing few-shot methods are task-specific and fail to generalize across diverse dense prediction tasks with varying output structures (e.g., discrete or continuous labels). A universal few-shot learner must address three key challenges: (1) designing a task-agnostic architecture capable of handling unseen tasks with arbitrary label spaces, (2) enabling efficient task-specific adaptation to avoid overfitting with limited data, and (3) ensuring parameter-sharing across tasks to enhance generalization. This paper introduces **Visual Token Matching (VTM)**, a universal few-shot learning framework that addresses these challenges by leveraging a unified architecture and a flexible adaptation mechanism.\n\n---\n\n### Method\nVTM is inspired by human analogy-making, where similarities between inputs and outputs are identified and adapted to new contexts. The model employs a **non-parametric, patch-based matching approach** to predict query labels by comparing query and support patches in a shared embedding space. The key components of VTM include:\n\n1. **Unified Architecture**:\n   - A hierarchical encoder-decoder structure is used, with Vision Transformers (ViTs) as backbones for both image and label encoding. The image encoder extracts patch-level embeddings for query and support images, while the label encoder processes support labels.\n   - A convolutional decoder generates the final output by combining multi-level features from the matching module.\n\n2. **Task-Specific Adaptation**:\n   - Task-specific parameters (bias terms) are introduced in the image encoder to modulate the matching algorithm, enabling flexible adaptation to diverse task semantics while avoiding overfitting. These parameters are fine-tuned during meta-testing.\n\n3. **Multi-Level Matching**:\n   - A matching module computes patch-wise predictions by aligning query and support tokens across multiple feature hierarchies using multi-head attention. This mechanism dynamically adjusts attention to task-relevant features, ensuring robust token matching.\n\n4. **Training and Inference**:\n   - The model is trained using an episodic meta-learning protocol, where each episode involves predicting query labels using a small support set. During meta-testing, only the task-specific parameters are fine-tuned, allowing the model to generalize to unseen tasks with minimal labeled data.\n\n---\n\n### Experiments\nVTM was evaluated on a modified version of the **Taskonomy dataset**, which includes ten diverse dense prediction tasks such as semantic segmentation, surface normal estimation, and edge detection. The dataset was split into training and testing tasks to ensure semantic diversity, and labels were standardized to [0,1]. The experimental setup included comparisons with fully supervised baselines (DPT, InvPT) and few-shot baselines (HSNet, VAT, DGPNet). Key findings include:\n\n1. **Few-Shot Performance**:\n   - VTM significantly outperformed all few-shot baselines across tasks. For example, in 10-shot evaluations, VTM achieved a mean Intersection over Union (mIoU) of 0.4097 for semantic segmentation, compared to 0.1069 (HSNet), 0.0353 (VAT), and 0.0261 (DGPNet).\n   - With only 10 labeled examples (<0.004% of full supervision), VTM sometimes outperformed fully supervised baselines. With 0.1% of full supervision, VTM consistently matched or exceeded the performance of supervised models.\n\n2. **Robustness and Efficiency**:\n   - VTM demonstrated robustness to support set selection, with minimal performance variance across different support sets.\n   - The model is highly parameter-efficient, adapting to new tasks using only 0.28% of its parameters. This makes it suitable for resource-constrained settings, such as medical imaging.\n\n3. **Ablation Studies**:\n   - Removing the task-specific adaptation module or the non-parametric matching mechanism significantly degraded performance, highlighting the importance of these components.\n   - Increasing the number of meta-training tasks improved generalization, with diminishing variance as more tasks were included.\n\n4. **Qualitative Results**:\n   - Visualizations showed that VTM effectively attends to task-relevant features, such as focusing on chairs for semantic segmentation or planes orthogonal to the camera for surface normal estimation. Few-shot baselines failed to generalize to novel tasks, while VTM successfully adapted to all tasks.\n\n---\n\n### Results\nVTM achieved state-of-the-art few-shot performance on dense prediction tasks, demonstrating competitive results with fully supervised baselines while requiring minimal labeled data. Key results include:\n- **10-shot evaluations**: VTM outperformed few-shot baselines and matched or exceeded supervised baselines on many tasks.\n- **Impact of support size**: Increasing the support set size improved performance, allowing VTM to surpass supervised baselines on several tasks.\n- **Robustness to incomplete datasets**: VTM maintained strong performance even when trained on incomplete datasets, demonstrating its adaptability to realistic scenarios with heterogeneous data.\n\n---\n\n### Conclusion\nThis paper introduces **Visual Token Matching (VTM)**, the first universal few-shot learning framework for arbitrary dense prediction tasks. By combining a unified architecture with a flexible adaptation mechanism, VTM generalizes across tasks with minimal labeled data. The model's performance on the Taskonomy dataset demonstrates its robustness, efficiency, and potential for applications in resource-constrained domains. Future work includes exploring more diverse training datasets and applying VTM to continual learning settings. The code and dataset are publicly available, ensuring reproducibility and facilitating further research.",
    "text_only_review": "#### 1. Summary\nThe paper introduces **Visual Token Matching (VTM)**, a universal few-shot learning framework designed to address diverse dense prediction tasks such as semantic segmentation, depth estimation, and edge detection. VTM tackles the challenges of task-agnostic architecture design, efficient task-specific adaptation, and parameter-sharing across tasks. The model employs a hierarchical encoder-decoder structure with Vision Transformers (ViTs) as backbones and a non-parametric, patch-based matching mechanism for query-support alignment. Task-specific adaptation is achieved through fine-tuning bias terms, enabling the model to generalize effectively with minimal labeled data. VTM is evaluated on the Taskonomy dataset, demonstrating state-of-the-art few-shot performance, robustness, and parameter efficiency. The paper also provides extensive qualitative and quantitative analyses, ablation studies, and comparisons with existing baselines.\n\n---\n\n#### 2. Strengths\n1. **Innovative Framework**:\n   - VTM introduces a universal few-shot learning framework that generalizes across diverse dense prediction tasks, addressing a significant gap in the field.\n   - The use of a non-parametric, patch-based matching mechanism is novel and aligns well with the goal of task-agnostic generalization.\n\n2. **Strong Empirical Performance**:\n   - VTM achieves state-of-the-art few-shot performance across all evaluated tasks, significantly outperforming existing baselines.\n   - The model demonstrates competitive or superior performance compared to fully supervised baselines, even with minimal labeled data.\n\n3. **Robustness and Efficiency**:\n   - The framework is robust to support set selection and performs well even with incomplete datasets, making it suitable for real-world applications.\n   - The parameter efficiency of the model (adapting with only 0.28% of its parameters) is particularly noteworthy for resource-constrained settings.\n\n4. **Comprehensive Evaluation**:\n   - The paper provides thorough experimental results, including quantitative metrics, qualitative visualizations, and ablation studies, which highlight the importance of each component in the framework.\n   - The episodic meta-learning protocol used for training and evaluation is well-designed and ensures fair comparisons.\n\n5. **Reproducibility**:\n   - The authors provide access to the code and dataset, ensuring that the research can be reproduced and extended by the community.\n\n---\n\n#### 3. Weaknesses\n1. **Limited Dataset Diversity**:\n   - The evaluation is conducted on the Taskonomy dataset, which, while diverse, may not fully represent the range of dense prediction tasks encountered in real-world scenarios. Testing on additional datasets (e.g., Cityscapes, NYU Depth, or medical imaging datasets) would strengthen the claims of generalizability.\n\n2. **Scalability to Larger Tasks**:\n   - While the model demonstrates strong performance on the tasks evaluated, it is unclear how well it scales to tasks with significantly larger label spaces or higher resolution images. This limitation is not explicitly addressed in the paper.\n\n3. **Task-Specific Adaptation Complexity**:\n   - The introduction of task-specific bias terms, while effective, adds a layer of complexity to the adaptation process. A more detailed analysis of the computational overhead during meta-testing would be beneficial.\n\n4. **Lack of Comparison with Broader Baselines**:\n   - The paper compares VTM with a limited set of few-shot baselines. Including comparisons with broader meta-learning approaches (e.g., MAML, ProtoNet) or task-agnostic frameworks would provide a more comprehensive evaluation.\n\n5. **Limited Discussion on Failure Cases**:\n   - The paper does not provide sufficient analysis of failure cases or scenarios where VTM underperforms. Understanding these limitations would be valuable for future research and practical deployment.\n\n---\n\n#### 4. Clarity & Reproducibility\n- **Clarity**:\n  - The paper is well-written, with clear explanations of the methodology, experimental setup, and results. The figures and visualizations effectively support the narrative.\n  - The modular breakdown of the VTM framework (unified architecture, task-specific adaptation, multi-level matching) is logical and easy to follow.\n\n- **Reproducibility**:\n  - The availability of code and dataset ensures reproducibility. The episodic meta-learning protocol and training details are described in sufficient depth.\n  - However, some hyperparameters and implementation details (e.g., specific configurations of the Vision Transformers and decoder) could be elaborated further to facilitate exact replication.\n\n---\n\n#### 5. Novelty & Significance\n- **Novelty**:\n  - The paper introduces a novel approach to universal few-shot learning for dense prediction tasks, combining a unified architecture with a flexible, non-parametric matching mechanism. The task-agnostic design and patch-based matching are significant contributions to the field.\n  - The use of task-specific bias terms for efficient adaptation is an innovative solution to the challenge of overfitting with limited data.\n\n- **Significance**:\n  - The ability to generalize across diverse dense prediction tasks with minimal labeled data has broad implications for fields such as medical imaging, autonomous driving, and robotics, where labeled data is scarce and expensive.\n  - The framework's robustness and parameter efficiency make it a practical choice for deployment in resource-constrained environments.\n\n---\n\n#### Overall Recommendation\nThis paper presents a significant contribution to the field of few-shot learning and dense prediction tasks. The proposed VTM framework is innovative, well-motivated, and demonstrates strong empirical performance. While there are some limitations, such as the reliance on a single dataset and the lack of broader baseline comparisons, these do not detract significantly from the overall quality of the work. I recommend **acceptance**, with minor revisions to address the identified weaknesses.",
    "merged_image_path": "temp_cache\\5139_universal_few_shot_learning_of_merged.png",
    "image_only_review": "### 1. Summary\n\nThe paper presents a novel approach for universal few-shot learning of dense prediction tasks using vision transformers. The authors propose a method that leverages the flexibility and scalability of vision transformers to address the challenges associated with few-shot learning in dense prediction tasks, such as semantic segmentation and object detection. The proposed method is evaluated on several benchmark datasets, demonstrating competitive performance compared to state-of-the-art methods.\n\n### 2. Strengths\n\n- **Innovative Approach**: The paper introduces a novel method that effectively combines the strengths of vision transformers with few-shot learning techniques, addressing a significant gap in the literature for dense prediction tasks.\n- **Comprehensive Evaluation**: The authors provide extensive empirical evaluations on multiple benchmark datasets, showcasing the effectiveness of their approach across different tasks and scenarios.\n- **Detailed Methodology**: The paper includes a thorough explanation of the proposed method, including architectural details, training procedures, and hyperparameter settings, which aids in understanding and potential replication of the work.\n- **Strong Results**: The proposed method achieves competitive results, often outperforming existing state-of-the-art methods in few-shot dense prediction tasks, indicating its potential impact and applicability.\n\n### 3. Weaknesses\n\n- **Limited Theoretical Analysis**: While the empirical results are strong, the paper could benefit from a more in-depth theoretical analysis of why the proposed method works well, particularly in comparison to other approaches.\n- **Complexity and Scalability**: The approach relies on vision transformers, which can be computationally intensive. The paper does not sufficiently address the potential limitations regarding computational resources and scalability to very large datasets or real-time applications.\n- **Generalization to Other Tasks**: The paper focuses primarily on dense prediction tasks. It would be beneficial to discuss the potential for generalizing the approach to other types of tasks or domains.\n\n### 4. Clarity & Reproducibility\n\n- **Clarity**: The paper is generally well-written and structured, with clear explanations of the methodology and results. The use of figures and tables aids in the presentation of the results.\n- **Reproducibility**: The authors provide detailed descriptions of the experimental setup, including datasets, training procedures, and hyperparameters. However, the paper would benefit from the inclusion of code or pseudo-code to further facilitate reproducibility.\n\n### 5. Novelty & Significance\n\n- **Novelty**: The integration of vision transformers with few-shot learning for dense prediction tasks is a novel contribution, addressing a previously underexplored area in the field.\n- **Significance**: Given the increasing importance of few-shot learning and the widespread application of dense prediction tasks in computer vision, the proposed method has significant potential to impact both academic research and practical applications.\n- **Potential Impact**: The method's ability to perform well across multiple datasets and tasks suggests that it could be a valuable tool for researchers and practitioners working on few-shot learning and dense prediction problems.\n\nIn conclusion, the paper presents a promising approach with strong empirical results, though it could be improved with additional theoretical insights and considerations of computational efficiency.",
    "multimodal_review": "**1. Summary**  \nThe paper introduces Visual Token Matching (VTM), a universal few-shot learning framework designed for dense prediction tasks such as semantic segmentation and depth estimation. VTM employs a unified architecture with a hierarchical encoder-decoder structure using Vision Transformers (ViTs) for both image and label encoding, and a non-parametric, patch-based matching approach for task-specific adaptation. The framework is evaluated on a modified Taskonomy dataset, demonstrating superior performance over existing few-shot and fully supervised baselines, especially in scenarios with minimal labeled data. The results highlight VTM's robustness, efficiency, and adaptability across diverse tasks.\n\n**2. Strengths**  \n- The paper addresses a significant gap in few-shot learning by proposing a universal framework applicable to various dense prediction tasks, which is a notable advancement over task-specific methods.\n- VTM's architecture effectively leverages Vision Transformers and a novel patch-based matching mechanism, allowing for efficient task adaptation and parameter sharing.\n- The experimental results are comprehensive, showing VTM's superior performance in few-shot scenarios and its ability to match or exceed fully supervised baselines with minimal labeled data.\n- The inclusion of ablation studies and qualitative results strengthens the paper's claims by demonstrating the importance of VTM's components and its adaptability to novel tasks.\n\n**3. Weaknesses**  \n- **Clarity in Methodology**: The description of the task-specific adaptation mechanism in Section 3.2 could be more detailed. It would be beneficial to include a more explicit explanation of how the bias terms are integrated and fine-tuned. Suggestion: Expand the explanation of the adaptation mechanism with illustrative examples or pseudocode.\n- **Baseline Comparisons**: In Table 2, the choice of few-shot baselines (HSNet, VAT, DGPNet) could be expanded to include more recent methods for a more robust comparison. Suggestion: Consider including additional recent few-shot learning baselines to provide a more comprehensive evaluation.\n- **Figure Clarity**: Figure 5's caption is somewhat ambiguous regarding the specific tasks being visualized. Suggestion: Clarify the caption to explicitly state which tasks are being depicted and how they relate to the qualitative results discussed in the text.\n- **Reproducibility Details**: Section 4.3 lacks specific details on the hyperparameters used during training and meta-testing. Suggestion: Include a detailed table or appendix section listing all hyperparameters and their values to enhance reproducibility.\n- **Generalization to Other Datasets**: While the Taskonomy dataset is diverse, the paper could benefit from evaluating VTM on additional datasets to further validate its generalization capabilities. Suggestion: Consider including results from other datasets or discuss potential limitations in generalizing to different domains.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**  \n  The paper is generally well-written, with a logical flow of ideas and clear explanations of the methodology and experiments. However, some sections, such as the task-specific adaptation mechanism, could benefit from more detailed explanations. The assumptions and limitations are briefly mentioned but could be more explicitly stated.\n\n  **(b) Figure & Caption Clarity**  \n  Figures are generally well-designed and support the paper's claims. However, some captions, such as that of Figure 5, could be more descriptive to ensure they are self-sufficient. Axes and labels are consistent and readable, and the figures correlate well with the textual descriptions.\n\n  **(c) Reproducibility Transparency**  \n  The paper provides a good overview of the experimental setup, but specific details on hyperparameters, hardware, and training times are sparse. The mention of code and dataset availability is a positive step towards reproducibility, but more detailed information on the experimental configurations would be beneficial. Ablation studies are well-conducted, demonstrating the importance of various components.\n\n**5. Novelty & Significance**  \nThe paper presents a novel approach to few-shot learning for dense prediction tasks by introducing a universal framework that can generalize across diverse tasks. This is a significant contribution, as it addresses the limitations of task-specific methods and provides a scalable solution for scenarios with limited labeled data. The use of Vision Transformers and a non-parametric matching mechanism is innovative and well-motivated. The work is well-substantiated with empirical results, demonstrating its potential impact on the field. The significance lies in its applicability to various domains, particularly those with resource constraints, making it a valuable contribution to the few-shot learning community.",
    "multimodal_rag_review": "**1. Summary**  \nThe paper introduces Visual Token Matching (VTM), a universal few-shot learning framework designed for dense prediction tasks in computer vision, such as semantic segmentation and depth estimation. VTM employs a non-parametric matching approach using patch-level embeddings and a hierarchical encoder-decoder architecture with Vision Transformer (ViT) backbones. The framework is trained using episodic meta-learning on a variant of the Taskonomy dataset, demonstrating strong performance with minimal labeled data. VTM outperforms fully supervised baselines in some cases, highlighting its robustness, parameter efficiency, and adaptability to diverse tasks.\n\n**2. Strengths**  \n- **Innovative Framework**: VTM presents a novel approach to few-shot learning for dense prediction tasks, offering a unified architecture that is task-agnostic and adaptable.\n- **Strong Performance**: The framework demonstrates competitive or superior performance compared to fully supervised baselines, even with extremely limited labeled data.\n- **Robustness and Efficiency**: VTM is robust to the choice of support set and requires fewer task-specific parameters, making it suitable for resource-constrained scenarios.\n- **Comprehensive Evaluation**: The paper provides extensive quantitative and qualitative evaluations, including ablation studies that underscore the importance of its components.\n\n**3. Weaknesses**  \n- **Clarity in Methodology**: The description of the hierarchical encoder-decoder design in Section 3.2 could benefit from clearer explanations of how the multi-scale features are integrated. Suggestion: Provide a more detailed explanation or a diagram illustrating the integration process.\n- **Baseline Comparisons**: In Table 2, there is no comparison with certain state-of-the-art methods like DPT or InvPT, which could provide a more comprehensive evaluation. Suggestion: Include comparisons with these methods to strengthen the empirical validation.\n- **Reproducibility Details**: Section 4.1 lacks specific details on hyperparameters and hardware used during training, which are crucial for reproducibility. Suggestion: Include a subsection detailing these aspects to enhance transparency.\n- **Figure Captions**: Some figure captions, such as those in Figure 5, are not sufficiently descriptive, which may hinder understanding. Suggestion: Expand captions to include more context and explanation of the visualized results.\n- **Theoretical Justification**: The paper lacks a theoretical analysis of why the non-parametric matching approach is effective across diverse tasks. Suggestion: Include a theoretical discussion or references to related work that could provide insights into the approach's effectiveness.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**: The paper is generally well-written, with logical explanations of the ideas and methods. However, some sections, such as the hierarchical encoder-decoder design, could benefit from additional clarity. Mathematical notations are well-defined, but the assumptions and limitations of the approach are not explicitly stated, which could be improved.\n  \n  **(b) Figure & Caption Clarity**: Figures are generally clear and support the paper's claims, but some captions lack detail. For example, Figure 5 could include more context about the results being presented. Axes and labels are consistent and readable, and diagrams correlate well with the textual descriptions.\n  \n  **(c) Reproducibility Transparency**: The paper provides a good overview of the experimental setup, but lacks specific details on hyperparameters, hardware, and training time. While the authors mention plans to release source code and datasets, these are not currently available, which limits reproducibility. Ablation studies are provided, but more detailed algorithmic steps could enhance transparency.\n\n**5. Novelty & Significance**  \nThe paper addresses the significant problem of few-shot learning for dense prediction tasks, offering a novel, task-agnostic framework that generalizes well across diverse tasks. The approach is well-motivated and contextualized within the existing literature, filling a gap where previous methods were limited to specific tasks. The claims are substantiated with rigorous empirical evaluations, demonstrating both the correctness and scientific rigor of the results. The work contributes new knowledge to the field, particularly in terms of parameter efficiency and adaptability, making it a valuable addition to the community. The visual elements are generally clear and effectively support the paper's claims, though some improvements in caption detail could enhance understanding."
}