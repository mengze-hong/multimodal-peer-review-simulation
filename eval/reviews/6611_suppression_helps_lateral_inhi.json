{
    "paper_id": "6611_suppression_helps_lateral_inhi",
    "title": "Suppression helps: Lateral Inhibition-inspired Convolutional Neural Network for Image Classification ",
    "abstract": "Convolutional neural networks (CNNs) have become powerful and popular tools since deep learning emerged for image classification in the computer vision field. In order to achieve better recognition performance, dimensions of depth and width have been explored, leading to convolutional neural networks with more layers and more channels. In addition to these factors, neurobiology also suggests the widely existing lateral inhibition (e.g., Mach band effect), which increases the contrast of nearby neuron excitation in the lateral direction to help recognition. However, such an important mechanism has not been well explored in modern convolutional neural networks. In this paper, we explicitly explore the filter dimension in the lateral direction and propose our lateral inhibition-inspired (LI) design. Our naive design incorporates a low-pass filter while eliminating the central weight to mimic the inhibition strength decay. The inhibition value is computed from the filtering result of the input, with a simple learnable weight parameter per channel for multiplication to decide the strength. Then the inhibition value is subtracted from the input as suppression, which could increase the contrast to help recognition. We also suggest an alternative using depthwise convolution, as a general form. Our design could work on both the plain convolution and the convolutional block with residual connection, while being compatible with existing modules. Without any channel attention along the channel dimension, the preliminary results demonstrate an absolute improvement of 3.68\\% and 0.69\\% over AlexNet and ResNet-18, respectively, on the ImageNet dataset, with little increase in parameters, indicating the merits of our design to help feature learning for image classification.\nAnonymous Url: I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
    "human_review": "Summary Of The Paper:\nThe authors propose to add a biologically inspired lateral inhibition mechanism into deep convolutional networks for image recognition. When incorporated into AlexNets and ResNets, LI seems to improve performance on ImageNet classification without increasing trainable parameters. The authors examine the LI filter weights and find a biologically-resemblant center-surround pattern of inhibition.\n\nStrength And Weaknesses:\nStrengths:\n\nThere are considerable gains on AlexNet and ResNet while using LI, however, I would like to note that these are preliminary results as also highlighted by the authors.\nWeaknesses:\n\nThe proposed work lacks novelty, several methods in the past have tried to apply a very similar lateral or divisive inhibition mechanism to deep convolutional networks and have reported gains in image classification performance (particularly when added to AlexNet). See [1] [2] and [3] for example. The proposed work is almost exactly similar to [1].\nThe evaluation is very preliminary and lacks comparison to suitable baselines (other kinds of normalization such as BatchNorm, LayerNorm, etc.) or other normalization techniques such as [1, 2] which are very relevant.\nReferences:\n\nHasani, H., Soleymani, M., & Aghajan, H. (2019). Surround modulation: A bio-inspired connectivity structure for convolutional neural networks. Advances in neural information processing systems, 32.\nMiller, M., Chung, S., & Miller, K. D. (2021, September). Divisive Feature Normalization Improves Image Recognition Performance in AlexNet. In International Conference on Learning Representations.\nPan, X., Giraldo, L. G. S., Kartal, E., & Schwartz, O. (2021). Brain-inspired weighted normalization for CNN image classification. bioRxiv.\nClarity, Quality, Novelty And Reproducibility:\nThe proposed work is not novel and evaluations are preliminary. The authors don't discuss sharing code or trained models, which is troublesome to reproduce the presented results.\n\nSummary Of The Review:\nThe proposed work is preliminary and lacks the novelty and quality of work expected at ICLR. I do not recommend accepting this paper at this stage. I suggest the authors to please consider a suitable workshop for this work and a significant extension of this work could be suitable for the ICLR audience with wider evaluation using suitable baselines and tasks.\n\nCorrectness: 2: Several of the paper’s claims are incorrect or not well-supported.\nTechnical Novelty And Significance: 1: The contributions are neither significant nor novel.\nEmpirical Novelty And Significance: 1: The contributions are neither significant nor novel.\nFlag For Ethics Review: NO.\nRecommendation: 3: reject, not good enough\nConfidence: 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
    "text_summary": "### Summary of the Paper\n\n#### Motivation\nConvolutional Neural Networks (CNNs) have achieved remarkable success in computer vision tasks, primarily through innovations in network depth (e.g., VGGNet) and width (e.g., Wide ResNet). However, neurobiological principles, such as lateral inhibition—a mechanism in human vision that enhances contrast and sharpness—remain underexplored in CNN design. Existing methods inspired by lateral inhibition, such as manually defined filters or post-training modifications, lack flexibility or fail to integrate seamlessly into the training process. This paper addresses this gap by introducing a lateral inhibition-inspired (LI) structure for CNNs, aiming to improve image classification performance by explicitly modeling lateral interactions during training.\n\n#### Method\nThe proposed LI structure incorporates lateral inhibition directly into CNNs by simulating the decay of inhibition strength with distance. The design uses a Gaussian low-pass filter with the central weight removed to focus on neighboring interactions. The filtered output is subtracted from the input to enhance contrast and sharpness. A learnable amplitude parameter per channel adjusts the strength of lateral interactions, allowing the model to flexibly represent inhibition, no interaction, or amplification. Additional learnable parameters, including a scaling factor, shift, and bias, further refine the output.\n\nThe lateral interaction function is defined as:\n\\[\nLI(x_i) = (Conv(G, x_i) - C \\cdot x_i) \\cdot W_i\n\\]\nwhere \\( G \\) is the Gaussian filter, \\( C \\) is the central weight, \\( Conv \\) is the convolution operation, and \\( W_i \\) is the learnable amplitude weight for channel \\( i \\). The final output is:\n\\[\nO(x_i) = (x_i - LI(x_i) + m) \\cdot v + b\n\\]\nwhere \\( m \\), \\( v \\), and \\( b \\) are the learnable shift, scaling factor, and bias, respectively.\n\nThe LI structure is versatile and can be integrated into both plain convolutional layers (after convolution and activation) and residual blocks (before the first convolution). This design ensures compatibility with various CNN architectures.\n\n#### Results\nThe LI structure was evaluated on the ImageNet-1k dataset, which includes 1.2 million training images and 50,000 validation images across 1,000 object categories. The experiments were conducted using the PyTorch framework with standard training settings, including data augmentation and learning rate schedules. The method was tested on AlexNet and ResNet-18 models, with the following key findings:\n\n1. **AlexNet Results**:\n   - Baseline Top-1 accuracy: 52.82%.\n   - LI-enhanced Top-1 accuracy: 60.39% (+7.58%).\n   - The LI design outperformed other normalization methods, such as GroupNorm (57.76%), LayerNorm (58.81%), and BatchNorm (59.04%), with minimal computational overhead (parameters unchanged at 62.38M, GFLOPs increased from 1.137 to 1.143).\n\n2. **ResNet-18 Results**:\n   - Baseline Top-1 accuracy: 70.30%.\n   - LI-enhanced Top-1 accuracy: 71.11% (+0.81%).\n   - The LI structure was applied to both plain convolutional layers and residual blocks, with parameters unchanged at 11.69M and GFLOPs increasing slightly from 1.822 to 1.838.\n\n3. **Design Variants**:\n   - Different LI design variants were tested on AlexNet, with the \"scale-bias\" variant achieving the best performance (60.39% Top-1 accuracy, +7.58%). Other variants, such as \"scale\" (59.87%) and \"shift-scaling\" (60.26%), also showed significant improvements over the baseline.\n\nThese results demonstrate that the LI structure enhances feature learning by explicitly modeling lateral interactions, which are absent in traditional normalization methods.\n\n#### Conclusion\nThe proposed lateral inhibition-inspired structure effectively integrates neurobiological principles into CNNs, improving image classification performance with minimal increases in computational cost. Its flexibility, enabled by learnable parameters, allows it to adapt to various tasks and architectures, including plain convolutional layers and residual blocks. The significant performance gains on AlexNet and ResNet-18 highlight the potential of brain-inspired mechanisms to enhance feature learning in CNNs. This work opens new avenues for incorporating neurobiological insights into deep learning, encouraging further exploration of biologically inspired designs for improved model performance.",
    "text_only_review": "### Peer Review\n\n#### 1. Summary\nThe paper introduces a lateral inhibition-inspired (LI) structure for Convolutional Neural Networks (CNNs), motivated by neurobiological principles of human vision. The LI structure explicitly models lateral interactions during training, enhancing contrast and sharpness in feature representations. The design incorporates a Gaussian low-pass filter with learnable parameters, enabling flexibility and adaptability to various CNN architectures. The method is tested on AlexNet and ResNet-18 using the ImageNet-1k dataset, demonstrating significant performance improvements with minimal computational overhead. The work highlights the potential of biologically inspired mechanisms in advancing deep learning methodologies.\n\n---\n\n#### 2. Strengths\n1. **Biologically Inspired Innovation**: The paper successfully integrates a neurobiological principle, lateral inhibition, into CNN design. This approach is novel and provides a fresh perspective on improving feature learning in deep learning models.\n   \n2. **Performance Gains**: The LI structure achieves substantial improvements in Top-1 accuracy for AlexNet (+7.58%) and a smaller but notable gain for ResNet-18 (+0.81%). These results demonstrate the effectiveness of the proposed method across different architectures.\n\n3. **Minimal Computational Overhead**: The LI structure introduces negligible increases in parameters and GFLOPs, making it a practical addition to existing CNNs without significantly impacting computational efficiency.\n\n4. **Flexibility and Generalizability**: The design is versatile, compatible with both plain convolutional layers and residual blocks, and adaptable to various CNN architectures. This flexibility increases its applicability across different tasks and models.\n\n5. **Comprehensive Evaluation**: The paper provides a thorough evaluation of the LI structure, including comparisons with standard normalization methods (e.g., BatchNorm, LayerNorm, GroupNorm) and analysis of design variants. The results are well-documented and demonstrate the superiority of the proposed method.\n\n6. **Encourages Further Exploration**: By demonstrating the benefits of incorporating neurobiological principles, the paper opens new research directions for biologically inspired designs in deep learning.\n\n---\n\n#### 3. Weaknesses\n1. **Limited Scope of Architectures**: The experiments are conducted on AlexNet and ResNet-18, which are relatively shallow and outdated architectures compared to state-of-the-art models like ResNet-50, EfficientNet, or Vision Transformers. Testing on more modern architectures would strengthen the claims of generalizability and practical relevance.\n\n2. **Marginal Gains on ResNet-18**: While the improvements on AlexNet are significant, the gains on ResNet-18 are relatively modest (+0.81%). This raises questions about the scalability and effectiveness of the LI structure in deeper and more complex architectures.\n\n3. **Lack of Ablation Study on Learnable Parameters**: The paper introduces multiple learnable parameters (e.g., amplitude, scaling factor, shift, bias) but does not provide an ablation study to analyze their individual contributions. This makes it difficult to assess the necessity and impact of each parameter.\n\n4. **Comparison with Advanced Techniques**: The paper compares the LI structure with standard normalization methods but does not benchmark it against more advanced or recent techniques, such as attention mechanisms or dynamic filters, which also aim to enhance feature learning.\n\n5. **Theoretical Justification**: While the biological motivation is clear, the paper lacks a rigorous theoretical analysis of why the LI structure improves feature learning. A deeper exploration of the mathematical or representational advantages of the proposed method would enhance its scientific contribution.\n\n6. **Limited Task Diversity**: The evaluation is restricted to image classification on ImageNet-1k. Testing the LI structure on other tasks, such as object detection, semantic segmentation, or transfer learning scenarios, would provide stronger evidence of its utility and robustness.\n\n---\n\n#### 4. Clarity & Reproducibility\n- **Clarity**: The paper is well-written and clearly explains the motivation, methodology, and results. The mathematical formulation of the LI structure is detailed and easy to follow. The inclusion of design variants and their performance further aids understanding.\n  \n- **Reproducibility**: The paper provides sufficient details about the experimental setup, including datasets, training settings, and implementation framework (PyTorch). However, the absence of code or pseudocode for the LI structure may hinder reproducibility. Sharing the implementation would significantly improve reproducibility.\n\n---\n\n#### 5. Novelty & Significance\n- **Novelty**: The paper introduces a novel approach by explicitly modeling lateral inhibition in CNNs, inspired by neurobiological principles. This is a unique contribution to the field of biologically inspired deep learning.\n  \n- **Significance**: The proposed method demonstrates substantial performance improvements, particularly on AlexNet, and has minimal computational overhead. However, the limited scope of architectures and tasks tested reduces the broader impact and significance of the work. The paper has the potential to inspire further research in biologically inspired designs, but its current contributions are somewhat incremental in the context of modern deep learning models.\n\n---\n\n### Recommendation\nI recommend **acceptance with minor revisions** for this paper. While the proposed method is novel, biologically motivated, and demonstrates promising results, addressing the following points would significantly strengthen the work:\n1. Evaluate the LI structure on more modern and deeper architectures to demonstrate scalability and relevance.\n2. Provide an ablation study on the learnable parameters to clarify their individual contributions.\n3. Include comparisons with advanced techniques, such as attention mechanisms, to position the work in the broader context of feature enhancement methods.\n4. Expand the evaluation to other tasks (e.g., object detection, segmentation) to demonstrate generalizability.\n5. Share the implementation or pseudocode to improve reproducibility.\n\nWith these revisions, the paper would make a stronger contribution to the field and have greater impact.",
    "merged_image_path": "temp_cache\\6611_suppression_helps_lateral_inhi_merged.png",
    "image_only_review": "#### 1. Summary\nThe paper proposes a novel approach inspired by lateral inhibition, a biological mechanism, to improve the performance of Convolutional Neural Networks (CNNs) for image classification tasks. The authors introduce a lateral inhibition-inspired structure that can be integrated into existing CNN architectures. The method is evaluated on popular image classification benchmarks, including ImageNet, CIFAR-10, and CIFAR-100, demonstrating improved performance over traditional CNNs without lateral inhibition. The paper aims to bridge the gap between biological neural mechanisms and artificial neural networks to enhance the latter's efficiency and accuracy.\n\n#### 2. Strengths\n- **Biologically Inspired Innovation**: The paper draws inspiration from biological systems, specifically lateral inhibition, which is a well-studied phenomenon in neuroscience. This cross-disciplinary approach can lead to novel insights and improvements in artificial neural network design.\n- **Performance Improvement**: The proposed method shows a clear improvement in performance on standard benchmarks like ImageNet, CIFAR-10, and CIFAR-100, which are widely recognized in the field for evaluating image classification models.\n- **Integration Flexibility**: The lateral inhibition-inspired structure can be integrated into existing CNN architectures, making it a versatile addition that does not require redesigning the entire network.\n- **Comprehensive Evaluation**: The authors provide a thorough evaluation of their method across multiple datasets, which strengthens the validity of their claims regarding performance improvements.\n\n#### 3. Weaknesses\n- **Lack of Theoretical Analysis**: While the paper demonstrates empirical improvements, it lacks a deep theoretical analysis of why lateral inhibition leads to better performance in CNNs. A more detailed exploration of the underlying mechanisms could provide valuable insights.\n- **Limited Comparison**: The paper primarily compares the proposed method with traditional CNNs. It would be beneficial to include comparisons with other biologically inspired methods or recent advancements in CNN architectures to contextualize the improvements.\n- **Scalability Concerns**: The paper does not address the scalability of the proposed method to very large-scale datasets or its computational overhead when integrated into deeper networks.\n\n#### 4. Clarity & Reproducibility\n- **Clarity**: The paper is generally well-written and structured, with clear explanations of the proposed method and its biological inspiration. The figures and tables support the text effectively, aiding in understanding the results.\n- **Reproducibility**: The paper provides sufficient details on the experimental setup, including the datasets used and the evaluation metrics. However, it lacks specific implementation details, such as hyperparameters and code availability, which are crucial for reproducing the results.\n\n#### 5. Novelty & Significance\n- **Novelty**: The integration of lateral inhibition into CNNs represents a novel approach, leveraging biological insights to enhance artificial neural networks. This cross-disciplinary innovation is a significant contribution to the field.\n- **Significance**: The improvements in performance on standard benchmarks highlight the potential impact of the proposed method. If the approach can be further refined and theoretically grounded, it could influence future CNN designs and inspire additional biologically inspired innovations in AI.\n\nIn conclusion, the paper presents an interesting and potentially impactful approach by integrating lateral inhibition into CNNs. While the empirical results are promising, further theoretical analysis and broader comparisons would strengthen the work. Additionally, addressing reproducibility concerns through detailed implementation information would enhance the paper's contribution to the field.",
    "multimodal_review": "**1. Summary**  \nThe paper introduces a lateral inhibition-inspired (LI) structure for Convolutional Neural Networks (CNNs), aiming to enhance image classification by incorporating neurobiological principles directly into the training process. The LI structure uses a Gaussian low-pass filter to model lateral interactions, with learnable parameters to adjust inhibition strength. The method was tested on AlexNet and ResNet-18 using the ImageNet-1k dataset, showing significant performance improvements over baseline and existing normalization methods. The results suggest that integrating lateral inhibition can enhance feature learning in CNNs with minimal computational overhead.\n\n**2. Strengths**  \n- The paper introduces a novel approach by integrating neurobiological principles, specifically lateral inhibition, into CNNs, which is a relatively unexplored area in deep learning.\n- The proposed method demonstrates significant improvements in image classification performance on standard benchmarks like ImageNet-1k, particularly with AlexNet.\n- The LI structure is versatile, being applicable to both plain convolutional layers and residual blocks, making it compatible with various CNN architectures.\n- The paper provides a comprehensive evaluation of the proposed method against several normalization techniques, highlighting its competitive edge.\n\n**3. Weaknesses**  \n- **Theory Justification**: The paper lacks a detailed theoretical analysis of why lateral inhibition, as implemented, should lead to improved performance. While Section 3.2 introduces the lateral interaction function, a deeper theoretical exploration or justification of the choice of Gaussian filters and learnable parameters would strengthen the argument. Suggestion: Include a theoretical discussion or empirical analysis justifying the choice of the Gaussian filter and its parameters.\n- **Baseline Comparisons**: In Table 2, the paper compares the LI structure with other normalization methods but does not include comparisons with state-of-the-art methods beyond normalization techniques. Suggestion: Include comparisons with recent state-of-the-art architectures or methods to provide a broader context for the performance improvements.\n- **Ablation Study**: The paper does not provide an ablation study on the impact of different components of the LI structure, such as the learnable parameters \\( m \\), \\( v \\), and \\( b \\). Suggestion: Conduct an ablation study to isolate and evaluate the contribution of each component of the LI structure.\n- **Clarity in Method Description**: Section 3 could benefit from clearer explanations of the integration process of the LI structure into existing CNN architectures. Suggestion: Provide a step-by-step guide or diagram illustrating the integration process for clarity.\n- **Presentation of Results**: The results in Table 1 and Table 2 are presented without a clear discussion of statistical significance or variance. Suggestion: Include statistical analysis or confidence intervals to support the reported improvements.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**  \n  The paper is generally well-written, with clear explanations of the motivation and methodology. However, some sections, particularly the method description, could benefit from additional clarity and detail, especially regarding the integration of the LI structure into CNNs.\n\n  **(b) Figure & Caption Clarity**  \n  Figures are generally clear and relevant, with Figure 1 effectively illustrating the concept of lateral inhibition. However, Figure 2 could be improved by providing more detailed annotations or a legend to clarify the components of the LI structure.\n\n  **(c) Reproducibility Transparency**  \n  The paper provides a reasonable level of detail regarding the experimental setup, including datasets and training settings. However, it lacks specific information on hyperparameters, hardware used, and random seeds, which are essential for reproducibility. Additionally, there is no mention of code or data availability. Suggestion: Include detailed hyperparameter settings, hardware specifications, and random seeds, and consider providing access to code and data for reproducibility.\n\n**5. Novelty & Significance**  \nThe paper presents a novel integration of lateral inhibition into CNNs, which is a significant contribution given the limited exploration of neurobiological principles in deep learning. The approach is well-motivated and demonstrates substantial performance improvements, particularly for AlexNet. While the novelty lies in the application of lateral inhibition, the significance could be further enhanced by broader comparisons with state-of-the-art methods and a deeper theoretical exploration. The work opens new avenues for incorporating biologically inspired designs into CNNs, potentially leading to further advancements in the field.",
    "multimodal_rag_review": "**1. Summary**  \nThe paper introduces a Lateral Inhibition-Inspired (LI) structure for convolutional neural networks (CNNs), aiming to enhance image classification by mimicking neurobiological processes. The LI structure incorporates a learnable Gaussian low-pass filter to simulate lateral interactions, providing flexibility and adaptability. This approach is applicable to both plain convolutional layers and residual blocks, making it versatile for various CNN architectures. Experimental results demonstrate significant performance improvements on AlexNet and ResNet18, showcasing the potential of biologically inspired designs in advancing deep learning architectures.\n\n**2. Strengths**  \n- The paper presents a novel approach by integrating lateral inhibition, a well-known neurobiological process, into CNNs, which is a fresh perspective in the field of deep learning.\n- The proposed LI structure is versatile, being applicable to both plain convolutional layers and residual blocks, which enhances its utility across different architectures.\n- Experimental results show significant improvements in image classification performance, particularly with AlexNet, indicating the effectiveness of the proposed method.\n- The paper provides a comprehensive comparison with existing normalization techniques, highlighting the advantages of the LI structure.\n\n**3. Weaknesses**  \n- **Clarity in Methodology**: The explanation of the lateral interaction design in Section 3.1 could be clearer. The mathematical formulation and its implications on the network's behavior are not sufficiently detailed. Suggestion: Provide a more intuitive explanation and possibly a visual representation of how the Gaussian low-pass filter interacts with the input features.\n- **Baseline Comparisons**: The experimental setup in Section 4 lacks a comparison with more recent state-of-the-art architectures beyond AlexNet and ResNet18. Suggestion: Include comparisons with more modern architectures like EfficientNet or Vision Transformers to demonstrate the broader applicability of the LI structure.\n- **Ablation Studies**: The paper does not include detailed ablation studies on the hyperparameters of the LI structure, such as the size of the Gaussian filter or the impact of learnable parameters. Suggestion: Conduct and include ablation studies to provide insights into the sensitivity and robustness of the proposed method.\n- **Justification of Design Choices**: The choice of using a Gaussian low-pass filter and the specific design of the LI structure are not thoroughly justified in Section 3. Suggestion: Provide empirical or theoretical justification for these design choices to strengthen the validity of the approach.\n- **Figure Clarity**: Figure 2, which illustrates the placement of the LI structure, lacks clarity in its depiction of the interaction between convolutional layers and the LI structure. Suggestion: Enhance the figure with more detailed labels and a clearer legend to improve understanding.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**  \n  The paper is generally well-written, with clear articulation of the problem and the proposed solution. However, some sections, particularly the methodology, could benefit from more detailed explanations and justifications of design choices. Mathematical notations are mostly well-defined, but additional context could improve understanding.\n\n  **(b) Figure & Caption Clarity**  \n  Figures are relevant but could be improved in clarity. For instance, Figure 2's depiction of the LI structure placement could be more detailed, with clearer labels and legends to aid comprehension. Captions are generally informative but could be expanded to provide more context.\n\n  **(c) Reproducibility Transparency**  \n  The paper provides a solid description of the experimental setup, including datasets, training details, and hyperparameters. However, it lacks details on the specific values of the learnable parameters within the LI structure. The paper does not mention code availability, which would enhance reproducibility.\n\n**5. Novelty & Significance**  \nThe paper introduces a novel approach by integrating lateral inhibition into CNNs, which is a significant contribution to the field of deep learning. This biologically inspired design offers a new perspective on feature learning and demonstrates substantial performance improvements over traditional normalization methods. While the novelty is clear, the significance could be further enhanced by demonstrating the approach's effectiveness across a wider range of modern architectures. Overall, the work is a valuable contribution, providing both theoretical insights and practical advancements in image classification."
}