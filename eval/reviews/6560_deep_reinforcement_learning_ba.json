{
    "paper_id": "6560_deep_reinforcement_learning_ba",
    "title": "Deep Reinforcement Learning based Insight Selection Policy",
    "abstract": "We live in the era of ubiquitous sensing and computing. More and more data is being collected and processed from devices, sensors and systems. This opens up opportunities to discover patterns from these data that could help in gaining better understanding into the source that produces them. This is useful in a wide range of domains, especially in the area of personal health, in which such knowledge could help in allowing users to comprehend their behaviour and indirectly improve their lifestyle. Insight generators are systems that identify such patterns and verbalise them in a readable text format, referred to as insights. The selection of insights is done using a scoring algorithm which aims at optimizing this process based on multiple objectives, e.g., factual correctness, usefulness and interestingness of insights. In this paper, we propose a novel Reinforcement Learning (RL) framework for insight selection where the scoring model is trained by user feedback on interestingness and their lifestyle quality estimates. With the use of highly reusable and simple principles of automatic user simulation based on real data, we demonstrate in this preliminary study that the RL solution may improve the selection of insights towards multiple pre-defined objectives.",
    "human_review": "Summary Of The Paper:\nThis work proposes a reinforcement learning (RL) framework for the insight selection problem, aiming to generate insights that are both relevant to user preferences and beneficial to users’ health and well-being. The framework is evaluated through two experiments to demonstrate its feasibility. Preliminary experimental results using the American Time Use Survey (ATUS) 2003–2020 dataset show that the proposed RL solution outperforms insights generated from multiple pre-defined objectives.\n\nStrength And Weaknesses:\n\nStrengths:\n- The idea of applying reinforcement learning to the problem of insight generation and selection is novel and interesting.\n- The paper is well organized, with a clear hierarchical structure and readable presentation.\n\nWeaknesses:\n- The relationship between the two categories of insights—those appreciated by users and those beneficial to life quality—is not clearly explained. It is unclear whether these goals are aligned, conflicting, or sequential.\n- Given the availability of a labeled dataset (ATUS), the paper should compare its RL-based approach to a supervised learning baseline to better contextualize performance gains.\n- If a direct supervised learning comparison is infeasible, the paper should at least include comparisons to other MDP-based frameworks (e.g., as surveyed in Afsar et al., 2021) to clarify the advantages of the proposed design.\n- The modeling of real-life behavior as a state machine based on offline ATUS data raises generalization concerns. Specifically, if a state unseen in training appears in deployment, it is unclear how the insight selection network would respond or adapt.\n- There are several grammatical and stylistic issues throughout the paper. For example:\n  - “Those subjects have been selected to tell to the policy network about what measurement is the insight, if it compares it to the benchmark value and to which day of the week it refers to.”\n  - “On figure 6 is presented the behavior of the policy network after 12.000.000 steps of training.”\n  These sentences should be revised for clarity and fluency.\n\nClarity, Quality, Novelty And Reproducibility:\nThe paper is reasonably well-motivated and structured, with a clear problem statement. The primary contribution lies in applying reinforcement learning to the new domain of insight selection rather than introducing a fundamentally new algorithmic development. More extensive experimental comparisons and discussion are needed to strengthen the paper’s technical contribution and clarify its practical significance.\n\nSummary Of The Review:\nThe paper presents a reinforcement learning approach for the insight selection problem, showing promising preliminary results on the ATUS dataset. However, the paper lacks rigorous experimental baselines, deeper analyses, and broader discussion of its technical novelty. The current contribution primarily consists of applying existing RL methods to a new problem domain. Thus, the reviewer views the paper as marginally below the acceptance threshold.\n\nCorrectness: 3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\nTechnical Novelty And Significance: 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\nEmpirical Novelty And Significance: 2: The contributions are only marginally significant or novel.\nFlag For Ethics Review: NO.\nRecommendation: 5: marginally below the acceptance threshold\nConfidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
    "text_summary": "### Motivation\nThis study introduces a novel Deep Reinforcement Learning (DRL) framework designed to dynamically recommend personalized health insights to users, aiming to improve life quality and align with user preferences. The motivation stems from the increasing availability of data from sensors and devices, which can be analyzed to generate actionable insights, particularly in personal health. Insights are defined as natural language statements describing patterns in user data (e.g., \"You went to sleep later during weekends than weekdays\") to encourage better lifestyle choices. Traditional methods for insight generation, such as statistical algorithms or supervised learning, often oversimplify user preferences, require large datasets, or fail to account for long-term user engagement. Previous frameworks, like the generalized insight generator (Genf), used an \"overgenerate and rank\" approach but lacked adaptability to user-specific needs and the ability to model sequential user interactions. This study addresses these limitations by leveraging DRL to optimize insight recommendations based on user feedback and lifestyle metrics.\n\n### Method\nThe proposed framework models the recommendation process as a Markov Decision Process (MDP), enabling it to consider both short-term and long-term impacts of insights on user behavior. It incorporates a probabilistic simulation of user behavior and a DRL policy network to optimize for multiple objectives, such as factual correctness, usefulness, and user interest. The system operates in three stages: insight generation, insight selection, and user lifestyle simulation.\n\n#### Insight Generation\nInsights are generated using predefined schemas for comparisons, such as:\n1. Comparing measurements across time periods (e.g., \"You sleep less on Mondays than other days\").\n2. Comparing measurements to benchmarks (e.g., \"You sleep less than 8 hours on Mondays\").\nInsights are validated using statistical significance tests (e.g., Kolmogorov-Smirnov test, p-value < 0.05) and scored for relevance based on data completeness and contextual differences. Insights are represented as feature vectors using a bag-of-words (BoW) embedding, clustered via K-means, and filtered to retain the most relevant insights.\n\n#### Insight Selection\nThe DRL policy network selects one insight per day from the clustered insights or benchmark comparisons. The network is trained to optimize user outcomes, such as improved sleep quality or exercise duration, based on user feedback and lifestyle metrics.\n\n#### User Lifestyle Simulation\nA probabilistic state machine simulates user behavior using minimal input data (e.g., activity start and stop times). Bayesian Gaussian Mixture Models (GMMs) model transitions between states (e.g., Sleeping, Working, Exercising), capturing the probabilistic nature of human behavior. The simulation adjusts user behavior based on selected insights, enabling the policy network to learn through trial and error without requiring counterfactuals or manually designed reward functions.\n\n#### Reinforcement Learning\nThe policy network, implemented using the Proximal Policy Optimization (PPO) algorithm, observes a history of selected insights and their impact on life quality metrics. It selects insights to influence user behavior, with rewards based on improvements in metrics like sleep quality, exercise duration, and user satisfaction. The reward function is designed to balance multiple objectives, such as aligning with user preferences and optimizing life quality metrics.\n\n### Results\nThe framework was evaluated using simulated user data based on the American Time Use Survey (ATUS) dataset, focusing on working adults with poor sleep habits. Key experiments included:\n\n#### Experiment 1: Insight Selection for Healthcare\nThe policy network was tested with different reward functions (e.g., optimizing sleep quality, exercise duration, or both). Results showed that policies prioritizing sleep quality (e.g., PSQI, FULL) improved sleep metrics fastest, achieving a Pittsburgh Sleep Quality Index (PSQI) score of 4 within 35 days. Policies focusing on exercise duration (e.g., EXE) maximized exercise hours but were less effective for sleep improvement. Balancing multiple objectives (e.g., PSQI and exercise) proved challenging, highlighting the need for refined reward structures.\n\n#### Experiment 2: Insight Selection with Feedback\nThis experiment prioritized user preferences over lifestyle metrics, with rewards based solely on user feedback. The policy network adapted to user interests, discarding ineffective insights and diversifying recommendations over time. After extensive training, the network demonstrated the ability to align insights with user preferences while maintaining robust performance.\n\n#### Simulation Results\nThe system effectively influenced user behavior, encouraging earlier sleep and wake times. Policies targeting specific metrics (e.g., PSQI, EXE) outperformed baseline methods, demonstrating the framework's ability to adapt to user feedback and optimize life quality metrics.\n\n### Conclusion\nThis study demonstrates the potential of DRL to deliver personalized health insights that improve user life quality and align with their preferences. By leveraging probabilistic simulations and eliminating the need for counterfactuals or manual reward design, the framework achieves robust and scalable training. The system successfully adapts to user feedback, selecting relevant insights while minimizing complexity and time spent on ratings. However, challenges remain in balancing multiple objectives and addressing performance gaps in specific topics, such as sleep duration.\n\nFuture work will focus on deploying the system in real-world applications, such as patient and personal health monitoring, to study its impact on user behavior. Additional research will explore:\n- Automatically encoding the relative impact of insights on user behavior.\n- Training the policy network with diverse user simulations to adapt to varying behaviors and reward functions.\n- Expanding the scope of user interests beyond the current topics, potentially using weighted random sampling to reflect individual preferences.\n\nThe study marks a significant advancement in personalized health insight generation, offering a flexible and robust tool for improving user life quality through tailored recommendations.",
    "text_only_review": "#### 1. Summary\nThis paper proposes a novel Deep Reinforcement Learning (DRL) framework for dynamically generating and recommending personalized health insights to users. The framework models the recommendation process as a Markov Decision Process (MDP) and leverages a Proximal Policy Optimization (PPO) algorithm to optimize multiple objectives, such as improving user life quality metrics (e.g., sleep quality, exercise duration) and aligning with user preferences. The system consists of three main stages: insight generation, insight selection, and user lifestyle simulation. Insights are generated using predefined schemas and statistical validation, while a DRL policy network selects the most relevant insights based on user feedback and lifestyle metrics. A probabilistic simulation of user behavior enables the system to learn through trial and error without requiring counterfactuals or manual reward design. The framework is evaluated using simulated user data, demonstrating its ability to optimize life quality metrics and adapt to user preferences.\n\n#### 2. Strengths\n- **Innovative Approach**: The paper introduces a novel application of DRL to personalized health insight generation, addressing limitations of traditional methods such as lack of adaptability and inability to model sequential user interactions.\n- **Comprehensive Framework**: The proposed system integrates multiple components (e.g., insight generation, selection, and user simulation) into a cohesive pipeline, enabling end-to-end optimization of user outcomes.\n- **Probabilistic User Simulation**: The use of Bayesian Gaussian Mixture Models (GMMs) to simulate user behavior is a notable strength, as it captures the probabilistic nature of human activities and allows for scalable training without requiring real-world data.\n- **Multi-Objective Optimization**: The framework balances multiple objectives, such as factual correctness, user interest, and life quality metrics, showcasing its flexibility and adaptability.\n- **Promising Results**: Experimental results demonstrate the framework's effectiveness in improving sleep quality and exercise duration, outperforming baseline methods and adapting to user feedback over time.\n- **Scalability**: The elimination of counterfactuals and manual reward design enhances the scalability and generalizability of the approach.\n\n#### 3. Weaknesses\n- **Evaluation Limitations**: The framework is evaluated solely on simulated user data, which may not fully capture the complexities and variability of real-world user behavior. The lack of real-world validation limits the generalizability of the results.\n- **Reward Function Design**: While the paper highlights the challenges of balancing multiple objectives, it does not provide a detailed discussion of how reward functions could be refined to address trade-offs effectively.\n- **Limited Scope of Insights**: The predefined schemas for insight generation focus primarily on sleep and exercise metrics, which may limit the system's applicability to other health domains or broader user interests.\n- **User Feedback Modeling**: The probabilistic simulation of user feedback may oversimplify the nuances of real-world user preferences and engagement, potentially impacting the system's ability to generalize.\n- **Complexity of Metrics**: The reliance on metrics like the Pittsburgh Sleep Quality Index (PSQI) may introduce complexity in real-world deployment, as such metrics require detailed and accurate user data that may not always be available.\n\n#### 4. Clarity & Reproducibility\n- **Clarity**: The paper is well-written and provides a clear explanation of the framework's components, methodologies, and experimental results. The use of examples (e.g., insight schemas) enhances understanding.\n- **Reproducibility**: While the methodology is described in detail, the paper does not provide sufficient information about implementation specifics (e.g., hyperparameters, training configurations, or code availability) to ensure full reproducibility. Sharing code or pseudocode would significantly enhance reproducibility.\n- **Experimental Setup**: The use of the American Time Use Survey (ATUS) dataset for simulation is appropriate, but details about how the dataset was processed and used for evaluation are limited.\n\n#### 5. Novelty & Significance\n- **Novelty**: The application of DRL to personalized health insight generation is novel and represents a significant departure from traditional methods like statistical algorithms or supervised learning. The integration of probabilistic user simulation and multi-objective optimization further enhances the novelty.\n- **Significance**: The framework addresses a critical gap in personalized health monitoring by providing a scalable and adaptable solution for generating actionable insights. If validated in real-world settings, the system could have significant implications for improving user life quality and engagement in health-related behaviors.\n- **Potential Impact**: The proposed approach has the potential to advance the field of personalized health monitoring and recommendation systems, particularly in applications involving wearable devices and sensor data.\n\n---\n\nIn conclusion, this paper presents a promising and innovative framework for personalized health insight generation using DRL. While the approach is well-motivated and demonstrates strong preliminary results, the lack of real-world validation and detailed discussion of certain design choices (e.g., reward functions) are notable limitations. Future work addressing these issues could significantly enhance the framework's impact and applicability.",
    "merged_image_path": "temp_cache\\6560_deep_reinforcement_learning_ba_merged.png",
    "image_only_review": "#### 1. Summary\nThe paper presents a novel approach to insight selection in marketing using deep reinforcement learning (DRL). The authors propose a policy that leverages DRL to optimize the selection of marketing insights, aiming to improve decision-making processes in marketing strategies. The methodology involves the use of a deep Q-network (DQN) to learn optimal policies for selecting insights based on historical data. The paper includes experiments conducted on real-world datasets, demonstrating the effectiveness of the proposed approach compared to traditional methods.\n\n#### 2. Strengths\n- **Innovative Approach**: The application of deep reinforcement learning to the problem of insight selection in marketing is innovative and addresses a significant gap in the literature.\n- **Comprehensive Methodology**: The paper provides a detailed explanation of the methodology, including the architecture of the DQN and the process of training the model.\n- **Empirical Validation**: The authors conduct experiments using real-world datasets, providing empirical evidence of the proposed method's effectiveness.\n- **Clear Problem Definition**: The paper clearly defines the problem of insight selection and articulates the challenges associated with it.\n\n#### 3. Weaknesses\n- **Limited Baseline Comparisons**: The paper could benefit from a broader comparison with more baseline methods to better contextualize the performance improvements.\n- **Scalability Concerns**: While the approach is validated on real-world datasets, there is limited discussion on the scalability of the method to larger datasets or more complex marketing scenarios.\n- **Generalization**: The paper does not sufficiently address how the proposed method generalizes across different industries or types of marketing campaigns.\n\n#### 4. Clarity & Reproducibility\n- **Clarity**: The paper is generally well-written and structured, with clear explanations of the methodology and results. However, some sections, particularly those detailing the experimental setup, could benefit from additional clarification.\n- **Reproducibility**: The paper provides a reasonable level of detail regarding the implementation of the DQN and the experimental setup. However, the inclusion of code or pseudocode would enhance reproducibility. Additionally, more detailed information on hyperparameter settings and training procedures would be beneficial.\n\n#### 5. Novelty & Significance\n- **Novelty**: The application of DRL to insight selection in marketing is novel and represents a significant contribution to the field. The approach leverages advanced machine learning techniques to address a practical problem, which is a commendable effort.\n- **Significance**: The proposed method has the potential to significantly impact marketing strategies by improving the efficiency and effectiveness of insight selection. This could lead to better-informed decision-making and more successful marketing campaigns. However, the significance could be further enhanced by demonstrating the method's applicability across a wider range of scenarios and industries.",
    "multimodal_review": "**1. Summary**  \nThe paper introduces a novel Deep Reinforcement Learning (DRL) framework aimed at dynamically recommending personalized health insights to users, with the goal of improving life quality and aligning with user preferences. The framework models the recommendation process as a Markov Decision Process (MDP) and employs a DRL policy network to optimize insight recommendations based on user feedback and lifestyle metrics. The system operates in three stages: insight generation, insight selection, and user lifestyle simulation, and is evaluated using simulated user data from the American Time Use Survey (ATUS) dataset. The results demonstrate the framework's ability to adapt to user feedback and optimize life quality metrics, although challenges remain in balancing multiple objectives.\n\n**2. Strengths**  \n- The paper addresses a relevant and timely problem of personalized health insight generation, leveraging the growing availability of sensor data.\n- The use of DRL to model sequential user interactions and optimize recommendations based on user feedback is innovative and well-motivated.\n- The framework is comprehensive, incorporating insight generation, selection, and user lifestyle simulation, which allows for a holistic approach to recommendation.\n- The evaluation using simulated data demonstrates the potential of the framework to improve user life quality metrics, such as sleep quality and exercise duration.\n\n**3. Weaknesses**  \n- **Theory and Justification**: The paper lacks a detailed theoretical justification for the choice of the Proximal Policy Optimization (PPO) algorithm over other DRL algorithms. This is important for understanding the suitability of PPO for the specific problem context. *Suggestion*: Include a comparative analysis or justification in Section 3.4 for the choice of PPO.\n- **Novelty and Baseline Comparison**: The paper does not provide a comparison with existing baseline methods for insight recommendation, such as the generalized insight generator (Genf). This makes it difficult to assess the novelty and improvement over existing methods. *Suggestion*: Include a baseline comparison in Section 4 to highlight the improvements offered by the proposed framework.\n- **Clarity and Presentation**: Some sections, such as the Insight Generation (Section 3.1), are dense and could benefit from clearer explanations or examples to aid understanding. *Suggestion*: Simplify the explanation and provide additional examples or diagrams to illustrate the insight generation process.\n- **Reproducibility and Transparency**: The paper lacks detailed information on the hyperparameters used in the DRL training process, which is crucial for reproducibility. *Suggestion*: Include a table or appendix detailing the hyperparameters and training setup in Section 4.\n- **Figure and Caption Clarity**: Figures such as Figure 4 and Figure 5 lack detailed captions and explanations, making it difficult to interpret the results. *Suggestion*: Enhance the captions to provide a clear explanation of what each figure illustrates and how it relates to the text.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity** – The paper generally explains the ideas and methods clearly, but some sections, such as Insight Generation, could benefit from simplification and additional examples. Mathematical notations are well-defined, but the assumptions and limitations of the approach are not explicitly discussed.\n  **(b) Figure & Caption Clarity** – Figures are relevant to the claims but lack detailed captions and explanations. Axes and labels are generally consistent, but some figures could be more closely aligned with the textual descriptions.\n  **(c) Reproducibility Transparency** – The paper provides an overview of the experimental setup but lacks detailed information on hyperparameters and training configurations. There is no mention of code or data availability, and while the use of simulated data is described, real-world applicability is not addressed.\n\n**5. Novelty & Significance**  \nThe paper presents a novel application of DRL to the problem of personalized health insight generation, which is a significant contribution given the increasing availability of personal health data. The approach is well-motivated and contextualized within the literature, addressing limitations of previous methods such as lack of adaptability and sequential modeling. The framework's ability to adapt to user feedback and optimize life quality metrics is a valuable contribution, although the lack of baseline comparisons and detailed theoretical justification limits the assessment of its novelty and significance. The work has the potential to impact real-world applications in personal health monitoring, but further validation with real-world data is needed to fully realize its significance.",
    "multimodal_rag_review": "**1. Summary**  \nThe paper presents a novel Deep Reinforcement Learning (DRL) framework aimed at recommending personalized health insights to users, thereby enhancing their lifestyle quality. The framework dynamically selects insights using DRL, adapting to user feedback and preferences, and addresses the limitations of previous static methods. Key contributions include the use of a probabilistic user simulator for training, multi-objective optimization to balance factual correctness and user interest, and a three-stage daily training pipeline involving insight generation, selection, and user lifestyle simulation. The evaluation demonstrates improvements in life quality metrics and adaptability to user preferences, although challenges remain in balancing multiple objectives.\n\n**2. Strengths**  \n- The framework introduces a dynamic approach to insight recommendation, leveraging DRL to adaptively cater to individual user preferences, which is a significant advancement over static methods.\n- The use of a probabilistic user simulator for generating training data is innovative, reducing the dependency on large retrospective datasets and manual reward function design.\n- The multi-objective optimization ensures that insights are not only factually correct but also useful and engaging for users, enhancing the practical applicability of the system.\n- The evaluation demonstrates the framework's potential in improving life quality metrics, particularly in sleep quality and exercise duration, showcasing its effectiveness.\n\n**3. Weaknesses**  \n- **Novelty and Contribution Clarity**: The paper does not clearly distinguish its novel contributions from existing methods, particularly in the introduction and methodology sections. It would benefit from explicitly outlining what sets this framework apart from prior work (e.g., Section 1, Introduction).\n- **Comparison with Related Work**: There is insufficient comparison with related literature, which raises questions about the novelty and relevance of the proposed approach. Including a more thorough comparison would help establish the framework's unique contributions (e.g., Section 2, Related Work).\n- **Generalizability of Results**: The discussion on the generalizability of results is lacking, especially given the reliance on simulated user feedback. It is important to address how well these findings might translate to real-world scenarios (e.g., Section 5, Discussion).\n- **Complexity vs. Simplicity**: The framework employs complex models without a clear justification for their necessity over simpler baselines. Including a comparison with simpler models could validate the need for such complexity (e.g., Section 4, Experiments).\n- **Figure Clarity**: Some figures, such as Figure 3, lack clarity in their captions and do not effectively convey the intended information. Improving the captions and ensuring they are self-sufficient would enhance understanding (e.g., Figure 3).\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**  \n  The paper is generally well-written, with ideas and methods logically presented. However, the introduction and methodology sections could benefit from clearer articulation of the novel contributions. Mathematical notations are well-defined, but assumptions and limitations are not explicitly stated, which could aid in understanding the scope of the work.\n\n  **(b) Figure & Caption Clarity**  \n  Figures are generally informative, but some captions, such as those for Figure 3, lack detail and do not fully explain the figure's relevance. Ensuring that captions are comprehensive and that figures correlate well with the text would improve clarity.\n\n  **(c) Reproducibility Transparency**  \n  The paper provides a reasonable level of detail regarding the experimental setup, including datasets and evaluation metrics. However, there is no mention of code or data availability, which is crucial for reproducibility. Additionally, the lack of ablation studies on key parameters limits the understanding of the framework's robustness.\n\n**5. Novelty & Significance**  \nThe paper addresses the significant problem of personalized health insight recommendation using a novel DRL framework. The approach is well-motivated, particularly in its attempt to overcome the limitations of static methods through dynamic adaptation to user preferences. While the paper substantiates its claims with empirical results, the novelty is somewhat undermined by insufficient comparison with existing work. The framework's potential impact is notable, particularly in enhancing user engagement and lifestyle quality, but further validation in real-world settings and a clearer articulation of its novel contributions would strengthen its significance."
}