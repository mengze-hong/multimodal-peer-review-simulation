{
  "title": "Suppression helps: Lateral Inhibition-inspired Convolutional Neural Network for Image Classification",
  "abstract": "Convolutional neural networks (CNNs) have become powerful and popular tools\nsince deep learning emerged for image classification in the computer vision field.",
  "summary": "### Overview of the Paper\n\nThis paper introduces a **Lateral Inhibition-Inspired (LI) Structure** for convolutional neural networks (CNNs) to enhance image classification performance by mimicking neurobiological processes. Lateral inhibition, a phenomenon in biological vision systems that enhances contrast and sharpness (e.g., the Mach band effect), is explicitly modeled in the proposed design. The LI structure incorporates a learnable Gaussian low-pass filter to simulate lateral interactions, offering flexibility and adaptability compared to prior fixed or unlearnable designs. This approach is compatible with both plain convolutional layers and residual blocks, making it versatile for various CNN architectures.\n\n---\n\n### Background and Related Work\n\n#### Neurobiological Inspiration\nLateral inhibition is a key mechanism in biological vision systems that enhances contrast by suppressing neighboring signals. While this concept has been explored in computer vision for tasks like boundary detection and color constancy, its integration into CNNs for feature learning has been limited.\n\n#### Traditional Methods in Neural Networks\n1. **Normalization Techniques**:\n   - **Local Response Normalization (LRN)**: Applies lateral inhibition across channels but lacks trainability.\n   - **Batch Normalization (BN)**: Normalizes within channels using batch statistics, widely used in CNNs.\n   - **Layer and Group Normalization**: Normalize across channels or groups of channels, addressing small batch size issues.\n   - **Weighted Normalization and DivisiveNorm**: Extend LRN with scaling and bias parameters but lack flexibility in modeling lateral interactions.\n\n2. **Lateral Inhibition in Vision Tasks**:\n   - **Yang et al. (2015)**: Proposed a double-opponent mechanism for boundary detection, mimicking human vision.\n   - **Cao et al. (2018)**: Used lateral inhibition for saliency detection but applied it only during inference.\n   - **Hasani et al. (2019)**: Introduced surround modulation with fixed Gaussian filters, which lacked flexibility and caused performance drops in deeper layers.\n\nThese methods either lack trainability, flexibility, or fail to fully integrate lateral inhibition into CNNs during training.\n\n---\n\n### Proposed Method: Lateral Inhibition-Inspired (LI) Structure\n\n#### Design and Implementation\nThe LI structure explicitly models lateral inhibition by:\n1. **Lateral Interaction Modeling**:\n   - A Gaussian low-pass filter (with the central weight removed) models the decay of interaction strength with distance.\n   - Learnable weights allow the structure to adapt to different interaction types (e.g., inhibition, none, or excitation).\n2. **Contrast Enhancement**:\n   - The convolution result of the input features and the Gaussian filter is subtracted from the input to enhance contrast.\n3. **Learnable Adjustments**:\n   - Additional parameters (scaling factor, shift, and bias) are introduced for fine-tuning the output.\n4. **Placement**:\n   - The LI structure is applied after plain convolutions or inside residual blocks before the first convolution.\n\nThis design is flexible, trainable, and biologically inspired, addressing limitations of prior methods.\n\n---\n\n### Experimental Setup\n\n#### Dataset and Training\n- **Dataset**: ImageNet-1k, containing 1.2M training images and 50k validation images across 1,000 categories.\n- **Training Details**:\n  - Models were trained using PyTorch on 4 GPUs with a batch size of 128 (batch accumulation of 2).\n  - AlexNet was trained for 90 epochs, and ResNet for 100 epochs.\n  - Standard training configurations were used, including 224Ã—224 image crops, learning rate decay, momentum, weight decay, and data augmentation.\n\n---\n\n### Results and Analysis\n\n#### Performance Improvements\n1. **AlexNet**:\n   - Baseline Top-1 accuracy: **52.82%**.\n   - LI design achieved **60.39%**, a **7.58% improvement** with minimal parameter and GFLOP increases.\n   - Outperformed other normalization methods:\n     - GroupNorm: 57.76% (+4.94%).\n     - LayerNorm: 58.81% (+5.99%).\n     - BatchNorm: 59.04% (+6.22%).\n     - WeightedNorm-c: 59.41% (+6.59%).\n     - DivisiveNorm: 59.54% (+6.72%).\n\n2. **ResNet18**:\n   - Baseline Top-1 accuracy: **70.30%**.\n   - LI design achieved **71.11%**, a **0.81% improvement**, despite ResNet already using BatchNorm.\n\n#### Design Variants\n- LI with scaling only: 59.87% (+7.05% improvement).\n- LI with shift-scaling: 60.26% (+7.44% improvement).\n- LI with scale-bias: 60.39% (+7.58% improvement, best result).\n\n#### Key Insights\n- The LI structure outperforms existing normalization methods by explicitly modeling lateral interactions within the same channel, which other methods lack.\n- Among design variants, scaling with bias performs best, though the difference between the top two variants is small.\n\n---\n\n### Contributions and Significance\n\n1. **Biologically Inspired Design**:\n   - The LI structure explicitly integrates lateral inhibition into CNNs during training, offering a new perspective for feature learning in image classification.\n2. **Flexibility and Adaptability**:\n   - Unlike fixed designs (e.g., Difference of Gaussian filters), the LI structure is trainable and can adapt to various layers and channels.\n3. **Performance Gains**:\n   - Significant accuracy improvements were achieved on AlexNet and ResNet18 with minimal computational overhead.\n\n---\n\n### Conclusion\n\nThe proposed lateral inhibition-inspired (LI) structure enhances CNN performance by mimicking neurobiological processes. By explicitly modeling lateral interactions, the LI design improves contrast and sharpness in feature learning, outperforming traditional normalization methods. This work highlights the potential of biologically inspired designs for advancing deep learning architectures, offering both flexibility and significant performance gains in image classification tasks.",
  "ref": {
    "weaknesses": [
      "Lack of clarity in distinguishing key concepts or terms, leading to potential confusion.",
      "Arbitrary or insufficiently justified methodological choices that undermine the study's validity.",
      "Experimental design flaws, such as reliance on suboptimal or outdated architectures, limiting the generalizability of findings.",
      "Limited novelty or overlap with existing work, reducing the contribution to the field.",
      "Underwhelming or inconclusive experimental results that fail to substantiate claims.",
      "Insufficient exploration of alternative approaches or comparisons to state-of-the-art methods.",
      "Poor explanation of architectural or methodological components, leaving key aspects ambiguous."
    ],
    "improvements": [
      "Provide clearer definitions and distinctions for key concepts to enhance reader understanding.",
      "Justify methodological choices with theoretical or empirical evidence to strengthen validity.",
      "Design experiments using modern, state-of-the-art architectures to ensure relevance and robustness.",
      "Highlight novel contributions and differentiate the work from prior studies to demonstrate originality.",
      "Present stronger and more conclusive experimental results to substantiate claims effectively.",
      "Explore alternative approaches or comparisons to existing methods to contextualize findings.",
      "Improve clarity in describing architectural components and methodologies to avoid ambiguity."
    ]
  },
  "rev": "**1. Summary**  \nThe paper introduces a Lateral Inhibition-Inspired (LI) structure for convolutional neural networks (CNNs), aiming to enhance image classification by mimicking neurobiological processes. The LI structure incorporates a learnable Gaussian low-pass filter to simulate lateral interactions, providing flexibility and adaptability. This approach is applicable to both plain convolutional layers and residual blocks, making it versatile for various CNN architectures. Experimental results demonstrate significant performance improvements on AlexNet and ResNet18, showcasing the potential of biologically inspired designs in advancing deep learning architectures.\n\n**2. Strengths**  \n- The paper presents a novel approach by integrating lateral inhibition, a well-known neurobiological process, into CNNs, which is a fresh perspective in the field of deep learning.\n- The proposed LI structure is versatile, being applicable to both plain convolutional layers and residual blocks, which enhances its utility across different architectures.\n- Experimental results show significant improvements in image classification performance, particularly with AlexNet, indicating the effectiveness of the proposed method.\n- The paper provides a comprehensive comparison with existing normalization techniques, highlighting the advantages of the LI structure.\n\n**3. Weaknesses**  \n- **Clarity in Methodology**: The explanation of the lateral interaction design in Section 3.1 could be clearer. The mathematical formulation and its implications on the network's behavior are not sufficiently detailed. Suggestion: Provide a more intuitive explanation and possibly a visual representation of how the Gaussian low-pass filter interacts with the input features.\n- **Baseline Comparisons**: The experimental setup in Section 4 lacks a comparison with more recent state-of-the-art architectures beyond AlexNet and ResNet18. Suggestion: Include comparisons with more modern architectures like EfficientNet or Vision Transformers to demonstrate the broader applicability of the LI structure.\n- **Ablation Studies**: The paper does not include detailed ablation studies on the hyperparameters of the LI structure, such as the size of the Gaussian filter or the impact of learnable parameters. Suggestion: Conduct and include ablation studies to provide insights into the sensitivity and robustness of the proposed method.\n- **Justification of Design Choices**: The choice of using a Gaussian low-pass filter and the specific design of the LI structure are not thoroughly justified in Section 3. Suggestion: Provide empirical or theoretical justification for these design choices to strengthen the validity of the approach.\n- **Figure Clarity**: Figure 2, which illustrates the placement of the LI structure, lacks clarity in its depiction of the interaction between convolutional layers and the LI structure. Suggestion: Enhance the figure with more detailed labels and a clearer legend to improve understanding.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**  \n  The paper is generally well-written, with clear articulation of the problem and the proposed solution. However, some sections, particularly the methodology, could benefit from more detailed explanations and justifications of design choices. Mathematical notations are mostly well-defined, but additional context could improve understanding.\n\n  **(b) Figure & Caption Clarity**  \n  Figures are relevant but could be improved in clarity. For instance, Figure 2's depiction of the LI structure placement could be more detailed, with clearer labels and legends to aid comprehension. Captions are generally informative but could be expanded to provide more context.\n\n  **(c) Reproducibility Transparency**  \n  The paper provides a solid description of the experimental setup, including datasets, training details, and hyperparameters. However, it lacks details on the specific values of the learnable parameters within the LI structure. The paper does not mention code availability, which would enhance reproducibility.\n\n**5. Novelty & Significance**  \nThe paper introduces a novel approach by integrating lateral inhibition into CNNs, which is a significant contribution to the field of deep learning. This biologically inspired design offers a new perspective on feature learning and demonstrates substantial performance improvements over traditional normalization methods. While the novelty is clear, the significance could be further enhanced by demonstrating the approach's effectiveness across a wider range of modern architectures. Overall, the work is a valuable contribution, providing both theoretical insights and practical advancements in image classification.",
  "todo": [
    "Clarify methodology: Provide a more intuitive explanation and possibly a visual representation of how the Gaussian low-pass filter interacts with input features [Section 3.1]",
    "Expand baseline comparisons: Include comparisons with more modern architectures like EfficientNet or Vision Transformers to demonstrate the broader applicability of the LI structure [Section 4]",
    "Conduct ablation studies: Include detailed ablation studies on the hyperparameters of the LI structure, such as the size of the Gaussian filter and the impact of learnable parameters [Section 4]",
    "Justify design choices: Provide empirical or theoretical justification for the use of a Gaussian low-pass filter and the specific design of the LI structure [Section 3]",
    "Enhance figure clarity: Improve Figure 2 with more detailed labels and a clearer legend to better illustrate the interaction between convolutional layers and the LI structure [Page 5, Figure 2]",
    "Improve reproducibility: Include details on the specific values of the learnable parameters within the LI structure and mention code availability to enhance reproducibility [Section 4]",
    "Expand figure captions: Provide more context in figure captions to improve understanding [Throughout the paper]"
  ],
  "timestamp": "2025-10-30T15:27:01.168766",
  "manuscript_file": "manuscript.pdf",
  "image_file": "concat_image.png"
}