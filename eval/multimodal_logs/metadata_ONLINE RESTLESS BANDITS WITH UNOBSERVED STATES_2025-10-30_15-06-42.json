{
  "title": "ONLINE RESTLESS BANDITS WITH UNOBSERVED STATES",
  "abstract": "We study the online restless bandit problem, where each arm evolves according to\na Markov chain independently, and the reward of pulling an arm depends on both\nthe current state of the corresponding Markov chain and the action. The agent\n(decision maker) does not know the transition kernels and reward functions, and\ncannot observe the states of arms even after pulling. The goal is to sequentially\nchoose which arms to pull so as to maximize the expected cumulative rewards col-\nlected. In this paper, we propose TSEETC, a learning algorithm based on Thomp-\nson Sampling with Episodic Explore-Then-Commit. The algorithm proceeds in\nepisodes of increasing length and each episode is divided into exploration and\nexploitation phases. In the exploration phase in each episode, action-reward sam-\nples are collected in a round-robin way and then used to update the posterior as\na mixture of Dirichlet distributions. At the beginning of the exploitation phase,",
  "summary": "### Structured Overview of the Paper\n\n#### **Introduction and Problem Setting**\nThe paper addresses the **online restless bandit problem**, a challenging decision-making framework where multiple arms evolve independently according to unknown Markov chains. The agent's goal is to maximize cumulative rewards over a time horizon \\( T \\), despite not knowing the transition dynamics, reward functions, or even the states of the arms—states remain unobserved even after pulling an arm. This problem is particularly relevant in applications such as wireless communication, sensor maintenance, and healthcare.\n\n#### **Key Challenges**\n1. **Unobserved States**: Unlike prior work, the algorithm must operate without direct state observations, relying instead on belief states derived from historical actions and rewards.\n2. **Unknown Parameters**: Transition probabilities and reward functions are unknown and must be estimated during the learning process.\n3. **Theoretical Tightness**: Achieving a regret bound close to the theoretical lower bound for restless bandits is a significant challenge, especially in the unobserved state setting.\n\n---\n\n#### **Contributions**\nThe paper introduces **TSEETC (Thompson Sampling with Episodic Explore-Then-Commit)**, a novel algorithm that balances exploration and exploitation in a structured, episodic manner. The key contributions are:\n\n1. **Algorithm Design**:\n   - **Exploration Phase**: The algorithm collects action-reward samples in a round-robin manner and updates posterior distributions using a mixture of Dirichlet distributions.\n   - **Exploitation Phase**: Parameters are sampled from the posterior, and the optimal policy for the sampled model is followed for the remainder of the episode.\n   - Episode lengths increase deterministically over time, ensuring efficient exploration while bounding exploration-induced regret.\n\n2. **Theoretical Results**:\n   - TSEETC achieves a **Bayesian regret bound of \\( \\tilde{O}(\\sqrt{T}) \\)**, which is near-optimal and matches the theoretical lower bound for restless bandits.\n   - The regret analysis incorporates belief state updates, posterior sampling, and episodic exploration-exploitation trade-offs.\n\n3. **Empirical Validation**:\n   - Simulations demonstrate that TSEETC outperforms existing algorithms, such as SEEU and UCB-based methods, in terms of cumulative regret.\n   - The algorithm achieves a regret slope close to 0.5 in log-log plots, consistent with the theoretical bound.\n\n---\n\n#### **Algorithmic Framework**\n1. **Belief State Updates**:\n   - The belief state encodes the agent's knowledge about the system, updated using observed rewards and historical actions.\n   - Unknown parameters (transition probabilities and reward functions) are modeled as Dirichlet distributions, and posterior updates incorporate all possible state sequences weighted by their likelihoods.\n\n2. **Posterior Sampling**:\n   - Transition probabilities and reward functions are updated using historical data, with posterior distributions expressed as mixtures of Dirichlet distributions.\n   - The algorithm ensures that pseudo-counts closely approximate true counts, improving parameter estimation accuracy.\n\n3. **Episodic Structure**:\n   - Episodes are divided into exploration and exploitation phases, with lengths increasing deterministically to balance exploration and exploitation.\n   - The total number of episodes is \\( O(\\sqrt{T}) \\), ensuring efficient learning over the time horizon.\n\n---\n\n#### **Theoretical Results**\n1. **Regret Analysis**:\n   - The exploration phase contributes a regret bound of \\( O(\\sqrt{T}) \\), while the exploitation phase achieves a similar bound.\n   - The total regret is bounded as:\n     \\[\n     R_T \\leq 48C_1C_2S\\sqrt{NT\\log(NT)} + (\\tau_1\\Delta_R + H + 4C_1C_2SN)\\sqrt{T} + C_1C_2,\n     \\]\n     where constants \\( C_1 \\) and \\( C_2 \\) depend on problem parameters, including the number of arms \\( N \\), state size \\( S \\), and maximum reward \\( r_{\\text{max}} \\).\n\n2. **Belief Error Control**:\n   - The belief error is bounded as:\n     \\[\n     \\| b_t(\\cdot, R^*, P^*) - \\hat{b}_t(\\cdot, R_k, P_k) \\|_1 \\leq L_1 \\| R_k - R^* \\|_1 + L_2 \\max_s \\| P^*(m, :) - P_k(m, :) \\|_2,\n     \\]\n     where \\( L_1 \\) and \\( L_2 \\) depend on the minimum elements of the transition and reward functions.\n\n3. **Comparison to Prior Work**:\n   - TSEETC achieves a tighter regret bound (\\( \\tilde{O}(\\sqrt{T}) \\)) compared to prior methods like SEEU (\\( \\tilde{O}(T^{2/3}) \\)) and UCB-based algorithms.\n   - Unlike pseudo-count-based methods, TSEETC's posterior updates ensure accurate parameter estimation, leading to improved performance.\n\n---\n\n#### **Empirical Results**\n1. **Experimental Setup**:\n   - Simulations were conducted with two arms, two hidden states per arm, and a time horizon \\( T = 50,000 \\).\n   - TSEETC was compared against baseline algorithms, including \\( \\epsilon \\)-greedy, Sliding-Window UCB, RUCB, Q-learning, and SEEU.\n\n2. **Performance**:\n   - TSEETC achieved the lowest cumulative regret among all algorithms.\n   - The algorithm's regret slope in log-log plots was close to 0.5, consistent with the theoretical bound, while other methods exhibited linear regret (slope ≈ 1).\n\n---\n\n#### **Context and Related Work**\n1. **Restless Multi-Armed Bandits (RMABs)**:\n   - RMABs are widely applicable in domains like healthcare and communication systems, where arms transition based on action-dependent Markov chains.\n   - Prior work often assumes either known parameters or observable states, which are unrealistic in many practical scenarios.\n\n2. **Comparison to Existing Algorithms**:\n   - **Restless-UCB**: Achieves regret bounds of \\( \\tilde{O}(T^{2/3}) \\), weaker than the theoretical lower bound.\n   - **SEEU and Related Methods**: Address unobserved states but fail to match the lower bound \\( \\tilde{O}(\\sqrt{T}) \\).\n   - **Thompson Sampling Approaches**: Provide better theoretical guarantees but often assume observable states or known parameters.\n\n---\n\n#### **Conclusion and Future Directions**\nThe paper advances the state-of-the-art in restless bandit problems by introducing TSEETC, which achieves near-optimal regret bounds (\\( \\tilde{O}(\\sqrt{T}) \\)) and demonstrates superior empirical performance. The algorithm effectively balances exploration and exploitation in environments with unobserved states, leveraging belief states, Dirichlet-based posterior updates, and episodic strategies.\n\n**Open Questions**:\n- Extending the framework to scenarios where transition functions depend on actions remains an open challenge for future research.",
  "ref": {
    "weaknesses": [
      "Lack of clarity in explaining methodologies and algorithms, leading to confusion.",
      "Insufficient experimental details and lack of comprehensive analysis of results.",
      "Over-reliance on citations without adequate explanation of their relevance or application.",
      "Inconsistencies in notation and definitions, causing misunderstandings.",
      "Failure to clearly differentiate the proposed methods from existing ones."
    ],
    "improvements": [
      "Enhance clarity by providing detailed explanations of methodologies and algorithms.",
      "Include comprehensive experimental details and thorough analysis of results.",
      "Provide clear definitions and consistent use of notations throughout the paper.",
      "Clearly differentiate proposed methods from existing ones to highlight novelty.",
      "Discuss the relevance and application of cited works to improve understanding."
    ]
  },
  "rev": "**1. Summary**  \nThe paper presents TSEETC (Thompson Sampling with Episodic Explore-Then-Commit), a novel algorithm designed to address the online restless bandit problem where states are unobserved, and transition dynamics and reward functions are unknown. The algorithm employs a structured, episodic approach to balance exploration and exploitation, utilizing belief states derived from historical actions and rewards. Theoretical analysis demonstrates that TSEETC achieves a Bayesian regret bound of \\( \\tilde{O}(\\sqrt{T}) \\), which is near-optimal. Empirical results show that TSEETC outperforms existing algorithms in terms of cumulative regret, validating its effectiveness.\n\n**2. Strengths**  \n- The paper addresses a significant and challenging problem in the field of online decision-making under uncertainty, particularly in scenarios with unobserved states.\n- TSEETC's design is innovative, leveraging a combination of belief state updates, Dirichlet-based posterior sampling, and an episodic exploration-exploitation strategy.\n- Theoretical contributions are robust, providing a near-optimal regret bound that advances the state-of-the-art for restless bandit problems.\n- Empirical validation is thorough, demonstrating superior performance of TSEETC compared to existing methods across various scenarios.\n\n**3. Weaknesses**  \n- **Clarity in Methodology**: The explanation of the belief state updates and posterior sampling in Section 3.1 is dense and could benefit from additional clarification. Suggestion: Include a step-by-step example to illustrate the belief state update process.\n- **Experimental Details**: The paper lacks detailed information about the experimental setup, such as specific hyperparameters used for TSEETC and baseline algorithms (Section 5.1). Suggestion: Provide a table summarizing the hyperparameters and their values for reproducibility.\n- **Comparison with Baselines**: While TSEETC is compared with several baselines, the paper does not discuss why certain popular algorithms, such as Thompson Sampling with known states, were not included (Section 5.2). Suggestion: Justify the choice of baselines and consider including additional comparisons.\n- **Notation Consistency**: There are inconsistencies in the notation used for belief states and posterior distributions across Sections 3 and 4. Suggestion: Ensure consistent notation throughout the paper to avoid confusion.\n- **Discussion of Limitations**: The paper does not sufficiently address the limitations of the proposed approach, particularly in scenarios with more complex state dynamics (Section 6). Suggestion: Include a discussion of potential limitations and directions for future work.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**  \n  The paper is generally well-structured, with clear section titles and logical progression of ideas. However, the dense mathematical notation in some sections could be better explained with examples or diagrams. Assumptions and limitations are not explicitly stated, which could improve the reader's understanding of the scope of the work.\n\n  **(b) Figure & Caption Clarity**  \n  Figures effectively illustrate the main claims, with clear axes and legends. However, the captions could be more descriptive to ensure they are self-sufficient. For instance, Figure 2's caption should explain the significance of the regret slope observed.\n\n  **(c) Reproducibility Transparency**  \n  The paper provides a high-level description of the experimental setup but lacks specific details on datasets, hyperparameters, and hardware used. There is no mention of code availability, which is crucial for reproducibility. Including these details and making the code publicly available would enhance the paper's transparency.\n\n**5. Novelty & Significance**  \nThe paper makes a significant contribution to the field of online decision-making by addressing the challenging problem of restless bandits with unobserved states. The introduction of TSEETC, with its novel episodic exploration-exploitation strategy, represents a meaningful advancement over existing methods. The theoretical and empirical results substantiate the claims of improved performance and near-optimal regret bounds. The work is well-motivated and contextualized within the literature, offering new insights and methodologies that are likely to influence future research in this area.",
  "todo": [
    "Revise methodology explanation: Include a step-by-step example to clarify belief state updates and posterior sampling [Section 3.1]",
    "Add experimental details: Provide a table summarizing hyperparameters used for TSEETC and baseline algorithms for reproducibility [Section 5.1]",
    "Justify baseline selection: Explain the choice of baselines and consider including comparisons with Thompson Sampling with known states [Section 5.2]",
    "Ensure notation consistency: Review and correct inconsistencies in notation for belief states and posterior distributions [Sections 3 and 4]",
    "Discuss limitations: Include a discussion of potential limitations of the proposed approach, especially in complex state dynamics [Section 6]",
    "Enhance figure captions: Make Figure 2's caption more descriptive by explaining the significance of the observed regret slope [Page 10, Figure 2]",
    "Improve reproducibility: Provide specific details on datasets, hyperparameters, and hardware used in experiments, and mention code availability [Section 5.1]",
    "Consider code availability: Make the code publicly available to enhance transparency and reproducibility [Section 5.1]"
  ],
  "timestamp": "2025-10-30T15:06:42.913446",
  "manuscript_file": "manuscript.pdf",
  "image_file": "concat_image.png"
}