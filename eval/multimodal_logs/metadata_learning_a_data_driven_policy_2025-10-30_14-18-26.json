{
  "title": "Learning a Data-Driven Policy Network for Pre-Training Automated Feature Engineering",
  "abstract": "Feature engineering is widely acknowledged to be pivotal in tabular data analysis\nand prediction. Automated feature engineering (AutoFE) emerged to automate\nthis process managed by experienced data scientists and engineers conventionally.",
  "summary": "### Overview of FETCH: A Novel Framework for Automated Feature Engineering\n\nThe paper, presented at ICLR 2023, introduces **FETCH (FEature SET Data-Driven Search)**, a groundbreaking framework for automated feature engineering (AutoFE) tailored for tabular data. FETCH addresses key limitations in existing AutoFE methods by adopting a **data-driven Markov Decision Process (MDP)** setup and leveraging reinforcement learning (RL) to iteratively generate and refine features. The framework is inspired by human experts' ability to analyze datasets and transfer knowledge across tasks, offering a more efficient, interpretable, and transferable solution for feature engineering.\n\n---\n\n### Key Contributions\n\n1. **Data-Driven MDP Setup**:\n   - FETCH directly observes and analyzes tabular datasets as the state input to a policy network, mimicking human experts. This contrasts with traditional NAS-like AutoFE methods (e.g., NFS, DIFER), which are \"data-unobserved\" and generate feature engineering (FE) plans without directly analyzing the data.\n\n2. **Transferability**:\n   - FETCH introduces a pre-training paradigm for AutoFE, enabling the policy network to generalize across unseen datasets. By training on diverse datasets, FETCH accumulates knowledge that can be applied to new datasets with minimal exploration, significantly improving efficiency and performance.\n\n3. **Improved Performance**:\n   - Extensive experiments demonstrate that FETCH systematically outperforms state-of-the-art AutoFE methods, achieving superior results in both classification and regression tasks. Its ability to generate higher-order features and optimize feature sets leads to better model fitting and generalization.\n\n4. **Interpretability and Flexibility**:\n   - FETCH generates features with interpretable names based on their construction paths, aiding expert understanding. It also supports variable-length and permutation-invariant tabular data, making it highly adaptable to real-world datasets.\n\n---\n\n### FETCH's Workflow and Architecture\n\n#### **Core Workflow**:\n- FETCH formulates feature engineering as a control problem using RL. The framework employs a policy network to map input data tables to optimal FE actions, iteratively refining these actions to create higher-order features.\n- The RL setup includes:\n  - **State**: The dataset or its bootstrapped version.\n  - **Actions**: Feature transformation operations (e.g., addition, deletion, or transformation of columns).\n  - **Reward Function**: Evaluates the quality of the FE plan based on cross-validation performance metrics (e.g., F1-score).\n\n#### **Policy Network**:\n- Inspired by Transformer models, the policy network includes:\n  - A fully connected network (FCN) to standardize feature vector dimensions.\n  - A multi-head self-attention module (without positional encoding) to capture relationships between features while maintaining permutation invariance.\n  - An action decoder and softmax layer to map feature relationships to action probabilities.\n\n#### **Training with Proximal Policy Optimization (PPO)**:\n- FETCH uses PPO to train the policy network, optimizing feature selection and generation. The reward function combines average cross-validation performance with a penalty term for suboptimal results, improving training stability and sample efficiency.\n\n#### **Pre-Training and Transferability**:\n- FETCH is pre-trained on diverse datasets with varying schemas, row/column sizes, and structures. This enables the model to generalize to unseen datasets, akin to pre-training paradigms in NLP (e.g., BERT) and vision (e.g., SimCLR). Unlike traditional pre-training, FETCH does not require standardized input shapes, making it highly flexible for real-world tabular data.\n\n---\n\n### Experimental Results\n\n#### **Performance Across Datasets**:\n- FETCH was evaluated on 27 datasets (11 regression, 16 classification) from OpenML, UCI, and Kaggle. It outperformed state-of-the-art AutoFE methods (e.g., DIFER, NFS) on 25 out of 27 datasets and ranked second on the remaining two.\n- Example results:\n  - **Airfoil (Regression)**: FETCH scored 0.6463, outperforming DIFER (0.6125) and NFS (0.6226).\n  - **Adult Income (Classification)**: FETCH scored 0.8537, slightly below AutoGluon (0.8738) but above DIFER (0.8584).\n\n#### **Transferability**:\n- FETCH demonstrated significant improvements when pre-trained on diverse datasets:\n  - Pre-trained models achieved higher scores earlier in the training process, with the largest performance gap around 150 epochs.\n  - Example improvements:\n    - **Housing Boston**: 2.31% improvement with pre-trained models.\n    - **OpenML 616**: 4.67% improvement.\n    - **Wine Quality Red**: 2.59% improvement.\n\n#### **Scalability and Efficiency**:\n- FETCH was tested on large datasets (e.g., Medical Charges, Give Me Some Credit, Poker Hand) with over 50K rows. It achieved the highest scores compared to NFS and DIFER within a 2-day runtime.\n- FETCH converged within 100-150 exploration epochs, making it **11x more sample-efficient** than NFS.\n\n#### **Flexibility Across Models**:\n- FETCH improved evaluation scores across various machine learning models (e.g., Logistic Regression, Random Forest, XGBoost), demonstrating its compatibility with different algorithms.\n\n---\n\n### Advantages of FETCH\n\n1. **Human-Like Feature Engineering**:\n   - FETCH bridges the gap between automated and manual feature engineering by directly analyzing datasets and iteratively refining features, emulating human experts.\n\n2. **Higher-Order Feature Generation**:\n   - FETCH supports recursive application of binary operations, enabling the creation of complex features (e.g., BMI as Weight/HeightÂ²).\n\n3. **Interpretability**:\n   - Generated features are named based on their construction paths, aiding expert understanding and real-world applicability.\n\n4. **Real-World Applicability**:\n   - FETCH handles variable-length and permutation-invariant tabular data, eliminating the need for standardized preprocessing.\n\n5. **Transferability**:\n   - FETCH's pre-training capability allows it to generalize across datasets, reducing computational costs and exploration time on new datasets.\n\n---\n\n### Limitations of Existing Methods Addressed by FETCH\n\n1. **Data-Unobserved Setup**:\n   - Traditional NAS-like AutoFE frameworks (e.g., NFS, DIFER) do not directly observe datasets, leading to data-unrelated FE plans. FETCH overcomes this by fully analyzing datasets before proposing FE actions.\n\n2. **Lack of Transferability**:\n   - Existing methods cannot leverage knowledge from prior datasets to accelerate feature engineering on new datasets. FETCH introduces a pre-training paradigm to address this gap.\n\n3. **Feature Dimension Explosion**:\n   - FETCH avoids the feature dimension explosion and computational inefficiency seen in prior methods by iteratively refining feature sets.\n\n---\n\n### Conclusion and Future Directions\n\nFETCH establishes itself as a superior AutoFE framework, excelling in performance, transferability, and flexibility. It outperforms state-of-the-art methods across diverse datasets and tasks, demonstrating its potential for real-world applications. Future work aims to scale FETCH to extremely large datasets and further explore its pre-training capabilities, including potential extensions to multimodal data.\n\nIn summary, FETCH represents a significant advancement in automated feature engineering, offering a novel, interpretable, and transferable solution for tabular data analysis.",
  "ref": {
    "weaknesses": [
      "Lack of clarity in presenting key contributions and their significance.",
      "Over-reliance on combining multiple techniques without a clear emphasis on the primary innovation.",
      "Insufficient explanation of experimental setup or methodology, leading to potential confusion for readers.",
      "Limited discussion on the broader implications or generalizability of the proposed approach.",
      "Presence of minor grammatical and typographical errors, which detract from the overall readability."
    ],
    "improvements": [
      "Provide a clearer and more explicit articulation of the main contributions and their impact on the field.",
      "Include a detailed and transparent explanation of the experimental design and methodology to ensure reproducibility.",
      "Conduct thorough ablation studies to isolate and highlight the significance of individual components in the proposed approach.",
      "Discuss broader implications and potential applications of the work to enhance its relevance and appeal to the community.",
      "Ensure rigorous proofreading to eliminate grammatical and typographical errors, improving the overall presentation quality."
    ]
  },
  "rev": "**1. Summary**  \nThe paper introduces FETCH, a novel framework for automated feature engineering (AutoFE) specifically designed for tabular data. FETCH leverages a data-driven Markov Decision Process (MDP) and reinforcement learning (RL) to iteratively generate and refine features, mimicking human experts' ability to analyze datasets and transfer knowledge across tasks. The framework is notable for its pre-training paradigm, enabling transferability across datasets, and demonstrates superior performance over existing AutoFE methods in extensive experiments. FETCH also emphasizes interpretability and flexibility, supporting variable-length and permutation-invariant data.\n\n**2. Strengths**  \n- **Innovative Approach**: FETCH introduces a novel data-driven MDP setup for AutoFE, which is a significant departure from traditional NAS-like methods that do not directly analyze datasets.\n- **Transferability**: The pre-training paradigm allows FETCH to generalize across datasets, reducing exploration time and computational costs, which is a substantial advancement in the field.\n- **Performance**: The framework consistently outperforms state-of-the-art methods in both classification and regression tasks, as evidenced by experiments on 27 datasets.\n- **Interpretability and Flexibility**: FETCH's ability to generate interpretable feature names and handle variable-length data makes it highly applicable to real-world scenarios.\n\n**3. Weaknesses**  \n- **Clarity in Methodology**: The explanation of the MDP setup and the role of the policy network in Section 3 could be more detailed to enhance understanding. Consider adding a step-by-step walkthrough of the process.\n- **Experimental Setup Details**: The specifics of the experimental setup, such as hyperparameters and hardware configurations, are insufficiently detailed in Section 5. Including these details would improve reproducibility.\n- **Ablation Studies**: Section 6 lacks ablation studies to isolate the impact of individual components, such as the multi-head self-attention module, on the overall performance. Adding these studies would clarify the contribution of each component.\n- **Broader Implications**: The discussion on the broader implications and potential applications of FETCH in Section 7 is limited. Expanding this discussion could enhance the paper's relevance to the community.\n- **Typographical Errors**: Minor grammatical errors are present throughout the paper, such as in the abstract and Section 4. A thorough proofreading would improve the overall presentation quality.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**  \n  The paper is generally well-structured, with clear section titles and logical progression of ideas. However, some sections, particularly the methodology, could benefit from more detailed explanations and examples to aid comprehension. Mathematical notations are well-defined, but assumptions and limitations are not explicitly stated.\n\n  **(b) Figure & Caption Clarity**  \n  Figures are generally clear and support the text well, but some captions, such as those in Figure 3, could be more descriptive to be self-sufficient. Axes and labels are consistent and readable, and diagrams correlate well with the textual descriptions.\n\n  **(c) Reproducibility Transparency**  \n  The paper provides a good overview of the experimental results but lacks detailed information on datasets, hyperparameters, and hardware used. There is no mention of code or data availability, which is crucial for reproducibility. Including these details, along with ablation studies, would significantly enhance transparency.\n\n**5. Novelty & Significance**  \nFETCH addresses a significant gap in automated feature engineering by introducing a data-driven approach that directly analyzes datasets, which is a novel contribution. The framework's ability to generalize across datasets through pre-training is a noteworthy advancement, offering substantial improvements in efficiency and performance. While the paper substantiates its claims with empirical evidence, the lack of detailed methodological explanations and broader implications limits its impact. Nonetheless, FETCH represents a valuable contribution to the field, with potential applications in various domains requiring efficient and interpretable feature engineering.",
  "todo": [
    "Revise methodology explanation: Provide a detailed step-by-step walkthrough of the MDP setup and the role of the policy network [Section 3]",
    "Enhance experimental setup details: Include specifics on hyperparameters and hardware configurations to improve reproducibility [Section 5]",
    "Conduct ablation studies: Isolate the impact of individual components, such as the multi-head self-attention module, on overall performance [Section 6]",
    "Expand discussion on broader implications: Elaborate on potential applications and relevance of FETCH to the community [Section 7]",
    "Proofread for typographical errors: Correct grammatical errors throughout the paper, especially in the abstract and Section 4 [Entire paper]",
    "Improve figure captions: Make captions, particularly for Figure 3, more descriptive to ensure they are self-sufficient [Page 5, Figure 3]",
    "Include reproducibility details: Provide information on datasets, hyperparameters, hardware used, and mention code or data availability [Section 5]",
    "Explicitly state assumptions and limitations: Clarify any assumptions made and discuss limitations of the methodology [Section 3]"
  ],
  "timestamp": "2025-10-30T14:18:26.144717",
  "manuscript_file": "manuscript.pdf",
  "image_file": "concat_image.png"
}