{
  "title": "When and Why Vision-Language Models Behave like Bags-Of-Words, and What to Do About It?",
  "abstract": "Despite the use of large vision and language models (VLMs) in many downstream\napplications, it is unclear how well they encode the compositional relationships\nbetween objects and attributes. Here, we create the Attribution, Relation, and Order\n(ARO) benchmark to systematically evaluate the ability of VLMs to understand\ndifferent types of relationships, attributes, and order information. ARO consists\nofVisual Genome Attribution , to test the understanding of objects’ properties;",
  "summary": "### Structured Overview of the Paper\n\n#### **Introduction and Motivation**\nThe paper, presented at ICLR 2023, investigates the compositional understanding of Vision-Language Models (VLMs), highlighting their significant limitations in encoding relationships, attributes, and order in visual and textual data. Despite their strong performance on tasks like image-text retrieval, VLMs often fail to capture compositional structures, treating captions as unordered \"bags-of-words.\" This deficiency arises from the reliance on contrastive pretraining and retrieval-based evaluations, which do not incentivize learning compositionality.\n\nTo address these challenges, the authors introduce the **Attribution, Relation, and Order (ARO) benchmark**, a large-scale evaluation framework with over 50,000 test cases. ARO enables fine-grained testing of VLMs' ability to understand compositionality, including object properties, relational order, and word order in captions.\n\n---\n\n#### **ARO Benchmark and Tasks**\nThe ARO benchmark comprises four tasks designed to evaluate compositional understanding:\n1. **Visual Genome Attribution**: Tests models' ability to attribute properties correctly (e.g., \"the crouched cat and the open door\" vs. \"the open cat and the crouched door\").\n2. **Visual Genome Relation**: Evaluates relational understanding (e.g., \"the dog is behind the tree\" vs. \"the tree is behind the dog\").\n3. **COCO Order** and **Flickr30k Order**: Assess sensitivity to word order in captions by introducing systematic permutations.\n\nEach test case includes an image, a correct caption, and a swapped caption, with models evaluated on their ability to select the correct caption. The benchmark is significantly larger than prior datasets like Winoground (400 examples), enabling more robust statistical analysis.\n\n---\n\n#### **Key Findings**\n1. **Deficiencies in VLMs**:\n   - State-of-the-art VLMs, including CLIP, BLIP, Flava, and X-VLM, struggle with compositional tasks, particularly relational understanding and word order sensitivity.\n   - Models often perform near or below chance level on relational tasks, indicating a failure to encode relationships and order.\n   - For example, CLIP achieves only 56% accuracy on spatial relations, while Flava performs poorly across most tasks (e.g., 25% accuracy on relations).\n\n2. **Limitations of Current Training and Evaluation**:\n   - Despite being trained on large datasets with rich compositional structures, VLMs excel at retrieval tasks without truly understanding compositionality. Retrieval tasks allow models to rely on shortcuts, bypassing the need to encode relationships, attributes, or order.\n   - Perturbation experiments reveal that models like BLIP and Flava perform poorly when captions or images are shuffled, further highlighting their insensitivity to compositional cues.\n\n---\n\n#### **Proposed Solution: Composition-Aware Hard Negative Mining**\nTo improve VLMs' compositional understanding, the authors propose **composition-aware hard negative mining**, a modification to contrastive learning that introduces challenging negative examples during training:\n1. **Hard Negative Captions**: Generate alternative captions by swapping linguistic elements (e.g., nouns, verbs) to challenge the model's understanding of word order.\n2. **Hard Negative Images**: Identify visually similar images using CLIP and add them as hard negatives to force models to capture fine-grained differences.\n\nThe modified training objective incorporates these hard negatives, computing a similarity matrix that includes both original and negative captions, but excludes loss computation for unmatched negative captions.\n\n---\n\n#### **Experimental Results**\n1. **Compositional Tasks**:\n   - The proposed method, **NegCLIP**, significantly outperforms baseline models on ARO tasks:\n     - VG-Relation: CLIP (59%) → NegCLIP (81%)\n     - VG-Attribution: CLIP (62%) → NegCLIP (71%)\n     - COCO Order: CLIP (46%) → NegCLIP (86%)\n     - Flickr30k Order: CLIP (59%) → NegCLIP (91%)\n\n2. **Downstream Tasks**:\n   - NegCLIP maintains comparable performance to CLIP on standard tasks like image classification (CIFAR-10, CIFAR-100, ImageNet) and retrieval (COCO, Flickr30k), demonstrating that improved compositional understanding does not come at the cost of general performance.\n\n3. **Comparison to Other Models**:\n   - NegCLIP becomes the best-performing model on VG-Relations and is competitive with advanced models like X-VLM and BLIP on VG-Attribution.\n   - XVLM demonstrates strong robustness to text and image shuffling, while Flava underperforms across most tasks.\n\n---\n\n#### **Broader Implications**\n1. **Critique of Retrieval-Based Evaluations**:\n   - High retrieval performance masks deficiencies in compositional and order understanding. Current evaluation metrics fail to surface these limitations, necessitating richer benchmarks like ARO.\n\n2. **Ethical Considerations**:\n   - The study acknowledges concerns about biases and stereotypes in VLMs, as well as the societal implications of their use in applications like surveillance. For example, models like DALL-E misrepresent prompts involving unconventional attributes, defaulting to stereotypes.\n\n3. **Reproducibility**:\n   - The authors provide open-source code and detailed documentation for replicating experiments, ensuring transparency and reproducibility.\n\n---\n\n#### **Contributions**\n1. **ARO Benchmark**: A comprehensive test bed for evaluating VLMs' compositional understanding, addressing limitations of prior datasets like Winoground.\n2. **Critique of Contrastive Pretraining**: The study highlights how current training objectives fail to incentivize compositional learning, calling for alignment between pretraining objectives and compositional understanding.\n3. **NegCLIP**: A practical solution that demonstrates the potential of composition-aware hard negatives to improve VLMs' performance on compositional tasks without sacrificing general capabilities.\n\n---\n\n#### **Conclusion and Future Directions**\nThe study underscores the need for algorithmic improvements, rather than simply scaling dataset size, to enhance VLMs' compositional understanding. Future work should explore:\n- Composition-aware contrastive pretraining.\n- Interactions between pretraining objectives and compositional understanding.\n- Broader evaluations of VLMs to better understand their capabilities and limitations.\n\nBy addressing these challenges, the research aims to advance the development of VLMs that can truly understand and represent compositional structures in vision and language.",
  "ref": {
    "weaknesses": [
      "Lack of clarity in technical explanations and notation, making it difficult for readers to grasp key concepts.",
      "Insufficient discussion on the scalability and applicability of the proposed methods to broader or more practical domains.",
      "Limited exploration of failure cases and performance limitations in experiments.",
      "Presentation issues, such as missing equation numbers and unclear references, which hinder readability and reviewability."
    ],
    "improvements": [
      "Enhance clarity by providing more examples to illustrate theoretical concepts and improve accessibility for readers.",
      "Include a detailed discussion on the scalability of the proposed methods, particularly in terms of computational and memory costs for larger problems.",
      "Expand the analysis of failure cases and limitations to provide a deeper understanding of the model's shortcomings.",
      "Improve the presentation by adding equation numbers, refining notation, and ensuring clear references to material within the paper.",
      "Discuss the broader applicability of the methods to other domains and provide guidelines for adapting the approach to new contexts.",
      "Incorporate comparisons to related work to situate the contributions within the existing literature and inspire future research directions."
    ]
  },
  "rev": "**1. Summary**  \nThe paper introduces the Attribution, Relation, and Order (ARO) benchmark to evaluate the compositional understanding of Vision-Language Models (VLMs), addressing their limitations in encoding relationships, attributes, and order in visual and textual data. The benchmark includes four tasks that test models on visual genome attributions, relations, and word order sensitivity using COCO and Flickr30k datasets. The authors propose a novel training method, composition-aware hard negative mining, to improve VLMs' compositional understanding. The experimental results demonstrate that the proposed NegCLIP model significantly outperforms existing VLMs on compositional tasks while maintaining performance on standard tasks.\n\n**2. Strengths**  \n- The paper addresses a critical gap in the evaluation of VLMs by introducing a comprehensive benchmark, ARO, that focuses on compositional understanding.\n- The proposed method, composition-aware hard negative mining, is a practical and innovative approach to enhancing VLMs' ability to capture compositional structures without sacrificing general performance.\n- The experimental results are robust, showing significant improvements in compositional tasks and maintaining performance on standard benchmarks, demonstrating the effectiveness of the proposed approach.\n- The paper provides open-source code and detailed documentation, ensuring transparency and reproducibility of the experiments.\n\n**3. Weaknesses**  \n- **Clarity in Technical Explanations**: The explanation of the composition-aware hard negative mining method in Section 4 is somewhat dense and could benefit from additional examples or diagrams to enhance understanding. Consider providing a step-by-step illustration of the method.\n- **Scalability Discussion**: The paper lacks a detailed discussion on the scalability of the proposed methods, particularly in terms of computational and memory costs for larger datasets or more complex models (Section 5.3). It would be beneficial to include a scalability analysis or discussion.\n- **Failure Cases and Limitations**: The exploration of failure cases and limitations of the proposed method is limited (Section 6). Expanding this analysis could provide a deeper understanding of the model's shortcomings and guide future improvements.\n- **Presentation Issues**: Some figures, such as Figure 3, have captions that are not fully self-explanatory and could be clarified to better convey the information presented. Consider revising the captions to ensure they are comprehensive and informative.\n- **Broader Applicability**: While the paper focuses on VLMs, it does not discuss the potential applicability of the proposed methods to other domains or types of models (Section 7). Including a discussion on broader applicability could enhance the impact of the work.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**  \n  The paper is generally well-written, with ideas and methods explained logically. However, some sections, particularly those detailing the technical aspects of the proposed method, could benefit from additional clarity and examples. The mathematical notations are well-defined, but the assumptions and limitations could be more explicitly stated.\n\n  **(b) Figure & Caption Clarity**  \n  Figures are generally effective in illustrating the main claims, but some captions, such as those for Figure 3, could be more detailed to ensure they are self-sufficient. Axes, labels, and legends are consistent and readable, and diagrams correlate well with the textual descriptions.\n\n  **(c) Reproducibility Transparency**  \n  The paper provides adequate detail on the experimental setups, including datasets, hyperparameters, and evaluation metrics. The availability of open-source code and detailed documentation enhances reproducibility. However, the paper could benefit from more detailed descriptions of the algorithmic steps and any specific hardware or software requirements.\n\n**5. Novelty & Significance**  \nThe paper makes a significant contribution by addressing a critical gap in the evaluation of VLMs' compositional understanding. The introduction of the ARO benchmark and the novel composition-aware hard negative mining method represent important advancements in the field. The work is well-motivated and contextualized within the existing literature, and the experimental results substantiate the claims made. The significance of the work lies in its potential to improve VLMs' ability to capture compositional structures, which is crucial for their application in real-world scenarios. The paper's contributions are likely to inspire further research in improving the compositional understanding of VLMs and other types of models.",
  "todo": [
    "Revise explanation of composition-aware hard negative mining: Add examples or diagrams for clarity [Section 4]",
    "Expand scalability discussion: Include analysis of computational and memory costs for larger datasets or complex models [Section 5.3]",
    "Explore failure cases and limitations: Provide a deeper analysis to guide future improvements [Section 6]",
    "Revise Figure 3 caption: Ensure it is comprehensive and self-explanatory [Page 5, Figure 3]",
    "Discuss broader applicability: Address potential use of proposed methods in other domains or model types [Section 7]",
    "Enhance textual clarity: Add explicit statements of assumptions and limitations in technical sections [Throughout the paper]",
    "Provide detailed algorithmic steps: Include specific hardware or software requirements for reproducibility [Throughout the paper]"
  ],
  "timestamp": "2025-10-30T12:55:42.811872",
  "manuscript_file": "manuscript.pdf",
  "image_file": "concat_image.png"
}