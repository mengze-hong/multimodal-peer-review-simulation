{
  "title": "Transfer NAS with Meta-learned Bayesian Surrogates",
  "abstract": "While neural architecture search (NAS) is an intensely-researched area, approaches\ntypically still suffer from either (i) high computational costs or (ii) lack of robust-\nness across datasets and experiments. Furthermore, most methods start searching\nfor an optimal architecture from scratch , ignoring prior knowledge. This is in\ncontrast to the manual design process by researchers and engineers that leverage\nprevious deep learning experiences by, e.g., transferring architectures from previ-\nously solved, related problems. We propose to adopt this human design strategy and\nintroduce a novel surrogate for NAS, that is meta-learned across prior architecture\nevaluations across different datasets. We utilize Bayesian Optimization (BO) with\ndeep-kernel Gaussian Processes, graph neural networks for obtaining architecture\nembeddings and a transformer-based dataset encoder. As a result, our method\nconsistently achieves state-of-the-art results on six computer vision datasets, while\nbeing as fast as one-shot NAS methods.\n1 I NTRODUCTION",
  "summary": "### Overview of the Paper\n\nThe paper, presented at ICLR 2023, introduces **Transferrable Neural Architecture Search (TNAS)**, a novel approach to Neural Architecture Search (NAS) that addresses two critical challenges: high computational costs and lack of robustness across datasets. Unlike traditional NAS methods that start from scratch, TNAS leverages prior knowledge through a transfer-based framework, inspired by human designers who adapt architectures from related problems. The method employs **meta-learned Bayesian surrogates** to transfer architecture evaluations across datasets, significantly improving efficiency and robustness.\n\n---\n\n### Key Contributions\n\n1. **Transfer Learning for NAS**:\n   - TNAS treats NAS as a transfer or few-shot learning problem, inspired by manual architecture design.\n   - A meta-learned kernel for Bayesian Optimization (BO) encodes both architecture and dataset information, enabling effective transfer across tasks.\n\n2. **Novel Methodology**:\n   - **Deep-kernel Gaussian Processes (GPs)**: Used for sample-efficient Bayesian Optimization.\n   - **Graph Neural Network (GNN) Encoder**: Encodes neural network architectures, which are naturally represented as graphs.\n   - **Transformer-based Dataset Encoder**: Captures dataset-specific characteristics for context-aware architecture evaluation.\n   - **Joint Encoding**: Combines architecture and dataset encodings via a fully connected neural network, feeding into a deep kernel for the GP surrogate.\n\n3. **Improvements Over Prior Work**:\n   - Addresses limitations of MetaD2A (Lee et al., 2021) by:\n     - Balancing exploration and exploitation through a probabilistic GP surrogate.\n     - Dynamically adapting to new tasks during meta-testing using feedback from function evaluations.\n   - Extends the few-shot Bayesian Optimization framework (Wistuba & Grabocka, 2021) to handle NAS by learning a deep kernel that spans both architecture and dataset spaces.\n\n4. **State-of-the-Art Performance**:\n   - TNAS achieves superior performance on six computer vision benchmarks, outperforming both black-box NAS methods and one-shot NAS methods.\n   - Combines the reliability of black-box optimization with computational efficiency comparable to one-shot approaches.\n\n---\n\n### Methodology\n\n#### Core Components:\n1. **Deep Kernel Learning**:\n   - TNAS employs a trainable kernel function, represented by neural networks, to adapt to the learning task.\n   - This improves over manually designed kernels, which often rely on suboptimal assumptions.\n\n2. **Architecture and Dataset Encoding**:\n   - **Graph Neural Networks (GNNs)**: Encode architectures as directed acyclic graphs (DAGs) using GRU cells for forward and backward traversal.\n   - **Transformer-based Dataset Encoder**: Captures intra-class and inter-class interactions using stacked Set-Transformer layers.\n   - The encodings are merged via a fully connected network to create a joint representation.\n\n3. **Bayesian Optimization (BO)**:\n   - A Gaussian Process (GP) surrogate, conditioned on architecture and dataset encodings, is used for BO.\n   - The Expected Improvement (EI) acquisition function selects the next architecture to evaluate.\n   - During meta-testing, the GP surrogate dynamically adapts to new tasks using feedback from function evaluations.\n\n4. **Meta-Learning Framework**:\n   - The GNN, transformer, and kernel parameters are meta-trained jointly to maximize the marginal likelihood of the GP surrogate.\n   - The REPTILE algorithm is used for parameter updates during meta-training, while stochastic gradient descent (SGD) is employed for optimization.\n\n---\n\n### Experimental Results\n\n#### Datasets and Search Spaces:\n- **Datasets**: CIFAR-10, CIFAR-100, SVHN, Aircraft, Oxford IIT Pets, and MNIST.\n- **Search Spaces**: NAS-Bench-201 and MobileNetV3.\n\n#### Baselines:\n- Compared against Random Search (RS), Bayesian Optimization with GP surrogate (GP-UCB), HEBO, BANANAS, NASBOWL, and MetaD2A.\n\n#### Key Findings:\n1. **Performance**:\n   - TNAS consistently outperforms baselines across datasets and search spaces.\n   - Achieves higher accuracy with fewer GPU hours compared to alternatives.\n   - For example:\n     - On CIFAR-10, TNAS achieves ~94% accuracy faster than other methods.\n     - On MNIST, TNAS reaches ~99.7% accuracy with minimal GPU usage.\n\n2. **Efficiency**:\n   - TNAS achieves search times comparable to one-shot NAS methods while maintaining the reliability of black-box optimization.\n\n3. **Consistency**:\n   - Demonstrates consistent performance across benchmarks, with lower variance in rankings compared to other NAS methods.\n   - Performs well under both small and large computational budgets.\n\n4. **Comparison with MetaD2A**:\n   - TNAS surpasses MetaD2A by:\n     - Balancing exploration and exploitation through a probabilistic GP surrogate.\n     - Adapting to new datasets during meta-testing, while MetaD2A stagnates due to its deterministic predictor.\n\n---\n\n### Ablation Studies\n\n1. **Design Components**:\n   - Removing either the GNN-based architecture encoder or the transformer-based dataset encoder reduces performance, validating the combined design.\n\n2. **Initialization and Meta-Learning**:\n   - Starting with the top-5 architectures from the meta-training set yields better results than random initialization.\n   - Meta-learning the surrogate model significantly improves performance compared to a non-meta-learned version.\n\n---\n\n### Best Practices and Reproducibility\n\nThe authors adhered to NAS best practices (Lindauer & Hutter, 2020):\n1. Released code for the training pipeline, search space, hyperparameters, and random seeds.\n2. Ensured fair comparisons by using the same NAS benchmark, hardware, and evaluation protocols.\n3. Conducted ablation studies and compared performance over time, including random search baselines.\n\n---\n\n### Conclusion and Future Work\n\nTNAS represents a significant advancement in NAS by combining the consistency of black-box optimization with the efficiency of one-shot methods. By leveraging meta-learned deep-kernel Gaussian Processes, TNAS achieves state-of-the-art performance across diverse datasets and search spaces. The method's ability to adapt dynamically to new tasks during meta-testing addresses a key limitation in prior NAS methods.\n\nThe authors highlight the potential of their deep surrogate model to capture interactions between architectures and hyperparameter configurations, an underexplored area in NAS research. Future work could explore further improvements in transferability and scalability, as well as the integration of hyperparameter optimization into the NAS framework.\n\n---\n\n### Acknowledgments and Resources\n\nThe research acknowledges contributions from Hadi Jomaa and funding from the Eva-Mayr-Stihl Stiftung, the Deutsche Forschungsgemeinschaft (DFG, grant 417962828), and the BrainLinks-BrainTools center of excellence. The code is made available at [GitHub](https://github.com/TNAS-DCS/TNAS-DCS), and experimental protocols are detailed in the \"NAS Best Practices Checklist\" in Appendix F.",
  "ref": {
    "weaknesses": [
      "Lack of clarity in explaining the methodological contributions and their generalizability.",
      "Insufficient theoretical grounding or justification for the proposed approach.",
      "Weak experimental design, including limited benchmarks and inadequate baselines for comparison.",
      "Over-reliance on specific datasets or scenarios, reducing the generalizability of the results.",
      "Inadequate exploration of alternative methods or approaches within the same problem space.",
      "Superficial treatment of multi-objective optimization without demonstrating its broader applicability.",
      "Lack of detailed explanation for experimental choices and parameter settings."
    ],
    "improvements": [
      "Provide a clearer and more thorough explanation of the methodology, including its scalability and generalizability to broader contexts.",
      "Incorporate stronger theoretical justifications and references to existing literature to support the proposed approach.",
      "Design experiments with more diverse and challenging benchmarks to demonstrate robustness and generalizability.",
      "Include a wider range of baselines, including state-of-the-art methods, to provide a more comprehensive evaluation.",
      "Explore alternative methods or embeddings to strengthen the comparative analysis.",
      "Demonstrate the applicability of the approach to a broader range of objectives and scenarios, particularly those with less correlated objectives.",
      "Justify experimental choices, such as sample sizes and parameter settings, with clear reasoning and supporting evidence."
    ]
  },
  "rev": "**1. Summary**  \nThe paper introduces Transferrable Neural Architecture Search (TNAS), a novel approach to Neural Architecture Search (NAS) that leverages transfer learning to address high computational costs and lack of robustness across datasets. TNAS employs meta-learned Bayesian surrogates to transfer architecture evaluations across datasets, significantly improving efficiency and robustness. Key components include deep-kernel Gaussian Processes for Bayesian Optimization, a Graph Neural Network encoder for architecture representation, and a Transformer-based dataset encoder. The method demonstrates state-of-the-art performance across six computer vision benchmarks, outperforming both black-box and one-shot NAS methods.\n\n**2. Strengths**  \n- The paper presents a novel approach by integrating transfer learning into NAS, which is a significant advancement over traditional NAS methods.\n- The use of meta-learned Bayesian surrogates and deep-kernel Gaussian Processes is innovative and addresses key limitations in existing methods.\n- The experimental results are robust, showing superior performance across multiple datasets and search spaces, which highlights the method's effectiveness and generalizability.\n- The paper adheres to NAS best practices, providing code and detailed experimental protocols, which enhances reproducibility.\n\n**3. Weaknesses**  \n- **Clarity in Methodology**: The explanation of the joint encoding process in Section 3.2 is somewhat dense and could benefit from additional clarification. A more detailed breakdown of how the architecture and dataset encodings are merged would improve understanding.  \n  *Suggestion*: Include a flow diagram or pseudocode to illustrate the joint encoding process clearly.\n\n- **Theoretical Justification**: While the method is empirically validated, the theoretical underpinnings of the meta-learned kernel for Bayesian Optimization in Section 3.1 could be more thoroughly justified.  \n  *Suggestion*: Provide a more detailed theoretical analysis or reference existing literature that supports the choice of the meta-learned kernel.\n\n- **Baseline Comparisons**: Although the paper compares TNAS with several baselines, the inclusion of more recent state-of-the-art NAS methods would strengthen the evaluation.  \n  *Suggestion*: Expand the experimental section to include comparisons with newer NAS methods that have been published recently.\n\n- **Figure Clarity**: Some figures, such as Figure 4 on page 6, have captions that are not fully descriptive, which might lead to confusion about the results being presented.  \n  *Suggestion*: Revise the captions to be more comprehensive, explaining the significance of each result shown.\n\n- **Ablation Study Detail**: The ablation studies in Section 4.5 are insightful but could be expanded to explore the impact of different hyperparameters on the performance of TNAS.  \n  *Suggestion*: Include additional ablation studies focusing on hyperparameter sensitivity to provide a more comprehensive understanding of the method's robustness.\n\n**4. Clarity & Reproducibility**  \n\n  **(a) Textual Clarity**  \n  The paper is generally well-written, with a logical flow of ideas. However, certain sections, particularly those detailing the methodology, could benefit from clearer explanations and more intuitive descriptions. The mathematical notations are well-defined, but the assumptions and limitations could be more explicitly stated to aid comprehension.\n\n  **(b) Figure & Caption Clarity**  \n  Figures are generally effective in illustrating the main claims, but some captions lack detail, which can hinder understanding. Axes and labels are consistent and readable, but ensuring that all figures are self-sufficient with comprehensive captions would enhance clarity.\n\n  **(c) Reproducibility Transparency**  \n  The paper provides a solid foundation for reproducibility, with detailed descriptions of datasets, search spaces, and experimental setups. The availability of code and adherence to NAS best practices further support reproducibility. However, additional details on hyperparameter settings and training times would be beneficial.\n\n**5. Novelty & Significance**  \nThe paper addresses a significant problem in NAS by introducing a transfer learning approach that enhances efficiency and robustness. The integration of meta-learned Bayesian surrogates and deep-kernel Gaussian Processes is novel and contributes new insights to the field. The work is well-motivated and contextualized within the existing literature, providing a substantial contribution to NAS research. The demonstrated state-of-the-art performance across diverse benchmarks underscores the method's significance and potential impact on the community.",
  "todo": [
    "Clarify joint encoding process: Add a flow diagram or pseudocode to illustrate how architecture and dataset encodings are merged [Section 3.2]",
    "Enhance theoretical justification: Provide a detailed theoretical analysis or reference existing literature to support the choice of the meta-learned kernel for Bayesian Optimization [Section 3.1]",
    "Expand baseline comparisons: Include more recent state-of-the-art NAS methods in the experimental evaluation [Section 4]",
    "Revise Figure 4 caption: Make it more comprehensive by explaining the significance of each result shown [Page 6, Figure 4]",
    "Expand ablation studies: Include additional studies focusing on hyperparameter sensitivity to provide a more comprehensive understanding of TNAS's robustness [Section 4.5]",
    "Improve textual clarity: Provide clearer explanations and more intuitive descriptions in the methodology section, explicitly stating assumptions and limitations [Section 3]",
    "Add details on hyperparameter settings and training times: Enhance reproducibility by providing additional information on these aspects [Section 4 and Appendix F]"
  ],
  "timestamp": "2025-10-30T13:14:48.938963",
  "manuscript_file": "manuscript.pdf",
  "image_file": "concat_image.png"
}