{
  "title": "Learning Group Importance using the Differentiable Hypergeometric Distribution",
  "abstract": "Partitioning a set of elements into subsets of a priori unknown sizes is essential\nin many applications. These subset sizes are rarely explicitly learned - be it the\ncluster sizes in clustering applications or the number of shared versus independent\ngenerative latent factors in weakly-supervised learning. Probability distributions\nover correct combinations of subset sizes are non-differentiable due to hard con-\nstraints, which prohibit gradient-based optimization. In this work, we propose\nthe differentiable hypergeometric distribution. The hypergeometric distribution\nmodels the probability of different group sizes based on their relative importance.",
  "summary": "### Structured Overview of the Paper\n\n#### Introduction\nThe paper, presented at ICLR 2023, introduces the **differentiable hypergeometric distribution**, a novel approach to address challenges in learning subset sizes in machine learning tasks. Traditional methods struggle with the non-differentiable nature of discrete distributions, limiting their integration into gradient-based optimization frameworks. The proposed method overcomes this limitation by enabling reparameterizable gradients for the hypergeometric distribution, which models sampling without replacement. This distribution is widely used in fields such as biology, social science, and computer science.\n\n---\n\n#### Key Contributions\n1. **Differentiable Hypergeometric Distribution**:\n   - The authors extend the hypergeometric distribution to support gradient-based optimization, enabling its use in modern stochastic networks.\n   - A reparameterization trick is introduced, allowing low-variance gradients and efficient integration into learning pipelines.\n\n2. **Applications**:\n   - **Weakly-Supervised Learning**: The method learns the number of shared and independent generative factors between paired images, outperforming heuristic-based approaches.\n   - **Clustering**: By integrating the hypergeometric distribution into a variational clustering algorithm, the method adaptively models the number of samples per cluster, overcoming the i.i.d. assumption and establishing dependencies between dataset samples.\n\n3. **Evaluation**:\n   - The approach was validated using a Kolmogorov-Smirnov (KS) test, showing accuracy comparable to a non-differentiable reference implementation.\n   - It demonstrated superior performance in weakly-supervised learning and clustering tasks compared to prior methods.\n\n---\n\n#### Context and Related Work\nThe paper builds on advancements in differentiable sampling techniques, such as the Gumbel-Softmax trick, which enabled reparameterized gradients for categorical distributions. However, these methods do not extend to more complex distributions like the hypergeometric distribution. Previous uses of the hypergeometric distribution were limited to modeling assumptions or theorem proofs due to its non-differentiable nature. The authors address this gap by introducing a differentiable formulation.\n\n---\n\n#### Technical Details\n1. **Hypergeometric Distribution**:\n   - Models the probability of group sizes based on their relative importance, making it suitable for tasks where the choice of one element affects the probability of others.\n   - The authors focus on Fisher's noncentral hypergeometric distribution, introducing a class importance parameter (\\( \\omega \\)) to model latent factors like importance or fitness of classes.\n\n2. **Reparameterizable Sampling**:\n   - The multivariate distribution is reformulated as a sequence of conditional univariate hypergeometric distributions, scaling linearly with the number of classes (\\( c \\)).\n   - Sampling involves the Gumbel-Softmax trick for differentiability, enabling gradient-based learning.\n\n3. **Sequential Sampling**:\n   - Conditional distributions are modeled as univariate hypergeometric distributions with parameters derived for two groups: the current class and the remaining classes.\n   - Approximation errors due to merging operations are mitigated by adjusting \\( \\omega \\).\n\n4. **Algorithm Overview**:\n   - **Algorithm 1**: Sequentially samples from the differentiable hypergeometric distribution by iterating over classes and reformulating the multivariate distribution into univariate distributions.\n   - **Algorithm 2**: Details the PMF calculation and sampling process using the Gumbel-Softmax trick.\n\n---\n\n#### Experiments and Results\n1. **Kolmogorov-Smirnov (KS) Test**:\n   - The proposed differentiable hypergeometric distribution was compared to a non-differentiable reference implementation.\n   - Results showed small KS test distances and high p-values, confirming the accuracy of the proposed method.\n\n2. **Weakly-Supervised Learning**:\n   - Tested on synthetic mpi3D datasets where pairs of images share generative factors.\n   - Compared three methods:\n     - **LabelVAE**: Assumes the number of independent factors is known.\n     - **AdaVAE**: Uses a heuristic based on KL divergence to infer shared factors.\n     - **HGVAE (proposed)**: Dynamically estimates shared and independent factors using the differentiable hypergeometric distribution.\n   - Results:\n     - HGVAE outperformed baselines in estimating shared factors, achieving lower mean squared error (MSE) and better classification accuracy for independent factors.\n\n3. **Clustering**:\n   - Applied to deep clustering tasks using a variational approach.\n   - The hypergeometric prior improved modeling of cluster sizes, outperforming traditional priors (categorical and uniform) in accuracy, normalized mutual information (NMI), and adjusted Rand index (ARI), especially on imbalanced datasets.\n\n4. **Runtime and Efficiency**:\n   - HGVAE demonstrated competitive runtimes compared to simpler methods like LabelVAE, with minimal computational overhead.\n\n---\n\n#### Analysis and Implications\n1. **Hyperparameter Sensitivity**:\n   - The learning rate was identified as the most critical hyperparameter for stability and convergence.\n   - Temperature annealing for the Gumbel-Softmax trick mitigated issues with vanishing or exploding gradients.\n\n2. **Disentanglement and Representation Quality**:\n   - HGVAE achieved superior disentanglement of latent representations, with better downstream performance in classification tasks.\n\n3. **Generalization**:\n   - HGVAE is not explicitly designed for weakly-supervised learning but achieves results comparable to or better than specialized models.\n\n---\n\n#### Conclusion\nThe differentiable hypergeometric distribution represents a significant advancement in enabling gradient-based optimization for complex probability distributions. It demonstrates robust performance in weakly-supervised learning and clustering tasks, outperforming baseline methods in estimating shared and independent factors, modeling cluster sizes, and achieving better latent representations. The method is computationally efficient, scalable, and applicable to a wide range of machine learning tasks.\n\n---\n\n#### Future Directions\nThe authors suggest exploring applications of the differentiable hypergeometric distribution in fields like biology and social sciences, where sampling without replacement and dependencies between samples are critical. Further work could also investigate extensions to other complex distributions and their integration into deep learning frameworks.\n\n---\n\n#### Reproducibility\nThe paper provides detailed derivations, algorithms, and empirical results. Public datasets and code are available for replication, ensuring transparency and reproducibility of the findings.",
  "ref": {
    "weaknesses": [
      "Lack of clarity in explaining key concepts or assumptions, leading to potential misunderstandings.",
      "Insufficient theoretical justification or grounding for proposed methods or results.",
      "Inadequate experimental comparisons with relevant or state-of-the-art methods in the field.",
      "Over-reliance on specific datasets or examples that may not generalize well to broader contexts.",
      "Ambiguity in the presentation of results, such as unclear visualizations or missing explanations for statistical measures.",
      "Failure to address potential limitations or assumptions of the proposed approach."
    ],
    "improvements": [
      "Provide clearer and more detailed explanations of key concepts, assumptions, and methodologies.",
      "Strengthen theoretical grounding by referencing relevant literature or providing additional justifications for the approach.",
      "Expand experimental validation by including comparisons with a broader range of baseline methods, especially those closely related to the proposed approach.",
      "Incorporate diverse datasets and scenarios to demonstrate the generalizability of the method.",
      "Improve the clarity of result presentations, such as defining statistical measures, confidence intervals, or visual elements in plots.",
      "Discuss limitations and assumptions explicitly to provide a balanced perspective on the method's applicability."
    ]
  },
  "rev": "**1. Summary**  \nThe paper introduces the differentiable hypergeometric distribution, a novel approach that extends the hypergeometric distribution to support gradient-based optimization. This is achieved through a reparameterization trick that allows for low-variance gradients, facilitating its integration into learning pipelines. The method is applied to weakly-supervised learning and clustering tasks, demonstrating superior performance over traditional methods. The approach is validated using a Kolmogorov-Smirnov test, confirming its accuracy, and its applications show improved results in estimating shared factors and modeling cluster sizes.\n\n**2. Strengths**  \n- The paper presents a significant advancement by making the hypergeometric distribution differentiable, which is a notable contribution to the field of machine learning.\n- The proposed method is well-integrated into existing frameworks, allowing for efficient gradient-based learning.\n- The experimental results are robust, demonstrating superior performance in both weakly-supervised learning and clustering tasks.\n- The paper provides comprehensive details on the methodology, including algorithms and theoretical underpinnings, enhancing the understanding of the proposed approach.\n\n**3. Weaknesses**  \n- **Clarity of Key Concepts**: The explanation of the reparameterization trick in Section 3.2 could be clearer. It would be beneficial to include a more intuitive explanation or visualization to aid understanding.\n- **Theoretical Justification**: Section 4.1 lacks a detailed theoretical justification for the choice of the class importance parameter (\\( \\omega \\)). Providing a more thorough explanation or empirical analysis would strengthen the paper.\n- **Experimental Comparisons**: The experiments in Section 5.2 do not include comparisons with some recent state-of-the-art methods in clustering, which could provide a more comprehensive evaluation of the proposed method's performance.\n- **Generalization of Results**: The paper primarily focuses on synthetic datasets for weakly-supervised learning. Including experiments on real-world datasets in Section 5.3 would better demonstrate the generalizability of the method.\n- **Presentation of Results**: The captions for Figures 3 and 4 are somewhat ambiguous and could be expanded to provide more context and explanation of the results shown.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**  \n  The paper is generally well-written, with a logical flow of ideas and clear explanations of methods and claims. However, certain sections, such as the reparameterization trick, could benefit from more intuitive explanations. Mathematical notations are well-defined, but the assumptions and limitations of the approach could be more explicitly stated.\n\n  **(b) Figure & Caption Clarity**  \n  Figures effectively illustrate the main claims, but captions for some figures, such as Figures 3 and 4, could be more detailed to ensure they are self-sufficient. Axes and labels are consistent and readable, but additional context in captions would enhance understanding.\n\n  **(c) Reproducibility Transparency**  \n  The paper provides adequate details on experimental setups, including datasets, hyperparameters, and algorithmic steps. Code and data availability are mentioned, supporting reproducibility. However, more detailed ablation studies on key parameters would further enhance transparency.\n\n**5. Novelty & Significance**  \nThe introduction of a differentiable hypergeometric distribution is a novel contribution that addresses a significant gap in the field. The approach is well-motivated and contextualized within the literature, building on recent advancements in differentiable sampling techniques. The paper substantiates its claims with rigorous experimental validation, demonstrating the method's effectiveness and potential impact. The work contributes new knowledge by enabling gradient-based optimization for complex distributions, offering valuable insights and applications in machine learning. The significance of the work lies in its potential to be applied to a wide range of tasks, particularly those involving sampling without replacement and dependencies between samples.",
  "todo": [
    "Clarify reparameterization trick: Provide a more intuitive explanation or visualization [Section 3.2]",
    "Justify class importance parameter: Add detailed theoretical justification or empirical analysis for the choice of \\( \\omega \\) [Section 4.1]",
    "Expand experimental comparisons: Include comparisons with recent state-of-the-art methods in clustering [Section 5.2]",
    "Demonstrate generalization: Conduct experiments on real-world datasets for weakly-supervised learning [Section 5.3]",
    "Enhance figure captions: Expand captions for Figures 3 and 4 to provide more context and explanation [Page 9, Figures 3 and 4]",
    "State assumptions and limitations: Explicitly state the assumptions and limitations of the approach [Throughout the paper]",
    "Conduct ablation studies: Provide detailed ablation studies on key parameters to enhance transparency [Experimental Section]"
  ],
  "timestamp": "2025-10-30T14:11:20.844019",
  "manuscript_file": "manuscript.pdf",
  "image_file": "concat_image.png"
}