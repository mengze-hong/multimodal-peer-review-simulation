{
  "title": "Scaling Up Probabilistic Circuits by Latent Variable Distillation",
  "abstract": "Probabilistic Circuits (PCs) are a unified framework for tractable probabilistic\nmodels that support efficient computation of various probabilistic queries (e.g.,\nmarginal probabilities). One key challenge is to scale PCs to model large and high-\ndimensional real-world datasets: we observe that as the number of parameters in",
  "summary": "### Structured Overview of the Paper\n\n#### **Introduction and Motivation**\nThe paper, presented at ICLR 2023, addresses the challenge of scaling **Probabilistic Circuits (PCs)** for large, high-dimensional datasets. PCs are a framework for **tractable probabilistic models (TPMs)** that allow efficient computation of probabilistic queries, such as marginals and conditionals. However, their performance plateaus as the number of parameters increases, indicating that existing optimization techniques fail to fully exploit their expressive power. To overcome this limitation, the authors propose **Latent Variable Distillation (LVD)**, a novel method that leverages supervision from **deep generative models (DGMs)** to improve the training of PCs and other probabilistic models like **Hidden Markov Models (HMMs)**.\n\n---\n\n#### **Latent Variable Distillation (LVD): Methodology**\nLVD enhances the training of PCs and HMMs by introducing a systematic approach to materialize and optimize latent variables (LVs). The method involves three key steps:\n\n1. **Materializing Latent Variables (LVs):**\n   - PCs are augmented to explicitly represent latent variables \\( Z \\), creating an augmented model \\( p_{\\text{aug}}(X, Z) \\). This process enforces determinism in sum units, reducing the latent space and simplifying optimization.\n   - Algorithm 1 in the paper provides a systematic way to associate LVs with specific variable scopes in the PC.\n\n2. **Inducing Latent Variable Assignments:**\n   - LVD uses DGMs (e.g., **BERT** for text or **Masked Autoencoders (MAEs)** for images) to generate embeddings for observed data. These embeddings are clustered (e.g., using **K-means**) to assign semantics-aware values to latent variables.\n   - For example, in language modeling, BERT embeddings are clustered to assign latent states in HMMs, while in image modeling, MAEs extract features from image patches for clustering.\n\n3. **Parameter Learning:**\n   - The augmented PC \\( p_{\\text{aug}}(X, Z; \\theta) \\) is trained on an augmented dataset \\( D_{\\text{aug}} = \\{(x^{(i)}, z^{(i)})\\} \\), where \\( z^{(i)} \\) are the induced LV assignments.\n   - Training involves optimizing a lower-bound objective for the log-likelihood of the original PC, followed by fine-tuning on the original dataset.\n\n---\n\n#### **Applications and Experimental Results**\nThe paper demonstrates the effectiveness of LVD across both **image modeling** and **language modeling** tasks, achieving significant improvements in performance and efficiency.\n\n1. **Image Modeling:**\n   - **Datasets:** CIFAR, ImageNet32, and ImageNet64.\n   - **Methodology:** Images are divided into patches, and MAEs are used to extract latent features. These features are clustered to assign categorical latent variables, which are then used to train PCs.\n   - **Results:**\n     - LVD-trained PCs achieve competitive performance in terms of **bits-per-dimension (bpd)**, outperforming TPM baselines like **Hidden Chow-Liu Trees (HCLTs)**, **Einsum Networks (EiNet)**, and **Random Sum-Product Networks (RAT-SPNs)**.\n     - On ImageNet32, LVD achieves 4.39 bpd, better than HCLT (4.82) and EiNet (5.63), and approaches the performance of DGMs like Glow (4.09) and RealNVP (4.28).\n\n2. **Language Modeling:**\n   - **Dataset:** WikiText-2.\n   - **Methodology:** HMMs are trained on token sequences, with BERT embeddings used to assign latent states. The augmented dataset is used to optimize HMM parameters via the Expectation-Maximization (EM) algorithm.\n   - **Results:**\n     - HMMs trained with LVD show significant improvements in **test perplexity**, especially for larger models, compared to random initialization.\n\n3. **Comparison with Deep Generative Models (DGMs):**\n   - LVD-trained PCs achieve competitive results against state-of-the-art DGMs like **variational autoencoders (VAEs)** and **flow-based models** (e.g., Glow, RealNVP), while maintaining tractability.\n\n---\n\n#### **Advantages of LVD**\n1. **Improved Utilization of Large Models:**\n   - LVD enables PCs and HMMs to better exploit their increased capacity as the number of parameters grows, overcoming the performance plateau observed with traditional training methods.\n\n2. **Faster Training:**\n   - By leveraging supervision from latent variable assignments, LVD accelerates the training process. For example, training a 500M-parameter PC on CIFAR with LVD takes ~10 GPU hours, compared to ~1 GPU day with standard optimizers.\n\n3. **Scalability:**\n   - LVD generalizes across different TPMs, including HMMs and PCs, and is computationally efficient due to the conditional independence introduced by materialized LVs.\n\n---\n\n#### **Key Insights and Theoretical Contributions**\n1. **Efficient Parameter Learning:**\n   - Materialized LVs partition observed variables into disjoint subsets, allowing the joint probability \\( p(X, Z) \\) to decompose into independent components. This significantly reduces computational costs.\n\n2. **Theoretical Proofs:**\n   - The paper provides proofs (e.g., Lemma 1) demonstrating the conditional independence properties of PCs, which are leveraged to optimize the augmented model.\n\n3. **Reproducibility:**\n   - The authors provide detailed descriptions of models, training setups, and hyperparameters, along with official implementations on GitHub.\n\n---\n\n#### **Limitations and Future Work**\n1. **Performance Gap on CIFAR:**\n   - While LVD achieves competitive results on ImageNet datasets, it shows a larger performance gap on CIFAR compared to DGMs. The authors attribute this to data inefficiency caused by the independent construction of sub-PCs and suggest parameter tying as a potential solution.\n\n2. **Potential for Further Optimization:**\n   - The most time-intensive step in LVD is training latent-conditioned distributions. Future work could explore parameter sharing or other techniques to further reduce training time.\n\n---\n\n#### **Conclusion**\nLVD represents a significant advancement in scaling probabilistic models for generative tasks. By distilling latent information from DGMs, LVD enhances the training of PCs and HMMs, achieving competitive performance with state-of-the-art DGMs while maintaining tractability. The method is versatile, efficient, and opens new possibilities for integrating tractable probabilistic models with neural networks.\n\nThe code and implementation details are available at [GitHub](https://github.com/UCLA-StarAI/LVD).",
  "ref": {
    "weaknesses": [
      "Lack of detailed explanation and clarity in the methodology and execution of proposed ideas.",
      "Insufficient practical implementation details and unclear public accessibility of methods.",
      "Inadequate literature review and comparison with existing methods, especially in targeted regimes.",
      "Unclear focus and motivation of the study, leading to difficulty in assessing the contribution."
    ],
    "improvements": [
      "Provide a clearer and more detailed explanation of the methodology and execution of ideas.",
      "Include comprehensive practical implementation details and ensure public accessibility of methods.",
      "Expand literature review and comparisons to include relevant existing methods, particularly in targeted regimes.",
      "Clarify the focus and motivation of the study to better highlight its contribution and relevance."
    ]
  },
  "rev": "**1. Summary**  \nThe paper introduces Latent Variable Distillation (LVD), a novel method designed to enhance the training of Probabilistic Circuits (PCs) and Hidden Markov Models (HMMs) by leveraging supervision from deep generative models (DGMs). LVD systematically materializes and optimizes latent variables to improve the scalability and performance of PCs on large, high-dimensional datasets. The approach is validated through experiments in image and language modeling tasks, demonstrating competitive performance against state-of-the-art DGMs while maintaining computational efficiency and tractability.\n\n**2. Strengths**  \n- **Innovative Approach**: The introduction of LVD as a method to leverage DGMs for improving the training of PCs and HMMs is novel and addresses a significant challenge in scaling probabilistic models.\n- **Comprehensive Experimental Validation**: The paper provides extensive experimental results across multiple datasets and tasks, showcasing the effectiveness of LVD in both image and language modeling.\n- **Theoretical Contributions**: The paper includes theoretical proofs that support the conditional independence properties leveraged by LVD, enhancing the scientific rigor of the work.\n- **Practical Relevance**: The method demonstrates improved training efficiency, which is crucial for practical applications involving large-scale data.\n\n**3. Weaknesses**  \n- **Lack of Detailed Explanation in Methodology**: The description of the LVD process, particularly the clustering of embeddings and the assignment of latent variables, could be more detailed for clarity (Section 3.2). Providing a step-by-step explanation or a flowchart could enhance understanding.\n- **Insufficient Baseline Comparisons**: While the paper compares LVD with several TPM baselines, it lacks a comprehensive comparison with more recent or alternative methods in the literature (Table 2). Including additional baselines or discussing why certain methods were excluded would strengthen the evaluation.\n- **Clarity in Figures and Captions**: Some figures, such as Figure 4, could benefit from more descriptive captions and clearer labeling to better convey the results and their implications. Ensuring that all figures are self-explanatory would improve the paper's readability.\n- **Limited Discussion on Limitations**: The paper briefly mentions performance gaps on certain datasets (e.g., CIFAR) but does not thoroughly explore potential reasons or mitigation strategies (Section 7). A more in-depth discussion on limitations and future work could provide valuable insights for readers.\n- **Reproducibility Details**: While the paper mentions the availability of code, it could provide more explicit details on the experimental setup, such as specific hyperparameters, hardware used, and random seeds (Appendix C). This would facilitate easier replication of results by other researchers.\n\n**4. Clarity & Reproducibility**  \n  **(a) Textual Clarity**: The paper is generally well-written, with a clear structure and logical flow of ideas. However, certain sections, particularly those detailing the methodology, could benefit from additional clarity and detail to ensure that readers can fully grasp the proposed approach. Mathematical notations are well-defined, but assumptions and limitations could be more explicitly stated.\n  \n  **(b) Figure & Caption Clarity**: Figures are relevant and support the main claims of the paper, but some captions lack sufficient detail to be fully self-contained. Ensuring that all figures are clearly labeled and that captions provide a comprehensive explanation of the visual content would enhance the paper's clarity.\n  \n  **(c) Reproducibility Transparency**: The paper provides a good overview of the experimental setup, but more detailed information on datasets, hyperparameters, and computational resources would improve reproducibility. The mention of code availability is a positive aspect, but explicit links or references to specific repositories would be beneficial.\n\n**5. Novelty & Significance**  \nThe paper addresses a critical problem in the field of probabilistic modeling by proposing a novel method that effectively scales PCs and HMMs for large datasets. The approach is well-motivated and contextualized within the existing literature, offering a significant contribution to the community by bridging the gap between tractable probabilistic models and deep generative models. The empirical results substantiate the claims, demonstrating that LVD can achieve competitive performance with state-of-the-art DGMs while maintaining tractability. The work is significant as it opens new avenues for integrating probabilistic models with neural networks, offering both theoretical insights and practical benefits.",
  "todo": [
    "Revise methodology description: Provide a more detailed explanation of the LVD process, especially the clustering of embeddings and assignment of latent variables, possibly with a step-by-step guide or flowchart [Section 3.2].",
    "Expand baseline comparisons: Include additional baseline methods or justify the exclusion of certain methods to strengthen the evaluation [Table 2].",
    "Enhance figure clarity: Add more descriptive captions and clearer labeling to figures, particularly Figure 4, to ensure they are self-explanatory [Page 7, Figure 4].",
    "Discuss limitations in depth: Elaborate on the performance gaps observed on datasets like CIFAR and suggest potential mitigation strategies [Section 7].",
    "Improve reproducibility details: Provide explicit details on the experimental setup, including specific hyperparameters, hardware used, and random seeds [Appendix C].",
    "Add explicit links to code: Include direct links or references to specific repositories for easier access to the code [Appendix C].",
    "Clarify assumptions and limitations: Explicitly state any assumptions and limitations in the methodology section to enhance understanding [Throughout the paper]."
  ],
  "timestamp": "2025-10-30T13:11:32.114224",
  "manuscript_file": "manuscript.pdf",
  "image_file": "concat_image.png"
}