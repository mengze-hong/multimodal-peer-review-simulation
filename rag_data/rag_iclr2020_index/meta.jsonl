{"id": "iclr2020_0", "title": "A Stochastic Derivative Free Optimization Method with Momentum | OpenReview", "abstract": "Abstract:###We consider the problem of unconstrained minimization of a smooth objective function in in setting where only function evaluations are possible. We propose and analyze stochastic zeroth-order method with heavy ball momentum. In particular, we propose, SMTP, a momentum version of the stochastic three-point method (STP) Bergou et al. (2019). We show new complexity results for non-convex, convex and strongly convex functions. We test our method on a collection of learning to continuous control tasks on several MuJoCo Todorov et al. (2012) environments with varying difficulty and compare against STP, other state-of-the-art derivative-free optimization algorithms and against policy gradient methods. SMTP significantly outperforms STP and all other methods that we considered in our numerical experiments. Our second contribution is SMTP with importance sampling which we call SMTP_IS. We provide convergence analysis of this method for non-convex, convex and strongly convex objectives.", "review": "Review:###This paper studies the so called problem of derivative-free optimization, which is relevant for cases when the evaluation function is continuous but access to gradients is not possible. The paper improves on top of the stochastic three points method (STP), an existing work (published in arXiv), by proposing adding momentum (SMTP). The intuition behind both STP and SMTP is rather straighforward: you sample a random direction s, then given your current position x you check x+as and x-as. You then move to the best position from (x, x+as, x_as). In a way, this is like computing the numerical derivatives (instead of the gradient) given a random location and its mirror, and then applying gradient descent given the best numerical derivative. However, take this analogy with a large grain of salt, as there are many differences with GD. The proposed algorithm adds momentum and importance sampling. Momentum helps speed up convergence, as the paper shows for non-convex, convex and strongly convex functions. All three cases are individually examined and bounds are derived regarding the speed of convergence. For the non-convex case the speed of convergence is 1/sqrt{K}, K being the number of iterations. For the convex case it is 1/K. For the strongly convex case the (unrealistic) assumption of knowing the optimal value is removed while maintaining the same speed of convergence. Importance sampling helps computing the derivatives focusing on those coordinate dimensions that are more critical to the objective function f(x), improving the speed of convergence further. The importance sampling is proportional to the coordinate-wise Lipschitz constants, assuming that the objective function is coordinate-wise Lipschitz smooth. The methods are validated on five different cases of MuJoCo. Results seem good when compared to the STP ones. Compared to policy gradient methods, the results seem much better. Strengths: + The paper presents a small but interesting and well-motivated addition to the original algorithm STP. I particularly liked how straightforward the final algorithm is: applying momentum and sampling according to the Lipschitz constants. + At least at a first glance the results look good. Compared to STP in figure 1 there is a clear improvement not only in the final optimum but also in the speed of attaining the said optimum. + I liked a lot the presentation and clarity of writing. While quite mathematically dense, it was easy to follow the big story and understand that underlying points. Weaknesses: + While interesting and useful, I am not completely convinced whether the added novelty over (Bergou et al, 2019) is significant enough. At the end of the day, the final algorithm is the conglomeration of two existing algorithms, that is STP and momentum. STP is very similar to the final algorithm, after all it is the basis for it. The authors argue that it is not trivial to select the next points under the momentum term. To this end, they propose to rely on yet another existing approach, that is the virtual iterates analysis from (Yang et al. 2016). However, it is not clear why these points are *optimal*, what is so *non-trivial* about selecting them? This is basically skimmed over in two lines. + In the strongly convex case one assumption (knowing the f(x*) ) is replaced with another assumption, that all points lie on a hypersphere (|s|_2=1). I suppose this would assume a spherical normalization of the input space. While this is not an unrealistic assumption, it does place a constraint which could be problematic in the case of high dimensions for s? In that case the high dimensionality would render distances rather unreliable and in turn could hurt convergence? This is also perhaps the reason that only the MuJoCo enviroments were tested? In general, I would say that the strongly convex case was discussed less clearly and it is not exactly clear the final result. In the end, eq (25) does contain f(x*), whereas in the convex case K does not (K approx 2 R_0^2 L ?_D/(??_D^2). + Some statements are unclear. ++ In p. 2 some symbols are not explained, e.g., ?. While it is quite clear for peopled versed in the field, in my opinion it is bad practice to leave notation not explained. ++ In assumption 3.1 seems rather trivial? Wouldn*t ?_D by definition be always positive, since is the expectation of a squared norm (always positive)? Does this need to be an assumption? ++ Between eq. (11) and (12) there is reference to (35)? What is (35)? ++ It is not clear in practice how the importance sampling is performed. In Algorithm 2 the probabilities p_i are defined as function inputs and then never updated. Is that true? If yes, how is p_i decided in the first place? What is the connection to the Lipschitz constants L_i? + A highly relevant field appears to be Bayesian Optimization, where also one cannot compute gradients and must optimize a black-box function. Some relevant recent works are [1] and [2] for continuous and discrete inputs. It would be interesting to discuss what are the distinct differences with bayesian optimization methods in [1] and [2]. + I would say that the paper is rather on the light side regarding experiments. Only MuJoCo is used as an experimental setup. It would be nice to also report results on synthetic experiments with known functions to better understand the limitations of the algorithm. Synthetic and realistic setups can be found in [1] and [2]. What is more, the experimental choices are not entirely clear. What is the *predefined reward threshold* and why was that chosen? For instance, the leaderboard for *Swimmer* is in: https://www.endtoend.ai/envs/gym/mujoco/swimmer/. How does the proposed algorithm fair compared to these works? Also, *maybe* it would be interesting to compare even against [1] or [2] (I guess [2] is harder as it is for discete inputs), assuming that a relatively low number of iterations is performed. [1] BOCK: Bayesian Optimization with Cylindrical Kernels, C. Oh, E. Gavves, M. Welling, ICML 2018 [2] BOCS: Bayesian Optimization of Combinatorial Structures, R. Baptista, M. Poloczek, ICML 2018"}
{"id": "iclr2020_1", "title": "Higher-Order Function Networks for Learning Composable 3D Object Representations | OpenReview", "abstract": "Abstract:###We present a new approach to 3D object representation where the geometry of an object is encoded directly into the weights and biases of a second *mapping* network. This mapping network can be used to reconstruct an object by applying its encoded transformation to points randomly sampled from a simple geometric space, such as the unit sphere. Next, we extend this concept to enable the composition of multiple mapping functions. This capability provides a method for mixing features of different objects through function composition in a latent function space. Our experiments examine the effectiveness of our method on a subset of the ShapeNet dataset. We find that this representation can reconstruct objects with accuracy equal to or exceeding state-of-the-art methods with orders of magnitude fewer parameters. Our smallest reconstruction network has only about 7000 parameters and shows reconstruction quality on par with state-of-the-art object representation architectures with millions of parameters.", "review": " Summary: This paper describes a contextual encoding scheme for reconstruction of 3D pointclouds from 2D images. An encoder outputs the parameters of a hierarchy of reconstruction networks that can be applied in succession to map random samples on a unit sphere to the surface of the reconstructed shape. Strengths: The author*s model was quite novel in my opinion. Deep 2D->3D is becoming a crowded space and there are many other models that encode image inputs, and many others that perform recursive or composition-based decoding. However, the particular link here was interesting, and I appreciate the small number of parameters resulting in solid reconstruction performance. While most related work was covered well, I believe the authors could have a more up-to-date list of recent work that reconstructs triangle-mesh representations from images [A-C] (especially since several of these methods has an architecture that involves encoding and subsequent compositional refinement). Some of the reconstructions shown in this paper are quite impressive, and the quantitative results show outperforming 2 recent methods. I did appreciate also the novel path-based evaluation of shape accuracy in the Appendix, although it would have been helpful to see more discussion of this in the main paper. Areas for improvement: I found that the core technical description was quite brief and would have benefited from simply more detail and space. You have argued that your method is sensible to try (cog. sci motivations), and shown that one instance works, but what can we expect in a more mathematical or general sense? Can any sizes of encoder and mapping network fit together? How does the number of mapping layers effect performance? Won*t we eventually expect vanishing/exploding gradients with particular activation and can one address this in some way? I note that recent papers in this field tend to perform significantly more extensive experimental evaluation, typically selecting a wider range of competitors and using a number of more standardized metrics including IOU, F1 score and CD and typically repeating these at a variety of resolutions or on additional datasets or category splits etc. Decision: Weak reject because the idea is quite interesting, but I believe a more thorough explanation and expanded experimental comparison would be of great help to ensure the community can appreciate this work. Additional citations suggested: [A] Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images. Wang, Zhang, Li, Fu, Liu and Jiang. ECCV 2018. [B] MeshCNN: A Network with an Edge. Hanocka, Hertz, Fish, Giryes, Fleishman and Cohen-Or. SIGGRAPH 2019. [C] GEOMetrics: Exploiting Structure for Graph-Encoded Objects. Smith, Fujimoto, Romero and Meger. ICML 2019."}
{"id": "iclr2020_2", "title": "Deep probabilistic subsampling for task-adaptive compressed sensing | OpenReview", "abstract": "Abstract:###The field of deep learning is commonly concerned with optimizing predictive models using large pre-acquired datasets of densely sampled datapoints or signals. In this work, we demonstrate that the deep learning paradigm can be extended to incorporate a subsampling scheme that is jointly optimized under a desired minimum sample rate. We present Deep Probabilistic Subsampling (DPS), a widely applicable framework for task-adaptive compressed sensing that enables end-to end optimization of an optimal subset of signal samples with a subsequent model that performs a required task. We demonstrate strong performance on reconstruction and classification tasks of a toy dataset, MNIST, and CIFAR10 under stringent subsampling rates in both the pixel and the spatial frequency domain. Due to the task-agnostic nature of the framework, DPS is directly applicable to all real-world domains that benefit from sample rate reduction.", "review": "Review:###The paper proposes a learning-based adaptive compressed sensing framework in which both the sampling and the task functions (e.g., classification) are learned jointly end-to-end. The main contribution includes using the Gumbel-softmax trick to relax categorical distributions and use back-propagation to estimate the gradient jointly with the tas neural network. The proposed solution has the flexibility of able to be used in several different tasks, such as inverse problems ( super-resolution or image completion) or classification tasks. The paper is very well written. The paper locates itself well in current baselines and explains Experiments mostly well. However, there are significant limitations in demonstrating the effectiveness/impact of the proposed technique: 1) The only comparison to another non-fixed sampling baseline is Kool et al. 2019. The visualization and a thorough comparison were missing in MNIST classification. This baseline was also missing in image reconstruction. 2) Compressive Sensing incorporates vast literature of algorithms focusing on different aspects of improvements; algorithms focused on classification and inverse problems. Even if done disjointly, how does the proposed joint learning is compared to those algorithms in these domains? 3) Top row of Figure 3 nicely explains how the learned sampling paradigm performs compared to other mechanisms (such as uniform, random, low-pass). But there is no comparision against other non-fixed techniques."}
{"id": "iclr2020_3", "title": "PatchFormer: A neural architecture for self-supervised representation learning on images | OpenReview", "abstract": "Abstract:###Learning rich representations from predictive learning without labels has been a longstanding challenge in the field of machine learning. Generative pre-training has so far not been as successful as contrastive methods in modeling representations of raw images. In this paper, we propose a neural architecture for self-supervised representation learning on raw images called the PatchFormer which learns to model spatial dependencies across patches in a raw image. Our method learns to model the conditional probability distribution of missing patches given the context of surrounding patches. We evaluate the utility of the learned representations by fine-tuning the pre-trained model on low data-regime classification tasks. Specifically, we benchmark our model on semi-supervised ImageNet classification which has become a popular benchmark recently for semi-supervised and self-supervised learning methods. Our model is able to achieve 30.3% and 65.5% top-1 accuracies when trained only using 1% and 10% of the labels on ImageNet showing the promise for generative pre-training methods.", "review": "Review:###This paper attempts unsupervised representation learning, via a patch prediction task on ImageNet. The paper is sparse on details, but the method appears to be: (1) split the image into non-overlapping visible and masked patches, (2) from features extracted from the visible patches, predict the masked patches. Rather than predict RGB, they choose to predict 2-bit grayscale images. Also, rather than use the full patches, they use random crops of the input ones, and a center crop of the output ones. The paper seems to be an early draft of something bigger, submitted with the hope of getting some feedback. The method description is mostly composed of tiny details, such as the number and sizes of the patches; I recommend rewriting this to focus on the big idea first, and pack the details into another sub section like *Implementation Details*. The paper barely includes any evaluation. Also, the method does not appear to be very novel: I recommend the authors look at and compare against *Unsupervised Visual Representation Learning by Context Prediction* (ICCV 2015), which is conceptually very similar. The evaluation right now is not good. *Unknown* is not a valid point of comparison. I understand the code for CPC++ might not be released yet, but the authors could at least implement their best approximation of it, and also find older works (which CPC compared against in their paper), to fill out the results and make a convincing argument. In Table 2, the proposed model performs worse than CPC++, yet its values are bolded anyway. Please only put the best-performing result in bold."}
{"id": "iclr2020_4", "title": "Yet another but more efficient black-box adversarial attack: tiling and evolution strategies | OpenReview", "abstract": "Abstract:###We introduce a new black-box attack achieving state of the art performances. Our approach is based on a new objective function, borrowing ideas from -white box attacks, and particularly designed to fit derivative-free optimization requirements. It only requires to have access to the logits of the classifier without any other information which is a more realistic scenario. Not only we introduce a new objective function, we extend previous works on black box adversarial attacks to a larger spectrum of evolution strategies and other derivative-free optimization methods. We also highlight a new intriguing property that deep neural networks are not robust to single shot tiled attacks. Our models achieve, with a budget limited to queries, results up to of success rate against InceptionV3 classifier with queries to the network on average in the untargeted attacks setting, which is an improvement by queries of the current state of the art. In the targeted setting, we are able to reach, with a limited budget of , of success rate with a budget of queries on average, i.e. we need queries less than the current state of the art.", "review": "Review:###This paper proposes a black box adversarial attacks to deep neural networks. The proposed approaches consist of tiling technique proposed by Ilyas et al (2018) and derivative free approaches. The proposed approaches have been applied to targeted and untargeted adversarial attacks against modern neural network architectures such as VGG16, ResNet50, and InceptionV3 trained on ImageNet and CIFAR10 datasets. Experimental results show higher attack success rate with a smaller number of queries. The experimental results look quite promising, i.e., revealing the vulnerability of the deep neural network against black-box adversarial attacks. A possible weakness in the experimental design is that the authors haven*t apply any defense methodology to the classification models to be attacked. Yet the results are promising. From the viewpoint of technical soundness, the approach is a simple combination of the existing approaches. The tiling technique is used in Ilyas et al (2018) combined with a bandit approach. The current paper simply replaces the bandit with evolution strategies. The introduction of the evolution strategies is motivated by their good performance as a zeroth order optimization algorithm. A small novelty appears in a way to handle a bounded search space. The authors claim that many DFO algorithms are designed for unbounded real search space and need some constraint handling. The authors proposed two ways of transforming the bounded search space to the unbounded real search space. However, there must be existing approaches for this type fo constraint (rectangle constraint) in DFO settings. I can not list such approaches here as there are huge number of papers addressing the constraint of this type. There is not enough discussion in the paper why these two proposed approaches are promising. Formulation (2) makes the problem ill-posed and technically the optimal point may not exist. Formulation (3) with softmax representation makes the optimization problem noisy, hence it may annoy the optimizer. Nonetheless, I believe the combination of these constraint handling technique and evolutionary approaches are not new. Some minor comments / questions below: P5: How are the original images to be attacked selected for Fig 2? P6: *we highlight that neural neural networks are not robust to l? tiled random noise. * Isn*t it the contribution of (Ilyas et al., 2018b)? P7: What are the number of queries in Figure 3 and Table 1? Are they the number of queries spent until these algorithms found an adversarial example which is categorized to a wrong class for the first time?"}
{"id": "iclr2020_5", "title": "Permutation Equivariant Models for Compositional Generalization in Language | OpenReview", "abstract": "Abstract:###Humans understand novel sentences by composing meanings and roles of core language components. In contrast, neural network models for natural language modeling fail when such compositional generalization is required. The main contribution of this paper is to hypothesize that language compositionality is a form of group-equivariance. Based on this hypothesis, we propose a set of tools for constructing equivariant sequence-to-sequence models. Throughout a variety of experiments on the SCAN tasks, we analyze the behavior of existing models under the lens of equivariance, and demonstrate that our equivariant architecture is able to achieve the type compositional generalization required in human language understanding.", "review": "Review:###This work focuses on learning equivariant representations and functions over input/output words for the purposes of SCAN task. Basically, the focus is on local equivariances (over vocabulary) such that the effect of replacing and action verb like RUN in the input with the verb JUMP causes a similar change in the output. However, effects requiring global equivariances like learning relationship between *twice* and *thrice*, or learning relationships between different kinds of conjunctions are not handled in this work. For learning equivariant functions over vocabulary, group convolutions are used at each step over vocabulary items in both the sequence encoder and decoder. The results on SCAN task are impressive for verb replacement based experiments and improve over other relevant baselines. Also, improvement is shown on another word replacement task (*around right*), which requires learning corresponding substitutions in output based on the word changes in the input. As expected, for experiments that require global equivariances or no equivariance (simple, length), the difference ion performance is not very pronounced over other baselines. While this paper does show that modelling effects of word substitution can be handled by the locally equivariant functions, it still cannot account for more complex generalization phenomena which are likely to be much more prevalent especially for domains dealing with natural language that are other than SCAN. Therefor, I think the applicability of the proposed equivariant architectures is rather limited if interesting."}
{"id": "iclr2020_6", "title": "Uncertainty-aware Variational-Recurrent Imputation Network for Clinical Time Series | OpenReview", "abstract": "Abstract:###Electronic Health Records (EHR) comprise of longitudinal clinical observations portrayed with sparsity, irregularity, and high-dimensionality which become the major obstacles in drawing reliable downstream outcome. Despite greatly numbers of imputation methods are being proposed to tackle these issues, most of the existing methods ignore correlated features or temporal dynamics and entirely put aside the uncertainty. In particular, since the missing values estimates have the risk of being imprecise, it motivates us to pay attention to reliable and less certain information differently. In this work, we propose a novel variational-recurrent imputation network (V-RIN), which unified imputation and prediction network, by taking into account the correlated features, temporal dynamics, and further utilizing the uncertainty to alleviate the risk of biased missing values estimates. Specifically, we leverage the deep generative model to estimate the missing values based on the distribution among variables and a recurrent imputation network to exploit the temporal relations in conjunction with utilization of the uncertainty. We validated the effectiveness of our proposed model with publicly available real-world EHR dataset, PhysioNet Challenge 2012, and compared the results with other state-of-the-art competing methods in the literature.", "review": "Review:###The paper deals with the problem of missing data imputation in multivariate time series and subsequent classification of the imputed time series. It combines a time-step-wise imputation step using a VAE with a full-time-series imputation using an RNN. The RNN uses special GRU cells that take the missigness pattern and the output variance of the VAE into account. Generally, I believe the idea has conceptual merit, but the empirical evaluation is not sufficient to finally judge its practical value. Evaluations of the actual imputations, experiments on simple benchmark tasks, and more thorough ablation studies of different parts of the model would greatly strengthen this work. MAJOR COMMENTS -------------- - It seems plausible that the prediction loss (L_pred) and the imputation losses (L_reg, L_vae) do not always favor the same solution. It could for instance happen that a *wrong* imputation of the time series (compared to the ground truth) would be easier to classify than the correct imputation. Can we be sure that this is not happening here? It would be useful to actually evaluate the quality of the imputations themselves on held-out data with and without L_pred in the loss function. - In Figure 3, it looks like the lowest value for alpha almost consistently outperforms the other values. Does this suggest that the VAE might actually not be useful in this architecture? Section 4.3.1 says that the performance degrades with an increasing beta parameter and that the VAE loss is thus still important. I don*t think this conclusion is fully supported by the data, since it does not take the prediction loss (L_pred) into account. Increasing beta may well degrade the prediction performance because it weakens the influence of L_pred on the total loss, without any knowledge about the influence of L_vae. - Looking at the error bars in Table 2, I don*t think the conclusion that the full V-RIN model poses a *significant enhancement* over the other one is statistically supported. The confidence intervals seem to be well overlapping. Looking more closely, they actually also seem to overlap with some of the RITS and BRITS models on some measures. Those values should probably also be printed in bold face and the claim in the text should be respectively weakened. MINOR COMMENTS -------------- - The grammar and orthography should be checked here and there. - The reconstruction loss in the VAE-ELBO (Eq. 9) seems to encourage hat{x} to be close to \tilde{x}. But \tilde{x} contains zeros in place of all the missing features. Would this not encourage the VAE to just also put zeros there and therefore lead to _x0008_ar{x} being roughly equal to \tilde{x}? Maybe the hat{x} could actually be shown in the experiments to give an intuition for whether or not this is happening. - While I agree that medical time series are a challenging task to tackle and appreciate the usage of real data in the experiments, I think some aspects of the model could be more easily studied on some simpler benchmark task, where for instance the imputations could be visualized and ground-truth data for the missing values could be used for evaluation."}
{"id": "iclr2020_7", "title": "Understanding Isomorphism Bias in Graph Data Sets | OpenReview", "abstract": "Abstract:###In recent years there has been a rapid increase in classification methods on graph structured data. Both in graph kernels and graph neural networks, one of the implicit assumptions of successful state-of-the-art models was that incorporating graph isomorphism features into the architecture leads to better empirical performance. However, as we discover in this work, commonly used data sets for graph classification have repeating instances which cause the problem of isomorphism bias, i.e. artificially increasing the accuracy of the models by memorizing target information from the training set. This prevents fair competition of the algorithms and raises a question of the validity of the obtained results. We analyze 54 data sets, previously extensively used for graph-related tasks, on the existence of isomorphism bias, give a set of recommendations to machine learning practitioners to properly set up their models, and open source new data sets for the future experiments.", "review": " The paper is concerned with the presence of isomorphism bias in commonly used graph learning benchmarks. In particular, the paper analyzes the amount of isomorphic graphs in 54 graph datasets and evaluates the performance of three graph classification methods under two isomorphism settings. Careful analyses of commonly used benchmarks can be important contributions that provide new insights into the performance of state-of-the-art models. The present paper*s results on graph isomorphism properties can indeed be valuable for the ablation of models and testing their performance with regard to this property. I also found the relatively high label disagreements on some datasets (even under stronger isomorphism constraints) to be a surprising and useful result. However, the main assumption of the paper -- which equates the quality of a graph learning benchmark with the amount of isomorphic graphs that it contains, i.e., the lower the better -- seems questionable. The paper argues that isomorphic graphs are akin to duplicate images in computer vision and should be removed from a dataset. While completely identical graphs are certainly problematic, the case seems different for isomorphic graphs. In the latter, a learning method is required to identify the correct bijection form V_1 to V_2 which is a non-trivial task. Testing on isomorphic graphs evaluates the ability of a model to infer these equivalence classes from data which is an important property. Moreover, being able to capture the equivalence relation can be important for various graph learning tasks, e.g., to facilitate that two topologically equivalent graphs are be classified similarly. Going back to the computer vision analogy: it seems a more adequate comparison for graph isomorphism would be translation and scale invariance which are certainly desirable properties for CV models. In addition, the dataset analysis could also be improved. For instance, the SYNTHETIC dataset includes continuous node attributes that are essential for classification and make the graphs non-isomorphic (when considering, for instance, each attribute vector as a unique node label). However, the attributes are not considered in the analysis what leads to a large number of isomorphic graphs. On a side note: the paper also incorrectly attributes the SYNTHETIC dataset to (Morris et al, 2016), but it is in fact from [1]. The synthetic dataset of Morris et al (SYNTHIE) does not consist of isomorphic graphs, while the SYNTHETIC dataset of [1] does so intentionally. The results of Section 5 seem also not very surprising: After removing node labels, it is expected that the number of isomorphic graphs increases since a discriminating feature has been removed. Moreover, when accounting for node labels, many standard benchmarks seem to consist of significantly less isomorphic and mismatched graphs (as can be seen in the appendix). Since graph isomorphism != graph identity, the assumption Y_iso sub Y_train in Property 6 seems also not appropriate. The results of Theorem 6.1 on the other hand seems straightforward and would hold for any classification task for which the true label for an equivalence class of instances is known."}
{"id": "iclr2020_8", "title": "Optimizing Data Usage via Differentiable Rewards | OpenReview", "abstract": "Abstract:###To acquire a new skill, humans learn better and faster if a tutor, based on their current knowledge level, informs them of how much attention they should pay to particular content or practice problems. Similarly, a machine learning model could potentially be trained better with a scorer that “adapts” to its current learning state and estimates the importance of each training data instance. Training such an adaptive scorer efficiently is a challenging problem; in order to precisely quantify the effect of a data instance at a given time during the training, it is typically necessary to first complete the entire training process. To efficiently optimize data usage, we propose a reinforcement learning approach called Differentiable Data Selection (DDS). In DDS, we formulate a scorer network as a learnable function of the training data, which can be efficiently updated along with the main model being trained. Specifically, DDS updates the scorer with an intuitive reward signal: it should up-weigh the data that has a similar gradient with a dev set upon which we would finally like to perform well. Without significant computing overhead, DDS delivers strong and consistent improvements over several strong baselines on two very different tasks of machine translation and image classification.", "review": "Review:###Summary: This paper introduces a simple idea to optimize the weights of a weighted empirical training distributions. The goal is to optimize the population risk, and the idea is to optimize a distribution over the training examples to maximize the cosine similarity between training set gradients and validation set gradients. The distribution over the training set is parameterized by a neural network taking as arguments the Strengths: - The method is quite simple. - The results appear to be strong, although I am less familiar with the NMT baselines. The imagenet results seem quite strong to me. Weaknesses: - I couldn*t find a particularly clear description of the scoring networks architecture. Given that it observes the whole dataset, this seems like a critical choice that could have a big impact on the complexity of this approach. At the very least, this should be clearly reported, and I recommend a more thorough investigation of this choice. - The authors report that their method takes 1.5x to 2x longer to run than the uniform baseline. Yet, they ran all methods for the same number of steps / epochs. It seems to me that a fairer comparison might be letting all methods enjoy the same total budget measure roughly by wall time. Questions: - I didn*t follow why the computation of the per example gradient grad l(x_i, y_i, theta_t-1) is so onerous. Isn*t that computed on line 5 already?"}
{"id": "iclr2020_9", "title": "Parameterized Action Reinforcement Learning for Inverted Index Match Plan Generation | OpenReview", "abstract": "Abstract:###Match plan generation in the inverted index at Microsoft Bing is used to be based on hand-crafted rules. We formulate the generation process as a Parameterized Action MDP with sharing parameters and purpose a reinforcement learning algorithm on such formulation. We combine deterministic policy learning on discrete and continuous action spaces and several recent advances in deep reinforcement learning. For exploring in the parameterized action space, the agent outputs softmax values for discrete actions and applies Parameter Space Noise on policy network to unify the exploration direction in both spaces. We apply prioritized recurrent replay on match plan sequences and pad short match plans. We also use invertible value function rescaling and n-step return to stabilize the training. The agent is evaluated on our environment and some benchmarks. It outperforms the well-designed production match plan and beats the baselines on the benchmarks.", "review": "Review:###This paper presents a parameterized action RL match plan generation method which extends the plan generation to the general case without any predefined knowledge. Key to address the problem are normalized softmax values of discrete actions to enable gradients backpropagation, parameter space noise on parameters of the policy for unifying the exploration direction in both discrete and continuous spaces, and recurrent deterministic policies with prioritized replay buffer to accelerate and stabilize the training. The paper is well written. Strength: This paper applied RL method on match plan generation. It shows superior performance than manual methods. Weakness: Although the experimental results are exciting, the method itself is nothing novel. I do believe this is a good paper but not sure it is the best fit to this conference. I am not an expert in this specific area. I*ll leave the the decisions to other reviewers and ACs."}
{"id": "iclr2020_10", "title": "Incorporating Horizontal Connections in Convolution by Spatial Shuffling | OpenReview", "abstract": "Abstract:###Convolutional Neural Networks (CNNs) are composed of multiple convolution layers and show elegant performance in vision tasks. The design of the regular convolution is based on the Receptive Field (RF) where the information within a specific region is processed. In the view of the regular convolution*s RF, the outputs of neurons in lower layers with smaller RF are bundled to create neurons in higher layers with larger RF. As a result, the neurons in high layers are able to capture the global context even though the neurons in low layers only see the local information. However, in lower layers of the biological brain, the information outside of the RF changes the properties of neurons. In this work, we extend the regular convolution and propose spatially shuffled convolution (ss convolution). In ss convolution, the regular convolution is able to use the information outside of its RF by spatial shuffling which is a simple and lightweight operation. We perform experiments on CIFAR-10 and ImageNet-1k dataset, and show that ss convolution improves the classification performance across various CNNs.", "review": "Review:###This paper proposes a simple “spatial shuffling” operation for modifying CNNs based on a permutation matrix created at initialization time. The approach is motivate by a very high-level discussion of biological brains. Improvements are claimed on Cifar10 but results are not near the state of the art. The results do seem to improve incrementally over the previous vanilla results on the particular architecures on which they are applied. The authors dedicate some space to a qualitative analysis of why the models improve although it is at best intuition-y. In the end I’m left with inconclusive results, a weakly motivated story, and a paper that despite exceeding the page limit by a page lacks information density. Minor: Convolutional neural networks achieve … “elegant performance in computer vision tasks” >>> wrong adjective."}
{"id": "iclr2020_11", "title": "Learning Heuristics for Quantified Boolean Formulas through Reinforcement Learning | OpenReview", "abstract": "Abstract:###We demonstrate how to learn efficient heuristics for automated reasoning algorithms for quantified Boolean formulas through deep reinforcement learning. We focus on a backtracking search algorithm, which can already solve formulas of impressive size - up to hundreds of thousands of variables. The main challenge is to find a representation of these formulas that lends itself to making predictions in a scalable way. For a family of challenging problems, we learned a heuristic that solves significantly more formulas compared to the existing handwritten heuristics.", "review": "Review:###This paper proposed a GNN to improve an existing search based solver for 2-QBF solvers. The neural network is being used to predict the next steps in the search procedure, in this case, assignment to literals of the 2-QBF formula. Justifiably, a reinforcement learning formulation is used. I thought that the paper was very impressive. All necessary concepts were clearly introduced, the claims were very clear and thoroughly validated. Limitations of the current approach are also properly discussed. Only comment I have is that using shallow networks with one iteration sounds not enough for a problem like 2-QBF solving. I noticed that you have this exploration in the appendix, would recommend moving it to the main paper. I would also have liked 2-QBF to be mentioned more explicitly in the abstract and early introduction. Minor errors: *and* --> *an* in Sec 1 *variale* --> *variable* in Sec 4.2 Reference ? in Sec 6"}
{"id": "iclr2020_12", "title": "Tensor Decompositions for Temporal Knowledge Base Completion | OpenReview", "abstract": "Abstract:###Most algorithms for representation learning and link prediction in relational data have been designed for static data. However, the data they are applied to usually evolves with time, such as friend graphs in social networks or user interactions with items in recommender systems. This is also the case for knowledge bases, which contain facts such as (US, has president, B. Obama, [2009-2017]) that are valid only at certain points in time. For the problem of link prediction under temporal constraints, i.e., answering queries of the form (US, has president, ?, 2012), we propose a solution inspired by the canonical decomposition of tensors of order 4. We introduce new regularization schemes and present an extension of ComplEx that achieves state-of-the-art performance. Additionally, we propose a new dataset for knowledge base completion constructed from Wikidata, larger than previous benchmarks by an order of magnitude, as a new reference for evaluating temporal and non-temporal link prediction methods.", "review": "Review:###Review: This paper extends the ComplEx model (Trouillon et al., 2016) for completing temporal knowledge bases by augmenting it with timestamp embeddings. Besides, based on the assumption that these timestamp representations evolve slowly over time, the paper introduces this prior as a regularizer. Also, the paper adds a non-temporal component to the model to deal with static facts in knowledge bases. The proposed model has been evaluated using the current benchmark temporal event datasets, showing state-of-the-art performance. This paper could be weakly accepted because the paper introduces new regularization schemes based on the tensor nuclear norm for tensor decomposition over temporal knowledge graphs, which could be a significant algorithmic contribution. Additionally, the submission empirically studies the impact of regularizations for the proposed model, and weight different regularizers according to the joint marginal of timestamps and predicates, which achieves good experimental results. Feedback to improve the paper: 1. For novelty, the paper does not clearly point out that Ma et al. (2018) already augmented the ComplEx model with time embedding for completing temporal knowledge graphs. 2. The paper does not point out whether the units of the timestamps affects the model and how to adjust the model accordingly. For example, the time granularity of the ICEWS dataset is 24 hours. If we switch the unit of timestamps from hours to days, do the results of the proposed model change? If yes, how to peak the best time unit for a given dataset? 3. For the Wikidata, the author reports the filtered Mean Reciprocal Rank of the conducted experiments, where the author provides not only the overall score but also the temporal MRR. However, the paper does not provide information about error bars as well as the unfiltered version of the experiment results. Since the results of TComplEx and TNTComplEx are only slightly better than ComplexE, the reviewer doubts whether the proposed model can really beat the ComplEx model when considering the error bars. Also, NT-MRR and T-MRR are not clearly defined. 4. The submission proposes a new large-scale temporal event dataset, and states that this dataset is more realistic than the current benchmarks. However, the reviewer does not find any argument in the paper to support this statement. References: The?o Trouillon, Johannes Welbl, Sebastian Riedel, E?ric Gaussier, and Guillaume Bouchard. Com- plex embeddings for simple link prediction. In International Conference on Machine Learning, pp. 2071–2080, 2016. Yunpu Ma, Volker Tresp, and Erik A Daxberger. Embedding models for episodic knowledge graphs. Journal of Web Semantics, pp. 100490, 2018."}
{"id": "iclr2020_13", "title": "Shifted Randomized Singular Value Decomposition | OpenReview", "abstract": "Abstract:###We extend the randomized singular value decomposition (SVD) algorithm (Halko et al., 2011) to estimate the SVD of a shifted data matrix without explicitly constructing the matrix in the memory. With no loss in the accuracy of the original algorithm, the extended algorithm provides for a more efficient way of matrix factorization. The algorithm facilitates the low-rank approximation and principal component analysis (PCA) of off-center data matrices. When applied to different types of data matrices, our experimental results confirm the advantages of the extensions made to the original algorithm.", "review": " This paper adapts the approach by Halko to get a SVD using a low rank concept to the case where the matrix is implicit shifted. Honestly - there is nothing wrong with this paper except the level of contribution. I consider this work to be widely irrelevant. You can report this on arxiv if you like but I do not think it is important in general. The results show some effect - but not a relevant one. For ICLR this is much to less. And there is not much more to say. ----------------------------------------------------------------------------------- ----------------------------------------------------------------------------------- ----------------------------------------------------------------------------------- ----------------------------------------------------------------------------------- ----------------------------------------------------------------------------------- ----------------------------------------------------------------------------------- ----------------------------------------------------------------------------------- ----------------------------------------------------------------------------------- ----------------------------------------------------------------------------------- ----------------------------------------------------------------------------------- ----------------------------------------------------------------------------------- -----------------------------------------------------------------------------------"}
{"id": "iclr2020_14", "title": "DCTD: Deep Conditional Target Densities for Accurate Regression | OpenReview", "abstract": "Abstract:###While deep learning-based classification is generally addressed using standardized approaches, a wide variety of techniques are employed for regression. In computer vision, one particularly popular such technique is that of confidence-based regression, which entails predicting a confidence value for each input-target pair (x, y). While this approach has demonstrated impressive results, it requires important task-dependent design choices, and the predicted confidences often lack a natural probabilistic meaning. We address these issues by proposing Deep Conditional Target Densities (DCTD), a novel and general regression method with a clear probabilistic interpretation. DCTD models the conditional target density p(y|x) by using a neural network to directly predict the un-normalized density from (x, y). This model of p(y|x) is trained by minimizing the associated negative log-likelihood, approximated using Monte Carlo sampling. We perform comprehensive experiments on four computer vision regression tasks. Our approach outperforms direct regression, as well as other probabilistic and confidence-based methods. Notably, our regression model achieves a 1.9% AP improvement over Faster-RCNN for object detection on the COCO dataset, and sets a new state-of-the-art on visual tracking when applied for bounding box regression.", "review": "Review:###This paper proposes Deep Conditional Target Densities (DCTD) for confidence-based regression problems. Under the core of the framework, DCTD adopts the Monte Carlo approximations to calculate the integral in the negative log-likelihood Eqn(2). The authors perform comprehensive experiments on four computer vision regression tasks. The main concern about this paper is the novelty and the technological contributions. The Monte Carlo approximation is an off-the-shelf numeric integration method. The novelty of this paper is very limited."}
{"id": "iclr2020_15", "title": "Effective and Robust Detection of Adversarial Examples via Benford-Fourier Coefficients | OpenReview", "abstract": "Abstract:###Adversarial examples have been well known as a serious threat to deep neural networks (DNNs). To ensure successful and safe operations of DNNs on realworld tasks, it is urgent to equip DNNs with effective defense strategies. In this work, we study the detection of adversarial examples, based on the assumption that the output and internal responses of one DNN model for both adversarial and benign examples follow the generalized Gaussian distribution (GGD), but with different parameters (i.e., shape factor, mean, and variance). GGD is a general distribution family to cover many popular distributions (e.g., Laplacian, Gaussian, or uniform). It is more likely to approximate the intrinsic distributions of internal responses than any specific distribution. Besides, since the shape factor is more robust to different databases rather than the other two parameters, we propose to construct discriminative features via the shape factor for adversarial detection, employing the magnitude of Benford-Fourier coefficients (MBF), which can be easily estimated using responses. Finally, a support vector machine is trained as the adversarial detector through leveraging the MBF features. Through the Kolmogorov-Smirnov (KS) test, we empirically verify that: 1) the posterior vectors of both adversarial and benign examples follow GGD; 2) the extracted MBF features of adversarial and benign examples follow different distributions. Extensive experiments in terms of image classification demonstrate that the proposed detector is much more effective and robust on detecting adversarial examples of different crafting methods and different sources, in contrast to state-of-the-art adversarial detection methods.", "review": " This paper presents a new discriminator metric for adversarial attack*s detection by deriving the different properties of l-th neuron network layer on different adv/benign samples. This method can achieve good AUC score comparing to other start-of-art detection methods and also achieve good robustness under corresponding adaptive attack. The framework is clear and the experiment is solid. However, I have several concerns: Major: 1. It seems that the whole process assumes that there is difference for the parameters in the environment of GGD with adv/benign samples, and the goal is to search for the major components of it and use a classifier to detect. To extract the approximation of parameters, the authors use the *response entries* of l-th layer for several observations => this means the authors regard all the *response entries* of one layer as different samples on one certain GGD. This makes me feel a bit tricky, and it would be great if you the authors can provide some evidence or explanation here. 2. In the experiment*s remark, the authors mentioned that the mean parameter of GGD is set to 0 and most of them are actually close to 0 (around 1e-2) so the assumption is right. However 1e-2 is not a value *very close to zero* and it would be great to show / explain the variance here. 3. I can*t find the parameter of your evaluated attack method (like confidence, eps, etc.) Please also provide experimental details for reproducibility. Minor: Here are some reference error (e.g. P6, *For each database, as described in Section ??*). Please fix that. Overall, this paper is a interesting based on the performance of detection. But the assumptions made by the paper are a bit confusing and it would be good to clarify and provide clarification for them. Authors should explain the assumptions and give some extra experiment results if needed."}
{"id": "iclr2020_16", "title": "EXPLOITING SEMANTIC COHERENCE TO IMPROVE PREDICTION IN SATELLITE SCENE IMAGE ANALYSIS: APPLICATION TO DISEASE DENSITY ESTIMATION | OpenReview", "abstract": "Abstract:###High intra-class diversity and inter-class similarity is a characteristic of remote sensing scene image data sets currently posing significant difficulty for deep learning algorithms on classification tasks. To improve accuracy, post-classification methods have been proposed for smoothing results of model predictions. However, those approaches require an additional neural network to perform the smoothing operation, which adds overhead to the task. We propose an approach that involves learning deep features directly over neighboring scene images without requiring use of a cleanup model. Our approach utilizes a siamese network to improve the discriminative power of convolutional neural networks on a pair of neighboring scene images. It then exploits semantic coherence between this pair to enrich the feature vector of the image for which we want to predict a label. Empirical results show that this approach provides a viable alternative to existing methods. For example, our model improved prediction accuracy by 1 percentage point and dropped the mean squared error value by 0.02 over the baseline, on a disease density estimation task. These performance gains are comparable with results from existing post-classification methods, moreover without implementation overheads.", "review": "Review:###The authors propose a method to extract features utilizing the adjacency between patches, for better classification/regression of satellite image patches. The proposed method achieves better results compared to a straightforward baseline method. I have several significant concerns: - In the abstract, the authors claim that existing approaches such as post-classification add computational overhead to the task, whereas the proposed method does not add significant overhead. However, to me, post-classification can be very simple and straightforward, whereas the proposed method adds a series of computations: the proposed method not only extracts features from the input image, but also for another neighboring image; then features are combined (if two images are similar), before feeding into the network. The authors need to validate the claim that their method is more efficient. - The baseline the authors compare to is weak. There are existing works on satellite image classification/regression. Many of them also use semantic/contextual information, or aim to improve the robustness of features. For example: [1] Derksen et al. Spatially Precise Contextual Features Based on Superpixel Neighborhoods for Land Cover Mapping with High Resolution Satellite Image Time Series. IGARSS 2018. [2] Ghassemi et al. Learning and Adapting Robust Features for Satellite Image Segmentation on Heterogeneous Data Sets. Geoscience and Remote Sensing 2019. I understand that the authors cannot compare to everything. But the authors should compare to representative baseline methods. Methods mentioned in the related work section (Section 2.1) can also be compared to. - The proposed method is very application specific. The author only discussed the remote sensing application. Given the ICLR community*s interest in general methods that can be applied to (or already been tested on) multiple applications, the paper would have been stronger if the methods applicabilityto other domains was discussed (and even better demonstrated)."}
{"id": "iclr2020_17", "title": "Learning Nearly Decomposable Value Functions Via Communication Minimization | OpenReview", "abstract": "Abstract:###Reinforcement learning encounters major challenges in multi-agent settings, such as scalability and non-stationarity. Recently, value function factorization learning emerges as a promising way to address these challenges in collaborative multi-agent systems. However, existing methods have been focusing on learning fully decentralized value function, which are not efficient for tasks requiring communication. To address this limitation, this paper presents a novel framework for learning nearly decomposable value functions with communication, with which agents act on their own most of the time but occasionally send messages to other agents in order for effective coordination. This framework hybridizes value function factorization learning and communication learning by introducing two information-theoretic regularizers. These regularizers are maximizing mutual information between decentralized Q functions and communication messages while minimizing the entropy of messages between agents. We show how to optimize these regularizers in a way that is easily integrated with existing value function factorization methods such as QMIX. Finally, we demonstrate that, on the StarCraft unit micromanagement benchmark, our framework significantly outperforms baseline methods and allows to cut off more than 80\\% communication without sacrificing the performance.", "review": "Review:###The paper tackles the collaborative multi-agent RL problem as the problem of finding almost-decentralized value functions, where the loss tries to minimize communications between the agents. The core idea is around maximizing the mutual information between each massage and the existing knowledge of its receiver. This way, redundant messages are naturally removed. The authors then assert an entropy regularization to (almost) prevent the agents from *cheating*. The paper is in general well-written and motivated. There are however certain issues that should be addressed. [The second one is my main issue.] 1. The term *message* has been used repeatedly but not defined. You should define precisely what you mean by a message in the background section. In particular, it is not clear from the text what a message looks like mathematically. You may also consider giving intuitive examples of how different ways of designing a message alters the behaviour in a given context before start talking about how to optimise them. 2. Section 3.1 needs a revisit. I assume by *optimal action policy of agent j* you mean *optimal policy of agent j*. Formally, an optimal policy is greedy to the optimal value function and is deterministic unless there exist multiple actions with same optimal value at a given state. Therefore, the mutual information is not mathematically well-defined since most of the time the optimal policy is deterministic and does not induce a probability distribution. 2.1 What is the optimal value function of an agent? The agents are not optimizing their own value function, so even if your complex model converges, it does not imply optimality of agent-level value functions (and they shouldn’t be locally optimal). If by that you mean the agent-level value functions *after* convergence of , then you need to clarify it to avoid the confusion. Even so, it is still not clear how you conceive the agent-level policies from these value functions. 2.2. Minor: As for the notation, I found quite strange for a policy; you may want to consider something like . 3. If is learnt, then message encoding is not stationary at least in the training time. It may make the training potentially become unstable. Specifically, there is no condition to assure stability of your model. Suggestion: your entropy regularization might be sufficient to induce stability. More discussion on this (or a simple experiment to show if an instability exists and will be alleviated with regularization) would be quite helpful. 4. If I understand it correctly [and it wasn*t clear from the text], the encoder is conditioned on local history, which induces that all the agents must share same history shape (tensor-wise), which in turn means that all the local agents has to have same local state shape (computationally, they should for example have same output/internal-state shape in their neural networks). This sounds like a limitation, and it may only be OK in domains where agents are homogenous."}
{"id": "iclr2020_18", "title": "All Simulations Are Not Equal: Simulation Reweighing for Imperfect Information Games | OpenReview", "abstract": "Abstract:###Imperfect information games are challenging benchmarks for artificial intelligent systems. To reason and plan under uncertainty is a key towards general AI. Traditionally, large amounts of simulations are used in imperfect information games, and they sometimes perform sub-optimally due to large state and action spaces. In this work, we propose a simulation reweighing mechanism using neural networks. It performs backwards verification to public previous actions and assign proper belief weights to the simulations from the information set of the current observation, using an incomplete state solver network (ISSN). We use simulation reweighing in the playing phase of the game contract bridge, and show that it outperforms previous state-of-the-art Monte Carlo simulation based methods, and achieves better play per decision.", "review": "Review:###This paper deals imperfect information games, and builds a Bayesian method to model the unknown part of the current state, making use of the past moves (which constrain the game, here Contract Bridge). This new Bayesian method is compared to Monte Carlo style techniques, which are much more computationally expensive (they draw random samples of the unknown part of the information and then solve the perfect-information version of the game, for each simulated possible full-state). The work also introduces a Neural Network (NN) to estimate the best moves in the perfect-information version of the game (instead of making the full tree search of the optimal moves to play). The final model proposed (ISSN+SSN) uses a NN combined with Bayesian computation, using the NN at each time step of the past to update the belief about the current missing information. Overall the paper is written clearly: as a non-expert in RL, I was able to follow rather easily what is done. However, the results are not convincingly good. Maybe it is just the interpretation/contextualization that is insufficient. I have 3 important remarks: 1. It is stated in the abstract *that it [the new method] outperforms previous state-of-the-art Monte Carlo simulation based methods, and achieves better play per decision.* However, the costs in the *reweighing DDS* section (table 2) are lower (better) than those of the *reweighing SSN* section. The results show improvement upon using some reweighing, as ISSN+sthg is better than without ISSN (although by a very small relative decrease in cost). 1.a. I understand that the NN-based methods (last three lines of table 2) are incomparably faster than the DDS-based approaches (baseline). And the total rate of tricks loss per initial state remains low (6% seems small as an absolute value). But, they are almost double of the DDS-based loss rates ! It is not clear how this can be considered *outperforming* or *better play per decision*. If you can explain in which sense those are good results for the SSN vs DDS, please do so. If they are not, please do acknowledge it, and eventually contextualize (maybe a 2-fold increase of loss rate is okay given the large speedup obtained?). 1.b. in the same way, it is not clear how ISSN outperforms its no-memory counterparts, given the loss decreases are essentially negligible. Maybe one needs to increase the value of T to make ISSN*s success more obvious? 2. In addition, why not compare this Bayesian method with other Bayesian methods (quoted at the very end of section 3)? Here the paper focuses on comparison with *deterministic* methods, i.e. methods which sample complete states (simulations) to then solve the complete-information version of the game (via exhaustive tree search). Those are what I would call brute-force methods. However, although simulations may be done (expensively) for contract bridge, in some other cases this kind of sampling may become prohibitively expensive, so that only Bayesian methods are left. If this kind of argument is the justification for your work, please make it more explicitly. Otherwise please correct me and explain the context (the role of Bayesian methods, what has been done and what hasn*t been) more clearly. If you are to situate the work within the Bayesian-based approaches, the question remains: is ISSN+SSN better than other Bayesian-inspired, *information-completing* methods ? To summarize, the paper convincingly shows that Bayesian-NN methods can compete with expensive brute force methods, but it would be very nice to see how the method introduced compares with other recent Bayesian approaches. (Or if no such comparison can be done, explain why). 3. In addition, here it seems your baseline is based on GIB, which was winning tournaments ~20 years ago: why not compare with Wbridge5 or JackBridge ? I think you need to at least explain your choice. Because of these weak points in terms of experimental results, I lean to reject the paper. However, depending on the authors answers and clarifications on my 3 important remarks above, I am ready to change my rating. Also, I have a couple of more or less minor remarks to improve the paper: The definition of the cost is not explicitly given: *We set up the evaluation metric by tricks loss per deal. The ground truth play is DDS result for the original deal and all simulation deals. We compare all the following algorithmS with the ground truth to get costs of the policy.* You should rephrase this to make the definition of *cost* very explicit. Is it simply the average rate of lost tricks (per given set of 13 cards in the agent hands) ? This is quite crucial and make the reading of results a bit complicated (especially since results fo not match the conclusion announced in the abstract). *DDS) 3 computes the maximum tricks each side can get if all the plays are optimal* In this place and a couple others, you should explicitly recall whether you mean *assuming perfect information*, or not. Sometimes it can get confusing, and a bit of repetition won*t hurt. I think I understood correctly that DDS solves (perfectly) the perfect information game, but at times I thought other methods also made use of the full information (?) Figure 4c is nicely explained and this part really illustrates well the idea of the method, I like it. Although, the notation AQ3 was not obvious for me at first, and I think it is worth improving this figure, making use of the right-hand-side space, to make it an autonomously explanatory figure. *position*: the term is not defined. I would guess it means the current state of the game (the agent*s hand and the cards played in the past or during the current trick). This should be said explicitly. Also *hands* seem to refer to the 4 hands (1 for each player) (is that correct?) There are wrong singular/plurals (*s*/no *s*) in several places. This is simple to correct and should be corrected. in section 7, ablation studies. This is a very nice study, but you should precise how much you augment the data here (or recall by how much, if you say it earlier). This passage is unclear and should be rephrased for clarity: *We run DDS on all 2.4 million positions, to get the corresponding results for each available action. This serves as ground truth of the current position. For each position, we run 50 simulations to compute the baselines and serve as training target of the pretrained network. Each call to DDS takes about 5 milliseconds.* This passage is unclear and should be rephrased for clarity: *For simplicity, in this work we just use discarding information.* This passage is slightly unclear and should be rephrased for clarity: *For each simulation, the moves with the optimal number of tricks are marked with optimal moves. We sum up the optimal moves counter in these k simulations. This results in a counter for each legal move and we treat this as the training target. The process is described in Figure 1.**"}
{"id": "iclr2020_19", "title": "Sequence-level Intrinsic Exploration Model for Partially Observable Domains | OpenReview", "abstract": "Abstract:###Training reinforcement learning policies in partially observable domains with sparse reward signal is an important and open problem for the research community. In this paper, we introduce a new sequence-level intrinsic novelty model to tackle the challenge of training reinforcement learning policies in sparse rewarded partially observable domains. First, we propose a new reasoning paradigm to infer the novelty for the partially observable states, which is built upon forward dynamics prediction. Different from conventional approaches that perform self-prediction or one-step forward prediction, our proposed approach engages open-loop multi-step prediction, which enables the difficulty of novelty prediction to flexibly scale and thus results in high-quality novelty scores. Second, we propose a novel dual-LSTM architecture to facilitate the sequence-level reasoning over the partially observable state space. Our proposed architecture efficiently synthesizes information from an observation sequence and an action sequence to derive meaningful latent representations for inferring the novelty for states. To evaluate the efficiency of our proposed approach, we conduct extensive experiments on several challenging 3D navigation tasks from ViZDoom and DeepMind Lab. We also present results on two hard-exploration domains from Atari 2600 series in Appendix to demonstrate our proposed approach could generalize beyond partially observable navigation tasks. Overall, the experiment results reveal that our proposed intrinsic novelty model could outperform several state-of-the-art curiosity baselines with considerable significance in the testified domains.", "review": "Review:###This paper extends the prediction-error based model by Pathak et. al., 2019 by learning a forward (and inverse) dynamics model for predicting a state feature multiple steps into the future (say, K-steps) given an open loop sequence of K actions as opposed to 1 step into the future, with the caveat that instead of using learnable state features, a random network is used for computing state features, similar to Random Network Distillation (RND) by Burda et. al., 2019. Also, their inverse dynamics models predicts the entire sequence of actions up to K steps. Experiments on VizDoom point-navigation tasks show that the proposed model does better than baselines as rewards get sparser. Ablations are provided to justify the choice of K in multi-step prediction, the choice of inverse dynamics and the choice of RND state features. My decision is weak reject as: 1. The paper does a good job at clearly explaining their model, presenting results on relevant experiments and baseline comparisons for their model and justifying each modeling choice with ablations. 2. The novelty in their contribution is moderate - the idea of long-term future prediction is not new (e.g.: Ke et. al., 2019) (but using it for giving a curiosity bonus is new), the architecture choice is not significantly new. 3. I expected a more detailed ablation for the choice of K, given that “multi-step” has major emphasis in the paper, but Figure 7(a) only shows ablations for 3 vs 1 step predictions. Elaborating on (3): - My major concern is that the gap between 1-step prediction and 3-step prediction (in Figure 7 (a)) is not significant. Note that the version of their model with 1-step predictions does not completely reduce to Pathak et. al. 2019’s ICM model, as RND state features are used. I feel that it may be the case that the 1-step version of the proposed model is actually good enough to beat all the baselines and adding multiple steps gives marginal gains. This hypothesis needs to be verified by the authors with more experiments - I would like to see all the main experiments have an additional baseline of 1-step predictions. - Only two values of K are tested - 1 and 3, what happens with larger values of K? Other comments: The motivation for using long-term predictions to “infer more meaningful novelty” is fine on it’s own but seems to conflict with the choice of random network (RND) state features. Random features imply that a random “hash” of the observations is being computed which has no reason to have similar features for two nearby states. If there is any small amount of noise in the state transitions, this would mean that predicting far into the future is practically impossible given that the random feature of slightly incorrect states will be very different. Can the authors give reasons/motivations as to why such a model would work in the case of stochastic transitions and K is large or will it be brittle to stochasticity? References: All references are same as those cited in paper."}
{"id": "iclr2020_20", "title": "A FRAMEWORK FOR ROBUSTNESS CERTIFICATION OF SMOOTHED CLASSIFIERS USING F-DIVERGENCES | OpenReview", "abstract": "Abstract:###Formal verification techniques that compute provable guarantees on properties of machine learning models, like robustness to norm-bounded adversarial perturbations, have yielded impressive results. Although most techniques developed so far requires knowledge of the architecture of the machine learning model and remains hard to scale to complex prediction pipelines, the method of randomized smoothing has been shown to overcome many of these obstacles. By requiring only black-box access to the underlying model, randomized smoothing scales to large architectures and is agnostic to the internals of the network. However, past work on randomized smoothing has focused on restricted classes of smoothing measures or perturbations (like Gaussian or discrete) and has only been able to prove robustness with respect to simple norm bounds. In this paper we introduce a general framework for proving robustness properties of smoothed machine learning models in the black-box setting. Specifically, we extend randomized smoothing procedures to handle arbitrary smoothing measures and prove robustness of the smoothed classifier by using -divergences. Our methodology achieves state-of-the-art}certified robustness on MNIST, CIFAR-10 and ImageNet and also audio classification task, Librispeech, with respect to several classes of adversarial perturbations.", "review": "Review:###Summary: This submission proposes a unified framework for black-box adversarial certification. Based on cohen*s and lee*s result, it extends the lambda_TV distance to more general f-divergence. Besides, some other techniques, e.g. reference distribution, are also included in the framework. For L_0 and L_1 perturbation, the proposed framework comes to better results than all the related methods. Strengths: [+] Detailed analysis and theorem. [+] Describing the background and conclusions clearly. Weaknesses: [-] In the experiment section, for ImageNet, baselines use ResNet-50 but the authors use ResNet-152. I wonder whether this is fair enough. [-] Renyi divergence is not a proper f-divergence. Therefore, maybe the title should be changed. [-] The theorem part contains too many details and some important parts (e.g. tightness) are in appendix. It should be re-organized. Questions: [.] In Appendix A.7, Lagrangian strong-duality has been used to show the tightness, but I wonder whether the tightness holds for any kind f-divergence? If the authors can write down some theorem about the tightness, it will be better. p.s. Could you give me some quick responses to my questions (e.g. ResNet-152 v.s. ResNet-50) ? Once you convince me, I would revise the score to 6. If not, I will turn down the score to 3."}
{"id": "iclr2020_21", "title": "Rethinking the Security of Skip Connections in ResNet-like Neural Networks | OpenReview", "abstract": "Abstract:###Skip connections are an essential component of current state-of-the-art deep neural networks (DNNs) such as ResNet, WideResNet, DenseNet, and ResNeXt. Despite their huge success in building deeper and more powerful DNNs, we identify a surprising emph{security weakness} of skip connections in this paper. Use of skip connections \textit{allows easier generation of highly transferable adversarial examples}. Specifically, in ResNet-like (with skip connections) neural networks, gradients can backpropagate through either skip connections or residual connections. We find that using more gradients from the skip connections rather than the residual connections according to a decay factor, allows one to craft adversarial examples with high transferability. Our method is termed emph{Skip Gradient Method} (SGM). We conduct comprehensive transfer attacks against 10 state-of-the-art DNNs including ResNets, DenseNets, Inceptions, Inception-ResNet, Squeeze-and-Excitation Network (SENet) and robustly trained DNNs. We show that employing SGM on the gradient flow can greatly improve the transferability of crafted attacks in almost all cases. Furthermore, SGM can be easily combined with existing black-box attack techniques, and obtain high improvements over state-of-the-art transferability methods. Our findings not only motivate new research into the architectural vulnerability of DNNs, but also open up further challenges for the design of secure DNN architectures.", "review": " Summary: This paper proposes a modification to standard Projected Gradient Descent to improve transferability of adversarial examples, when the source model is a ResNet-like model containing skip connections. The method, Skip Gradient Method (SGM) modifies the backwards pass to scale down the gradient computed in each residual branch of the model, before these gradients are combined with the gradient from the skip connection. This thus upweights the gradients from the skip connections as opposed to residual modules. The paper demonstrates significant improvements in the single-model black-box transfer setting, against a variety of undefended and defended models. Strengths: - Lots of interesting empirical results here! One result which stands out to me is Table 4, where across a wide range of target models, adding SGM to existing techniques cuts defender accuracy by ~1/2 (e.g. 79.9% to 89.66% for SE154). The results in Table 3, showing that even without additional techniques, SGM results in large improvements and outperforms previous approaches, are also quite nice. - It*s notable that such a simple approach leads to significant improvements. - Results are very clearly presented, and writing is clear throughout. Suggestions for improvement: I have 3 major concerns: (1) discrepancies between baselines and previously published results (2) unrealistic threat model (3) framing / conclusions drawn by paper. I suspect (1) is easily addressed, but was not clear to me from the current paper. (1) Baselines: - For multi-step transfer against undefended models (Table 3), the attack success rates seem low compared to numbers reported in e.g. Liu et al. For example, using ResNet-152 as the source, and VGG-19 as the target, Liu et al reports 19% defender accuracy = 81% attacker success. This is significantly higher than the 65.52% reported for MI (both are non-regularized optimization attacks), also stronger than the 80.68% reported for SGM. In general, considering these are *untargeted* attacks, with eps=16, the transfer rates seem pretty low. - For multi-step transfer against defended models (Table 5), the numbers are slightly lower than previously reported. E.g. attacking IncV3_ens3, Dong et al 19 reports 46.9% for MI, but here, 44.28% is reported. (I realize these differences are slight.) - In general, it would be nice if the paper were structured so as to make these comparisons easier. For example, the appendix could include tables comparing the baselines reported here, to values previously reported, and explain any discrepancies. Particularly with black-box transfer, where baseline performance is so sensitive to small choices, it*s important to ensure baselines are properly implemented, and the current writing of the paper makes it impossible for the reader to assess this unless they are very familiar with the black-box transfer literature. (2) Unrealistic threat model: - For black-box transfer, all the strongest attacks use multiple source models (see e.g. NeurIPS 2017 Adversarial Examples contest, the baselines cited in the paper). While this paper shows significant improvements in the single-source setting, these results are significantly weaker than any multiple-source attack. For instance, Liu et al 17 achieve near 100% untargeted success (and near 100% targeted success) against all undefended models they study (many which overlap), and Dong et al 19 report ~85% accuracy against IncV3_ens3 (compared to <60% here). - There*s no reason in practice that an adversary would not employ an ensemble-based attack if they wanted to fool an unknown model. (3) Framing / conclusions: - The paper frames the results as a *security vulnerability of ResNets,* but the results don*t show this. In particular, they show that ResNets make effective *source* models to be used by *attackers*, but they don*t imply which models defenders should use in order to be robust to black-box attacks. In this way, the main message of the paper seems misleading to practitioners. - The main message of the paper thus seems to be *on the transferability of adversarial examples generated with resnets* as opposed to the *security of skip connections.* I would encourage rewriting of the title/intro/conclusion as such. Overall, there are several very interesting empirical findings in this paper. I view the two main impacts these results could later have would be leading to (1) improved understanding of what causes transfer of adversarial examples and (2) improved understanding of ResNet-like architectures (for example, the results provide some support for the view of ResNets as ensembles of shallow models, cited in the paper). If the paper were written with this view, then concern (2) above becomes unimportant. Suggestions I believe could strengthen the paper, but I do not view as weaknesses, or necessities: - Can you decouple the effect of SGM on optimization and transfer? The paper does a bit of this, but restricts the analysis to the single-step case. For instance, if the main effect of SGM is on optimization, this has interesting implications for optimization of ResNets. If the main effect is on transfer, this has interesting implications for what causes transfer. Minor: - I*d suggest moving Table 1 (one-step attacks) to the Appendix. The results are less impressive than multi-step, and the community as a whole favors stronger attacks. Overall, there are several interesting empirical findings in this paper (modulo concerns about baselines indicated above). I suggest that the paper either consider more realistic threat models to be useful to the adversarial examples field, or focus on the insights revealed by these findings. I hope that the authors can address the concerns outlined here, and I would be happy to adjust my score if so. Note that my current indicated confidence rating is for the current state of the paper. I would be happy to adjust my evaluation if revisions to the paper can one/several of the concerns indicated above. __________________ EDIT: Rating adjusted from 3: Weak Reject to 8: Accept, after accounting for revisions and author rebuttal. See reply below."}
{"id": "iclr2020_22", "title": "Stabilizing Neural ODE Networks with Stochasticity | OpenReview", "abstract": "Abstract:###Neural Ordinary Differential Equation (Neural ODE) has been proposed as a continuous approximation to the ResNet architecture. Some commonly used regularization mechanisms in discrete neural networks (e.g. dropout, Gaussian noise) are missing in current Neural ODE networks. In this paper, we propose a new continuous neural network framework called Neural Stochastic Differential Equation (Neural SDE) network, which naturally incorporates various commonly used regularization mechanisms based on random noise injection. Our framework can model various types of noise injection frequently used in discrete networks for regularization purpose, such as dropout and additive/multiplicative noise in each block. We provide theoretical analysis explaining the improved robustness of Neural SDE models against input perturbations/adversarial attacks. Furthermore, we demonstrate that the Neural SDE network can achieve better generalization than the Neural ODE and is more resistant to adversarial and non-adversarial input perturbations.", "review": "Review:###The paper introduces Neural Stochastic Differential Equations models which is a follow-up of last year paper on Neural Ordinary Differential Equations. The stochasticity is obtained by adding an injection noise term. Pros: As there is still no published paper that introduces Neural SDE, this paper would be well placed in the literature. Apart from introducing the Neural SDE architecture, the authors also study both empirically and analytically the robustness of suggested models in comparison with Neural ODE models. Overall the paper is clear and well written. Cons: The transfer from *Ordinary* to *Stochastic* is achieved by only adding a Dropout (injection noise) term. So the idea seems to be not *that* big. There is another papers, not cited by the authors, that suggests stochastic versions of Neural ODEs based on ResNets with a specific initialization distribution on the NN parameters (Peluchetti and Favaro, 2019). Their model can be considered as a continuous version of ResNets with applied stochastic gradient descent. The current interpretation of Neural SDE also can be considered, just I find the suggestion of Peluchetti and Favaro to be more natural. Stefano Peluchetti and Stefano Favaro. Neural Stochastic Differential Equations (2019) Major suggestions: I don*t have major suggestions. Minor suggestions: * In the first and last sentences of subsection “Gaussian noise injection”, Section 3.1, there must be references to equation (3) instead of (25), which is in the appendix. And also further in the main text of the paper there are references to (25). * In equation (7) the identity matrix must be written in bold. * Section 3.2. we can also calculates -> calculate * There are also some missed articles throughout the paper. * Appendix: - I think it would look better to put Equation (12) in Lemma A.3. - After Equation (16), there is another letter epsilon. - Appendix D contains only Figure 7 which is the same as Figure 3 (left). * References: - Weinen E. -> full name - Some papers have already been published, e.g. Xavier Gastaldi’s paper was at ICLR 2017. - Conference names are not homogeneous, such as ‘Advances in Neural …’ and only ‘Neural …’, written with capital and not capital letters."}
{"id": "iclr2020_23", "title": "Distillation $approx$ Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized NN | OpenReview", "abstract": "Abstract:###Distillation is a method to transfer knowledge from one model to another and often achieves higher accuracy with the same capacity. In this paper, we aim to provide a theoretical understanding on what mainly helps with the distillation. Our answer is *early stopping*. Assuming that the teacher network is overparameterized, we argue that the teacher network is essentially harvesting dark knowledge from the data via early stopping. This can be justified by a new concept, Anisotropic In- formation Retrieval (AIR), which means that the neural network tends to fit the informative information first and the non-informative information (including noise) later. Motivated by the recent development on theoretically analyzing overparame- terized neural networks, we can characterize AIR by the eigenspace of the Neural Tangent Kernel(NTK). AIR facilities a new understanding of distillation. With that, we further utilize distillation to refine noisy labels. We propose a self-distillation al- gorithm to sequentially distill knowledge from the network in the previous training epoch to avoid memorizing the wrong labels. We also demonstrate, both theoret- ically and empirically, that self-distillation can benefit from more than just early stopping. Theoretically, we prove convergence of the proposed algorithm to the ground truth labels for randomly initialized overparameterized neural networks in terms of l2 distance, while the previous result was on convergence in 0-1 loss. The theoretical result ensures the learned neural network enjoy a margin on the training data which leads to better generalization. Empirically, we achieve better testing accuracy and entirely avoid early stopping which makes the algorithm more user-friendly.", "review": "Review:###This paper proposes a new perspective on understanding knowledge distillation as a transfer of information defined with respect to the neural tangent kernel. Additionally, a new framework for learning the classifier on a noisy labeled dataset is proposed based on the knowledge transfer framework. Overall, I think the paper lacks justification (and explanation) for its main statement on how knowledge distillation is related to the early stopping of the teacher network. Especially, it is confusing since Section 2 and 3 make different statements. Specifically, Section 2 shows that early stopping *helps* knowledge distillation while Section 3 shows that knowledge distillation can *replace* early stopping. The former observation implies that early stopping is complementary to knowledge distillation, while the latter implies otherwise. Furthermore, Section 2 mainly explains why the eigenspaces associated with the largest eigenvalue of the neural tangent kernel is *informative information*. However, there is no elaboration on how the knowledge distillation process leads to the transfer of such information, i.e., there is no connection between the neural tangent kernel and the knowledge distillation process. Although Figure 1. suggests that early stopping indeed improves the knowledge distillation process, they are not enough to support the statement convincingly enough. Without proper support on the main statement of this paper, the paper looses much of its claimed contributions. The label refinery algorithm for the noisy labeled dataset is interesting, but it is not evaluated thoroughly enough to demonstrate its superiority over existing algorithms. It also does not have much originality when compared to similar algorithms [1, 2]. Bagherinezhad et al., [1] also tried to remove *noisy supervisions* that were generated by harsh augmentation on images. Han et al., [2] and Li et al., [3] also use distillation-like processes to learning noisy datasets. [1] Label Refinery: Improving ImageNet Classification through Label Progression, Bagherinezhad et al., 2018 [2] Co-teaching: Robust Training of Deep NeuralNetworks with Extremely Noisy Labels, Han et al., 2018 [3] Learning from Noisy Labels with Distillation, Li et al., 2017"}
{"id": "iclr2020_24", "title": "An Information Theoretic Approach to Distributed Representation Learning | OpenReview", "abstract": "Abstract:###The problem of distributed representation learning is one in which multiple sources of information X1,...,XK are processed separately so as to extract useful information about some statistically correlated ground truth Y. We investigate this problem from information-theoretic grounds. For both discrete memoryless (DM) and memoryless vector Gaussian models, we establish fundamental limits of learning in terms of optimal tradeoffs between accuracy and complexity. We also develop a variational bound on the optimal tradeoff that generalizes the evidence lower bound (ELBO) to the distributed setting. Furthermore, we provide a variational inference type algorithm that allows to compute this bound and in which the mappings are parametrized by neural networks and the bound approximated by Markov sampling and optimized with stochastic gradient descent. Experimental results on synthetic and real datasets are provided to support the efficiency of the approaches and algorithms which we develop in this paper.", "review": "Review:###In this paper, the authors studied the distributed representation learning problem, where multiple sources of data are processed to provide information about Y. They studied this problem from information-theoretic point of view. Their main contribution can be summarized as follows. 1. The optimal trade-off between the accuracy and complexity were studied for discrete memoryless data model as well as memoryless vector Gaussian model. 2. A variational bound were constructed in order to connect the optimal encoder and decoder mappings with the solution of an optimization algorithm. 3. If only samples from an unknown distribution are available, an algorithm were proposed to find the optimal encode and decoder. Moreover, some experiment were conducted to support the approach. In general, I think the paper is well-organized. The definition of the problem and the motivation of the approach are clear. The theorems, algorithms and experiments are solid enough to support the whole story of this paper. Generally I wish to see this paper being accepted."}
{"id": "iclr2020_25", "title": "Thwarting finite difference adversarial attacks with output randomization | OpenReview", "abstract": "Abstract:###Adversarial input poses a critical problem to deep neural networks (DNN). This problem is more severe in the *black box* setting where an adversary only needs to repeatedly query a DNN to estimate the gradients required to create adversarial examples. Current defense techniques against attacks in this setting are not effective. Thus, in this paper, we present a novel defense technique based on randomization applied to a DNN*s output layer. While effective as a defense technique, this approach introduces a trade off between accuracy and robustness. We show that for certain types of randomization, we can bound the probability of introducing errors by carefully setting distributional parameters. For the particular case of finite difference black box attacks, we quantify the error introduced by the defense in the finite difference estimate of the gradient. Lastly, we show empirically that the defense can thwart three adaptive black box adversarial attack algorithms.", "review": "Review:###This paper proposes applying randomization to the output layer of a DNN to defend against query-based attacks based on finite difference estimates. Then some theoretical analysis is provided, showing that with perturbation of a suitable scale, the randomization layer will not affect the accuracy of the model, while causing a large estimation error of finite difference methods that prevents finite-difference based attacks. Empirical results verify that the proposed defense is still effective against adaptive attacks where the randomness is averaged. Pros: The proposed method is simple, straightforward, yet novel. Its working mechanism is easy to understand and analyze, so it should be useful against finite-difference based attacks. Limitation: The proposed method is not useful to defense against white-box attacks and transfer-based attacks, since basically it does not change the predictive model. Some other randomization methods like [26], by contrast, change the predictive model, hence they may be useful against white-box attacks and transfer-based attacks. Questions and suggestions: This part is my main concern. It seems that the experimental results are very good. For example, in Figure 3a, the defense is effective even if sigma^2<1e-6. However, by the analysis in Section 4.2 (the formula below Line 3, Page 6), when sigma^2=1e-6, |E[g_i-gamma_i]| should be rather small, hence it should not block finite-difference based attacks. I think more explanation is needed for the good performance in the experiments. Finite differences are extremely sensitive to small random perturbation of the function value when the spacing (step size) h is small. For example, g_i=frac{L(f(x+he_i))-L(f(x-he_i))}{2h}, when h is very small, f(x+he_i) and f(x-he_i) is very close, hence adding perturbation to them will change g_i a lot. To present stronger adaptive attacks to output randomization, my suggestion is that a larger h can be adopted. It will be better if the results are investigated against attacks with different values of h. A mistake: In Section 4.1 on Page 4: *we can express the probability that x is misclassified in the vector d(p) as: sum_{i=2}^C P(d(p_i)>d(p_m))*. I think this is wrong, since P(A or B happens)=P(A happens)+P(B happens) only when A and B are mutually exclusive. However, *d(p_i)>d(p_m)* and *d(p_j)>d(p_m)* are not mutually exclusive. Hence, the probability that x is misclassified in the vector should be less than or equal to that sum of probabilities. By the way, the writing in Section 4.1 is not clear: - The overall misclassification probability is presented first, but after that K only represents the misclassification probability into a specific class. The connection between them is unclear. - At the beginning of Section 4.1, the distribution of epsilon is epsilonsimmathcal{N}(mu,sigma^2cdotmathbf{I}_C): a unique sigma is used. But after that, the variance of epsilon_i becomes sigma_i^2 instead of sigma^2. - In the second to the last line on Page 4, *level of noise (sigma^2) can be set for each class separately*, but the authors did not explain how to set them, and in the experiments sigma^2 is set as the same scalar. - In Figure 1b, the line style of *K=5.0e-3* and *K=1.0e-1* in the legend is very similar. The line style of *K=2.0e-01* in the legend is not clear: I do not know whether it refers to *-.-.-.* or *------*. Typos: Section 6, Page 8: *unintentionlly* => *unintentionally* Some missing spaces after punctuation: - Section 4.2, Page 5, *... gradient estimate.When the ...* => should add a space before *When* - Section 5, Page 7, *In addition,input randomization ...* => should add a space before *input*"}
{"id": "iclr2020_26", "title": "Bias-Resilient Neural Network | OpenReview", "abstract": "Abstract:###Presence of bias and confounding effects is inarguably one of the most critical challenges in machine learning applications that has alluded to pivotal debates in the recent years. Such challenges range from spurious associations of confounding variables in medical studies to the bias of race in gender or face recognition systems. One solution is to enhance datasets and organize them such that they do not reflect biases, which is a cumbersome and intensive task. The alternative is to make use of available data and build models considering these biases. Traditional statistical methods apply straightforward techniques such as residualization or stratification to precomputed features to account for confounding variables. However, these techniques are not in general applicable to end-to-end deep learning methods. In this paper, we propose a method based on the adversarial training strategy to learn discriminative features unbiased and invariant to the confounder(s). This is enabled by incorporating a new adversarial loss function that encourages a vanished correlation between the bias and learned features. We apply our method to a synthetic, a medical diagnosis, and a gender classification (Gender Shades) dataset. Our results show that the learned features by our method not only result in superior prediction performance but also are uncorrelated with the bias or confounder variables. The code is available at http://blinded_for_review/.", "review": "Review:###The authors propose a method based on GAN to classify data while automatically removing confounding effects during training, in order to obtain a classifier whose features are not biased by any confounding effect. The proposed idea is based on the extension of classical classification architectures to account for bias prediction. In practice, the parameters of the feature extraction component are updated to solve both bias prediction and the desired classification problem in adversarial fashion. Pearson’s correlation was chosen as a metric for bias. The paper is interesting and addresses an important problem for the application of machine learning methods in several real context. The feeling is however that the paper should have better explored the implication of the proposed model of bias, and better investigated the relationship with simpler approaches relying on similar hypothesis. Here are my main comments for this work: - Why Pearson’s correlation should be a reliable metric to quantify bias? This metric is insensitive to affine scaling of the data, which is a quite common form of bias (for example in medical images). - The authors should have investigated the relationship between the proposed method and bias removal through canonical correlation analysis (CCA), and perhaps its non-linear variants. At the end this is what their network is doing, although in an end-to-end fashion. Using the CCA projections in the latent space for classification would be the closest approach to the state of the art for bias removal in statistical analysis (residual analysis). - The experimental setting illustrated in 4.1 is not clear. In which sense sigma_A is a common factor for the two groups? Why the theoretical maximum classification accuracy is 90%? Figure 2 is not clear either and doesn’t help understanding the structure of the generated data (e.g. axis labels missing, colorbars units not specified). - It is not clear why authors quantify the correlation in the latent space with tSNE projections. tSNE is highly sensitive to the choice of parameters and it would be important to ensure that it was a “fair competition” between all the methods, when showing the results of the dimensionality reduction. This is another modelling step relying on specific assumptions which decreases interpretability of the findings. The authors also proposed to assess the decorrelation of the estimated features throughout the different methods by measuring the squared distance correlation. Naturally, their method is the one which exhibits the best performances using this metric. However, this way of assessing the decorrelation of the features with the biases is unfair to the other methods, as in their case they specifically built their model to avoid statistical correlation between features and biases. - Experiment 4.2 has some controversial aspects, as the bias correction is performed on the control population only, while the model is trained on the entire population. I understand the fact that confounding can be estimated only on healthy conditions, however in this case the network is going to be biased by the control group by construction. The effect of such a choice in the end-to-end optimisation scheme is really not clear. - We also observe that the results of the baseline CNN are very close to the BR-Net. The main difference lies in the fact that the CNN tends to have an unbalanced classification between true negatives and true positives. However, what would happen if we corrected for age before applying the CNN ? - In the case where the performances of the CNN would be improved, this last question would raise another one. Indeed, if I already know the confounding effects I want to correct for, why wouldn’t I correct them beforehand in order to avoid to train a complex GAN, which leads to more instability during training. This aspect points to the limit of having an online bias-correction (at least for the medical data case)."}
{"id": "iclr2020_27", "title": "Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization | OpenReview", "abstract": "Abstract:###Humans can learn task-agnostic priors from interactive experience and utilize the priors for novel tasks without any finetuning. In this paper, we propose Scoring-Aggregating-Planning (SAP), a framework that can learn task-agnostic semantics and dynamics priors from arbitrary quality interactions as well as the corresponding sparse rewards and then plan on unseen tasks in zero-shot condition. The framework finds a neural score function for local regional state and action pairs that can be aggregated to approximate the quality of a full trajectory; moreover, a dynamics model that is learned with self-supervision can be incorporated for planning. Many of previous works that leverage interactive data for policy learning either need massive on-policy environmental interactions or assume access to expert data while we can achieve a similar goal with pure off-policy imperfect data. Instantiating our framework results in a generalizable policy to unseen tasks. Experiments demonstrate that the proposed method can outperform baseline methods on a wide range of applications including gridworld, robotics tasks and video games.", "review": "Review:###I am not from this area and don*t know much about reinforcement learning. The paper discusses zero shot generalization (adaptation) into new environments. The authors propose an approach and then show results on Grid-World, Super Mario Bros, and 3D Robotics. In the training environment E1 = (S, A, p) the algorithm sees a bank of exploratory trajectories \tau_i = {(s_t, a_t)}_{t=1}^{T} but not rewards. The authors then say that algorithm is tested on the test environment E2. They * propose to only inform the new task per trajectory terminal evaluation r(? ) in E1* to give the training signal (where r is the reward). I am a bit confused by this setting. The model never sees any rewards for E1 but it does see rewards for E2? How is this zero shot? The authors then propose their approach, I wish some of it had been described more rigorously with math (e.g. the loss etc.) so it was easier to understand for people not in the domain and familiar with some of the terminology. Empirically the authors show results for 3 datasets and this seems thorough."}
{"id": "iclr2020_28", "title": "Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps | OpenReview", "abstract": "Abstract:###Modern neural network architectures use structured linear transformations, such as low-rank matrices, sparse matrices, permutations, and the Fourier transform, to improve inference speed and reduce memory usage compared to general linear maps. However, choosing which of the myriad structured transformations to use (and its associated parameterization) is a laborious task that requires trading off speed, space, and accuracy. We consider a different approach: we introduce a family of matrices called kaleidoscope matrices (K-matrices) that provably capture any structured matrix with near-optimal space (parameter) and time (arithmetic operation) complexity. We empirically validate that K-matrices can be automatically learned within end-to-end pipelines to replace hand-crafted procedures, in order to improve model quality. For example, replacing channel shuffles in ShuffleNet improves classification accuracy on ImageNet by up to 5%. Learnable K-matrices can also simplify hand-engineered pipelines---we replace filter bank feature computation in speech data preprocessing with a kaleidoscope layer, resulting in only 0.4% loss in accuracy on the TIMIT speech recognition task. K-matrices can also capture latent structure in models: for a challenging permuted image classification task, adding a K-matrix to a standard convolutional architecture can enable learning the latent permutation and improve accuracy by over 8 points. We provide a practically efficient implementation of our approach, and use K-matrices in a Transformer network to attain 36% faster end-to-end inference speed on a language translation task.", "review": "Review:###The authors propose learnable *kaleidoscope matrices* (K-matrices) in place of manually engineered structured and sparse matrices. By capturing *all* structured matrices in a way that can be learned, and without imposing a specific structure or sparsity pattern, these K-matrices can improve on existing systems by * capturing more structure (that was not handled by the existing manually engineered architecture), * running faster than dense implementations. The claim that *all* structured matrices can be represented efficiently is a strong one, and in section 2.3 the authors make it clear what they mean by this. Although the proof is long and beyond the expertise of this reviewer, the basic explanation given in section 2.3 makes their point clear for the non-expert reader. The balance of the paper empirically tests the claims of learnable structure and efficiency. On the basis that these experiments essentially bear out the claims of the paper, I selected to accept the paper. Weaknesses: 1. Regarding the ISWLT translation task result: With this dataset, it*s a bit of a stretch to say there was *only a 1 point drop in BLEU score*. That*s a significant drop, and in fact the DynamicConv paper goes to significant lengths to make a smaller 0.8 point improvement. There are probably many other ways to trade BLEU score for efficiency, and without showing those other methods (and the point drops they have), it*s not clear that K-matrices are a good way to speed up decoding a bit."}
{"id": "iclr2020_29", "title": "Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning | OpenReview", "abstract": "Abstract:###Off-policy estimation for long-horizon problems is important in many real-life applications such as healthcare and robotics, where high-fidelity simulators may not be available and on-policy evaluation is expensive or impossible. Recently, citet{liu18breaking} proposed an approach that avoids the curse of horizon suffered by typical importance-sampling-based methods. While showing promising results, this approach is limited in practice as it requires data being collected by a known behavior policy. In this work, we propose a novel approach that eliminates such limitations. In particular, we formulate the problem as solving for the fixed point of a *backward flow* operator and show that the fixed point solution gives the desired importance ratios of stationary distributions between the target and behavior policies. We analyze its asymptotic consistency and finite-sample generalization. Experiments on benchmarks verify the effectiveness of our proposed approach.", "review": "Review:###Summary This paper proposes using a black-box estimation method from Liu and Lee 2017 to estimate the propensity score for off-policy reinforcement learning. They suggest that, even when the full proposal distribution is available (as is the case in much of the off-policy RL literature), that estimating the propensity score can allow for a much lower variance off-policy correction term. This reduced variance allows the proposed method to *break the curse of horizon* as termed by Liu 2018. Review The proposed fixed point formulation for learning a parameterized off-policy state distribution allows for lower variance off-policy corrections decreasing the impact of the curse of horizon. This approach appears both theoretically sound and empirically well-supported. I am concerned with the consistency argument made in section 4.3. It appears from equation (10) and the final statement of theorem 4.1 that it is necessary to have two independent samples of x* given a single state x. In the general case, without access to the environment model, it is not possible to obtain two samples of x*. If the environment is continuing, then the probability of returning to state x to obtain a second sample is 0. Am I misunderstanding the requirements specified by the objective function in equation (10)? An additional concern with the consistency argument is that it appears to assume that the approximator for d_w can achieve 0 error according to the MMD. If d_pi is not representable by the approximator, which could reasonably be the case in more challenging domains, it is unclear from this analysis or empirical results what the behavior of the system will be. It is difficult to assess how large of an assumption this is for consistency claim; for difficult problems will d_pi never be representable, or is this a fairly low concern? The empirical results look to have high enough variance in the final outcomes that it is difficult to consistently assess the performance of each algorithm (looking specifically at figure 3). However, the primary competitor algorithm is IPS which the proposed algorithm handily beats in three of the four problems. In the fourth problem, it is unclear that the competitor algorithm is winning, and could in fact be better only due to chance given the size of the respective error bars. It is worth noting that, because the parameter settings were tuned only for 50 trajectories, it is important to primarily assess performance based only on that point. It is likely that, given more trajectories to learn from, each algorithm would have chosen a smaller stepsize and effectively performed similarly. Additional comments (did not affect score) I would be interested in seeing the scalability of this approach empirically. Given the additional parameterized function to learn, I am unsure if this method would scale reasonably to much larger problems. However, I recognize that the scalability question is largely outside the scope of this paper."}
{"id": "iclr2020_30", "title": "All Simulations Are Not Equal: Simulation Reweighing for Imperfect Information Games | OpenReview", "abstract": "Abstract:###Imperfect information games are challenging benchmarks for artificial intelligent systems. To reason and plan under uncertainty is a key towards general AI. Traditionally, large amounts of simulations are used in imperfect information games, and they sometimes perform sub-optimally due to large state and action spaces. In this work, we propose a simulation reweighing mechanism using neural networks. It performs backwards verification to public previous actions and assign proper belief weights to the simulations from the information set of the current observation, using an incomplete state solver network (ISSN). We use simulation reweighing in the playing phase of the game contract bridge, and show that it outperforms previous state-of-the-art Monte Carlo simulation based methods, and achieves better play per decision.", "review": "Review:###This paper presents an approach to playing imperfect information games, an “Incomplete State Solver Network” (ISSN) within the domain of contact bridge. The paper’s primary technical contributions are the network, and a large dataset of contact bridge games, which the authors make publicly available. I believe that the work that the authors did in regards to this paper is valuable. However, I had a very hard time following along with the paper. The major issue is the language, which I mean both in terms of particular phrases or terms going unexplained, and the grammar and phrasing of the sentences. The most clear early example of terms and phrases going unexplained is the second section describing contract bridge. Many terms are used without any explanation. For example, what a target contract is or what a trump means in the case of contract bridge. This made the remainder of the paper, including the results difficult to understand. As an indication of the grammar and phrasing issues in the paper I have below included issues from just the first page of the paper: - “In real world”-> “In the real world” - “have to made decision” -> “have to make decisions” - “researchers steers towards” -> “researchers have focused on” - “Chess, best action” -> “Chess, the best action” - “it is independent of opponent and action history” -> “it is independent of the opponent or action history” - “history actions” -> “action histories” - “In early ages there are heuristic based system to assign different prior” -> “Early approaches employ heuristic based systems to assign different priors” - “try to convert the problem to a perfect” -> “try to convert the problem into a perfect” - “explicitly with neural networks, and try to optimize it” -> “explicitly with neural networks, trying” - (not a phrasing issue but I would have appreciated a citation for the claim at the end of the third paragraph of the intro) - “Simulation based approach usually requires large” -> “Simulation based approaches usually require a large” - “sequences of existing player and opponent” -> “sequences of the existing player and opponent” - “a few underlying complete information state” -> “a few underlying complete information states” This issue of readability came up in the figures as well. The figures in the paper are dense and very difficult to parse. There is little explanation in the text, and I found them difficult to glean anything from without an understanding of contact bridge. From what I can gather from the paper this is good and valuable work. However, I think the paper is not yet ready for publication as a communication of this work."}
{"id": "iclr2020_31", "title": "Learning Robust Representations via Multi-View Information Bottleneck | OpenReview", "abstract": "Abstract:###The information bottleneck method provides an information-theoretic view of representation learning. The original formulation, however, can only be applied in the supervised setting where task-specific labels are available at learning time. We extend this method to the unsupervised setting, by taking advantage of multi-view data, which provides two views of the same underlying entity. A theoretical analysis leads to the definition of a new multi-view model which produces state-of-the-art results on two standard multi-view datasets, Sketchy and MIR-Flickr. We also extend our theory to the single-view setting by taking advantage of standard data augmentation techniques, empirically showing better generalization capabilities when compared to traditional unsupervised approaches.", "review": " This paper extends the information bottleneck method of Tishby et al. (2000) to the unsupervised setting. By taking advantage of multi-view data, they provide two views of the same underlying entity. Experimetal results on two standard multi-view datasets validate the efficacy of the proposed method. I have three questions about this work. 1. The proposed method only provides two views of the same underlying entity, what about 3 or more views? 2. Can this method be used for multi-modality case? 3. What about the time efficiency of the proposed method?"}
{"id": "iclr2020_32", "title": "Regularizing activations in neural networks via distribution matching with the Wassertein metric | OpenReview", "abstract": "Abstract:###Regularization and normalization have become an indispensable component in deep learning because it enables faster training and improved generalization performance. We propose the projected error function regularization loss (PER) that encourages activations to follow the standard normal distribution. PER randomly projects activations to one dimensional space and computes the regularization in the projected space. PER acts like the Pseudo-Huber loss in the projected space, enabling robust regularization for training deep neural networks. In addition, PER can capture interaction between hidden units by projection vector drawn from unit sphere. By doing so, PER minimizes the upper bound of the Wasserstein distance of order one between an empirical distribution of activations and the standard normal distribution. To the best of the authors* knowledge, this is the first work to regularize activations concerning the target distribution in the probability distribution space. We evaluate the proposed method on image classification task and word-level language modeling task.", "review": " This submission belongs to the general field of neural networks and sub-field of activation regularisation. In particular, this submission proposes a novel approach for activation regularisation whereby a distribution of activations within minibatch are regularised to have standard normal distribution. The approach, projected error function regularisation (PER), accomplishes that by minimising an upper-bound on 1-Wasserstein distance between empirical and standard normal distributions. I think the idea described in this submission is interesting. Unfortunately, I have issues with 1) presentation, 2) experimental results, 3) English. The PER is presented as an objective function that minimises an upperbound on 1-Wasserstein. I believe I have seen no evidence to the origin of PER other than it is the upper-bound on 1-Wasserstein. Therefore, I find it strange to see a presentation where first an objective function is introduced, then 1-Wasserstein is described, and after applying standard inequality you obtain an expression that is PER. The current presentation seems to indicate that before this derivation has been done no one new the connection between PER and the upper bound on 1-Wasserstein. I disagree and say that you obtained the upper bound on 1-Wasserstein and called it PER. For unknown reasons you decided to present first PER, then upper bound and finally claim connection. This is a mistake as it is not a connection but merely a consequence. Simply looking up CIFAR-10 best numbers on any search engine I can find significantly better numbers. It is therefore unclear why did you decide to use sub-optimal configuration without commenting on that. The same applies to PTB and possibly to WikiText2. There are numerous places where English is not adequate. For instance, *new perspective of concerning the target distribution*. Following the rebuttal stage where the authors have made significant changes to the manuscript I have decided to increase my assessment score."}
{"id": "iclr2020_33", "title": "OmniNet: A unified architecture for multi-modal multi-task learning | OpenReview", "abstract": "Abstract:###Transformer is a popularly used neural network architecture, especially for language understanding. We introduce an extended and unified architecture that can be used for tasks involving a variety of modalities like image, text, videos, etc. We propose a spatio-temporal cache mechanism that enables learning spatial dimension of the input in addition to the hidden states corresponding to the temporal input sequence. The proposed architecture further enables a single model to support tasks with multiple input modalities as well as asynchronous multi-task learning, thus we refer to it as OmniNet. For example, a single instance of OmniNet can concurrently learn to perform the tasks of part-of-speech tagging, image captioning, visual question answering and video activity recognition. We demonstrate that training these four tasks together results in about three times compressed model while retaining the performance in comparison to training them individually. We also show that using this neural network pre-trained on some modalities assists in learning unseen tasks such as video captioning and video question answering. This illustrates the generalization capacity of the self-attention mechanism on the spatio-temporal cache present in OmniNet.", "review": "Review:###The authors propose an extended and unifying learning architecture – OmniNet- based on transformer, which tackles tasks with various modalities such as images, text and videos. This is attained with a spatio-temporal cache mechanism, that can both capture and store temporal information and spatial information. In addition, this proposed framework supports asynchronous multi-task learning with pre-trained neural networks on different modalities. OmiNet’s generalization capability is illustrated and demonstrated with experiments. The proposed model has multiple peripheral networks each majoring on one unique modality of data. These peripheral networks project input data into the same shared format/space that can be uniformly processed in a central neural processor working like a CPU. The central neural processor uses self-attention and RNN for temporal encoding and spatial encoding. The output of the encoding components are stored in temporal and spatial caches respectively. The two caches are then used as input to the spatial temporal decoder for different tasks. Overall, this paper is well-written, and technically sounds, with comprehensive experimental results. However, I still have two concerns below that prevent me from giving a direct acceptance. 1. However, considering the proposed model attempts to solve the multi-task learning problem, there seems no multi-task learning methods compared as baselines, making it hard to justify the performance. 2. Furthermore, in Table 1, the Omin-Net results are not as good as SOTA, without clear explanation. The parameter settings for SOTA are also missing."}
{"id": "iclr2020_34", "title": "Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness | OpenReview", "abstract": "Abstract:###Previous work shows that adversarially robust generalization requires larger sample complexity, and the same dataset, e.g., CIFAR-10, which enables good standard accuracy may not suffice to train robust models. Since collecting new training data could be costly, we focus on better utilizing the given data by inducing the regions with high sample density in the feature space, which could lead to locally sufficient samples for robust learning. We first formally show that the softmax cross-entropy (SCE) loss and its variants convey inappropriate supervisory signals, which encourage the learned feature points to spread over the space sparsely in training. This inspires us to propose the Max-Mahalanobis center (MMC) loss to explicitly induce dense feature regions in order to benefit robustness. Namely, the MMC loss encourages the model to concentrate on learning ordered and compact representations, which gather around the preset optimal centers for different classes. We empirically demonstrate that applying the MMC loss can significantly improve robustness even under strong adaptive attacks, while keeping state-of-the-art accuracy on clean inputs with little extra computation compared to the SCE loss.", "review": "Review:###This paper first shows some potential issues of softmax loss (i.e., cross-entropy loss with softmax function) and then propose the Max-Mahalanobis center (MMC) loss to encourge the intra-class compactness for better adversarial robustness. The MMC loss is essentially minimizing the distance between the feature and the pre-fixed class center. Different from center loss, these centers are determined by minimizing the maximum inner product between any two class centers. Since the norm of these class centers are normalized to a constant. It is equivalent to angles. This acutally reminds me of a number of works in angular margin-based softmax loss. Just to name a few: [1] Large-Margin Softmax Loss for Convolutional Neural Networks, ICML 2016 [2] SphereFace: Deep Hypersphere Embedding for Face Recognition, CVPR 2017 [3] Soft-margin softmax for deep classification, ICNIP 2017 [4] CosFace: Large Margin Cosine Loss for Deep Face Recognition, CVPR 2018 [5] ArcFace: Additive Angular Margin Loss for Deep Face Recognition, CVPR 2019 I think these works are closely related to what the authors aim to do, and therefore they should be discussed methodologically and compared empirically. Besides that, I think it is also worth conducting an ablation study for how to determine these class centers. This paper considers to minimize the maximum inner product. There are a few papers listed below that explicitly discusses how to make the class centers uniformly spaced. The authors may consider to compare these methods for determining the class centers. [1] Learning towards Minimum Hyperspherical Energy, NeurIPS 2018 [2] UniformFace: Learning Deep Equidistributed Representation for Face Recognition, CVPR 2019 For the experiments, the MMC loss indeed shows some advantages over the softmax loss. I am basically convinced by the experiments, although it can further strengthen the paper if the authors can conduct some evaluations on large-scale datasets like ImageNet. I appreciate the authors provide many theoretical justifications, which is inspiring. Intuitively speaking, I can understand that shrinking the feature space (i.e., make feature distribution more compact) can improve the adversarial robustness. As a result, I think this paper is naturally motivated and is also theoretically sound. The experiments can be further improved."}
{"id": "iclr2020_35", "title": "Training Neural Networks for and by Interpolation | OpenReview", "abstract": "Abstract:###In modern supervised learning, many deep neural networks are able to interpolate the data: the empirical loss can be driven to near zero on all samples simultaneously. In this work, we explicitly exploit this interpolation property for the design of a new optimization algorithm for deep learning. Specifically, we use it to compute an adaptive learning-rate in closed form at each iteration. This results in the Adaptive Learning-rates for Interpolation with Gradients (ALI-G) algorithm. ALI-G retains the main advantage of SGD which is a low computational cost per iteration. But unlike SGD, the learning-rate of ALI-G uses a single constant hyper-parameter and does not require a decay schedule, which makes it considerably easier to tune. We provide convergence guarantees of ALI-G in the stochastic convex setting. Notably, all our convergence results tackle the realistic case where the interpolation property is satisfied up to some tolerance. We provide experiments on a variety of architectures and tasks: (i) learning a differentiable neural computer; (ii) training a wide residual network on the SVHN data set; (iii) training a Bi-LSTM on the SNLI data set; and (iv) training wide residual networks and densely connected networks on the CIFAR data sets. ALI-G produces state-of-the-art results among adaptive methods, and even yields comparable performance with SGD, which requires manually tuned learning-rate schedules. Furthermore, ALI-G is simple to implement in any standard deep learning framework and can be used as a drop-in replacement in existing code.", "review": "Review:###This paper proposes a new adaptive learning rate method which is tailored to the optimization of deep neural networks. The motivating observation is that over-parameterized DNNs are able to interpolate the training data (i.e. they are able to reach near-zero training error). This enables application of the Polyak update rule to stochastic updates and a simplification by assuming a zero minimal training loss. A number of proofs for convergence in various convex settings are provided, and empirical evaluation on several benchmarks demonstrates (a) ability to optimize complex architectures, (b) performance improvements over, and (c) performance close to manually tuned SGD learning rates. I vote for accepting this paper. The approach is well-motivated, the method is described clearly and detail, and the experiments support the paper*s claims well. What I would still like to see are a few additional details regarding the experimental protocol. In particular, did you train a single or multiple models for each result that is reported? Do different runs start from the exact same weight initialization? What condition was used to stop the training? The results in section 5.2. are all very close to each other, and it would be helpful to have a sense of the variability of the different methods. The graphs in Figure 4 do look like the models did not converge yet. Generally, it would be nice to examine the behavior of the method in cases where the neural network is underparameterized or is otherwise unable to effectively interpolate the training data. Does the method lead to divergence in this case, or is it subpar to other methods? I think section 2.2. could benefit from a short motivational introduction; on the first read, I was not clear about the purpose of introducing the Polyak step size as it is not mentioned explicitly in the text leading to it."}
{"id": "iclr2020_36", "title": "Neural Module Networks for Reasoning over Text | OpenReview", "abstract": "Abstract:###Answering compositional questions that require multiple steps of reasoning against text is challenging, especially when they involve discrete, symbolic operations. Neural module networks (NMNs) learn to parse such questions as executable programs composed of learnable modules, performing well on synthetic visual QA domains. However, we find that it is challenging to learn these models for non-synthetic questions on open-domain text, where a model needs to deal with the diversity of natural language and perform a broader range of reasoning. We extend NMNs by: (a) introducing modules that reason over a paragraph of text, performing symbolic reasoning (such as arithmetic, sorting, counting) over numbers and dates in a probabilistic and differentiable manner; and (b) proposing an unsupervised auxiliary loss to help extract arguments associated with the events in text. Additionally, we show that a limited amount of heuristically-obtained question program and intermediate module output supervision provides sufficient inductive bias for accurate learning. Our proposed model significantly outperforms state-of-the-art models on a subset of the DROP dataset that poses a variety of reasoning challenges that are covered by our modules.", "review": "Review:###Paper Claims The paper offers a new deep learning approach to symbolic reasoning over text. They propose using Neural Module Networks to perform explicit reasoning steps that are nevertheless differentiable. The process is separated into a semantic parsing of the question, and a resolution using MNMs. Auxiliary tasks improve performance and enable using a BERT pretrained model as a seed. The proposed model*s performance surpasses previous SOTA on several question types. Decision I*m in favor of accepting this paper because it tackles an extremely challenging and important problem in a novel and successful way. Reasoning has progressed more slowly than other NLP domains, and answering complex multi-reasoning-step questions is a good way to tackle the core of the problem. Regarding the approach, I find the mix of explicit reasoning steps, deep learning modeling, and even heuristics for some of the data preparation, powerful and effective. I see this as a useful step to advance the body of work in this space -- despite not being the desired end-result, it is nevertheless very instructive of what might work for at least some parts of the larger, AI-complete reasoning problem. Also, the paper is clear, well-written, and well-motivated. Further details on Decision I*m more than satisfied with the breadth of question types tackled here, and correspondingly the variety of modules. There*s extensive, valuable work in designing these modules and making them work together. As the authors point out, it appears that other types of questions will require more intricate modules (or some other means), and I suspect that predetermined modules will not be what generalizes eventually. Nevertheless, the NMN approach taken here can be a stepping stone to further understanding how to tackle symbolic reasoning in a deep neural network. It will be instructive in designing a more ambitious, generalizable model. The auxiliary supervision tasks appear to be essential to obtaining the results, most notably the unsupervised loss for IE. I think this area has room for further improvement, but what is achieved in the paper is sufficient for publication. In particular, the writing of heuristics is a very specific solution targeting specific types of question and this will not scale to the full scope of natural language questions, and much less to all reasoning. Discussion of how to expand on them, scale them (automatic discovery, some other means?), etc. would be very welcome, as it is the main weakness of the paper. I also think the methodology is sound and the results are obtained in a reasonable and mostly reproducible way. This is truly great work that deserves to be published, discussed, and expanded upon."}
{"id": "iclr2020_37", "title": "Information Plane Analysis of Deep Neural Networks via Matrix--Based Renyi*s Entropy and Tensor Kernels | OpenReview", "abstract": "Abstract:###Analyzing deep neural networks (DNNs) via information plane (IP) theory has gained tremendous attention recently as a tool to gain insight into, among others, their generalization ability. However, it is by no means obvious how to estimate mutual information (MI) between each hidden layer and the input/desired output, to construct the IP. For instance, hidden layers with many neurons require MI estimators with robustness towards the high dimensionality associated with such layers. MI estimators should also be able to naturally handle convolutional layers, while at the same time being computationally tractable to scale to large networks. None of the existing IP methods to date have been able to study truly deep Convolutional Neural Networks (CNNs), such as the e.g. VGG-16. In this paper, we propose an IP analysis using the new matrix--based R*enyi*s entropy coupled with tensor kernels over convolutional layers, leveraging the power of kernel methods to represent properties of the probability distribution independently of the dimensionality of the data. The obtained results shed new light on the previous literature concerning small-scale DNNs, however using a completely new approach. Importantly, the new framework enables us to provide the first comprehensive IP analysis of contemporary large-scale DNNs and CNNs, investigating the different training phases and providing new insights into the training dynamics of large-scale neural networks.", "review": "Review:###In this paper, the authors try to resolve the problem of estimating mutual information between high-dimensional layers in neural network. Specifically, the authors try to tackle the convolutional neural networks, by propose a novel tensor-kernel based estimator. They use the proposed estimator to discover the similar trends on the information plane as the previous works. In general, I think the introduction of the tensor kernels for mutual information estimation is the key contribution of this paper. However, I think this contribution is a little bit incremental, compared to the multivariate matrix-based version introduced by Yu et.al. In formula (7), the authors use the Frobenius distance of two tensors as the input of the kernal, which is equivalent to vectorize the tensor and use Euclidean distance. This approach does not capture the special structure of tensor and seems incremental. Also, the phenomenon of *turning point* on information plane for neural network has been challenged since its first publication. Despite its high citation, the deep meaning of this phenomenon has not been clearly studied. So an incremental change on computing such a questionable phenomenon make the contribution of this paper not very strong. So I would like to give a weak reject."}
{"id": "iclr2020_38", "title": "Multi-Scale Representation Learning for Spatial Feature Distributions using Grid Cells | OpenReview", "abstract": "Abstract:###Unsupervised text encoding models have recently fueled substantial progress in Natural Language Processing (NLP). The key idea is to use neural networks to convert words in texts to vector space representations (embeddings) based on word positions in a sentence and their contexts. We see a strikingly similar situation in spatial analysis, which focuses on incorporating both absolute positions and spatial contexts of geographic objects such as Points of Interest (POIs) into models. A general space encoding method is valuable for a multitude of tasks such asPOI search, land use classification, point-based spatial interpolation and locationaware image classification. However, no such general model exists to date beyond simply applying discretizing or feed forward nets to coordinates, and little effort has been put into jointly modeling distributions with vastly different characteristics, which commonly emerges from GIS data. Meanwhile, Nobel Prize-winning Neuroscience research shows that grid cells in mammals provide a multi-scale periodic representation that functions as a metric for encoding space and are critical for recognizing places and for path-integration. Inspired by this research, wepropose a representation learning model called Space2vec to encode the absolutepositions and spatial relationships of places. We conduct experiments on realworld geographic data and predict types of POIs at given positions based on their1) locations and 2) nearby POIs. Results show that because of its multi-scale representations Space2vec outperforms well-established ML approaches such as RBF kernels, multi-layer feed forward nets, and tile embedding approaches.", "review": "Review:###This paper presents a new method called *Space2Vec* to compute spatial embeddings of a pixel in a spatial data. The primary motivation of Space2Vec is to integrate representations of different spatial scales which could potentially make the spatial representations more informative and meaningful as features. Space2Vec is trained as a part of an encoder-decoder framework, where Space2Vec encodes the spatial features of all the points that are fed as input to the framework. They conducted experiments on real world geographic data where they predict types of point of interests (POIs) at given positions based on their 1) locations (location-modeling) and 2) spatial neighborhood (spatial context modeling). They evaluated Space2Vec against other ML approaches for encoding spatial information including RBF kernels, multi-layer feed forward nets, and tile embedding approaches. Their results indicate that that Space2Vec approach performs better (albeit marginally) than other ML methods. I am giving this paper a weak reject rating mainly because of weak results and lack of motivation for location modeling problem (where their approach performs significantly better than baselines). I explain my concerns below under detailed comments. Detailed Comments: 1) Motivation of location modeling problem does not sound compelling enough to me, especially in the context of Point of Interest(POI) classification approach. I could not imagine any scenario where access to information from spatial neighborhood will be denied. If authors could present strong motivating examples for this problem and demonstrate the utility of their proposed approach in that setting, that will make the paper much stronger. 2) In spatial context modeling problem, the improvements in the results (Table 2) appear to be marginal(0.185 against 0.181, 25.7 against 25.3). Authors should try out more datasets to convincingly justify the superiority of their approach over other methods. EDIT: AFTER RECEIVING AUTHOR*S RESPONSE I am satisfied with author*s response to my comments. I am updating my rating to Weak Accept."}
{"id": "iclr2020_39", "title": "Learning Neural Surrogate Model for Warm-Starting Bayesian Optimization | OpenReview", "abstract": "Abstract:###Bayesian optimization is an effective tool to optimize black-box functions and popular for hyper-parameter tuning in machine learning. Traditional Bayesian optimization methods are based on Gaussian process (GP), relying on a GP-based surrogate model for sampling points of the function of interest. In this work, we consider transferring knowledge from related problems to target problem by learning an initial surrogate model for warm-starting Bayesian optimization. We propose a neural network-based surrogate model to estimate the function mean value in GP. Then we design a novel weighted Reptile algorithm with sampling strategy to learn an initial surrogate model from meta train set. The initial surrogate model is learned to be able to well adapt to new tasks. Extensive experiments show that this warm-starting technique enables us to find better minimizer or hyper-parameters than traditional GP and previous warm-starting methods.", "review": " POST-REBUTTAL FEEDBACK Thanks for your response. The justifications provided in the response have not convinced me to improve my score. They are at times hard to understand: For example, the authors have claimed that while their design choice is not reasonable, it is less unreasonable than the other. SUMMARY OF REVIEW The authors have proposed the use of a neural surrogate model in place of the GP posterior mean and a weighted Reptile algorithm to meta-learn the initial weights of the neural surrogate model. This approach appears interesting. However, there seems to be multiple highly restrictive (at times impractical) assumptions in this work that are atypical of the BO setting adopted by other meta BO algorithms and not discussed, as detailed below. Justifications are required. Clarifications are also needed with regards to how they exactly run their algorithm in the experiments and whether the prior/initial information from related problems/meta tasks provided to the tested algorithms is fair. DETAILED COMMENTS The authors say that *We still use the variance in Eq. (4) to measure uncertainty, because the estimation uncertainty should be independent in individual problems.* This does not seem to hold true. If a meta task or train set is indeed correlated (or provides information) to the new problem, the posterior variance/uncertainty at a point depends on the observations in the meta task or train set near to this point (see, for example, Feurer et al. (2018)). Can the authors discuss the implications of such an assumption in their work? Q and Q_i have always been referred to as problems. In Algorithm 3, Q is suddenly referred to as meta train set. On page 4, you have said that x^*_i is the minimizer of i-th problem Q_i(x) in meta train set. Based on these information, I assume that the authors consider x^*_i as the global minimizer and that x^*_i is known in order to compute the rewards. Can the authors discuss why is this a reasonable assumption? In their proposed weight Reptile algorithm (Algorithm 3), the authors have also assumed access to the black-box functions of the related problems or meta tasks, which is not typical of other meta BO works that only require the existing observations or datasets of the related problems/meta tasks. As a result, compared with the existing meta BO algorithms, their proposed weighted Reptile is considerably more expensive due to the need to additionally evaluate the black-box functions of the related problems or meta tasks many times during execution. Can the authors discuss the practical implications of such an assumption and how it affects the types of problems/applications that can be considered by this work? The authors have not provided any justification for their choice of reward on page 4. If the black-box function is indeed complex and highly varying, the distance between points may not work well at all. Can the authors provide a justification and discuss the practical implications and limitations with such a choice? Isn*t it more natural to consider a single Bayesian neural network instead of using a neural network for the mean and a GP for the variance? For the experiments, it would be good to see two other variants of the proposed algorithm to understand the individual contributions of the neural surrogate model and weighted Reptile algorithm: one without neural surrogate model and the other with simply the use of neural surrogate model. Can the authors explain in greater detail how they run their algorithms (Algorithms 2 and 3) in the experiments? For example, the authors say that *WRA-N starts with learned initial surrogate model*. I assume that WRA-N refers to Algorithm 3 based on its acronym. Isn*t the learned initial surrogate model the output of WRA-N in the first place? Also, the graphs in Fig. 2 seem to show iteration 1 to 13 in NOE (Algorithm 2). However, Algorithm 3 accepts N_T = 13 and executes NOE for N_T = 13 (and not 1, 2, or 3, ...) for each problem in each epoch. How do the authors generate the plot of WRA-N for iterations 1 to 12? What seems to make more sense to me is that the authors in fact run Algorithm 2 instead of Algorithm 3 for each experiment and they initialize w in Algorithm 2 to the output of Algorithm 3. In any case, a clarification is needed here. It is not clear to me whether the initial/prior information from related problems/meta tasks provided to WRA-N, TST-R, AND TSR-M is fair. Can the authors provide a justification? To clarify, for each related problem/function, only N_T number of datapoints are used to train a corresponding neural network with 1 hidden layer of 15 hidden units? The authors say that *Since TST-R needs base models for combination, we sample 20 points from uniform distribution in (?10, 10) to construct base models.* Is this sampling procedure the same as that in (Wistuba et al., 2016)? Can the authors explain the comparable performance of WRA-N and TST-R in Fig. 5? Why are the error bars missing? How does the proposed approach compare with that of Feurer et al. (2018)? Minor issues Page 1, 3: adapt well to new tasks. Page 2: The author says *depends on a GP-based surrogate model fitting function values without learnable parameters*. This is not true: The GP hyperparameters need to be learned and they adapt to new problems. Page 4: descent order? Pages 4, 5: Why is there an input x to Q_i? Algorithm 2: t^* should be at the superscript of x. Equation 7: What is N? Page 5: Does it make a difference in the performance when delta is set to 0? Page 5: well define meta-features?"}
{"id": "iclr2020_40", "title": "Representing Model Uncertainty of Neural Networks in Sparse Information Form | OpenReview", "abstract": "Abstract:###This paper addresses the problem of representing a system*s belief using multi-variate normal distributions (MND) where the underlying model is based on a deep neural network (DNN). The major challenge with DNNs is the computational complexity that is needed to obtain model uncertainty using MNDs. To achieve a scalable method, we propose a novel approach that expresses the parameter posterior in sparse information form. Our inference algorithm is based on a novel Laplace Approximation scheme, which involves a diagonal correction of the Kronecker-factored eigenbasis. As this makes the inversion of the information matrix intractable - an operation that is required for full Bayesian analysis, we devise a low-rank approximation of this eigenbasis and a memory-efficient sampling scheme. We provide both a theoretical analysis and an empirical evaluation on various benchmark data sets, showing the superiority of our approach over existing methods.", "review": " The contribution of the paper is marginal, as the principle of imposing Gaussians on the network to perform Bayes is not new. The Laplace-based approximation is half-baked and certainly much better techniques for Bayes exist in the recent literature; furthermore, it*s selection is not substantiated enough, and, of course, it represents no novelty. The experimental results are not convincing, as both the considered scenarios are limited and the comparisons are too poor (no consideration of state of the art alternatives). The provided corollaries are not actually helpful and should be put in the appendix."}
{"id": "iclr2020_41", "title": "On the Convergence of FedAvg on Non-IID Data | OpenReview", "abstract": "Abstract:###Federated learning enables a large amount of edge computing devices to jointly learn a model without data sharing. As a leading algorithm in this setting, Federated Averaging (\texttt{FedAvg}) runs Stochastic Gradient Descent (SGD) in parallel on a small subset of the total devices and averages the sequences only once in a while. Despite its simplicity, it lacks theoretical guarantees under realistic settings. In this paper, we analyze the convergence of \texttt{FedAvg} on non-iid data and establish a convergence rate of for strongly convex and smooth problems, where is the number of SGDs. Importantly, our bound demonstrates a trade-off between communication-efficiency and convergence rate. As user devices may be disconnected from the server, we relax the assumption of full device participation to partial device participation and study different averaging schemes; low device participation rate can be achieved without severely slowing down the learning. Our results indicate that heterogeneity of data slows down the convergence, which matches empirical observations. Furthermore, we provide a necessary condition for \texttt{FedAvg} on non-iid data: the learning rate must decay, even if full-gradient is used; otherwise, the solution will be away from the optimal.", "review": "Review:###Federated learning is distinguished from the standard distributed learning in the following sense: 1) training is distributed over a huge number (say N) of devices and communication between the central server and devices are slow. 2) The central server has no control of individual devices, and there are inactive devices that does not respond to the server; full participation of all devices is unrealistic. 3) The local data distribution at each device is different from each other; i.e., the data is non-iid. Due to property 1), communication-efficient algorithms such as Federated Averaging (FedAvg) have been proposed and studied. FedAvg runs SGD in parallel on K (?N) local devices using their local datasets, and updates the global parameter after E local iterations by aggregating the updates from the local devices. Properties 2) and 3) makes analysis of FedAvg difficult, and previous results have proven convergence of FedAvg assuming that the data is iid and/or all devices are active. In contrast, this paper studies FedAvg on the non-iid data and inactive devices setting and shows that, with adequately chosen aggregation schemes and decaying learning rate, FedAvg on strongly convex and smooth functions converges with a rate of O(1/T). Overall, I enjoyed reading this paper and I would like to recommend acceptance. This is the first result showing convergence rate analysis of FedAvg under presence of properties 2) and 3), which is a nontrivial, important, and timely problem. The paper is well-written and reads smoothly, except for some minor typos. The convergence bounds provide insights of practical relevance, e.g., the optimal choice of E, the effect of K in convergence rate, etc. The authors also provide empirical results supporting their theoretical analysis. Some questions I have in mind: - What is *transformed Scheme II*? Is it the scaling trick described at the end of Section 3.3? The name appears in the experiment section before being defined. - What happens if we choose eta_t that is decaying but slower than O(1/t), say O(1/sqrt t)? Can convergence be proved? If so, in what rate? Minor typos: - Footnote 3: know -> known - Assumptions 1 & 2: f in is math-bold - Choice of sampling schemes: *If the system can choose to active...* -> activate - mnist balanced and mnist unbalanced: the description after them suggests they should be switched - Apdx D.1: widely -> wide, summary -> summarize"}
{"id": "iclr2020_42", "title": "BlockSwap: Fisher-guided Block Substitution for Network Compression on a Budget | OpenReview", "abstract": "Abstract:###The desire to map neural networks to varying-capacity devices has led to the development of a wealth of compression techniques, many of which involve replacing standard convolutional blocks in a large network with cheap alternative blocks. However, not all blocks are created equally; for a required compute budget there may exist a potent combination of many different cheap blocks, though exhaustively searching for such a combination is prohibitively expensive. In this work, we develop BlockSwap: a fast algorithm for choosing networks with interleaved block types by passing a single minibatch of training data through randomly initialised networks and gauging their Fisher potential. These networks can then be used as students and distilled with the original large network as a teacher. We demonstrate the effectiveness of the chosen networks across CIFAR-10 and ImageNet for classification, and COCO for detection, and provide a comprehensive ablation study of our approach. BlockSwap quickly explores possible block configurations using a simple architecture ranking system, yielding highly competitive networks in orders of magnitude less time than most architecture search techniques (e.g. 8 minutes on a single CPU for CIFAR-10).", "review": "Review:###This paper introduces an approach to compressing a deep network so as to satisfy a given budget. In contrast to methods that replace all convolutional blocks with the same cheaper alternative, the authors argue that mixing different types of such cheap alternatives should be effective. To achieve this in a fast manner, they propose to randomly sample architectures with mixed blocktypes that satisfy the given budget and rank these architectures using Fisher information. Originality: - The approach is simple, but seems effective. Although the different components that make up the final method are known, they are put together so as to address a different task, in a way that I find interesting. Methodology: - From the paper, it is not entirely clear how sampling under budget constraints is achieved. I imagine that one could do it in a naive way, by choosing the first block uniformly randomly, and then removing the candidates that do not fit the budget for the subsequent blocks. This, however, would seem to give a different importance to the early blocks than to the later ones, since the former would essentially not have any budget constraints. I would appreciate it is the authors could explain their strategy in more detail. - The assumption that the number of blocks in the student is the same as in the teacher seems constraining. A better architecture might be obtained by having fewer, wider blocks, or more, thinner blocks. Do the authors see a potential way to address this limitation? - In principle, it seems that the proposed Fisher Information-based strategy could be used for general NAS, not just for compact architectures. Have the authors investigated this direction? Related work: - It seems to me that the literature review on compression/pruning is a bit shallow. I acknowledge, however, that most works do not tackle the scenario where a budget is given. However, Chen et al., *Constraint-aware Deep Neural Network Compression*, ECCV 2018, do, and it would be interesting to discuss and provide comparisons with this work. Experiments: - I appreciate the ablation study, which answered several of my questions. - In Table 1, it seems counterintuitive that the (negative) correlation becomes smaller as the number of mini-batches increases. Do the authors have an explanation for this? - In Section 5, are the baseline compact networks all trained using the same Attention Transfer algorithm as for the BlockSwap ones? - In Table 2, the budget values (P. (K)) seem fairly arbitrary? How were they obtained? They seem to match those of SNIP. Is this because the authors ran SNIP, and then set their budget accordingly? - Below Fig. 3, the authors mention that they are generous with sparsity-based methods because they count the number of non-zero parameters. Note that several structured-sparsity compression methods have been proposed (Alvarez & Salzmann, NIPS 2016, 2017; Wen et al., NIPS 2016), which, by contrast with regular sparsity ones, would cancel out entire filters. Summary: Overall, I like the simple idea proposed in this paper. I would however appreciate it if the authors could clarify the way they sample the architectures and address my questions about their experiments."}
{"id": "iclr2020_43", "title": "Your classifier is secretly an energy based model and you should treat it like one | OpenReview", "abstract": "Abstract:###We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model.", "review": "Review:###The paper uses energy-based model interpretation for the logits of standard discriminative neural network models to define a generative model inside a classifier that proves useful in many downstream tasks such as uncertainty quantification, out-of-distribution detection, etc. Although there has been previous work attempting to bridge discriminative classifiers with generative modeling, this work proves to be competitive with both specialized models on discriminative/generative tasks as well as in many downstream tasks such as out-of-distribution detection, calibration, and adversarial robustness. The paper provides a clear exposition of the method, succeeds to discuss related work it bases on, conducts a thorough experimental study providing convincing explanations for results and does not hide the limitations of the work (high computational requirements, optimization difficulties connected with training energy-based model and the method used, limited approximation of the true energy). Overall, the paper provides a substantial contribution and paves the way for further work improving this joint discriminative - generative setting. However, there are points I would like the paper to address for better exposition. 1. It would benefit the paper showing that samples with higher unnormalized likelihood are visually more compelling than those with lower likelihood. 2. On CIFAR100 the accuracy drop from the reference value is larger than for datasets with 10 classes, could it be due the logits dimension is higher and challenges optimization? 3. It would also be helpful to clarify whether application of the proposed method is primarily restricted by the computational complexity or is there any property inherent to energy-based models that makes treating high-dimensional data challenging? Minor remark - Although the paper doesn*t state on which dataset results shown in Table 1 were obtained, I suspect its CIFAR10, please specify this."}
{"id": "iclr2020_44", "title": "Locally Constant Networks | OpenReview", "abstract": "Abstract:###We show how neural models can be used to realize piece-wise constant functions such as decision trees. Our approach builds on ReLU networks that are piece-wise linear and hence their associated gradients with respect to the inputs are locally constant. We formally establish the equivalence between the classes of locally constant networks and decision trees. Moreover, we highlight several advantageous properties of locally constant networks, including how they realize decision trees with parameter sharing across branching / leaves. Indeed, only neurons suffice to implicitly model an oblique decision tree with leaf nodes. The neural representation also enables us to adopt many tools developed for deep networks (e.g., DropConnect (Wan et al., 2013)) while implicitly training decision trees. We demonstrate that our method outperforms alternative techniques for training oblique decision trees in the context of molecular property classification and regression tasks.", "review": "Review:###This paper proposes locally constant network (LCN), which is implemented via the gradient of piece-wise linear networks such as ReLU networks. The authors built the equivalence between LCN and decision trees, and also demonstrated that LCN with M neurons has the same representation capability as decision trees with 2^M leaf nodes. The experiments conducted in the paper disclose that training LCN outperforms other methods using decision trees. The detailed comments are as follows: 1) The idea of LCN is very interesting, and the equivalence to decision trees is also very valuable, as it provides interpretability and shines light on new training algorithms. 2) The derivation of LCN and the equivalence is clear. The analysis based on the shared parameterization in Section 3.5 is helpful to understand why LCN with M neurons could be of equal capability to decision trees with 2^M leaf nodes. 3) One weakness is that the performance of ELCN seems to be very close to RF, as shown in Table 2. I am not sure whether some similar ideas to LCN have been explored in the literature. But the topic studied in this work is very valuable, which connects deep neural networks and decision trees."}
{"id": "iclr2020_45", "title": "NEURAL EXECUTION ENGINES | OpenReview", "abstract": "Abstract:###Turing complete computation and reasoning are often regarded as necessary pre- cursors to general intelligence. There has been a significant body of work studying neural networks that mimic general computation, but these networks fail to generalize to data distributions that are outside of their training set. We study this problem through the lens of fundamental computer science problems: sorting and graph processing. We modify the masking mechanism of a transformer in order to allow them to implement rudimentary functions with strong generalization. We call this model the Neural Execution Engine, and show that it learns, through supervision, to numerically compute the basic subroutines comprising these algorithms with near perfect accuracy. Moreover, it retains this level of accuracy while generalizing to unseen data and long sequences outside of the training distribution.", "review": "Review:###his paper deals with the problem of designing neural network architectures that can learn and implement general programs. The authors are motivated by problems in such works, mainly generalization to testing distributions that do not necessarily correspond to the training distribution. It should be made clear that the latter is a general problem in machine learning (with phenomena such as covariate shift being commonplace), the authors particularly relate this work to predicting new values and longer sequences (i.e. strong generalization). The authors further motivate their work by assuming that these phenomena are due to lack of prior structure that can be alleviated by further supervision during training. The goal is for complex behaviour to emerge via composition of simple functions (which clearly follows the deep learning paradigm).Specifically, the authors propose a modification to the transformer architecture, that does not use positional encodings (the authors mention that this was detrimental to their work - it would be good to provide some more insight into that) and single-headed attention. The main contribution seems to be adding the self-attention mask that is learned, along with execution traces that have been used in previous work. An relatively small increase in performance is observed due to this, but it seems that the experiment is limited (no standard deviation in results, so I presume one run with one initialization). Therefore it seems to me that the contribution of this paper is limited in terms of technical contribution."}
{"id": "iclr2020_46", "title": "LOGAN: Latent Optimisation for Generative Adversarial Networks | OpenReview", "abstract": "Abstract:###Training generative adversarial networks requires balancing of delicate adversarial dynamics. Even with careful tuning, training may diverge or end up in a bad equilibrium with dropped modes. In this work, we introduce a new form of latent optimisation inspired by the CS-GAN and show that it improves adversarial dynamics by enhancing interactions between the discriminator and the generator. We develop supporting theoretical analysis from the perspectives of differentiable games and stochastic approximation. Our experiments demonstrate that latent optimisation can significantly improve GAN training, obtaining state-of-the-art performance for the ImageNet (128 x 128) dataset. Our model achieves an Inception Score (IS) of 148 and an Frechet Inception Distance (FID) of 3.4, an improvement of 17% and 32% in IS and FID respectively, compared with the baseline BigGAN-deep model with the same architecture and number of parameters.", "review": "Review:###The paper proposes a novel training scheme for GANs, which leads to improved scores w.r.t. state-of-the-art. The idea is to update the sampled latent code in a direction improving the inner maximization in the min-max problem. The work considers both, the gradient direction and a direction motivated by the natural gradient which is shown to yield excellent performance. The overall scheme is motivated as an approximation to the (prohibitively expensive) symplectic gradient adjustment method. While the work contains some typos, it was easy to read and overall very well written. The experimental results are impressive and clearly validate the usefulness of the approach. Therefore, I recommend acceptance of the paper. On the theoretical side, I feel some parts can be improved. I*m willing to further increase my rating if some of the following points are addressed: 1. The connection to SGA is a bit hand-wavy, as terms are dropped or approximated with the only reasoning that they are difficult to compute. In some approximations (e.g. Gauss-Newton approximation of the Hessian) one can argue quite well that certain second-order terms can be dropped under some assumptions (e.g. mild nonlinearity, or vanishing near the optimum). 2. I had some troubles Sec 4 related to the natural gradient. (a) What is p(t | z)? Is it the same p as in (37), Appendix C? I find the argument that the hinge loss pushing D(G(z)) < 0 during training hand-wavy, as for natural gradient p(t | z) should always be a valid distribution. There are also some typos which made it hard to follow the arguments there. It probably should read *an ideal generator can perfectly fool the discriminator* (not perfectly fool the generator). (b) Maybe it would be easier to directly argue that one wishes to approximate SGA as well as possible, rather than taking a detour through the natural gradient. 3. Why is the increment Delta z clipped in Algorithm 1? Is there a theoretical justification? If the goal of the clipping is to stay inside the support of the uniform distribution, shouldn*t it rather be z* = [z + Delta z]? A soft clipping (e.g. performing a mirror descent step) might give better gradients. Typos, minor comments (no influence on my rating): - In Eq. (5), it should be partial^2 in the last *Hessian-block* multiplication. - presribed -> prescribed - Eqs. (7) and (8) might be easier to parse if one uses a different notation to distinguish between total derivative and partial derivative, i.e., write on the left side df(z*) / d\theta. Also, I think it is clearer to write in the last terms in (7) and (8) partial f(z*) / partial delta z instead of partial f(z*) / partial z*. - In Appendix B.1, shouldn*t it be gamma=(1+eta)/2 instead of gamma=eta (1+eta)/2? - I*ve found ELU activations to work well in GAN models which involve the Jacobian w.r.t. z. Maybe it can stabilize things here as well."}
{"id": "iclr2020_47", "title": "The fairness-accuracy landscape of neural classifiers | OpenReview", "abstract": "Abstract:###That machine learning algorithms can demonstrate bias is well-documented by now. This work confronts the challenge of bias mitigation in feedforward fully-connected neural nets from the lens of causal inference and multiobjective optimisation. Regarding the former, a new causal notion of fairness is introduced that is particularly suited to giving a nuanced treatment of datasets collected under unfair practices. In particular, special attention is paid to subjects whose covariates could appear with substantial probability in either value of the sensitive attribute. Next, recognising that fairness and accuracy are competing objectives, the proposed methodology uses techniques from multiobjective optimisation to ascertain the fairness-accuracy landscape of a neural net classifier. Experimental results suggest that the proposed method produces neural net classifiers that distribute evenly across the Pareto front of the fairness-accuracy space and is more efficient at finding non-dominated points than an adversarial approach.", "review": "Review:###This paper proposes a method to approximate the *fairness-accuracy landscape* of classifiers based on neural networks. The key idea is to set up a multi-dimensional objective, where one dimension is about prediction accuracy and another about fairness. The fairness component relies on a definition of fairness based on causal inference, relying on the idea that a sensitive attribute should not causally affect model predictions. I found the causal idea intriguing, since it makes sense that we don*t want a sensitive attribute to have a causal effect. However, there may be several problems with this approach: 1) For a causal estimate to be valid we need several assumptions. For example, we need A (the sensitive attribute) to be independent of potential outcomes conditional on X --- the so-called *unconfoundedness assumption* in causal inference. We also need *positivity*, i.e., that 0< P(A=1|X) <1. These assumptions are not discussed in the paper. Furthermore, the particular context of the paper, where the treatment is actually an immutable characteristic, makes such discussion much more subtle. What will we do, for instance, if there are no A=1 in the sample when X = ...? 2) The authors seem to assume that the propensity score model is well specified. This can be tested, e.g., using [1]. What do we do when this fails? 3) Why do we want U to be small, i.e., why do we want the causal effect of A to be small, is never justified. In particular, its relation to *fairness* is never fleshed out, but just assumed to be so. This can be problematic when, say, we are missing certain important X that are important for A. Then, there will be a measurable causal effect of A on h(). Some other problems: - What is the reason for focusing on *neural classifiers*? There is nothing specific in the method or analysis that relates to neural networks, except for the use of the causal estimand in a *hidden layer*. - In the Introduction, the authors could cite the works of Amartya Sea, etc., on fairness. Certainly the study of fairness problems did not start in 2016. - What exactly is a *sensitive attribute*? If we don*t want to bias our predictions, then why include it in the analysis? - It is unclear what is new and what is related work in page 3. - Sec. 4: The claim that *causal inference is about situations where manipulation is impossible* discards voluminous work in causal inference through randomized experiments. In fact, many scientists would agree that causal inference is impossible without manipulation. - As mentioned above, why this particular estimand leads to more *fairness* is never explained. - Do we need a square or abs value in Eq (5)? - The experimental section is weak and does not illustrate the claims well. It would be important to explain the choice of the particular causal estimand, the choice of the hidden layer to put the estimand in, to explore the choice of the objective, and so on. Currently, none of these choices/design aspects are being investigated. [1] *A specification test for the propensity score using its distribution conditional on participation* (Shaikh et al, 2009)"}
{"id": "iclr2020_48", "title": "Hope For The Best But Prepare For The Worst: Cautious Adaptation In RL Agents | OpenReview", "abstract": "Abstract:###We study the problem of safe adaptation: given a model trained on a variety of past experiences for some task, can this model learn to perform that task in a new situation while avoiding catastrophic failure? This problem setting occurs frequently in real-world reinforcement learning scenarios such as a vehicle adapting to drive in a new city, or a robotic drone adapting a policy trained only in simulation. While learning without catastrophic failures is exceptionally difficult, prior experience can allow us to learn models that make this much easier. These models might not directly transfer to new settings, but can enable cautious adaptation that is substantially safer than na*{i}ve adaptation as well as learning from scratch. Building on this intuition, we propose risk-averse domain adaptation (RADA). RADA works in two steps: it first trains probabilistic model-based RL agents in a population of source domains to gain experience and capture epistemic uncertainty about the environment dynamics. Then, when dropped into a new environment, it employs a pessimistic exploration policy, selecting actions that have the best worst-case performance as forecasted by the probabilistic model. We show that this simple maximin policy accelerates domain adaptation in a safety-critical driving environment with varying vehicle sizes. We compare our approach against other approaches for adapting to new environments, including meta-reinforcement learning.", "review": "Review:###This paper studies the problem of safe adaptation to avoid catastrophic failure in a new environment. It draws intuition from human behavior. The proposed method (risk-averse domain adaptation (RADA)) learns probabilistic model-based RL agents from source domains, and uses them to select actions that has the best worst-case performance in the target domain. The paper mentions safety-critical applications like auto-driving. However, generally, I don*t think black-box models are suitable for these safety-critical applications."}
{"id": "iclr2020_49", "title": "Farkas layers: don*t shift the data, fix the geometry | OpenReview", "abstract": "Abstract:###Successfully training deep neural networks often requires either {batch normalization}, appropriate {weight initialization}, both of which come with their own challenges. We propose an alternative, geometrically motivated method for training. Using elementary results from linear programming, we introduce Farkas layers: a method that ensures at least one neuron is active at a given layer. Focusing on residual networks with ReLU activation, we empirically demonstrate a significant improvement in training capacity in the absence of batch normalization or methods of initialization across a broad range of network sizes on benchmark datasets.", "review": "Review:###The paper introduces a new type of layer, Farkas layers, that are designed to ameliorate the *dying ReLU problem*. The idea of repurposing Farkas* lemma from linear programming to a component in NNs is very nice. However, the paper doesn*t have convincing baselines and doesn*t dig deep enough into what the Farkas layer is actually doing. Comments: 1. Start of section 2.2: “Previous work on the dying ReLU problem, or vanishing gradient problem, in the case of using sigmoid-like activation functions, has heavily revolved around novel weight initializations”. Dying ReLUs and vanishing gradients are different problems. In particular, it doesn’t make sense to talk about the dying ReLU problem for sigmoid-like activation functions. 2. Batchnorm (BN) consists in two operations: shifting and scaling. Both are relevant to vanishing gradients. However, only shifting (that is, subtracting the mean and resetting to a learned value) is relevant to dying ReLUs because rescaling the input to a ReLU by a positive number doesn’t affect whether the ReLU is subsequently ON or OFF. 3. Given point #2, a natural baseline to compare the Farkas layer against is a “pared-down BN”, which shifts but does not rescale. 4. Similarly, when combining the Farkas operation with BN, it might be worth considering keeping BN’s rescaling but dropping the shift -- since Farkas is analogous to the shift operation in BN. 5. I claimed above that Farkas is analogous to the shift in BN, but haven’t thought about it deeply. Do you agree? Any comments on how they differ and why? 6. Our contributions, p2: “We empirically show an approximate 20% improvement on the first epoch over only using batch normalization.” I’m not sure what to make of this; improvements on the first epoch are only useful if they lead to overall improvements. 7. Figure 4 of “Shattered gradients”, https://arxiv.org/abs/1702.08591 looks at ReLU activations by layer, both with and without BN. It’s worth doing a similar analysis for Farkas layers. Concretely: how do Farkas layers change the pattern and frequency of activation, both at activation and during training? 8. Guaranteeing the activity of a single neuron per layer seems very weak. What is the empirical effect of Farkas on the number of live neurons? Is it really just making sure one ReLU is on, or does it do better? Is it possible to ensure more neurons are ON in each layer? The “shattered” paper above suggests BN sets close to half ReLUs as ON at initialization, and approximately controls how often ReLUs are ON or OFF during training via the shift. 9. As a broader point, the paper proposes an algorithm based on a hypothesis: that having (at least one) ReLU on helps training. It’s worth digging into the hypothesis a bit rather than just plotting training curves. 10. I would expect ResNets have much *less* of a problem with dying ReLUs than standard NNs because of the skip-connections. One would therefore expect Farkas layers to help more with standard NNs than ResNets. However, the reported results are for ResNets. What happens when there are no skip-connections?"}
{"id": "iclr2020_50", "title": "Finding Winning Tickets with Limited (or No) Supervision | OpenReview", "abstract": "Abstract:###The lottery ticket hypothesis argues that neural networks contain sparse subnetworks, which, if appropriately initialized (the winning tickets), are capable of matching the accuracy of the full network when trained in isolation. Empirically made in different contexts, such an observation opens interesting questions about the dynamics of neural network optimization and the importance of their initializations. However, the properties of winning tickets are not well understood, especially the importance of supervision in the generating process. In this paper, we aim to answer the following open questions: can we find winning tickets with few data samples or few labels? can we even obtain good tickets without supervision? Perhaps surprisingly, we provide a positive answer to both, by generating winning tickets with limited access to data, or with self-supervision---thus without using manual annotations---and then demonstrating the transferability of the tickets to challenging classification tasks such as ImageNet.", "review": " This paper studies the problem of finding sparse networks in a limited supervision setup. The authors build on the lottery ticket work of Frankle & Carbin and investigate the validity of their idea when one has few or no labels. This work is an immediate followup on Morcos et al. who investigated the transferability of lottery tickets. This work is more observational rather than algorithmic or theoretical. Authors study various small sample/label setups where network sparsification works well. Main contribution is Section 4.1 where self-supervision is investigated. However given that lottery tickets are transferable (Morcos paper) it is really not that surprising that semisupervised learning algorithms will do a decent job as well. I also don*t see a practical benefit beyond transfer learning setup. Section 4.2 essentially sweeps through supervised problem parameters such as reducing sample size, adding noise etc and . The main application seems to be extracting lottery tickets faster by downsampling the data however this aspect is again fairly obvious. In short, unfortunately, this paper doesn*t cut it for ICLR. As improvements, I would recommend adding standard semi-supervised training techniques to their comparison. I was surprised to not see pseudo-labeling or consistency training (e.g. virtual adversarial training)."}
{"id": "iclr2020_51", "title": "Training Interpretable Convolutional Neural Networks towards Class-specific Filters | OpenReview", "abstract": "Abstract:###Convolutional neural networks (CNNs) have often been treated as “black-box” and successfully used in a range of tasks. However, CNNs still suffer from the problem of filter ambiguity – an intricate many-to-many mapping relationship between filters and features, which undermines the models’ interpretability. To interpret CNNs, most existing works attempt to interpret a pre-trained model, while neglecting to reduce the filter ambiguity hidden behind. To this end, we propose a simple but effective strategy for training interpretable CNNs. Specifically, we propose a novel Label Sensitive Gate (LSG) structure to enable the model to learn disentangled filters in a supervised manner, in which redundant channels experience a periodical shutdown as flowing through a learnable gate varying with input labels. To reduce redundant filters during training, LSG is constrained with a sparsity regularization. In this way, such training strategy imposes each filter’s attention to just one or few classes, namely class-specific. Extensive experiments demonstrate the fabulous performance of our method in generating sparse and highly label- related representation of the input. Moreover, comparing to the standard training strategy, our model displays less redundancy and stronger interpretability.", "review": "Review:###This paper proposed an interesting idea of using Label Sensitive Gate (LSG) structure to enforce models to learn disentangled filters for better interpretability of the DNN model. By periodically training with the sparse LSG structure, the model is forced to extract features from only a few classes. The model is trained efficiently in an alternate fashion (with respect to both the network parameters and the sparse gate matrix.) By disentangling the class-specific filters, the model becomes less redundant and more interpretable. Overall the paper is well-written and well-organized. I like the idea of imposing a class-specific gated structure for disentangling the representation. And numerical experiments verify the effectiveness of the proposed method in terms of 1) Improved performance 2) Disentangled representation (a small L1 norm on the gate matrix G) 3) Consistent class activation map for different inputs. I do have some question though 1) Table 1 also reports the L1 norm and Phi of a STD CNN. What is the gated matrix G here for a STD CNN? 2) Is it very important to have a constraint in the model? What if an L-1 norm is used directly?"}
{"id": "iclr2020_52", "title": "Generalized Transformation-based Gradient | OpenReview", "abstract": "Abstract:###The reparameterization trick has become one of the most useful tools in the field of variational inference. However, the reparameterization trick is based on the standardization transformation which restricts the scope of application of this method to distributions that have tractable inverse cumulative distribution functions or are expressible as deterministic transformations of such distributions. In this paper, we generalized the reparameterization trick by allowing a general transformation. Unlike other similar works, we develop the generalized transformation-based gradient model formally and rigorously. We discover that the proposed model is a special case of control variate indicating that the proposed model can combine the advantages of CV and generalized reparameterization. Based on the proposed gradient model, we propose a new polynomial-based gradient estimator which has better theoretical performance than the reparameterization trick under certain condition and can be applied to a larger class of variational distributions. In studies of synthetic and real data, we show that our proposed gradient estimator has a significantly lower gradient variance than other state-of-the-art methods thus enabling a faster inference procedure.", "review": "Review:###This paper looks at the problem of making the reparameterization trick in Variational Inference applicable to a wider range of distributions. Reparameterized estimators have lower variance than score function estimators but their application is restricted to fewer distribution classes. Using a set of mathematical devices, they derive gradient estimator for the ELBO as a transport equation. On similar lines as Ruiz et al (The Generalized Reparameterization Gradient or G-REP), the paper starts off by assuming that the standardized variable (\rho in the paper, epsilon in Kingma and Welling, Ruiz et al.) is a weak function of the variational parameters. They then go on to develop transport equations which reduce to the reparameterization and score function methods as special cases. This is also what we see in the related papers - G-REP goes about the estimation process for q(z,\theta) more directly, and obtains two terms, one that looks like the reparameterization gradient and the other a score function like term. A particular form of the velocity field is then used for their polynomial based estimator. Applicability is shown for the gamma distribution (and therefore associated distributions such as the dirichlet, beta). The results show a lower variance than the other known approaches (G-REP, and Rejection Sampling VI), and the ELBO reaches a high value at a much smaller number of runs. Having said that, I am not so sure if the results are really that much better than the counterparts - especially RSVI. My thoughts: I very much enjoyed going over the derivations and associated papers. The other related paper with a transport equation involves pathwise derivatives (as the authors note - Jankowiak and Obermeyer - https://arxiv.org/abs/1806.01851) and appeals to Optimal Transport for the procedure. I am curious as to whether we can give a physical interpretation to the *velocity* in the transport problem which looks very much like the transport equations we encounter in physics and fluid mechanics. I do note as a minor point that some of the other related papers (Ruiz et al, Naesseth et al.) read much more clearly."}
{"id": "iclr2020_53", "title": "A Generalized Framework of Sequence Generation with Application to Undirected Sequence Models | OpenReview", "abstract": "Abstract:###Undirected neural sequence models such as BERT (Devlin et al., 2019) have received renewed interest due to their success on discriminative natural language understanding tasks such as question-answering and natural language inference. The problem of generating sequences directly from these models has received relatively little attention, in part because generating from such models departs significantly from the conventional approach of monotonic generation in directed sequence models. We investigate this problem by first proposing a generalized model of sequence generation that unifies decoding in directed and undirected models. The proposed framework models the process of generation rather than a resulting sequence, and under this framework, we derive various neural sequence models as special cases, such as autoregressive, semi-autoregressive, and refinement-based non-autoregressive models. This unification enables us to adapt decoding algorithms originally developed for directed sequence models to undirected models. We demonstrate this by evaluating various decoding strategies for a cross-lingual masked translation model (Lample and Conneau, 2019). Our experiments show that generation from undirected sequence models, under our framework, is competitive with the state of the art on WMT*14 English-German translation. We also demonstrate that the proposed approach enables constant-time translation with similar performance to linear-time translation from the same model by rescoring hypotheses with an autoregressive model.", "review": " This paper presents a general framework for sentence generation using a BERT-like model. The authors decompose the problem of sentence generation into two problems. One is selecting the positions at which changes should be made, and the other is actually replacing the current word with a new word. This framework enables them to represent many decoding strategies including that of Ghazvininejas et al. (2019) in a unified manner, and they propose a new decoding strategy that considers the prediction confidence of the current and the new word. The paper also presents a heuristic algorithm for beam search decoding to find the most likely generation path. Their experimental results on the WMT14 English-German dataset suggest that the proposed approach could achieve translation quality comparable to that of the standard autoregressive approach under a constant-time translation setting. It is nice to see existing decoding strategies represented in a generalized framework, but I was a bit disappointed that the authors do not seem to address the most critical problem in using a BERT-like model for sentence generation, namely, how to find the most likely sentence in a probabilistically sound way. It seems to me that the authors rely on at least two approximations. One is using pseudo-likelihood and the other is using the most likely generation path instead of performing marginalization. It is fine that the authors focus on empirical results of translation quality but then I would like to see more strong and extensive evidence that supports the use of such approximation."}
{"id": "iclr2020_54", "title": "FLUID FLOW MASS TRANSPORT FOR GENERATIVE NETWORKS | OpenReview", "abstract": "Abstract:###Generative Adversarial Networks have been shown to be powerful tools for generating content resulting in them being intensively studied in recent years. Training these networks requires maximizing a generator loss and minimizing a discriminator loss, leading to a difficult saddle point problem that is slow and difficult to converge. Motivated by techniques in the registration of point clouds and the fluid flow formulation of mass transport, we investigate a new formulation that is based on strict minimization, without the need for the maximization. This formulation views the problem as a matching problem rather than an adversarial one, and thus allows us to quickly converge and obtain meaningful metrics in the optimization path.", "review": "Review:###This paper proposed a generative network based on fluid flow solutions of mass transport problems. The paper is difficult to follow due to a poor structure and obvious technical mistakes. Detailed comments are as follows: 1. The dual formulation used in the objective of WGANS involves expectations with respect to data distributions. When authors introduced WGANs, it is extremely loose and essential wrong to state that *In the case of WGANs a simpler expression is derived where we use the score directly setting g to the identity and h = ?g, and require some extra regularity on the score function*. 2. The organisation of the paper makes it difficult to read: a) While the relevant work is discussed only briefly in Section 1 and contain incorrect statements (see an example above), detailed discussions of two toy examples in the introduction section distract the reading. b) Section 2 is a combination of related work and proposed work. For example, it starts more like a section introducing the dynamic transport formulation of mass transport problems (Equations 2.3a-2.3c), but in fact it contains the authors* proposed approach (2.4a-2.4b), which makes it difficult to tell the authors* contributions. Moreover, the connection between 2.3a-2.3c to 2.4a-2.4b is not clear to me. 3. Multiple notations are not properly defined or conflicting: what are and ? 4. Very limited experimental validation with no comparison to other algorithms. 5. Multiple typos in the paper, e.g. *Equation equation 1.1*."}
{"id": "iclr2020_55", "title": "Learning to Represent Programs with Property Signatures | OpenReview", "abstract": "Abstract:###We introduce the notion of property signatures, a representation for programs and program specifications meant for consumption by machine learning algorithms. Given a function with input type ?_in and output type ?_out, a property is a function of type: (?_in, ?_out) ? Bool that (informally) describes some simple property of the function under consideration. For instance, if ?_in and ?_out are both lists of the same type, one property might ask ‘is the input list the same length as the output list?’. If we have a list of such properties, we can evaluate them all for our function to get a list of outputs that we will call the property signature. Crucially, we can ‘guess’ the property signature for a function given only a set of input/output pairs meant to specify that function. We discuss several potential applications of property signatures and show experimentally that they can be used to improve over a baseline synthesizer so that it emits twice as many programs in less than one-tenth of the time.", "review": "Review:###** Summary The paper studies the problem of program synthesis from examples and it proposes the notion of property signature as a set of *features* describing the program that can be used to simplify the synthesis by combining together sub-programs with similar features. ** Evaluation The paper is outside my scope of research, so my review is an educated guess. The use of properties to summarize some features about the program and the possibility to evaluate them directly from samples seems a very interesting idea. As the authors mentioned, this may help in creating useful features and useful architectures to simplify the learning task. The concept of property signatures is very well introduced and explained. The authors also provide an extensive comparison to related work. The empirical results seem promising in showing how property signatures make the synthesis much faster and better. The downsides of the paper are: - While it is clear how to build property signatures, it is quite unclear to me how they simplify the generation of programs that combine smaller/simpler programs. - Sect 3.2 on how to learn useful properties is rather vague and it would need a much more detail explanation. - Although the authors release code for the paper, the description of the experiments seem rather shallow and it requires a much higher level of detail on how the learning process is set up and executed."}
{"id": "iclr2020_56", "title": "POLYNOMIAL ACTIVATION FUNCTIONS | OpenReview", "abstract": "Abstract:###Activation is a nonlinearity function that plays a predominant role in the convergence and performance of deep neural networks. While Rectified Linear Unit (ReLU) is the most successful activation function, its derivatives have shown superior performance on benchmark datasets. In this work, we explore the polynomials as activation functions (order ? 2) that can approximate continuous real valued function within a given interval. Leveraging this property, the main idea is to learn the nonlinearity, accepting that the ensuing function may not be monotonic. While having the ability to learn more suitable nonlinearity, we cannot ignore the fact that it is a challenge to achieve stable performance due to exploding gradients - which is prominent with the increase in order. To handle this issue, we introduce dynamic input scaling, output scaling, and lower learning rate for the polynomial weights. Moreover, lower learning rate will control the abrupt fluctuations of the polynomials between weight updates. In experiments on three public datasets, our proposed method matches the performance of prior activation functions, thus providing insight into a network’s nonlinearity preference.", "review": "Review:###The paper proposes using parameterized polynomials as activation functions. A regularisation is proposed to stabilize training Experiments are performed and it is shown that polynomials perform well. -------------------------- Unfortunately, I only had limited time reviewing this paper next to the other 6 I had to review and unfortunately i won*t find more time before the submission deadline. -------------------------- I am very conflicted with the premise of the paper. Using parameterized polynomial activation functions is hardly new and a very quick search revealed papers going back to the 90s. The paper does not seem to reference any of this prior work or the theoretical results that have been given (e.g. the unsurprising result that polynomial networks with finite depth are not universal approximators). The reason i don*t mention a concrete reference is that I do not know which of the old references is the most important. The paper does not seem to motivate why looking at parameterized polynomials should be important. In general, they are harder to use and have all kinds of degenerate cases. The paper proposes a way to bound the polynomial range of values and derivatives to alleviate the training problem, yet i miss why this should be important or better than any of the already developed solutions. Most polynomials learned seem to be quadratic or with very low higher order components, so it is not that the learned shapes are interesting. I did not have the time to assess the experimental results."}
{"id": "iclr2020_57", "title": "Perceptual Regularization: Visualizing and Learning Generalizable Representations | OpenReview", "abstract": "Abstract:###A deployable machine learning model relies on a good representation. Two desirable criteria of a good representation are to be understandable, and to generalize to new tasks. We propose a technique termed perceptual regularization that enables both visualization of the latent representation and control over the generality of the learned representation. In particular our method provides a direct visualization of the effect that adversarial attacks have on the internal representation of a deep network. By visualizing the learned representation, we are also able to understand the attention of a model, obtaining visual evidence that supervised networks learn task-specific representations. We show models trained with perceptual regularization learn transferrable features, achieving significantly higher accuracy in unseen tasks compared to standard supervised learning and multi-task methods.", "review": "Review:###Summary: In this study, the authors propose a new architecture for visualizing the latent space of a network. This architecture is achieved by appending a secondary decoder *head* to visualize the latent space by reconstructing the inputs. In summary, I found the paper to provide a simple architectural change for attempting to address this issue of network interpretability and improve transfer learning. The results all appear sensible and expected, but the experiments are somewhat weak. My primary concerns are: 1. The network architecture described is not terribly novel as many papers have explored pairing an auto-encoder with a classification task. Thus, I don*t find the methods to be much of a contribution. The main contribution of the paper is thus in the interpretation and analysis of the method. 2. The results on network interpretability are all qualitative. That is, I must make a judgement on the resulting reconstructed image. I find this unsatisfactory for providing any quantitative assessment of the performance of the method. In particular, I would want to see a quantitative assessment of this method and compare it with other network introspection techniques. Currently, there is no benchmark nor any other methods to compare against. 3. The results on transfer learning are quantitative but weak. It is not clear to me how other simple transfer learning methods may perform on the task presented. That is, I do not know how hard the actual task is. (Would retraining a logistic classifier with SIFT features work just as well?) I would expect to see far more experimental studies to justify these claims. Additionally, I would expect to see comparisons with other transfer learning techniques to see how well this method fares. Major Comments: - Why do the authors allow for the decoder to regularize the latent space? Why not just stop the gradients at the latent space to provide a microscope to visualize the latent space without effecting it? - This reference appears to be in the same spirit as this paper and probably should be cited. Alain, Guillaume, and Yoshua Bengio. *Understanding intermediate layers using linear classifier probes.* arXiv preprint arXiv:1610.01644 (2016). - Figure 2, 3 and 4 provide very nice qualitative demonstrations of the reconstruction providing insight into how an image was misclassified, however some issues remain: 1) A human must interpret the reconstruction. This is a subjective process and is not systematic. 2) Can you see examples in cross-validated test images where the visualization would predict a misclassification? Can you see counter-examples where the resulting image does not make sense? - Figure 6 and 7 provide interesting results indicating that the unsupervised objective improves transfer learning across several classification tasks related to faces. The results are encouraging but I am concerned that (a) the task is too easy, (b) tuning the hyperparameter for lambda can be difficult. For (b), I would like to ensure that lambda was tuned independent on a third validation test (and not the test set whose numbers are reported). For (a), I suspect that an important benchmark is to just take set lambda = infty and then perform linear classification on the embedding. It would be interesting to see how well these numbers fare as a baseline compared to the reported numbers."}
{"id": "iclr2020_58", "title": "Transfer Active Learning For Graph Neural Networks | OpenReview", "abstract": "Abstract:###Graph neural networks have been proved very effective for a variety of prediction tasks on graphs such as node classification. Generally, a large number of labeled data are required to train these networks. However, in reality it could be very expensive to obtain a large number of labeled data on large-scale graphs. In this paper, we studied active learning for graph neural networks, i.e., how to effectively label the nodes on a graph for training graph neural networks. We formulated the problem as a sequential decision process, which sequentially label informative nodes, and trained a policy network to maximize the performance of graph neural networks for a specific task. Moreover, we also studied how to learn a universal policy for labeling nodes on graphs with multiple training graphs and then transfer the learned policy to unseen graphs. Experimental results on both settings of a single graph and multiple training graphs (transfer learning setting) prove the effectiveness of our proposed approaches over many competitive baselines.", "review": "Review:###Positive 1. The paper studies a universal policy for labeling nodes on graphs with multiple training graphs which can be transferred to new unseen graphs. 2. The paper focuses on minimizing human efforts in obtaining labeled data. The model is based on active learning and transfer learning. Negative 1. The proposed method combines existing models as the solution, which is heuristic and lacks persuasive theoretical proofs. 2. In graphs, data (nodes) to be labeled are highly correlated. However, there is no method for solving this challenge. 3. In Section 3.2, ACTIVEL EARNING ON A SINGLE GRAPH, the authors formalize the problem, i.e., learning a policy for selecting a set of nodes for annotation, as a sequential decision process, and the reinforce algorithm is applied to optimize the objective function. However, explanation about the active learning is confusing. More details are needed to explain their respective goals and to explain how to integrate active learning and reinforcement learning. 4. The authors claim that the details of heuristic features are represented in Appendix, please add these information. 5. The settings of active learning need more consideration. The total budget for active learning is set as . How to choose these nodes? Is it to select all samples at once or in batches during iterative epoch? If the samples are selected in batches, what is the specific experimental setting? 6. The experimental method about active learning. The paper focuses on minimizing human efforts in obtaining labeled data. Compared to the results of the model before selecting, how significant is the improvement after selecting all the node in the budget? More experiments are needed. 7. Minor format issue: the fonts in Table 2 and Table 3 are different."}
{"id": "iclr2020_59", "title": "Generating Semantic Adversarial Examples with Differentiable Rendering | OpenReview", "abstract": "Abstract:###Machine learning (ML) algorithms, especially deep neural networks, have demonstrated success in several domains. However, several types of attacks have raised concerns about deploying ML in safety-critical domains, such as autonomous driving and security. An attacker perturbs a data point slightly in the pixel space and causes the ML algorithm to misclassify (e.g. a perturbed stop sign is classified as a yield sign). These perturbed data points are called adversarial examples, and there are numerous algorithms in the literature for constructing adversarial examples and defending against them. In this paper we explore semantic adversarial examples (SAEs) where an attacker creates perturbations in the semantic space. For example, an attacker can change the background of the image to be cloudier to cause misclassification. We present an algorithm for constructing SAEs that uses recent advances in differential rendering and inverse graphics.", "review": "Review:###This paper studies the problem of semantic adversarial attacks with a differentiable de-rendering and rendering pipeline. More specifically, this paper proposed a variant of FGSM (Goodfellow et al. 2015) and PGD (Madry et al. 2017) by extending the traditional Lp-bounded adversarial attacks in the rich semantic space. It considered a list of semantic parameters including color, weather, foliage, rotation, transformation, and object shape. To facilitate back-propagation and improve the quality of rendering, this paper re-implemented the differentiable equivalents of several image manipulation operations based on the previous work (Yao et al. 2018). For experimental evaluations, this paper selected the object detector SqueezeDet (We et al. 2016) as the target model for attack on the virtual KITTI dataset. Experiments demonstrated that the generated semantic adversarial examples (SAEs) can attack the SqueezeDet (see Table 1 and Table 2). By re-training with augmented data by the proposed method, the robustness of SqueezeDet (see Table 3) can be further improved. Overall, this is an okay paper with incremental technical novelty and clear presentation. Reviewer has several concerns regarding the experiments. (1) This paper only conducts experiments on virtual KITTI dataset, a synthetic benchmark for object detection and semantic segmentation. In general, studying the adversarial examples in the synthetic domain seems not a significant contribution. Reviewer would like to know the performance on the real dataset such as Cityscape (used in Yao et al. 2018) and other challenging indoor datasets such as ADE20K. At least, reviewer would like to know whether re-training on adversarial examples help to improve the performance on real dataset? (2) The conclusion is largely based on the 1547 semantic adversarial examples generated, while there are more than 4K synthetic images in the dataset. This seems contradicts against the flexibility of generating semantic adversarial examples described in the paper (e.g., single parameter modifications). Reviewer suspects the proposed differentiable rendering pipeline is not very effective so that generating SAEs requires quite a bit exhaustive search over the parameter space. Please comment on the average running time for generating a semantic adversarial example. How does that compare to generating a pixel-based adversarial example? (3) While several different quantitative analyses have been conducted, this paper only provides two examples as the qualitative result (see Figure 2). It would be more convincing if this paper provides more such examples in the appendix. In addition, ablation studies on semantic parameters are currently missing. Furthermore, reviewer wonders if it is possible to report the FID score and make sure the generated adversarial examples have the same distribution as ground-truth images. (4) SqueezeDet is the only model used in the paper. Please also consider other models and report the attack performance and transferability. In a high-level, reviewer would like to know whether the proposed differentiable rendering method generalizes to other tasks including semantic segmentation and depth prediction. (5) The following paper is related (see Figure 5 of MeshAdv paper), but not even mentioned here. Reviewer would like to see the comparison between the proposed method and the MeshAdv baseline. -- MeshAdv: Adversarial Meshes for Visual Recognition. Xiao et al. In CVPR 2019."}
{"id": "iclr2020_60", "title": "Factorized Multimodal Transformer for Multimodal Sequential Learning | OpenReview", "abstract": "Abstract:###The complex world around us is inherently multimodal and sequential (continuous). Information is scattered across different modalities and requires multiple continuous sensors to be captured. As machine learning leaps towards better generalization to real world, multimodal sequential learning becomes a fundamental research area. Arguably, modeling arbitrarily distributed spatio-temporal dynamics within and across modalities is the biggest challenge in this research area. In this paper, we present a new transformer model, called the Factorized Multimodal Transformer (FMT) for multimodal sequential learning. FMT inherently models the intramodal and intermodal (involving two or more modalities) dynamics within its multimodal input in a factorized manner. The proposed factorization allows for increasing the number of self-attentions to better model the multimodal phenomena at hand; without encountering difficulties during training (e.g. overfitting) even on relatively low-resource setups. All the attention mechanisms within FMT have a full time-domain receptive field which allows them to asynchronously capture long-range multimodal dynamics. In our experiments we focus on datasets that contain the three commonly studied modalities of language, vision and acoustic. We perform a wide range of experiments, spanning across 3 well-studied datasets and 21 distinct labels. FMT shows superior performance over previously proposed models, setting new state of the art in the studied datasets.", "review": "Review:###This paper presents a method for multimodal sequential learning. The input consists of three modalities (e.g. visual, language, acoustic data). The output is the classification label for multimodal sentiment analysis, multimodal emotion recognition, and multimodal speaker traits recognition. The approach explicitly accounts for possible unimodal, bimodal and trimodal interactions existing within the multimodal input space by using the factorization. Compared to previous works, this method is able to model the intra-model and inter-modal dynamics within asynchronous multimodal sequences. Strengths: 1. Based on an intuitive idea, this paper shows its method outperforms the previous works on several tasks. The brief method they proposed achieves a powerful result instead of using complex structures. 2. The overall experiments and analyses support the theory of the paper and show the necessity of the factorized method. the problem of multimodality is solved well. Weakness: 1. The paper dis not show the training epoch and the convergence time, so we could not compare the effectiveness of the proposed method with the other methods. 2. Theoretical justification is not sufficient."}
{"id": "iclr2020_61", "title": "Bayesian Residual Policy Optimization: Scalable Bayesian Reinforcement Learning with Clairvoyant Experts | OpenReview", "abstract": "Abstract:###Informed and robust decision making in the face of uncertainty is critical for robots that perform physical tasks alongside people. We formulate this as a Bayesian Reinforcement Learning problem over latent Markov Decision Processes (MDPs). While Bayes-optimality is theoretically the gold standard, existing algorithms do not scale well to continuous state and action spaces. We propose a scalable solution that builds on the following insight: in the absence of uncertainty, each latent MDP is easier to solve. We split the challenge into two simpler components. First, we obtain an ensemble of clairvoyant experts and fuse their advice to compute a baseline policy. Second, we train a Bayesian residual policy to improve upon the ensemble*s recommendation and learn to reduce uncertainty. Our algorithm, Bayesian Residual Policy Optimization (BRPO), imports the scalability of policy gradient methods as well as the initialization from prior models. BRPO significantly improves the ensemble of experts and drastically outperforms existing adaptive RL methods.", "review": "Review:###In this paper, the authors motivate and propose a learning algorithm, called Bayesian Residual Policy Optimization (BRPO), for Bayesian reinforcement learning problems. Experiment results are demonstrated in Section 5. The paper is well written in general, and the proposed algorithm is also interesting. However, I think the paper suffers from the following limitations: 1) This paper does not have any theoretical analysis or justification. It would be much better if the authors can rigorously prove the advantages of BRPO under some simplifying assumptions. 2) It would be better if the authors can provide more experiment results, like experiment results in more games."}
{"id": "iclr2020_62", "title": "Deep probabilistic subsampling for task-adaptive compressed sensing | OpenReview", "abstract": "Abstract:###The field of deep learning is commonly concerned with optimizing predictive models using large pre-acquired datasets of densely sampled datapoints or signals. In this work, we demonstrate that the deep learning paradigm can be extended to incorporate a subsampling scheme that is jointly optimized under a desired minimum sample rate. We present Deep Probabilistic Subsampling (DPS), a widely applicable framework for task-adaptive compressed sensing that enables end-to end optimization of an optimal subset of signal samples with a subsequent model that performs a required task. We demonstrate strong performance on reconstruction and classification tasks of a toy dataset, MNIST, and CIFAR10 under stringent subsampling rates in both the pixel and the spatial frequency domain. Due to the task-agnostic nature of the framework, DPS is directly applicable to all real-world domains that benefit from sample rate reduction.", "review": "Review:###The authors propose a new approach of deep probabilistic subsampling for compressed sensing, based on Gumbel-softmax, which is interesting. A few points should be clarified: - in compressed sensing one has e.g. the restricted isometry property (RIP) related to recovery. How does the new method relate to such theoretical results? Are the results and findings along similar lines as (classical) compressed sensing theory? - Methods in compressed sensing are typically convex, e.g. using l1-regularization. What are the drawbacks of using deep learning in this context, e.g. related to non-convexity? What is the role of initialization? - Does the method both work for underdetermined and overdetermined problems (number of data versus number of unknowns)? - What is the influence of the hyper-parameters mu and lambda in eq (14)? How should the model selection be done (currently lambda is set to 0.004 without further motivation)? - MNIST: 60,000 instead of 70,000?"}
{"id": "iclr2020_63", "title": "Lossless Data Compression with Transformer | OpenReview", "abstract": "Abstract:###Transformers have replaced long-short term memory and other recurrent neural networks variants in sequence modeling. It achieves state-of-the-art performance on a wide range of tasks related to natural language processing, including language modeling, machine translation, and sentence representation. Lossless compression is another problem that can benefit from better sequence models. It is closely related to the problem of online learning of language models. But, despite this ressemblance, it is an area where purely neural network based methods have not yet reached the compression ratio of state-of-the-art algorithms. In this paper, we propose a Transformer based lossless compression method that match the best compression ratio for text. Our approach is purely based on neural networks and does not rely on hand-crafted features as other lossless compression algorithms. We also provide a thorough study of the impact of the different components of the Transformer and its training on the compression ratio.", "review": "Review:###Summary: The paper investigates using the transformer architecture for neural network-based lossless compression of text. The resulting model, obtained through a thorough investigation of the architecture hyper-parameters are on par with standard SOTA compression. The paper is well-written. In particular the authors have done a great job reviewing existing compression literature and positioning their method within the space of prior work. Recommendation: Weak Reject While the paper considers an interesting application of the Transformer architecture, and is well-written, it is of limited novelty. Specifically, the bulk of the paper is concerned with describing experimental results of a thorough (but standard) hyper-parameter search - considering things like Transformer context size, learning rate (schedule), number of layers and key, value, query dimensionality; and does not offer any new architectural modifications / insights. Furthermore, only a single dataset - enwik8 - is considered in the experimental validation and little attention is given to the description of the dataset split and any distribution differences between splits. Taken together, the existing experimental setup potentially creates an unfair advantage for the neural network-based methods - while the standard methods can be expected to perform similarly across a wide range of datasets / texts, the neural-network based methods have been trained and tested on very similar data and could be expected to perform well on these data, but not in case of a distributional shift (e.g. compressing legal texts instead of Wikipedia). The paper does not answer the question of whether or not this is true. Furthermore, similar to autoregressive models, transformers are known to be slow at inference time. I expect this to lead to very slow decoding. Therefore, methods in table 1 should be compared in compression/decompression time to give a better overview of the practical impact of this work. Taken together, in its current form the paper may be better suited for a workshop publication rather than a full conference paper. Major comments: 1. For reasons mentioned above, the paper should include additional experimental evaluation. In particular, it should consider the effect of training the model on one dataset, but evaluating it on another dataset; and discuss how differences in performance (if any) compare to standard methods. 2. Compression/decompression times of the proposed method should be compared against the other compression methods in table 1. I expect the proposed transformer to be slow at decompressing. 3. The paper does not contain the loss that the transformer model was used to optimize. I assume that it is the softmax cross entropy, but this is worth mentioning explicitly. It would also be worthwhile to explain the training procedure - for how many epochs was the model trained (see also next question), what was the dataset size? 4. Description of the “training with revisits” is not very clear. My understanding is that it resembles a pass through the data, where some of it is considered again at specific intervals. My first assessment is that this should not be necessary - the data should already be considered multiple times during the training process. a) The authors should provide a more detailed description of the training-with-revisits procedure, contrasting it specifically with a procedure where revisits are not done (i.e. normal training). b) If the goal of the revisits training is to observe some training examples more than once, then it would be very interesting if simply training for a longer time (several epochs == passes through the data) has a similar effect. c) Is there any motivation for the choice of the revisits hyper-parameters F and M? Was a different batch size used during the revisits training? Is the learning rate evolved during the revisits training phase or is it still decayed? Minor comments: 1. There is some prior work on using Neural Networks for lossless image compression (e.g. [1], [2]. [3] that achieves SOTA compression ratios compared to standard methods. It may be interesting for the readers to mention these results. In particular the authors’ statement that “[...] purely neural network based models are still far from state of the art [...]” may give the wrong impression to the readers. 2. The authors mention that they “[...] propose several improvements to its (the Transformer) architecture and training to accelerate and stabilize [...] training”. In my view, the experiments described in the paper resemble a hyper-parameter search more than architectural improvements. The authors may want to clarify in the text which specific improvements they refer to. 3. Page 1, last paragraph: “[...] of all the important component [...]” -> “[...] of all the important components [...]” 4. Page 3: “[...] attention span size across all layers as it suggested [...]” -> “[...] attention span size across all layers as was suggested [...]” 5. Page 3: Missing references. 6. Page 3: Use of small n and capital N when talking about n-grams. Should be made consistent. 7. Page 8 (Conclusion): “wihtout” -> “without” [1] F. H. Kingma, P. Abbeel, and J. Ho. Bit-Swap: recursive bits-back coding for lossless compression with hierarchical latent variables. In International Conference on Machine Learning (ICML), 2019. [2] Emiel Hoogeboom, Jorn W. T. Peters, Rianne van den Berg, and Max Welling. Integer Discrete Flows and Lossless Compression. arXiv e-prints, 2019. [3] Jonathan Ho, Evan Lohn, and Pieter Abbeel. Compression with Flows via Local Bits-Back Coding. arXiv e-prints, 2019."}
{"id": "iclr2020_64", "title": "Support-guided Adversarial Imitation Learning | OpenReview", "abstract": "Abstract:###We propose Support-guided Adversarial Imitation Learning (SAIL), a generic imitation learning framework that unifies support estimation of the expert policy with the family of Adversarial Imitation Learning (AIL) algorithms. SAIL addresses two important challenges of AIL, including the implicit reward bias and potential training instability. We also show that SAIL is at least as efficient as standard AIL. In an extensive evaluation, we demonstrate that the proposed method effectively handles the reward bias and achieves better performance and training stability than other baseline methods on a wide range of benchmark control tasks.", "review": "Review:###This paper proposes an approach for improving adversarial imitation learning, by combining it with support-estimation-based imitation learning. In particular, the paper explores a combination of GAIL (Ho and Ermon, 2016) and RED (Wang et. al., 2019), where the reward for the policy-gradient is a product of the rewards obtained from them separately. The motivation is that, while AIL methods are sample-efficient (in terms of expert data) and implicitly promote useful exploration, they could be unreliable outside the support of the expert policy. Therefore, augmenting them by constraining the imitator to the support of the expert policy (with a method such as RED) could result in an overall better imitation learning algorithm. While the motivation and intuition are clear to me, I have reservations about the claims made in the abstract and the experimental sections: 1. SAIL is an effective method for solving the reward bias in AIL. The reward in SAIL is “always” non-negative (product of 2 non-negative terms), making the method a very ad-hoc way of getting around the reward bias problem, especially when compared to other methods such as those which estimate the value function of the absorbing state (Kostrikov et. al. 2019). Consider a simple chain MDP with 3 states A, B and a terminal state T. The actions are left/right from each state. Let the expert trajectory be A->B->T. Also, for SAIL, consider perfect support estimation with an optimal RED-network. When at B, the agent can terminate with a right-action and collect some reward. But taking left and collecting 0 reward (due to perfect support estimation) makes it land in A, from where it can now achieve a positive reward for the A->B transition, and repeat the process. Hence, one could always create MDPs where the Q value of B->A is higher than B->T. The Lunar-Lander environment (with certain parameters) in Section 4.1 appears to present a scenario where SAIL get arounds the reward bias, but this doesn’t remove my doubts over the generalization of this approach. Also, in Table 1, why does GAIL not hover above the landing spot even in the default case? If the reward bias is strong there, with sufficient exploration, the agent should converge to the same policy as in the modified case. Figure 3 is concerning for the same reason as above. It shows the immediate reward at the goal state, and points that SAIL has large reward for no-op action. The issue is that RL optimizes for actions that have the maximum Q value, not the action with the maximum immediate reward. 2. I would recommend that the authors refer to the original GAIL algorithm as “GAIL” in the experiments section, and their practical stabilization trick as “GAIL-bounded” (or something to that effect). Referring to original algorithm as GAIL-log, and the modification as GAIL could be misleading to readers. 3. The authors claim that SAIL has better training stability, leading to more robust policies. If this is due to the algorithmic contribution of combining AIL and Support-Estimation-IL, then GAIL-log and SAIL-log in Table 2. should show this in the standard deviation numbers. This doesn’t appear to be the case. Also, Figure 4 (Half-Cheetah) has unusually large variance for SAIL-log. 4. Figure 4 and Table 2 numbers are very different. Take Humanoid for instance. From Figure 4, it seems that SAIL is way better than GAIL. But if you look at Table 2, they both achieve mean-score in excess of 10k. What’s the difference between Table 2. and final performance in Figure 4?"}
{"id": "iclr2020_65", "title": "Deep Generative Classifier for Out-of-distribution Sample Detection | OpenReview", "abstract": "Abstract:###The capability of reliably detecting out-of-distribution samples is one of the key factors in deploying a good classifier, as the test distribution always does not match with the training distribution in most real-world applications. In this work, we propose a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks. Unlike the discriminative (or softmax) classifier that only focuses on the decision boundary partitioning its latent space into multiple regions, our generative classifier aims to explicitly model class-conditional distributions as separable Gaussian distributions. Thereby, we can define the confidence score by the distance between a test sample and the center of each distribution. Our empirical evaluation on multi-class images and tabular data demonstrate that the generative classifier achieves the best performances in distinguishing out-of-distribution samples, and also it can be generalized well for various types of deep neural networks.", "review": " Summary: Unlike the softmax classifier, the authors considered the generative classifier based on Gaussian discriminative analysis and showed that such deep generative classifiers can be useful for detecting out-of-distribution samples. For various benchmark tasks, the proposed method outperforms baselines based on the softmax classifier. Detailed comments: The novelty of this paper is not significant due to the following reasons: 1. The main message (i.e. the concept of the deep generative classifier can be useful for detecting out-of-distribution samples) is not really new because it has been explored before [Lee* 18]. Even though this paper considers training a deep generative classifier directly unlike [Lee* 18], the proposed method looks like a simple variant of [Lee* 18]. 2. Missing baselines for training the deep generative classifier: training the deep generative classifier directly has been studied by [Guerriero* 18] and [Pang* 18] but the authors did not compare the proposed training method with such baselines. Because of that, it is hard to say that contributions from proposing a training method are significant. Questions: 1. Could the authors consider a case without an identity covariance assumption? Most training methods for deep generative classifier assumes the identity covariance matrix because optimizing the log determinant is not easy. So, it would be interesting if the authors can handle this issue. 2. Even though the authors assume the identity covariance matrix, the covariance matrix of pre-trained features can not be an identity matrix. Could the authors report the performance of Mahalanobis detector using the proposed deep generative classifier? [Lee* 18] Lee, K., Lee, K., Lee, H. and Shin, J., 2018. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems (pp. 7167-7177). [Guerriero* 18] Samantha Guerriero, Barbara Caputo, Thomas Mensink, DeepNCM: Deep Nearest Class Mean Classifiers, ICLR workshop 2018. [Pang* 18] Pang, T., Du, C. and Zhu, J., Max-mahalanobis linear discriminant analysis networks. In ICML, 2018."}
{"id": "iclr2020_66", "title": "Evidence-Aware Entropy Decomposition For Active Deep Learning | OpenReview", "abstract": "Abstract:###We present a novel multi-source uncertainty prediction approach that enables deep learning (DL) models to be actively trained with much less labeled data. By leveraging the second-order uncertainty representation provided by subjective logic (SL), we conduct evidence-based theoretical analysis and formally decompose the predicted entropy over multiple classes into two distinct sources of uncertainty: vacuity and dissonance, caused by lack of evidence and conflict of strong evidence, respectively. The evidence based entropy decomposition provides deeper insights on the nature of uncertainty, which can help effectively explore a large and high-dimensional unlabeled data space. We develop a novel loss function that augments DL based evidence prediction with uncertainty anchor sample identification through kernel density estimation (KDE). The accurately estimated multiple sources of uncertainty are systematically integrated and dynamically balanced using a data sampling function for label-efficient active deep learning (ADL). Experiments conducted over both synthetic and real data and comparison with competitive AL methods demonstrate the effectiveness of the proposed ADL model.", "review": "Review:###The authors consider active deep learning. They propose decomposing predictive entropy into a) vacuity (lack of evidence) and b) dissonance (contradictory evidence). They frame this in terms of *subjective logic*. In practice this is achieved by having the NN output the parameters of a Dirichlet, which allows an additional degree of freedom describing variance/vacuity. Dissonance is defined in terms of the support of contradictory classes. To get improved estimates of vacuity they augment the loss with a term regularizing the Dirichlet parameters to be small (low precision) at unlabelled points with higher KDE(unlablled points) than KDE(labelled points). They propose initially weighting vacuity and later dissonance as AL proceeds. Encouraging results are presented on simulated 2D data, MNIST and CIFAR10. I think the basic idea of separating vacuity and dissonance is interesting, and the demonstration of the failings of existing ADL approaches is valuable. It wasn*t clear to me how this *subjective logic* theory gets you to the specific definitions of vacuity/dissonance, or whether these were just proposed by the authors. Equation 6 seems to come out of nowhere (whereas the rest of the derivations using the Dirichlet are very intuitive). The idea of encouraging the network to be uncertain far from data is also reasonable. While some Bayesian models such as Gaussian process regression with a RBF kernel give you this for free, it is certainly true that DL methods do not have this characteristic in general. Regularizing the r to be small seems like a reasonable way to do this, but I*m not convinced by the kernel density estimate part. DNNs can operate on very high-dimensional, structured inputs. Even the simplest of these, images, requires some degree of spatial invariance (achieved using convolutions) to obtain meaningful predictions. I find it very hard to believe a KDE can do anything meaningful in such spaces, even if you could find a sensible bandwidth (which isn*t discussed at all). It is possible of course that random selection of unlabelled points to regularize in this way would work just as well. Unfortunately no ablation study is performed, so we don*t know what the individual contribution of the three proposals (moving from vacuity to dissonance, augmented loss and Dirichlet likelihood) is. How sensitive is the method to the vacuity/dissonance weighting? The improvement over competing methods for MNIST and CIFAR10 appears to mostly manifest after the initial 20 or so acquistions. Do these results extend to batch AL? For many applications that*s more important. Overall I thought this paper had some promising ideas but they need to be more thoroughly tested empirically to give some sense of how robust and generalizable the approach is."}
{"id": "iclr2020_67", "title": "Multi-objective Neural Architecture Search via Predictive Network Performance Optimization | OpenReview", "abstract": "Abstract:###Neural Architecture Search (NAS) has shown great potentials in finding a better neural network design than human design. Sample-based NAS is the most fundamental method aiming at exploring the search space and evaluating the most promising architecture. However, few works have focused on improving the sampling efficiency for a multi-objective NAS. Inspired by the nature of the graph structure of a neural network, we propose BOGCN-NAS, a NAS algorithm using Bayesian Optimization with Graph Convolutional Network (GCN) predictor. Specifically, we apply GCN as a surrogate model to adaptively discover and incorporate nodes structure to approximate the performance of the architecture. For NAS-oriented tasks, we also design a weighted loss focusing on architectures with high performance. Our method further considers an efficient multi-objective search which can be flexibly injected into any sample-based NAS pipelines to efficiently find the best speed/accuracy trade-off. Extensive experiments are conducted to verify the effectiveness of our method over many competing methods, e.g. 128.4x more efficient than Random Search and 7.8x more efficient than previous SOTA LaNAS for finding the best architecture on the largest NAS dataset NasBench-101.", "review": "Review:###This paper proposed BOGCN-NAS that encodes current architecture with Graph convolutional network (GCN) and uses the feature extracted from GCN as the input to perform a Bayesian regression (predicting bias and variance, See Eqn. 5-6). They use Bayesian Optimization to pick the most promising next model with Expected Improvement, train it and take its resulting accuracy/latency as an additional training sample, and repeat. They tested the framework on both single-objective and multi-objective tasks. On the single-objective (accuracy task). They tested it on NasBench and LSTM-12K, two NAS datasets with pre-trained models and their performance. They obtained very good performance on both, beating LaNAS (previous SoTA) by 7.8x higher sample efficiency. On multiple-objective, they show higher efficiency in finding Pareto frontier models, compared to random search. One main question I have is whether the next model is chosen given the current prediction model? For NasBench, did you run your predictor for all (420k minus explored) models and pick the one that maximizes Expected Improvement? Note that LaNAS is more efficient in that manner by sampling directly on polytopes formed by linear constraints. If so, how do you pick the next candidate models in open domain setting? It looks like Eqn. 9 biases the training heavily towards high accuracy region, which is a hack. Although in the Appendix (Fig. 7) the authors have already perform some analysis on the effect of different weight terms, I wonder whether there is a more problem-independent way of doing it. The MCTS in LaNAS is one way that automatically focuses the search on the important regions. Currently, the proposed approach might limit the usability of the proposed method to other situations when accuracy is no longer that important. The performance is really impressive in the NasBench and LSTM dataset. The paper mentioned that “BO can really predict the precious model to explore next” but didn’t provide an examples in the paper. I would whether the author could visualize it in the next revision, which would be very cool. Do you have open domain search research in single-objective search? Why not use NasNet architecture for a fair comparison with other NAS papers? In Appendix, Fig. 6 shows that even without GCN and BO, a single MLP already achieves global optimal solution in LSTM-12K dataset with ~850 samples, already beating all the previous methods (Random, Reg Evolution, MCTS and LaNAS). If that’s the case, I wonder how much roles the proposed methods (BO + GCN) play during search? Also what do you mean by “without BO”? Do you only predict the mean and assume all variance is constant? =====Post Rebuttal====== I have read other reviewers* comments and the rebuttal. One of the main problems in this paper is an unfair comparison against LaNAS. LaNAS only uses a single sample at each leaf, while they sample 100% to 0.1% of the models, evaluate them with the current BO model and find the best. For NasBench-101 with 420K models, even sampling 0.1% each time means ~400 samples and the performance (from the rebuttal) seems to degrade substantially from 100% case (1464.4->4004.4). This means that almost 3x more samples are needed, compared to what they claimed. I agree with the authors that calling BO function is super fast so maybe this is fine. However, on the open domain experiments, their performance is also not better than LaNAS+c/o, which they didn*t list in the rebuttal. I listed it here: Model Params Top-1 err No. of samples truly evaluated ----------------------------------------------------------------------------------------------------------------- BOGCN+cutout (V1) 3.1M 2.74 200 BOGCN+cutout (V2) 3.5M 2.61 400 LaNAS+c/o 3.2M 2.53±0.05 803 Overall this paper is on the borderline. I don*t mind if the paper gets rejected. For now I lower the score to 3."}
{"id": "iclr2020_68", "title": "DP-LSSGD: An Optimization Method to Lift the Utility in Privacy-Preserving ERM | OpenReview", "abstract": "Abstract:###Machine learning (ML) models trained by differentially private stochastic gradient descent (DP-SGD) have much lower utility than the non-private ones. To mitigate this degradation, we propose a DP Laplacian smoothing SGD (DP-LSSGD) to train ML models with differential privacy (DP) guarantees. At the core of DP-LSSGD is the Laplacian smoothing, which smooths out the Gaussian noise used in the Gaussian mechanism. Under the same amount of noise used in the Gaussian mechanism, DP-LSSGD attains the same DP guarantee, but a better utility especially for the scenarios with strong DP guarantees. In practice, DP-LSSGD makes training both convex and nonconvex ML models more stable and enables the trained models to generalize better. The proposed algorithm is simple to implement and the extra computational complexity and memory overhead compared with DP-SGD are negligible. DP-LSSGD is applicable to train a large variety of ML models, including DNNs.", "review": " This paper introduces LSSGD [1] in the privacy-preserving empirical risk minimization to improve the utility upper bound under the same privacy budget. The experiments show that the proposed method called DP-LSSGD outperforms the baseline method--DP-SGD [2]. My major concerns are as follows. 1. The improvement of the utility upper bound is insignificant. Table 1 shows that the utility upper bounds of DP-LSSGD and DP-SGD have the same order. 2. The novelty is incremental. The key method in this paper--LSSGD--was proposed in [1]. 3. Table 2 in [3] shows that DP-SVRG [3] outperforms the baseline method--DP-SGD. The authors may want to conduct more experiments to compare DP-LSSGD with other related work including DP-SVRG. 4. The authors may want to provide the problem setup of the privacy-preserving empirical risk minimization in detail. [1] S. Osher, B. Wang, P. Yin, X. Luo, M. Pham, and A. Lin. Laplacian Smoothing Gradient Descent. ArXiv:1806.06317, 2018. [2] R. Bassily, A. Smith, and A. Thakurta. Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds. In 55th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2014) , 2014. [3] D. Wang, M. Ye, and J. Xu. Differentially Private Empirical Risk Minimization Revisited: Faster and More General. In Advances in Neural Information Processing Systems (NIPS 2017), 2017."}
{"id": "iclr2020_69", "title": "Teacher-Student Compression with Generative Adversarial Networks | OpenReview", "abstract": "Abstract:###More accurate machine learning models often demand more computation and memory at test time, making them difficult to deploy on CPU- or memory-constrained devices. Teacher-student compression (TSC), also known as distillation, alleviates this burden by training a less expensive student model to mimic the expensive teacher model while maintaining most of the original accuracy. However, when fresh data is unavailable for the compression task, the teacher*s training data is typically reused, leading to suboptimal compression. In this work, we propose to augment the compression dataset with synthetic data from a generative adversarial network (GAN) designed to approximate the training data distribution. Our GAN-assisted TSC (GAN-TSC) significantly improves student accuracy for expensive models such as large random forests and deep neural networks on both tabular and image datasets. Building on these results, we propose a comprehensive metric—the TSC Score—to evaluate the quality of synthetic datasets based on their induced TSC performance. The TSC Score captures both data diversity and class affinity, and we illustrate its benefits over the popular Inception Score in the context of image classification.", "review": " This paper proposes an approach for improving teacher-student compression by introducing the assistant of GANs. A conditional GAN is trained for generating synthetic data. Then, the generated data combined with training data is used for knowledge distillation. Experiments on large random forests and deep neural networks demonstrate the effectiveness of the proposed method on data-augmentation. Moreover, an evaluation metric is proposed to evaluate s across-class diversity and intra-class diversity for generative models. Pros: + While using synthetic data of GANs to assist supervised learning has been shown to be failed in pervious works (e.g. [1] and also shown in this paper, this paper presents a new perspective to utilize GAN as a successful data-augmentation technique in teacher student paradigm. + Experiments are conducted in several settings including different models (random forests, deep neural networks) and different datasets (images and tabular) to show the effectiveness of proposed method in various settings. + The proposed GAN-TSC can be combined with standard augmentation to achieve higher performance as shown in the experiments. + This paper is well written and easy to follow. Cons: - Knowledge distillation, as a classical model compression technique, has been applied in deep convolutional models for several years. The CIFAR-10 dataset is too simple to evaluate this kind of methods. The author should try to conduct experiments on large scale datasets such as ImageNet, unless the reliability of the proposed algorithm would be very limited. - The proposed TSCScore seems to be similar with [1], especially when its novelty mainly lies in intra-class diversity compared with IS. It’s necessary to discuss difference between TSCScore and [1]. [1] Konstantin Shmelkov, Cordelia Schmid, and Karteek Alahari. How good is my GAN? ECCV 2018."}
{"id": "iclr2020_70", "title": "The Probabilistic Fault Tolerance of Neural Networks in the Continuous Limit | OpenReview", "abstract": "Abstract:###The loss of a few neurons in a brain rarely results in any visible loss of function. However, the insight into what “few” means in this context is unclear. How many random neuron failures will it take to lead to a visible loss of function? In this paper, we address the fundamental question of the impact of the crash of a random subset of neurons on the overall computation of a neural network and the error in the output it produces. We study fault tolerance of neural networks subject to small random neuron/weight crash failures in a probabilistic setting. We give provable guarantees on the robustness of the network to these crashes. Our main contribution is a bound on the error in the output of a network under small random Bernoulli crashes proved by using a Taylor expansion in the continuous limit, where close-by neurons at a layer are similar. The failure mode we adopt in our model is characteristic of neuromorphic hardware, a promising technology to speed up artificial neural networks, as well as of biological networks. We show that our theoretical bounds can be used to compare the fault tolerance of different architectures and to design a regularizer improving the fault tolerance of a given architecture. We design an algorithm achieving fault tolerance using a reasonable number of neurons. In addition to the theoretical proof, we also provide experimental validation of our results and suggest a connection to the generalization capacity problem.", "review": "Review:###This paper investigates the problem of fault telorance on NN: basically how the predictions are affected by failure of certain neurons at prediction time. The analysis is theoretical under the classical assumption of lipschitz bounded non-linearities and looking at the limit case of an infinite number of neurons. The paper is well written and comes with public code that allows to replicate the experiments illustrating the theoretical derivations. The paper is well motivated and addresses a timely matter. Typos - *the error of the output of* -> *the error of the output*"}
{"id": "iclr2020_71", "title": "Curvature-based Robustness Certificates against Adversarial Examples | OpenReview", "abstract": "Abstract:###A robustness certificate against adversarial examples is the minimum distance of a given input to the decision boundary of the classifier (or its lower bound). For {it any} perturbation of the input with a magnitude smaller than the certificate value, the classification output will provably remain unchanged. Computing exact robustness certificates for deep classifiers is difficult in general since it requires solving a non-convex optimization. In this paper, we provide computationally-efficient robustness certificates for deep classifiers with differentiable activation functions in two steps. First, we show that if the eigenvalues of the Hessian of the network (curvatures of the network) are bounded, we can compute a robustness certificate in the norm efficiently using convex optimization. Second, we derive a computationally-efficient differentiable upper bound on the curvature of a deep network. We also use the curvature bound as a regularization term during the training of the network to boost its certified robustness against adversarial examples. Putting these results together leads to our proposed {_x0008_f C}urvature-based {_x0008_f R}obustness {_x0008_f C}ertificate (CRC) and {_x0008_f C}urvature-based {_x0008_f R}obust {_x0008_f T}raining (CRT). Our numerical results show that CRC outperforms CROWN*s certificate by an order of magnitude while CRT leads to higher certified accuracy compared to standard adversarial training and TRADES.", "review": "Review:###This paper develops computationally-efficient convex relaxations for robustness certification and adversarial attack problems given the classifier has a bounded curvature. The authors showed that the convex relaxation is tight under some general conditions. To be able to use proposed certification and attack convex optimizations, the authors derive global curvature bounds for deep networks with differentiable activation functions. The result is a consequence of a closed-form expression that the paper derived for the Hessian of a deep network. The empirical results indicate that the proposed curvature-based robustness certificate outperforms the CROWN certificate by an order of magnitude while being faster to compute as well. Furthermore, adversarial training using the attack method coupled with curvature regularization results in a significantly higher certified robust accuracy than the existing adversarial training methods. 1. I am not at all familiar with the robustness certification, therefore little knowledge of relevant related works. 2. I hope the authors can provide a more thorough survey of related works. 3. There exist good amount of theoretical analysis in the paper. However all empirical results are only from MNIST. This certainly weakens the method*s contribution."}
{"id": "iclr2020_72", "title": "Understanding Isomorphism Bias in Graph Data Sets | OpenReview", "abstract": "Abstract:###In recent years there has been a rapid increase in classification methods on graph structured data. Both in graph kernels and graph neural networks, one of the implicit assumptions of successful state-of-the-art models was that incorporating graph isomorphism features into the architecture leads to better empirical performance. However, as we discover in this work, commonly used data sets for graph classification have repeating instances which cause the problem of isomorphism bias, i.e. artificially increasing the accuracy of the models by memorizing target information from the training set. This prevents fair competition of the algorithms and raises a question of the validity of the obtained results. We analyze 54 data sets, previously extensively used for graph-related tasks, on the existence of isomorphism bias, give a set of recommendations to machine learning practitioners to properly set up their models, and open source new data sets for the future experiments.", "review": " This work probes graph classification datasets for isomorphism bias. They find substantial amount of bias in some datasets and show that they suffer from data leakage. They further perform a more fine-grained evaluation taking into consideration the node/edge types which reduce the perceived effects. They also provide some recommendations for measuring the *right metrics* and release clean versions of the considered datasets. Strengths: - The methodology is rigorous and the datasets considered is extensive - The paper is well written Concerns: - Isomorphism is not necessarily a bad thing in graph classification tasks. Especially in chemistry where a bond decides if a compound is poisonous or not. Also, as the authors themselves mention, taking node/edge labels decrease the isomorphism in most datasets. - The results and recommendations presented in the paper are intuitive and somewhat trivial - I am not sure if ICLR is the right venue for this work"}
{"id": "iclr2020_73", "title": "Adaptive Loss Scaling for Mixed Precision Training | OpenReview", "abstract": "Abstract:###Mixed precision training (MPT) is becoming a practical technique to improve the speed and energy efficiency of training deep neural networks by leveraging the fast hardware support for IEEE half-precision floating point that is available in existing GPUs. MPT is typically used in combination with a technique called loss scaling, that works by scaling up the loss value up before the start of backpropagation in order to minimize the impact of numerical underflow on training. Unfortunately, existing methods make this loss scale value a hyperparameter that needs to be tuned per-model, and a single scale cannot be adapted to different layers at different training stages. We introduce a loss scaling-based training method called adaptive loss scaling that makes MPT easier and more practical to use, by removing the need to tune a model-specific loss scale hyperparameter. We achieve this by introducing layer-wise loss scale values which are automatically computed during training to deal with underflow more effectively than existing methods. We present experimental results on a variety of networks and tasks that show our approach can shorten the time to convergence and improve accuracy, compared with using the existing state-of-the-art MPT and single-precision floating point.", "review": "Review:###The authors propose an adaptive loss scaling method during the backpropagation stage for the mix precision training to reduce the underflow. Compared with the previous work, which scales the loss by human design, and needs to be consistent in all layers. The authors state that they can decide the scale rate layer by layer automatically to reduce the underflow in a low precision situation. They calculate the scale rate using the statistic information of the layer weight and gradient. By adaptively scale each layer’s loss and gradient, this method can reduce the underflow rate better than the previous work. Additionally, the authors claim that the computation overhead is not significant, so it is efficient to use rather than searching from a set of fix scale rates. The experiments present on image classification and objective detection benchmarks. From the result, we can see that the adaptive loss scale reaches a relatively high point on all the tasks. ? Pros: ? - The method is straight forward and easy to understand. The motivation is good. They get some impressive results on ResNet110 and SSD512 comparing with the fixed scaling method. Besides, they give some analysis of their advantages and disadvantages in different networks, which looks promising to me. ? Cons: - One question in Section 3.2.1, the assumption that w, g, p can be treated as the random variable with Gaussian distribution seems not natural in the training process. Especially p is a zero-mean distribution. The cited paper uses this assumption in a more convincing case, such as the weight initialization task. Notice that He et al., 2015 claim that the product of weight and gradient can be a zero-mean normal distribution is based on the weight is a symmetric distribution around zero, which is not true in neither this paper’s assumption nor the real training situation. - In the objective detection part, I can not find which dataset the authors use. Though the author state that they follow Liu et al., 2016 ’s work, there are also several tasks in Liu’s paper, and I can not directly match the resulting point with any of those tasks, which makes me hard to confirm the experiment result. - The experiment setting is unclear. Here are two questions. 1, What is the initial scale at the last layer? It should be manually designed in the experiment, and I think this value may affect the other layer’s scale as well. If the algorithm is robust for this scale, it is better to show some study on that. 2, What update frequency is used in the experiment? The authors say that the overhead can be reduced by reducing the frequency, but they do not clearly show which frequency they use in their experiment, if the frequency does not affect the performance, it is also better to claim or show some study on that. Minor comments: - Figures can use a larger font. Figures 4a and 4b can be aligned better."}
{"id": "iclr2020_74", "title": "Semi-supervised Pose Estimation with Geometric Latent Representations | OpenReview", "abstract": "Abstract:###Pose estimation is the task of finding the orientation of an object within an image with respect to a fixed frame of reference. Current classification and regression approaches to the task require large quantities of labelled data for their purposes. The amount of labelled data for pose estimation is relatively limited. With this in mind, we propose the use of Conditional Variational Autoencoders (CVAEs) cite{Kingma2014a} with circular latent representations to estimate the corresponding 2D rotations of an object. The method is capable of training with datasets that have an arbitrary amount of labelled images providing relatively similar performance for cases in which 10-20% of the labels for images is missing.", "review": "Review:###This paper proposes to employ conditional variational autoencoder (CVAE) to estimate the geometry of 2D rotations of objects given images partially labeled. Here, the label represents the geometry of the 2D rotation. The proposed method introduces two latent representation. z is the ordinal latent variable and r a latent representation for the rotations where the latent variable is defined in the 1-dimensional circle in R^2 so that it can naturally represent a hyperspherical latent space. The construction of the proposed CVAE is straightforward. For labeled images, the (evidence lower bound of the) loglikelihood of the image-rotation pairs is maximized. For labeled images. For labeled images, the (evidence lower bound of the) loglikelihood of the images is maximized. The decision of the reviewer of this paper is weak reject. The major reason is the lack of technical originality. The construction itself would be novel while each component (e.g., CVAE, latent representation for the rotations, and semi-supervised construction of CVAE) have been already known. Experimental results are not surprising but show that the presented method is useful to some extent. In a sense, it is a bit disappointing that we need 50+% images needed to be labeled to achieve < 20-degree error. One interesting observation of this paper is that more labeled images give better results than giving greater number of renders. Expansion to 3D rotations would be a good challenge."}
{"id": "iclr2020_75", "title": "The Surprising Behavior Of Graph Neural Networks | OpenReview", "abstract": "Abstract:###We highlight a lack of understanding of the behaviour of Graph Neural Networks (GNNs) in various topological contexts. We present 4 experimental studies which counter-intuitively demonstrate that the performance of GNNs is weakly dependent on the topology, sensitive to structural noise and the modality (attributes or edges) of information, and degraded by strong coupling between nodal attributes and structure. We draw on the empirical results to recommend reporting of topological context in GNN evaluation and propose a simple (attribute-structure) decoupling method to improve GNN performance.", "review": "Review:###The paper presents four experimental set-ups to get a better understanding how the performance of Graph Neural Networks (GNN) depend on topological and/or nodal information. As the paper is not really in my research area, I would have liked the paper to be a bit more self-contained, but the writing of the paper is generally clear. The contributions of the paper are mainly experimental and show different *surprising* aspects about GNNs. I found the experimental results to be interesting, especially those presented in Section 5 about decoupling attribute and topological information. However, I think the presentation may be improved and more information (e.g. about how the experiments were conducted or more plots) should be reported in the main paper or the appendix. Regarding the experiments, the authors made choices about which subsets of datasets (Table 2) or which subsets of measures (Table 1) to use. It would be nice to explain how those choices are made and if they are well-justified. Otherwise, it feels that the presented experimental results have been hand-picked. Regarding Section 3, I guess that Figure 1 presents averaged results over datasets or over models. I think it would be important to share the results per datasets or per models, as averaging may hide some key aspects. On page 3, the third paragraph could be illustrated with some plots. Besides, the its first sentence seems to contradict the last sentence of the paragraph before Section 4. Moreover, the text in the fourth paragraph doesn*t seem to fit well Fig. 2b for Amazon computers. Am I misreading this plot? Regarding Section 4, I think it would have been interesting to report the metrics about the topological features for the perturbed graphs. Are they very different from those for the orignal graphs? Regarding Section 6, the sentence *In the previous sections, we have demonstrated that GNNs .. are robust to perturbations to it* on page 7 seems to contradict the conclusion of Section 4. I found the experiments in this section to be less convincing. The edge addition technique seems to be ad-hoc and it is not clear why it would improve the performance or not. For instance, how was the value 6°*density-G chosen? Overall, although I found some of the experimental results very intriguing, I think the paper may not be ready for publication in its current state."}
{"id": "iclr2020_76", "title": "Angular Visual Hardness | OpenReview", "abstract": "Abstract:###The mechanisms behind human visual systems and convolutional neural networks (CNNs) are vastly different. Hence, it is expected that they have different notions of ambiguity or hardness. In this paper, we make a surprising discovery: there exists a (nearly) universal score function for CNNs whose correlation with human visual hardness is statistically significant. We term this function as angular visual hardness (AVH) and in a CNN, it is given by the normalized angular distance between a feature embedding and the classifier weights of the corresponding target category. We conduct an in-depth scientific study. We observe that CNN models with the highest accuracy also have the best AVH scores. This agrees with an earlier finding that state-of-art models tend to improve on classification of harder training examples. We find that AVH displays interesting dynamics during training: it quickly reaches a plateau even though the training loss keeps improving. This suggests the need for designing better loss functions that can target harder examples more effectively. Finally, we empirically show significant improvement in performance by using AVH as a measure of hardness in self-training tasks.", "review": "Review:###Main Contribution: This paper is trying to bridge the gap between CNN and the human visual system by proposing a metric (angular visual distance) and validate that this metric is correlated to the human visual hardness and this metric has a stronger relation compared to the softmax score which has been viewed as a metric measuring the hardness of images in CNNs. Furthermore, this paper proposed a reasonable explanation for this observation, i.e., the norm is possibly not correlated to the human visual hardness and validate through the experiment. Finally, this paper shows that this metric is also useful in other applications. Innovative Part: The metric proposed in this paper is based on an interesting and also innovative observation that samples in each class will concentrate in a convex cone in the embedding space (e.g., shown in Figure 1) and the norm has no information on the visual hardness. I like this observation since several existing theoretical results have similar implications although in far simpler settings. For example, [1] shows that for LINEAR model with logistic loss, gradient descent converges to the maximum margin classifier while the norm (corresponding to ||x||_2 in this paper) diverges to infinity with log(T) rate. If we are looking into the Figure 1, we will see that ten convex cones almost form an equal partition of the two-dimensional space and this indicates that the classifier is very similar to the classifier with the maximum margin in the angular space (NOT in the Euclidean space). The observation is quite intuitive and has strong theoretical foundation, which is the main reason that I vote for the acceptance of this paper. Drawbacks: This paper also have several drawbacks but I do believe they can be addressed very easily. 1. The introduction is not well-written, especially the second paragraph. I strongly recommend modifying the introduction. For the first three sentences of the second paragraph, do you mean that CNNs are constructed based on some properties of the human visual systems and thus they should have had some connections but they actually fundamentally differ in practice? Otherwise, if these two are fundamentally different with each other, what is the point of showing some connections between them? For the sentence *we use this dataset to verify our hypothesis*, what is the hypothesis? Do you mean the hypothesis that human visual hardness should have had connections to the classifying hardness for CNNs? For the sentence *Given a CNN, we propose a novel score function that has strong correlation with human visual hardness*, I am not sure whether the word *strong* can be used here. 2. In table 1, I am not sure whether the author should assume that all audiences have some background on z-score, although I can understand it. I would also encourage the authors to use other correlation metrics with more intuitive explanations (e.g., correlation coefficients). 3. For the experiment, I would like to recommend authors adding the following experiments. 3.1) Show that on other datasets (e.g., CIFAR 10, 100), AVH converges fast to a plateau while the norm constantly diverges to infinity. 3.2) Introducing several other measurements to show the correlation. 3.3) I also would like to see similar results in Table 1 for different models. [1] Soudry, Daniel, et al. *The implicit bias of gradient descent on separable data.* The Journal of Machine Learning Research 19.1 (2018): 2822-2878."}
{"id": "iclr2020_77", "title": "Multi-source Multi-view Transfer Learning in Neural Topic Modeling with Pretrained Topic and Word Embeddings | OpenReview", "abstract": "Abstract:###Though word embeddings and topics are complementary representations, several past works have only used pretrained word embeddings in (neural) topic modeling to address data sparsity problem in short text or small collection of documents. However, no prior work has employed (pretrained latent) topics in transfer learning paradigm. In this paper, we propose a framework to perform transfer learning in neural topic modeling using (1) pretrained (latent) topics obtained from a large source corpus, and (2) pretrained word and topic embeddings jointly (i.e., multiview) in order to improve topic quality, better deal with polysemy and data sparsity issues in a target corpus. In doing so, we first accumulate topics and word representations from one or many source corpora to build respective pools of pretrained topic (i.e., TopicPool) and word embeddings (i.e., WordPool). Then, we identify one or multiple relevant source domain(s) and take advantage of corresponding topics and word features via the respective pools to guide meaningful learning in the sparse target domain. We quantify the quality of topic and document representations via generalization (perplexity), interpretability (topic coherence) and information retrieval (IR) using short-text, long-text, small and large document collections from news and medical domains. We have demonstrated the state-ofthe- art results on topic modeling with the proposed transfer learning approaches.", "review": "Review:###This is an emergency review. This work proposes a novel method to use pre-trained topic embeddings and pre-trained word embeddings obtained from various corpora in the transfer learning framework. Their model architecture is based on DocNADE, unsupervised neural-network based topic model, and the authors propose two strategies to use pre-trained topic embeddings and pre-trained word vectors. 1) Addition of a weighted sum of pre-trained word embeddings and the hidden vector of DocNADE. 2) L2-Regularization term between topic embedding of DocNADE and pre-trained topic embeddings. They propose to align these two embeddings by multiplying align matrix *A* to the topic embedding of DocNADE. They show the transfer learning performance of their model on various source/target domain datasets, including medical target corpora, and verify that their model outperforms on a short text and small document collection. Strengths. 1. Comparison with the data augmentation baseline shows the performance gain is not only from bigger training data. Even though comparison with the naive baseline (data augmentation) seems too obvious, I think the results clearly show their claim about the importance of using transfer learning in neural topic modeling domain. 2. As the first approach that introduces a novel transfer learning framework with pre-trained topic embeddings, they show tons of experimental results with various datasets and metrics to show the specification of their method. Their experimental setting is well designed. Weaknesses and comments: Their method to combine pre-trained word embeddings and pre-trained topic embeddings is too simple. Since this is the first approach to use topic embedding in the transfer learning field, the simplicity of the proposed method is somewhat necessary. However, a weighted sum of pre-trained topic/word vectors seems not enough to transfer multisource knowledge. For instance, word vectors obtained from individual training processes do not share embedding vector space. As you apply the alignment method to topic embeddings from various sources, you should align word embeddings too."}
{"id": "iclr2020_78", "title": "Gradients as Features for Deep Representation Learning | OpenReview", "abstract": "Abstract:###We address the challenging problem of deep representation learning--the efficient adaption of a pre-trained deep network to different tasks. Specifically, we propose to explore gradient-based features. These features are gradients of the model parameters with respect to a task-specific loss given an input sample. Our key innovation is the design of a linear model that incorporates both gradient features and the activation of the network. We show that our model provides a local linear approximation to a underlying deep model, and discuss important theoretical insight. Moreover, we present an efficient algorithm for the training and inference of our model without computing the actual gradients. Our method is evaluated across a number of representation learning tasks on several datasets and using different network architectures. We demonstrate strong results in all settings. And our results are well-aligned with our theoretical insight.", "review": "Review:###Summary: This paper proposes to use the gradients of specific layers of convolutional networks as features in a linearized model for transfer learning and fast adaptation. The method is theoretically backed by an appeal to the recently proposed neural tangent kernel and seems like it could be practically useful. Edit: Post rebuttal, I am somewhat satisfied by the authors* response and find their ablation study somewhat compelling hence am increasing my score to a weak accept. However, I*m not completely convinced that their choice of layers to use as gradient features is reasonable. If this paper does end up getting accepted, I*d very strongly advocate for being extremely clear as to 1) why the gradient features used are used, 2) a good choice of gradient features in practice 3) inclusion of all of the relevant ablation studies in the final version. I tend to weakly reject this paper currently despite liking the simplicitly and timeliness of the approach. Specifically, I would like further discussion of the choice of the layers to use as gradient features and an ablation study on supervised trained networks. Originality: I believe that this is one of the first papers to explicitly use the Taylor approximation (neural tangent kernel) in a transfer learning setting, making the approach timely and potentially practically useful. Significance: The approach is a quite nice merging of theoretical insights with a neat practical implementation. Although the main methodological advance is a straightforward application of the thinking behind Jacobian vector products, the method is well described and ought to be practically useful. However, I have a bit of a concern as to its practical necessity in comparison to simple fine-tuning of the final softmax layer (referred to in Table 1 as \theta_2) on a new dataset. Clarity: While the approach described in Section 3 is quite generic – theoretically, the method should simply consist of training the final layer of the neural network (w_1) and weights from the Jacobian features around the Taylor expansion. However, the experimental approaches suggest that different layers were used in each experiment – see e.g. “[we] … compute our gradient based features from one, two, or all of the top-3 conv layers” (Section 4.2 for the BiGAN architectures) versus “our gradient based features are from the last 2 conv layers” (Table 2 for the AlexNet architectures). Why was the entire network (modulo the last feed forward layer) not simply used for the Jacobian features? Similarly, why was the entirety of the model (again modulo the last feedforward layer) not used for forwards model in the AlexNet experiments? Based on the ablation study in Section 4.1, the authors find that “it suffices to set the very top layer as \theta_2 to enjoy a reasonably large performance gain.” When this is the case, it is a bit tough to distinguish the approach from standard final layer transferring approaches (Sharif Razavian et al, 2014) and indeed Bayesian final layer approaches that simply retrain an output layer (e.g. Perrone et al, 2018). Indeed this seems to be the case in Table 1, if that is so, then why not just re-train \theta_2 as well throughout the experiments as it will typically just be another stochastic convex optimization problem? Could the authors quickly describe an implementation of the scalable Jacobian inner products in Section 3.3? It seems like outputs will have to be cached during the forwards passes, thereby requiring a somewhat significant amount of software engineering (and memory overhead) to be have to do this process for each new layer. Is this the correct understanding of how one would implement the procedure? A quick paste of PyTorch pseudo-code would be sufficient here. Quality: The experiments seem to be relatively convincing – it seems exciting that a linear model + the network’s features itself can typically perform as well as fine-tuning and occasionally even better than fine-tuning itself. However, I am a bit concerned by the fact that the ablation studies themselves only utilize models trained in an un-supervised fashion. I’d instead like the authors to run an ablation study on supervised trained models (perhaps 8 layer conv nets or VGG16) in the same manner as in Table 1 and Section 4.1. Specifically, I’d like to see this done to see whether the gradients in fact are as interesting as features when they have been trained in a supervised fashion. Similarly, I’d like to see the features themselves used as a linear classifier (no network forwards passed) in the same two ablation studies. That is, could the authors use w_1^T J w_2 as the features for their linear classifier. If they have already done so and I’ve missed that in the tables somehow, I apologize. This experiment should help to test out how _useful_ the features defined by the Jacobian matrix are in comparison to the network’s forward pass itself. Minor Comments: Introduction: “After learning, the the (sic) activation of the deep network are considered as generic features.” Not only is there a small typo, but you should either include a citation here or be more specific as to what “the activation” of the deep network is here. Introduction: “And the accuracy of the rthe (sic) …” typo + please do not start sentences with and if at all possible. Section 3.1 (beneath eq. 2): “liner” should be linear. Table 3: “Self-supervise” should be “Self-supervised.” References: Perrone et al, Scalable Hyperparameter Transfer Learning, NeurIPS, 2018. http://papers.nips.cc/paper/7917-scalable-hyperparameter-transfer-learning.pdf Sharif Razavian et al, CNN Features off-the-Shelf: an Astounding Baseline for Image Recognition, CVPRW, 2014. https://arxiv.org/abs/1403.6382"}
{"id": "iclr2020_79", "title": "On Understanding Knowledge Graph Representation | OpenReview", "abstract": "Abstract:###Many methods have been developed to represent knowledge graph data, which implicitly exploit low-rank latent structure in the data to encode known information and enable unknown facts to be inferred. To predict whether a relationship holds between entities, their embeddings are typically compared in the latent space following a relation-specific mapping. Whilst link prediction has steadily improved, the latent structure, and hence why such models capture semantic information, remains unexplained. We build on recent theoretical interpretation of word embeddings as a basis to consider an explicit structure for representations of relations between entities. For identifiable relation types, we are able to predict properties and justify the relative performance of leading knowledge graph representation methods, including their often overlooked ability to make independent predictions.", "review": "Review:###Summary: The paper attempts to understand the latent structure underlying knowledge graph embedding methods. The work can be seen as an extension of understanding of PMI-based word embedding methods. They categorize knowledge graph relations into three categories based on their relation conditions: Relatedness (R), Specialisation (S), and Context-shift (C). For each category, they evaluate a representative of different types of knowledge graph embedding methods. Through results, they demonstrate that a model’s ability to represent a specific relation type depends on the limitations imposed by the model architecture with respect to satisfying the necessary relation conditions. Questions: 1. The results in Tables 3 and 4 demonstrate that MuRE is the most effective method for handling different types of relations but then how come its performance on FB15k-237 (.336 MRR) is significantly lower than other methods like TuckER (.358 MRR). Can you provide an explanation? 2. In Section 3.2, the authors list 4 predictions (P1-4). It would be great if authors could provide some more reasoning behind coming with these predictions. 3. In Section 4.2, it is stated that “ranking based metrics like MRR and hits@k are flawed if entities are related to more than k others”. It would be great if the authors could give an example to make it more clear."}
{"id": "iclr2020_80", "title": "Embodied Multimodal Multitask Learning | OpenReview", "abstract": "Abstract:###Visually-grounded embodied language learning models have recently shown to be effective at learning multiple multimodal tasks such as following navigational instructions and answering questions. In this paper, we address two key limitations of these models, (a) the inability to transfer the grounded knowledge across different tasks and (b) the inability to transfer to new words and concepts not seen during training using only a few examples. We propose a multitask model which facilitates knowledge transfer across tasks by disentangling the knowledge of words and visual attributes in the intermediate representations. We create scenarios and datasets to quantify cross-task knowledge transfer and show that the proposed model outperforms a range of baselines in simulated 3D environments. We also show that this disentanglement of representations makes our model modular and interpretable which allows for transfer to instructions containing new concepts.", "review": "Review:###The paper explores multi-task learning in embodied environments and proposes a Dual-Attention Model that disentangles the knowledge of words and visual attributes in the intermediate representations. It addresses two tasks, namely Semantic Goal Navigation (SGN) and Embodied Question Answering (EQA), using a simple synthetic environment. The paper compares against a few simple baselines and baselines adapted from models in each task. I would recommend for acceptance, as the experimental results show that the proposed approach successfully transfers knowledge across tasks. However, I would also like to note that the paper has a few drawbacks. First, the paper uses a new environment to evaluate the SGN and EQA task instead of the benchmark environments for these two tasks, making it difficult to compare performance to previous work. The environment in the paper is small (compared to e.g., House3D for EQA) and has a limited variety. Also, the paper only compares to relatively out-of-date approaches on EQA and SGN, instead of the state-of-the-art approaches on them. In addition, the paper should also discuss its connections to other multi-task learning approaches in the related work section."}
{"id": "iclr2020_81", "title": "MACER: Attack-free and Scalable Robust Training via Maximizing Certified Radius | OpenReview", "abstract": "Abstract:###Adversarial training is one of the most popular ways to learn robust models but is usually attack-dependent and time costly. In this paper, we propose the MACER algorithm, which learns robust models without using adversarial training but performs better than all existing provable l2-defenses. Recent work shows that randomized smoothing can be used to provide certified l2 radius to smoothed classifiers, and our algorithm trains provably robust smoothed classifiers via MAximizing the CErtified Radius (MACER). The attack-free characteristic makes MACER faster to train and easier to optimize. In our experiments, we show that our method can be applied to modern deep neural networks on a wide range of datasets, including Cifar-10, ImageNet, MNIST, and SVHN. For all tasks, MACER spends less training time than state-of-the-art adversarial training algorithms, and the learned models achieve larger average certified radius.", "review": "Review:###This paper proposes a new approach to training models robust to perturbations (or *attacks*) within an l_2 radius, by maximizing a surrogate---a soft randomized smoothing loss---for the *certified radius* (a lower bound for the l_2 attack radius) of the classifier. This approach has the advantage of not needing to explicitly train against specific attacks, and is thus much faster and easier to optimize. The authors provide certain theoretical guarantees and also demonstrate strong empirical results relative to two baseline approaches. This work builds on prior work where the goal of training a classifier that is robust to attacks is phrased as maximizing the *robust radius*, the largest l_2 ball within which a data point x can be perturbed without changing the (correct) classifications of trained classifier. Since directly maximizing this robust radius is intractable, prior work seeks to derive a lower bound which the authors term the *certified radius*. In order to directly maximize this, the authors use randomized smoothing- in which a randomly Gaussian-smoothed version of classifier f is used to make predictions- and then motivate and develop a *soft randomized smoothing* lower bound surrogate of the certified radius to maximize, which is differentiable and provably numerically stable. Overall, this paper is well explicated, starting with clearly written background on basic concepts and prior work, stating clear desiderata that the surrogate loss being developed should satisfy, and then providing theoretical proofs as to this. The experiments are then thorough including core and ablation experiments to showcase the method. One downside is that the paper does make fairly aggressive claims (e.g. *performs better than all existing provable l_2-defenses*), but then only compares to two prior / baseline approaches in the experiments. Given the density of the field recently, this seems a bit sparse (although this reviewer is not an expert in this area)?"}
{"id": "iclr2020_82", "title": "NAS evaluation is frustratingly hard | OpenReview", "abstract": "Abstract:###Neural Architecture Search (NAS) is an exciting new field which promises to be as much as a game-changer as Convolutional Neural Networks were in 2012. Despite many great works leading to substantial improvements on a variety of tasks, comparison between different methods is still very much an open issue. While most algorithms are tested on the same datasets, there is no shared experimental protocol followed by all. As such, and due to the under-use of ablation studies, there is a lack of clarity regarding why certain methods are more effective than others. Our first contribution is a benchmark of 8 NAS methods on 5 datasets. To overcome the hurdle of comparing methods with different search spaces, we propose using a method’s relative improvement over the randomly sampled average architecture, which effectively removes advantages arising from expertly engineered search spaces or training protocols. Surprisingly, we find that many NAS techniques struggle to significantly beat the average architecture baseline. We perform further experiments with the commonly used DARTS search space in order to understand the contribution of each component in the NAS pipeline. These experiments highlight that: (i) the use of tricks in the evaluation protocol has a predominant impact on the reported performance of architectures; (ii) the cell-based search space has a very narrow accuracy range, such that the seed has a considerable impact on architecture rankings; (iii) the hand-designed macrostructure (cells) is more important than the searched micro-structure (operations); and (iv) the depth-gap is a real phenomenon, evidenced by the change in rankings between 8 and 20 cell architectures. To conclude, we suggest best practices, that we hope will prove useful for the community and help mitigate current NAS pitfalls, e.g. difficulties in reproducibility and comparison of search methods. We provide the code used for our experiments at link-to-come.", "review": "Review:###In this submission, the authors conduct a series of experiments on five image classification datasets to compare several existing NAS methods. Based on these experimental results, they point out: 1) how a network is trained (i.e., training protocols/tricks such as DropPath, Cutout) plays an important role for the final accuracy; 2) within the search space, the existing NAS methods perform close to or slightly better than a random sampling baseline; 3) hyperparameters of NAS methods also have significant effect on the performance. With these interesting findings, I suggest rejecting this submission. The reasons are as follows: 1) For the first finding of training protocol, several existing papers and books already discussed it, such as Li & Talwalkar (2019) and the book chapter {Neural Architecture Search} by Thomas, Jan Hendrik and Frank. 2) For the second finding of the search space and the performance of a randomly sampled architecture, existing work from Facebook AI Research group has studied this. And the existing work gives more experiments and discussion than this submission (from my own perspective). https://arxiv.org/pdf/1904.01569.pdf 3) The conducted experiments in this submission also have certain risks to support the claims/conclusions of it. For example, only datasets of image classification are adopted. Another factor is the hyper-parameter tuning (actually, the authors also mention this in the last paragraph on Page 4). All the compared methods, either NAS methods or random sample baseline, should receive the same training procedure to get a fair experimental comparison. The above mentioned existing work makes the contributions of this submission less, and the experimental results may not be convincing enough. These lead to a reject. However, a great point is made by the authors in the last paragraph of Section 6: hyperparameter of NAS methods should be either stable enough or counted toward the cost."}
{"id": "iclr2020_83", "title": "Robustness and/or Redundancy Emerge in Overparametrized Deep Neural Networks | OpenReview", "abstract": "Abstract:###Deep neural networks (DNNs) perform well on a variety of tasks despite the fact that most used in practice are vastly overparametrized and even capable of perfectly fitting randomly labeled data. Recent evidence suggests that developing *compressible* representations is key for adjusting the complexity of overparametrized networks to the task at hand and avoiding overfitting (Arora et al., 2018; Zhou et al., 2018). In this paper, we provide new empirical evidence that supports this hypothesis, identifying two independent mechanisms that emerge when the network’s width is increased: robustness (having units that can be removed without affecting accuracy) and redundancy (having units with similar activity). In a series of experiments with AlexNet, ResNet and Inception networks in the CIFAR-10 and ImageNet datasets, and also using shallow networks with synthetic data, we show that DNNs consistently increase either their robustness, their redundancy, or both at greater widths for a comprehensive set of hyperparameters. These results suggest that networks in the deep learning regime adjust their effective capacity by developing either robustness or redundancy.", "review": "Review:###This paper proposes to evaluate two characteristics of DNNs, robustness and redundancy, in an attempt to link them with generalization and overparameterization. A central hypothesis in this paper is that increasing the number of parameters increases at least one of these two characteristics. As far as the empirical results show, this is true. Where the line gets blurred is in the causal relationships between generalization, overparameterization, robustness and redundancy (and other unstudied phenomenons). A strict interpretation of the results in this paper show that: when there is overparameterization, there is also robustness and/or redundancy. Beyond that, the experiments in the paper are not fully able to answer harder questions with certainty: - does overparameterization really increase *capacity*? Extra robustness and redundancy could both be explained by a structural failure of DNNs to take advantage of having more parameters (I don*t believe so, but it would be nice to [dis]prove!) - do robustness and redundancy arise from the same phenomenon that leads to generalization, or do they arise from the mechanisms that we usually associate with overfitting/fitting high-frequency-noise patterns? The paper*s results suggest the latter, since the same phenomenons are observed when fitting random lables. - is this phenomenon limited to classification? visual problems? The synthetic problems show weaker trends and happen to not have a lot of structure within them. Nonetheless, now that we know some consistent behaviours that emerge from overparameterization, we may be better able to continue digging. Overall I think the results of this paper are novel and valuable. They confirm a few hypotheses that were emerging in the literature concerning the odd behaviours of overparameterized DNNs. On the other hand, I think more could have been done to understand the _why_ of the observed phenomenons. One big aspect that is missing is time. Beyond Fig. C3 which only looks at behaviours after convergence, there is no data regarding the progression of robustness and redundancy during training. Such trends might be vital in order to understand the order in which phenomenons happen in DNNs during training, and superposing such trends with other measures that have been linked to generalization and overfitting may help clarifying the dynamics of DNN training. Some comments: - why weren*t the MLPs initialized with standard initializers (Glorot, etc.)? - The presentation of the paper is good. There are a few typos here and there but the paper was otherwise easy to read and understand. - I*m not sure I fully agree with the *He* initialization scheme being considered a *high-variance* scheme nor the *LeCun* initialization scheme being *medium-variance*. Both these schemes, as well as Glorot et al, were designed with the intention of keeping variance _constant_ in depth. A better test would have been to scale these initializers by some constant c around 1. - *Xavier/Glorot*, these are not two separate things but rather, Xavier is the first name of Xavier Glorot, author of the *Glorot et al.* paper. - I*m not sure where you found LeCun initialization to be sqrt(3) / fan_in, it should be 1/fan_in, see http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf - Even though the initializations, optimzers, models and data are fairly standard, it*s still expected to cite the original papers. You cite none of them."}
{"id": "iclr2020_84", "title": "Adversarial Training and Provable Defenses: Bridging the Gap | OpenReview", "abstract": "Abstract:###We propose a new method to train neural networks based on a novel combination of adversarial training and provable defenses. The key idea is to model training as a procedure which includes both, the verifier and the adversary. In every iteration, the verifier aims to certify the network using convex relaxation while the adversary tries to find inputs inside that convex relaxation which cause verification to fail. We experimentally show that this training method is promising and achieves the best of both worlds – it produces a model with state-of-the-art accuracy (74.8%) and certified robustness (55.9%) on the challenging CIFAR-10 dataset with a 2/255 L-infinity perturbation. This is a significant improvement over the currently known best results of 68.3% accuracy and 53.9% certified robustness, achieved using a 5 times larger network than our work.", "review": "Review:###Summary: This paper provides a promising new general training methodology to obtain provably robust neural networks (towards adversarial input perturbations). The paper provides promising experimental results on CIFAR-10 by obtaining state-of-the-art certified accuracy while also simultaneously improving clean accuracy. The paper is overall well-written and the algorithm is clearly described. Important questions to be answered: I find the need to clarify my understanding and request for more information in order to make a decision. --Methodology/motivation for the method: I am trying to understand abstractly what the proposed layerwise training is trying to optimize. To be concrete, let*s compare to the relaxation of Wong and Kolter (which this paper uses in the instantiation of layerwise adversarial training). What*s the exact difference? One way to view this is the following: The same training objective, but a different way to optimize. The new proposal to train involves freezing weights until one layer iteratively starting from the input layer. It is possible that this kind of training provides some inductive bias in finding better solutions. Is this an appropriate understanding? However, the paper’s experimental results unfortunately change the certification procedure. In other words, they haven’t evaluated the same training objective as that of Wong and Kolter. Hence, it’s not clear if the gains are from the better networks, or better certification method, or network being better suited for certification by the method used. The phrase “same relaxation” is not appropriately used. Their certification procedure uses a different (and tighter) relaxation. --Effect on latent adversarial examples: I am unable to understand why this training procedure would reduce the number of latent adversarial examples. The definition of latent adversarial examples seems to suggest that it’s the gap between the actual set of activations corresponding to the input perturbations and the convex hull. However, the proposed layerwise adversarial training procedure involves replacing the actual set S_i with the convex hull C_i when freezing things till below i-1. I do not follow how the proposed method tries to make C_i = S_i. Implicitly the optimization objective does try to make C_i small because the bounds being optimized are tighter when C_i is small. But this is true even for normal certified training, and not sure what changes in the new training procedure. Specific experimental results that would help: --Certified accuracy on using the same LP based certification procedure used in Wong and Kolter with the new layerwise trained networks --The paper’s own certification procedure (a combination of previous methods) on the network from Wong and Kolter or a note on why that doesn’t apply (if it doesn’t) --The paper currently provides only one data point to suggest this training method is superior. Would be good to try SVHN or MNIST. MNIST is perhaps “essentially” solved for small eps. But would be good to see if the training method offers gains at larger eps. In general, would be good to see more consistent gains. --The paper reports results on first 1000 examples of CIFAR10 test set. From my personal experience, there is a lot of variability in the robustness of test examples when evaluated on 1000 random test instances. Especially since the paper doesn*t take a random subset, it might be good to make sure the gains are consistent on some other subset. The Wong et al. baseline is evaluated on the entire test set for example, and hence might not be a fair comparison? What*s the Wong et al. accuracy on just the first 1000 test exampes Overall, I am leaning towards accept but need some conceptual and empirical clarification from the authors (detailed above)."}
{"id": "iclr2020_85", "title": "Understanding Why Neural Networks Generalize Well Through GSNR of Parameters | OpenReview", "abstract": "Abstract:###As deep neural networks (DNNs) achieve tremendous success across many application domains, researchers tried to explore in many aspects on why they generalize well. In this paper, we provide a novel perspective on these issues using the gradient signal to noise ratio (GSNR) of parameters during training process of DNNs. The GSNR of a parameter is simply defined as the ratio between its gradient*s squared mean and variance, over the data distribution. Based on several approximations, we establish a quantitative relationship between model parameters* GSNR and the generalization gap. This relationship indicates that larger GSNR during training process leads to better generalization performance. Futher, we show that, different from that of shallow models (e.g. logistic regression, support vector machines), the gradient descent optimization dynamics of DNNs naturally produces large GSNR during training, which is probably the key to DNNs’ remarkable generalization ability.", "review": "Review:###The paper defines the quantity of *gradient SNR* (GSNR), shows that larger GSNR leads to better generalization, and shows that SGD training of deep networks has large GSNR. It tells a great story on why SGD-trained DNNs have good generalization. This topic is highly relevant to this conference. However, I struggle to rate this paper, since I feel swamped with math. It is hard work to read this paper, and I can honestly say that I could semi-confidently follow until about Eq. (8). To even get there, I had to scroll back and forth to remember the definitions of the various symbols. The math may be very well correct, but it is infeasible to verify (or follow) it fully. It does not make it easier that one cannot really search a PDF for greek symbols with indices etc. Someone who reads theoretical papers all day long might do better here. This is the reason I rate the paper Weak Reject. Some feedback points: Section 2.1: Eq. (1): It seems the common definition of SNR is the ratio of mean standard deviation. Your SNR is its square. This should be explained. I think it would help the reader a lot to give some intuitive meaning to the GSNR value. Can you, in Section 2.1, explain with examples what typical (or extreme) values would be? Assumption 2.3.1: This is dropped on the reader without any motivation. It is also confusing: *we will make our derivation under the non-overfitting limit approximation* conflicts with *In the early training stage,...* So is this whole derivation only true in the early stages? Assumption 2.3.1 seems to address a thought I had when reading this: At the end of the training, I would expect mu_q(theta) to be zero (the definition of convergence). At the start, it is arbitrary as it entirely depends on the initial values. So this paper must look at some part between the two extremes to make sense. Is it? Is this assumption related? What is the difference between sigma and \rho? Seems one is on the data distribution and one on a sampled set. But then why is mu the same in both cases (Eq. (1) vs. Eq. (5))? All plots: The plot labels are far too small to be readable."}
{"id": "iclr2020_86", "title": "Stein Self-Repulsive Dynamics: Benefits from Past Samples | OpenReview", "abstract": "Abstract:###We propose a new Stein self-repulsive dynamics for obtaining diversified samples from intractable un-normalized distributions. Our idea is to introduce Stein variational gradient as a repulsive force to push the samples of Langevin dynamics away from the past trajectories. This simple idea allows us to significantly decrease the auto-correlation in Langevin dynamics and hence increase the effective sample size. Importantly, as we establish in our theoretical analysis, the asymptotic stationary distribution remains correct even with the addition of the repulsive force, thanks to the special properties of the Stein variational gradient. We perform extensive empirical studies of our new algorithm, showing that our method yields much higher sample efficiency and better uncertainty estimation than vanilla Langevin dynamics.", "review": "Review:###This paper proposed another variant of Langevin dynamics, called “Stein self-repulsive dynamics,” which simultaneously decreases the auto-correlation of Langevin dynamics and eliminates the need for running parallel chains in SVGD. They combined Langevin dynamics with Stein variational gradient descent and theoretically justified that the proposed method successfully converges to the stationary distribution with only a single chain, unlike SVGD. The proposed method decreases the auto-correlation of Langevin dynamics, so the proposed method increases the sample efficiency. The paper is well-written. The idea of the proposed method is natural, which is incorporating the functionality of SVGD to reduce the auto-correlation of Langevin dynamics. The idea is intuitive and justified by their theoretical analysis. The authors also well- placed their work in the literature, as described in Section 3. The intuitive explanation of the proposed method is given in Section 3. I have one technical question as follows. If the authors reply appropriately, I will raise the score to accept. In Theorem 4.3, the result holds for any k and M. The authors claim that if we take a limit of M -> ? with fixed k, the practical dynamics converges to the discrete-time mean-field limit, in Section 4. However, to state the result of Theorem 4.3, k should be bigger than M c_eta from the dentition of \tilde{\rho}_k^M, as shown under the equation (4). How do we take a limit of M -> ? ? Does k also go ?? Minor comments: - The definition of g should depend on only \theta_k^I and hat{delta}_k^M, not \theta_k^k. - The equation (1) should hold for any \theta’, not \theta. - The equation (1) should contain \rho, not p."}
{"id": "iclr2020_87", "title": "Learning-Augmented Data Stream Algorithms | OpenReview", "abstract": "Abstract:###The data stream model is a fundamental model for processing massive data sets with limited memory and fast processing time. Recently Hsu et al. (2019) incorporated machine learning techniques into the data stream model in order to learn relevant patterns in the input data. Such techniques were encapsulated by training an oracle to predict item frequencies in the streaming model. In this paper we explore the full power of such an oracle, showing that it can be applied to a wide array of problems in data streams, sometimes resulting in the first optimal bounds for such problems. Namely, we apply the oracle to counting distinct elements on the difference of streams, estimating frequency moments, estimating cascaded aggregates, and estimating moments of geometric data streams. For the distinct elements problem, we obtain the first memory-optimal algorithms. For estimating the -th frequency moment for we obtain the first algorithms with optimal update time. For estimating the -the frequency moment for we obtain a quadratic saving in memory. We empirically validate our results, demonstrating also our improvements in practice.", "review": "Review:###The paper talks about calculating various statistics over data streams. This is a very popular topic and is very relevant in big data analysis. A lot of work has been done in this general area and on the problems that are discussed in the paper. The new idea in the paper is better streaming algorithms under the assumption that there is a “heavy hitters” oracle that returns data items that have a lot of representation in the stream. The authors give provably better algorithms for the distinct elements problem, F_p moment problem (p > 2), and some more problems. These are important problems in streaming data analysis. They improve the space bounds and interestingly in some cases the bounds are better than what is possible without the oracle assumption. This also shows the power of such an oracle. There are experimental results to demonstrate the efficiency of the algorithms. At a high level the work seems good and interesting for a large audience interested in streaming data analysis. I have not gone over the proofs in detail (much of which is in the appendix). - Even though oracle results are interesting, to make it practical it may make sense to talk about a more realistic, weaker oracle where some of the queries may be incorrect. - It may even make sense to minimise the number of oracle calls which can be thought of as a resource and discuss the relationship between number of oracle calls and other resources such as space."}
{"id": "iclr2020_88", "title": "Mincut Pooling in Graph Neural Networks | OpenReview", "abstract": "Abstract:###The advance of node pooling operations in Graph Neural Networks (GNNs) has lagged behind the feverish design of new message-passing techniques, and pooling remains an important and challenging endeavor for the design of deep architectures. In this paper, we propose a pooling operation for GNNs that leverages a differentiable unsupervised loss based on the minCut optimization objective. For each node, our method learns a soft cluster assignment vector that depends on the node features, the target inference task (e.g., a graph classification loss), and, thanks to the minCut objective, also on the connectivity structure of the graph. Graph pooling is obtained by applying the matrix of assignment vectors to the adjacency matrix and the node features. We validate the effectiveness of the proposed pooling method on a variety of supervised and unsupervised tasks.", "review": "Review:###This paper proposes a solution to the important problem of pooling in graph neural networks. The method relies on minimizing a surrogate function inside the standard SGD loop and in conjunction with the optimization of the model parameters - such loss function aiming at optimizing the minCut on the graph. By that it aims to effective achieve a soft clustering of nodes that are both well connected and that have similar embeddings. This in an elegant choice, somewhat resembling the DiffPool method since it*s also end-to-end trainable. However it adds the local graph connectivity information due to the minCut loss (and related orthogonality penalty to achieve non trivial solutions on the relaxed minCut continuous problem). Such local graph connectivity is indeed important information to consider when carrying out pooling. Results show good performance improvement on different tasks of graph clustering, node and whole graph classification. The paper is well written and clear to read. The math is solid and the concept is well substantiated by results. I found no mention about code release and I would solicit the authors to release the code to reproduce the experiments."}
{"id": "iclr2020_89", "title": "Entropy Penalty: Towards Generalization Beyond the IID Assumption | OpenReview", "abstract": "Abstract:###It has been shown that instead of learning actual object features, deep networks tend to exploit non-robust (spurious) discriminative features that are shared between training and test sets. Therefore, while they achieve state of the art performance on such test sets, they achieve poor generalization on out of distribution (OOD) samples where the IID (independent, identical distribution) assumption breaks and the distribution of non-robust features shifts. Through theoretical and empirical analysis, we show that this happens because maximum likelihood training (without appropriate regularization) leads the model to depend on all the correlations (including spurious ones) present between inputs and targets in the dataset. We then show evidence that the information bottleneck (IB) principle can address this problem. To do so, we propose a regularization approach based on IB called Entropy Penalty, that reduces the model*s dependence on spurious features-- features corresponding to such spurious correlations. This allows deep networks trained with Entropy Penalty to generalize well even under distribution shift of spurious features. As a controlled test-bed for evaluating our claim, we train deep networks with Entropy Penalty on a colored MNIST (C-MNIST) dataset and show that it is able to generalize well on vanilla MNIST, MNIST-M and SVHN datasets in addition to an OOD version of C-MNIST itself. The baseline regularization methods we compare against fail to generalize on this test-bed.", "review": "Review:###In this work, the author(s) presented a regularization scheme that intends to suppress the identification of spurious features when learning deep representations. Their construction was inspired by the information bottleneck framework. By making Gaussian assumptions on the form of label conditioned feature distribution, the entropy penalty can be efficiently computed in the form of L2 loss, which is easy to implement. My major concerns for this submission are its clarity, novelty, and theoretical depth. The arguments provided are not very convincing and reported experimental results are based on toy-scale datasets. I recommend rejection for this submission, with more detailed comments attached below. Strength + The author(s) are trying to resolve the issue of learning spurious discriminant features for predictive models, which is a trendy topic with a potential impact on the field. + There are some interesting discussions in the related work section. Weakness - The presentation needs to be much improved. In its current form, the lack of clarity leads to serious confusion. Examples include but not limited to the following: - *violates the IID assumption which is the foundation of existing generalization theory*. Not sure what this IID assumption means, please briefly/intuitively describe these classical generalization theories. - *all the correlations btw inputs and targets.* - *throws away maximum possible information about the input distribution* - The author(s) have made a strong statement, quote *it is the second term that regularizes the model representations to become invariant to non-robust features* - Eqn (1) and Eqn (3) is equivalent, what*s the point??? There is no novelty here. - Prop 1. Modeling the conditional entropy H(f_{\theta}(X)|Y) nonparametrically is not any easier than modeling the marginal entropy H(f_{\theta}(X)). The assumption of a parametric form of f_{\theta}(X) given Y is very strong and needs to be justified (at least experimentally). Although the author(s) are honest about this limitation in the discussion. - The concept of distribution shift is not formally introduced in the manuscript. - Eqn (7) implicitly makes a strong prior assumption that the feature distribution condition on the label is isotropic Gaussian. This reminds me of Linear Discriminant Analysis (LDA), which followed from a similar heuristic, and might partly explain the empirical success of this practice (the model is forced to be LDA like, which combats the overfitting via appealing to simpler models). However, I have not found any discussion related to this, which evidence that the author(s) might lack a proper understanding of classical treatments. - Theoretical analyses of synthetic examples do not lend strong support to this paper. Questions # What is the fundamental difference btw the proposed work differs and domain adaption? Minors: % Conditional entropy H(f_{\theta}(X)|X) is zero. % I do not see the point of referencing adversarial robustness literature."}
{"id": "iclr2020_90", "title": "Autoencoder-based Initialization for Recurrent Neural Networks with a Linear Memory | OpenReview", "abstract": "Abstract:###Orthogonal recurrent neural networks address the vanishing gradient problem by parameterizing the recurrent connections using an orthogonal matrix. This class of models is particularly effective to solve tasks that require the memorization of long sequences. We propose an alternative solution based on explicit memorization using linear autoencoders for sequences. We show how a recently proposed recurrent architecture, the Linear Memory Network, composed of a nonlinear feedforward layer and a separate linear recurrence, can be used to solve hard memorization tasks. We propose an initialization schema that sets the weights of a recurrent architecture to approximate a linear autoencoder of the input sequences, which can be found with a closed-form solution. The initialization schema can be easily adapted to any recurrent architecture. We argue that this approach is superior to a random orthogonal initialization due to the autoencoder, which allows the memorization of long sequences even before training. The empirical analysis show that our approach achieves competitive results against alternative orthogonal models, and the LSTM, on sequential MNIST, permuted MNIST and TIMIT.", "review": "Review:###This paper proposes an initialization scheme for the recently introduced linear memory network (LMN) (Bacciu et al., 2019) and the authors claim that this initialization scheme can help improving the model performance of long-term sequential learning problems. My concerns lie with the novelty of the proposed model and the insufficiency of the experiments. First, the LMN seems to be a simpler version of LSTM and it has no significant advantages compared with other recurrent structures introduced in the past several years. Second, the autoencoder-based init scheme (Pasa&Sperduti, 2014) is not new while the only technical contribution of this paper is a minor change of this scheme so that it works for the LMN. In my opinion, combining these two (LMN and init scheme) can hardly be considered as a solid novelty contribution. For the experiment part, the first two tasks are a bit toyish in 2019 and I have not seen any significant improvement gained from the proposed model. Even for the TIMIT dataset, the results are a bit far from state-of-the-art which makes the paper*s claim less convincing. Overall I think the novelty contribution is marginal and I suggest the authors to test their models on larger-scale real problems. The writing is clear and easy to follow."}
{"id": "iclr2020_91", "title": "Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP | OpenReview", "abstract": "Abstract:###The lottery ticket hypothesis proposes that over-parameterization of deep neural networks (DNNs) aids training by increasing the probability of a “lucky” sub-network initialization being present rather than by helping the optimization process (Frankle& Carbin, 2019). Intriguingly, this phenomenon suggests that initialization strategies for DNNs can be improved substantially, but the lottery ticket hypothesis has only previously been tested in the context of supervised learning for natural image tasks. Here, we evaluate whether “winning ticket” initializations exist in two different domains: natural language processing (NLP) and reinforcement learning (RL).For NLP, we examined both recurrent LSTM models and large-scale Transformer models (Vaswani et al., 2017). For RL, we analyzed a number of discrete-action space tasks, including both classic control and pixel control. Consistent with workin supervised image classification, we confirm that winning ticket initializations generally outperform parameter-matched random initializations, even at extreme pruning rates for both NLP and RL. Notably, we are able to find winning ticket initializations for Transformers which enable models one-third the size to achieve nearly equivalent performance. Together, these results suggest that the lottery ticket hypothesis is not restricted to supervised learning of natural images, but rather represents a broader phenomenon in DNNs.", "review": "Review:###The paper used the lottery ticket hypothesis to study the over-parameterization of deep neural networks (DNNs). The main idea is that overparametrization increases the probability of a “lucky” sub-network initialization being present rather than by helping the optimization process. The paper conducted experiments to evaluate whether “winning ticket” initializations exist in two different domains: natural language processing (NLP) and reinforcement learning (RL). The authors confirm that winning ticket initializations generally outperform parameter-matched random initializations, even at extreme pruning rates for both NLP and RL. The results suggest that the lottery ticket hypothesis is not restricted to supervised learning The similarity between supervised learning and RL and NLP problem is obvious from a function approximation and optimization point of view. The paper is empirical in nature, and do not offer any additional insight. The experiments are not very conclusive."}
{"id": "iclr2020_92", "title": "Emergent Systematic Generalization In a Situated Agent | OpenReview", "abstract": "Abstract:###The question of whether deep neural networks are good at generalising beyond their immediate training experience is of critical importance for learning-based approaches to AI. Here, we demonstrate strong emergent systematic generalisation in a neural network agent and isolate the factors that support this ability. In environments ranging from a grid-world to a rich interactive 3D Unity room, we show that an agent can correctly exploit the compositional nature of a symbolic language to interpret never-seen-before instructions. We observe this capacity not only when instructions refer to object properties (colors and shapes) but also verb-like motor skills (lifting and putting) and abstract modifying operations (negation). We identify three factors that can contribute to this facility for systematic generalisation: (a) the number of object/word experiences in the training set; (b) the invariances afforded by a first-person, egocentric perspective; and (c) the variety of visual input experienced by an agent that perceives the world actively over time. Thus, while neural nets trained in idealised or reduced situations may fail to exhibit a compositional or systematic understanding of their experience, this competence can readily emerge when, like human learners, they have access to many examples of richly varying, multi-modal observations as they learn.", "review": "Review:###=============================== Update after revisions ===================================================== In my initial review, I had raised some issues with the interpretation of the results and suggested some control experiments to tighten the conclusions. The authors chose to weaken their initial claims by rephrasing their conclusions instead. I understand that there may not have been enough time to run many of the experiments I suggested, but I still think they are worth considering for the future. I*m mostly satisfied with the rephrasing of the conclusions in the revised paper, so as promised, I*m happy to increase my score and recommend acceptance. I spotted several typos in the revised paper, however: section 4.1: *we choose to consider negation ...*, p. 5: *for for ...*, a citation on p. 5 is not compiled correctly. There may be more. For the final version please make sure to go through the paper thoroughly a couple of times and fix all the typos. ======================================================================================================== The authors present a systematic study of generalization in agents embedded in a simulated 3d environment. I think there are some interesting results in this paper that might be useful for people to know about. I appreciate the thoroughness of the experiments, in particular. I have, however, some issues with the interpretation of several of the main results. I would be happy to increase my score if we can resolve some of these issues. Here are my main concerns: 1) In the experiments in section 3, only a limited test set is used. How is the train/test split decided in these experiments? Table 6 suggests that you have a much larger repository of objects. Why not use all possible objects in the test set? It is a bit premature to declare your results as systematic generalization if you can’t show that it actually works for a much larger set of test objects (ideally for all possible objects). 2) Section 4.1: in these experiments, the training set size is increased, but the test set size is kept constant (and small), so the train/test size ratio also increases. So, an alternative explanation of the results in this section is that the model behaves largely according to visual similarity and as the training set size is increased, it becomes easier to find a training set object that is visually similar to any test set object. I think the authors should run an experiment where both training and test set sizes increase by the same amount so that the train/test set size ratio stays constant. If the model can’t achieve systematic generalization in that case, it would be wrong to conclude, as the authors do now, that increasing the training set size itself improves systematic generalization. The correct conclusion would rather be that increasing the train/test size ratio improves generalization, which is a weaker conclusion. Please note that the results in this section are quite similar to those in Lake & Baroni (2018) and in Bahdanau et al. (2018) (see their Figure 3). Bahdanau et al. (2018), for example, also show that increasing train/test set size ratio (their “rhs/lhs” ratio) improves generalization in generic neural networks. It is interesting to note, however, that neither Lake & Baroni (2018) nor Bahdanau et al. (2018) interpret these results positively (i.e., these results don’t show systematicity), whereas the current paper seems to put a more positive spin on essentially the same result. I think these earlier results should be explicitly discussed here and the authors should justify why they are interpreting the results differently (if they are). It should also be noted that in the real world the train/test size ratio for humans is presumably very small, perhaps zero (given the compositional abilities of humans). 3) Section 4.3: I don’t think the results in this section are sufficient to establish the egocentric frame per se as the key factor. One possibility is that perhaps the frame doesn’t have to be centered on the agent, but as long as it has some systematic relationship to the agent’s location (for example, the center of the visibility window could be some distance away from the agent, and the agent itself may or may not be inside this window), that’s good enough to get generalization improvements. An even weaker possibility is that simply a moving frame is enough for improved generalization. In this case, the reference frame doesn’t even need to have a systematic relationship to the agent’s location. For example, the frame could be relative to a fictitious agent that randomly explores the environment. I think the authors should run some experiments to rule out these possibilities if they want to claim that the egocentric frame itself is responsible for generalization improvements. 4) Section 4.4: In the experiments in this section, I think there are two relevant factors that need to be better disentangled: 1) the number and variability of image frames experienced by the two models; 2) the active perception aspect (the fact that the agent interacts with the environment and affects its own perceptual experience in one case). The authors claim the second factor as the key aspect enabling better generalization, but 1) is equally likely (this would be more in line with a standard data augmentation type result). A good control experiment here would be to not just use the first frame but a larger number of more variable frames for training the non-situated agent (for example, one can use image frames that would be seen by a camera that more or less randomly moves in front of the objects perhaps with the constraint that both objects are always at least partially visible). If the classification model generalizes as well as the situated agent in this control condition, you cannot claim active perception as the key factor. 5) As a more general point, it’s a bit frustrating to have to judge systematic generalization by only looking at the results of some limited set of experiments. How do I interpret the results if the agent achieves only 84% accuracy in some experiment (as opposed to 100%)? It would be much better if the authors could somehow more rigorously prove systematicity. Here, I don’t necessarily mean “prove” in a mathematical sense, but just analyzing the learned representations a bit more rigorously and being able to say something along the lines of: here’s exactly how the trained model represents “lift”; because of reason X, Y, Z, this representation is completely disentangled from all object representations in the dataset (and ideally from all possible object representations, because that’s really what true systematicity entails, although I highly doubt that any generic model of the type studied in this paper will be able to achieve this, regardless of the amount and type of input it receives). More minor issues: 6) In Table 5, “table lamp” appears both in training and test sets. Is this a typo? 7) Some results are presented in the appendix without any mention in the main text (Appendix D. 2). I think this is not a good practice in general. In the main text, please make sure to mention, however briefly, every result that appears in the appendix (something along the lines of *This result could not be explained by confound X or Y (Appendix Z)* would suffice). 8) Font size in Figure 2 is tiny (axis labels are impossible to read), please make it bigger. You don’t need that many ticks on the axes."}
{"id": "iclr2020_93", "title": "Multi-Agent Hierarchical Reinforcement Learning for Humanoid Navigation | OpenReview", "abstract": "Abstract:###Multi-agent reinforcement learning is a particularly challenging problem. Current methods have made progress on cooperative and competitive environments with particle-based agents. Little progress has been made on solutions that could op- erate in the real world with interaction, dynamics, and humanoid robots. In this work, we make a significant step in multi-agent models on simulated humanoid robot navigation by combining Multi-Agent Reinforcement Learning (MARL) with Hierarchical Reinforcement Learning (HRL). We build on top of founda- tional prior work in learning low-level physical controllers for locomotion and add a layer to learn decentralized policies for multi-agent goal-directed collision avoidance systems. A video of our results on a multi-agent pursuit environment can be seen here", "review": "Review:###This paper proposes a multi-agent hierarchical reinforcement learning algorithm so that multiple humanoid robots can navigate in multi-agent settings (e.g. avoid collisions, collaboration, chase and escape) in a physically simulated environment. The key difference of this paper with the prior work on MARL is that it used an accurate physics simulation of humanoid robots. This is the main reason of using the hierarchical RL. In general, I like this paper. It is an important step towards multi-agent learning in complex physical environments. The results look appealing, too. However, I voted for *Weak Reject* for two reasons. First, the technical contribution is lean. Neither the multi-agent learning or the hierarchical learning of the algorithm is novel. The combination of these two methods seems straightforward. Once a low-level walking controller is trained, the high-level multi-agent navigation control is not much different from simple environments, e.g. point mass control, used in the previous works. I do not understand the *deep integration of MARL and HRL* that is claimed in the Introduction. I also do not agree with another claim that *We consider the simulation and training environment to be another novel contribution... few simulator support more than one agent, at most 2*. In most of the simulators that I am familiar with, such as Mujoco, Bullet, DART, it is straightforward to add multiple simulated robots. Second, the writing can be greatly improved. Almost half of the technical details are buried in *8. Supplementary material*. Since it is not fair to use *Supplementary material* as a way to extend the page limit, I will make my judgement of the paper solely based on the contents up to Section 7. In the main text (up to Section 7), there is no mentioning of how the low-level controllers are learned, and how to combine PPO in a MARL partial parameter sharing setting. I think that these are important details and may also be the contributions of this paper. Most of these should be moved to the main text. Here are some more suggestions on writing: 1) Certain paragraphs in the main text can be significantly shortened, such as the reward shaping in Section 5.2. 2) It would be great if the paper can clearly define the experiments: *waypoint*, *oncoming*, *mall*, and *bottleneck*. 3) The paper needs a thorough proof-reading. There are many grammar mistakes, typos, missing citations. For example, promiss->promise week signal->weak signal missing citation [?] in page 3 reuse the same symbol v_{com} for agent*s velocity and desired speed in eq(3)"}
{"id": "iclr2020_94", "title": "A GOODNESS OF FIT MEASURE FOR GENERATIVE NETWORKS | OpenReview", "abstract": "Abstract:###We define a goodness of fit measure for generative networks which captures how well the network can generate the training data, which is necessary to learn the true data distribution. We demonstrate how our measure can be leveraged to understand mode collapse in generative adversarial networks and provide practitioners with a novel way to perform model comparison and early stopping without having to access another trained model as with Frechet Inception Distance or Inception Score. This measure shows that several successful, popular generative models, such as DCGAN and WGAN, fall very short of learning the data distribution. We identify this issue in generative models and empirically show that overparameterization via subsampling data and using a mixture of models improves performance in terms of goodness of fit.", "review": "Review:###The paper proposes a new goodness of fit measure for generative models, and uses it to get insight into GAN*s. While this is an important topic and a novel approach, I do not think the paper delivers on what it promises. I think this paper should be rejected. First, while it claims to be a general method for generative models it, it is limited to only GANs and even for GANs it is limited. Second, most of the observations are nice but trivial, e.g. larger latent space leads to larger image. Detailed remarks: - The main point that the training set points x must have p(x)>0 under the model is naturally satisfied for almost all models except GANs such as VAEs, autoregressive model and flow models with standard implementations as the support is the whole space. This is in contrast to the claim in the paper that *its applications can be extended to other generative networks such as Variational Autoencoders.*. - Even for GANs as this measure only looks at the support and not the distribution it is not clear if this measure does more then evaluate mode collapse. While this is an important task, it falls short of the promises the authors claim. - The authors claim that *We demonstrate that our measure being minimized is a necessary and sufficient condition to detect mode collapse.* but only show that it is necessary. - Proposition 1 is a trivial statement. - The authors claim that * mode collapse happens if P(x) > 0 but minz ||G(z) ? x|| > 0*. This is a main point by the authors, but it ignores the probability and only looks at the support. It has been shown that mode collapse happens even in 2d distributions, e.g. veegan paper, where it is easy to get the support to be the whole distribution. - The results in sec. 5 are quiet obvious, with a larger latent space you can naturally get a larger support, same as with a mixture model. In general the method only looks at the support, ignoring the distribution over the support and is therefore very limited in evaluating generative models. minor details: - In eq. 3 the integration should be w.r.t dP(x) for it to be monte-carlo approximated as it is in eq. 4. - Not 100% I understand what the authors try to say here - *we pick the latent variable z and error ||G(z) ? x||2 that corresponds to the smallest error instead of picking the latent variable that Adam Kingma and Ba (2014) finds.*"}
{"id": "iclr2020_95", "title": "CAT: Compression-Aware Training for bandwidth reduction | OpenReview", "abstract": "Abstract:###Convolutional neural networks (CNNs) have become the dominant neural network architecture for solving visual processing tasks. One of the major obstacles hindering the ubiquitous use of CNNs for inference is their relatively high memory bandwidth requirements, which can be a main energy consumer and throughput bottleneck in hardware accelerators. Accordingly, an efficient feature map compression method can result in substantial performance gains. Inspired by quantization-aware training approaches, we propose a compression-aware training (CAT) method that involves training the model in a way that allows better compression of feature maps during inference. Our method trains the model to achieve low-entropy feature maps, which enables efficient compression at inference time using classical transform coding methods. CAT significantly improves the state-of-the-art results reported for quantization. For example, on ResNet-34 we achieve 73.1% accuracy (0.2% degradation from the baseline) with an average representation of only 1.79 bits per value. Reference implementation accompanies the paper.", "review": "Review:###The format of the paper does not meet the requirement of ICLR. Due to this, I will give a 3. I suggest the authors to change it as soon as possible. Besides that, the main idea of the paper is to regularize the training of a neural network to reduce the entropy of its activations. There are extensive experiments in the paper. The paper introduce two kinds of method to regularize the entropy. The first method is a soft version of the original entropy, and the second is the compressibility loss. After adding the regularization, the performance drop of the compressed network is reduced. The experiment performance is promising. I think the method is straightforward and reasonable with only a few questions: 1. Why do you quantize the weight? Seems it*s not necessary because the paper only address activation quantization. 2. What will happen if the weights are quantized to lower bits? For example, 4bit? 2. How about adding the regularization to weights?"}
{"id": "iclr2020_96", "title": "Robust training with ensemble consensus | OpenReview", "abstract": "Abstract:###Since deep neural networks are over-parametrized, they may memorize noisy examples. We address such memorizing issue under the existence of annotation noise. From the fact that deep neural networks cannot generalize neighborhoods of the features acquired via memorization, we find that noisy examples do not consistently incur small losses on the network in the presence of perturbation. Based on this, we propose a novel training method called Learning with Ensemble Consensus (LEC) whose goal is to prevent overfitting noisy examples by eliminating them identified via consensus of an ensemble of perturbed networks. One of the proposed LECs, LTEC outperforms the current state-of-the-art methods on MNIST, CIFAR-10, and CIFAR-100 despite its efficient memory usage.", "review": "Review:###Summary: This paper proposes a general method for eliminating noisy labels in supervised learning based on the combination of two ideas: outputs of noisy examples are less robust under noise, and noisy labels are less likely to have a low loss. The authors then propose 3 concrete instantiations of the idea, and do a thorough empirical study (including ablations) across multiple architectures, datasets, noise types, and comparing to multiple related methods. The results show pretty convincingly that one of the new methods (LTEC) that uses past networks outputs to build an ensemble performs really well. Caveats: 1) I’m an emergency reviewer and had less time to do an in-depth review. 2) While my research is sufficiently close to review the paper, I’m not an expert on label noise specifically, so I cannot comment much on novelty and related work questions. Comments: * The readability of the paper could be dramatically improved by reporting results visually (eg bar-plots) and moving all tables into the appendix. * The authors state that peak performance is valid because it *could* have been found using a validation set -- then why not just do that, and report this early-stopping performance everywhere, instead of always two numbers (peak and final)? * Please make the perturbation properties in section 3.2 more precise: do you mean “there exists a threshold and perturbation such that for some (x, y)”? Or “For any threshold and any perturbation then for all (x, y) it holds...”? Or something in-between? * For the “competing methods” paragraph, please cite the relevant papers in the main text, not only in the appendix. * “over 4 runs” did you randomize the data noise in each run (and in the same way for each method?), or only the network initialisation? * Table 5 is cool, but it raises the question: is 1.1epsilon the best, or would performance keep going up? * Looking at the actual implementation of LTEC (Algorithm 4), I cannot resist the thought that M=infinity could work even better (at no extra cost): just maintain a monotonically shrinking set of samples? Minor comments: - Define the threshold symbol in section 3.2 - Define M in Algorithm 1 - Define (initial) mathcal{P}_0 in Algorithm 4 - Fig 2: can you clarify what green is, does it correspond to “self-training”? - “label precision … does not decrease” -- well, not a lot, but it does decrease! - Fig 1, Fig 2: set max y to 100 - Table 4: Include M=1 (which is self-training) for comparison"}
{"id": "iclr2020_97", "title": "Ergodic Inference: Accelerate Convergence by Optimisation | OpenReview", "abstract": "Abstract:###Statistical inference methods are fundamentally important in machine learning. Most state-of-the-art inference algorithms are variants of Markov chain Monte Carlo (MCMC) or variational inference (VI). However, both methods struggle with limitations in practice: MCMC methods can be computationally demanding; VI methods may have large bias. In this work, we aim to improve upon MCMC and VI by a novel hybrid method based on the idea of reducing simulation bias of finite-length MCMC chains using gradient-based optimisation. The proposed method can generate low-biased samples by increasing the length of MCMC simulation and optimising the MCMC hyper-parameters, which offers attractive balance between approximation bias and computational efficiency. We show that our method produces promising results on popular benchmarks when compared to recent hybrid methods of MCMC and VI.", "review": "Review:###This paper proposes a new combination of Markov chain Monte Carlo (MCMC) and variational inference (VI) for improving approximate inference. The main contribution is the optimization objective that allows improving the quality of samples obtained from the combination of VI and MCMC. Specifically, the authors minimize the *approximate* version of the Kullback-Leibler (KL) divergence between the distribution of MCMC + VI and the true distribution. The authors validate the effectiveness of their formulation through experiments on 6 synthetic benchmarks and generative modeling of MNIST (experiments on Bayesian neural networks are also provided in the appendix). Overall, I think the paper provides a solid contribution towards combining MCMC and VI by proposing a way to optimize the MCMC part. The experiments validate the method by showing consistent improvement over existing methods. However, I believe the justification behind the proposed formulation, i.e., Equation (4) and (5), needs to be improved before being published at the conference. First, for Equation (4), the explanation behind *replacing* H(P_{T}) with ELBO w.r.t. P_{0} is confusing. Specifically, it is reasoned that ELBO w.r.t. P_{t} only increase after MCMC steps. This statement is misleading since the replacement was done for H(P_{T}), not the ELBO w.r.t. P_{T}. I also think the Equation (5) is not properly justified. it is stated that the constraint is needed for preventing P_{T} to be closer to P_{0}. However, nothing is stated about the reason on why P_{T} gets closer to pi when Equation (5) is satisfied. Note that even if the expected log-likelihood of the distribution is high, it does not necessarily mean that the distribution is more similar. Minor comments: - I was unable to understand why the algorithm is named *ergodic* inference. Both HVI and the proposed EI rely on the ergodic property of Markov chain for improving the variational distribution. I hope the authors could better illustrate on this point. I also think the term *ergodic approximation* in page 3. is hard to understand. - I (weakly) suggest changing y-axis of Figure 5. to log-scale for better readability. It almost seems that the brown plot does not converge in Fig 5-(a). - The paper could have been strengthened by performing experiments on more challenging datasets, e.g., CIFAR-10 or CIFAR-100."}
{"id": "iclr2020_98", "title": "Plug and Play Language Model: A simple baseline for controlled language generation | OpenReview", "abstract": "Abstract:###Large transformer-based generative models (e.g. GPT-2; 1.5B parameters) trained on a huge corpus (e.g. 40GB of text) have shown unparalleled language generation ability. While these models are powerful, fine-grained control of attributes of the generated language (e.g. gradually switching topic or sentiment) is difficult without modifying the model architecture to allow extra attribute inputs, or fine-tuning with attribute-specific data. Both would entirely change the original generative function, which, if done poorly, cannot be undone; not to mention the cost of retraining. We instead propose the Plug and Play Language Model for controlled language generation that consists of plugging in simple bag-of-words or one-layer classifiers as attribute controllers, and making updates in the activation space, without changing any model parameters. Such a control scheme provides vast flexibility and allows full recovery of the original generative function.The results demonstrate fine-grained control over a range of topics and sentiment styles, as well as the ability to detoxify generated texts. Our experiments, including human evaluation studies, show that text generated via this control scheme is aligned with desired attributes, while retaining fluency.", "review": "Review:###The paper introduces an approach to the conditional generation of text, relying on pre-trained decoders, without fine-tuning and, in certain cases, without any training at all. The approach they introduce is following the framework known in NLP as noisy-channel modeling, previously standard in machine translation (in its SMT days), but undergoing certain revival recently (https://arxiv.org/abs/1611.02554, https://arxiv.org/abs/1910.00553,https://arxiv.org/abs/1908.05731,https://arxiv.org/abs/1907.06616). The authors do not mention this connection (they should!). Very differently from these previous approaches attempting to integrate the two factors in the search process (e.g., using reranking), the authors instead rely on gradient descent in the latent space of their model (Transformer), similarly to plug-n-play generative networks in image generation. I find this approach interesting and like the paper overall. However, I do not see why authors do not compare to more direct ways of integrating the conditional component into the model. This would have been tricky in the NMT papers mentioned above, as the entire source sentences need to be reconstructred, however, it should be quite straightforward in this work, with conditioning on single categorical control variables (or maybe a couple in the additional experiments in sect 4.4). Especially, given that the authors already make the predictions of the control variable independently per prediction (e.g., see eq. (5) in section 4.2) / greedily per prefix (bottom lines, page 7). I would actually expect the proposed approach to work better (or at least differently) but it would be interesting to see it confirmed. E.g., for the experiments defining topics as sets of seed words (section 4.2), when integrating factors directly (unlike the proposed approach, Table 3), there will be no increase in the probability of generating relevant words before the first seed word is generated. Another limitation is the lack of comparison to standard controlled generation work, i.e. those requiring training a model or/and fine-tuning pretrained decoder. I understand that the proposed approach falls in a different category and, of course, do not expect it to beat a fine-tuned model, but I*d like to get some feel for how much one loses by using this simpler method. There has been a lot of work on controlled generation in recent ~3 years, and they can also be combined with intializing and fine-tuning off-the-shelf pretrained decoders. There is an interesting relation to the NIPS 2019 paper: https://arxiv.org/abs/1907.04944 They also rely on gradient descent to steer a pretrained language model. Their goal is to assess the degree of *steerability* rather than building a controlled-generation model. Given that style-controlled but otherwise unconditional generation may not have that many applications, I am curious how far you can push this approach. E.g., can you make it scale to more complicated data-to-text generation tasks (https://www.aclweb.org/anthology/D17-1239/)? Or, will the only application in this context be integrating new conditioning variables into pretrained conditional LMs? Minor: I am confused with the notation in *Post-norm Geometric Mean Fusion* section. It says that softmax is applied to the product of probabilities. Maybe to a linear interpolation of log-probs? Or maybe that*s not softmax at all? Something seems off here."}
{"id": "iclr2020_99", "title": "Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors | OpenReview", "abstract": "Abstract:###In this paper, we propose a new loss function for performing principal component analysis (PCA) using linear autoencoders (LAEs). Optimizing the standard L2 loss results in a decoder matrix that spans the principal subspace of the sample covariance of the data, but fails to identify the exact eigenvectors. This downside originates from an invariance that cancels out in the global map. Here, we prove that our loss function eliminates this issue, i.e. the decoder converges to the exact ordered unnormalized eigenvectors of the sample covariance matrix. For this new loss, we establish that all local minima are global optima and also show that computing the new loss (and also its gradients) has the same order of complexity as the classical loss. We report numerical results on both synthetic simulations, and a real-data PCA experiment on MNIST (i.e., a 60,000 x784 matrix), demonstrating our approach to be practically applicable and rectify previous LAEs* downsides.", "review": "Review:###This paper proposes a new loss function to compute the exact ordered eigenvectors of a dataset. The loss is motivated from the idea of computing the eigenvectors sequentially. However doing so would be computationally expensive, and the authors show that the loss function they propose (sum of sequential losses) has the same order (constant less than 7) of computational complexity as using the squared loss. A proof of the correctness of the algorithm is given, along with experiments to verify its performance. The loss function proposed in the paper is useful, and the decomposition in Lemma 1 shows that it is not computationally expensive. While the writing of the proofs of the theorems is clear, I find it hard to understand the flow of the paper. It would help if the authors could summarize the argument of the proof at the start of Sec 4. Along a similar vein, it would also help if the authors could describe in words the significance / claim of every theorem. The repetition in stating the theorems can be avoided. The main result (Theorem 2) is stated twice."}
{"id": "iclr2020_100", "title": "Neural Maximum Common Subgraph Detection with Guided Subgraph Extraction | OpenReview", "abstract": "Abstract:###Maximum Common Subgraph (MCS) is defined as the largest subgraph that is commonly present in both graphs of a graph pair. Exact MCS detection is NP-hard, and its state-of-the-art exact solver based on heuristic search is slow in practice without any time complexity guarantee. Given the huge importance of this task yet the lack of fast solver, we propose an efficient MCS detection algorithm, NeuralMCS, consisting of a novel neural network model that learns the node-node correspondence from the ground-truth MCS result, and a subgraph extraction procedure that uses the neural network output as guidance for final MCS prediction. The whole model guarantees polynomial time complexity with respect to the number of the nodes of the larger of the two input graphs. Experiments on four real graph datasets show that the proposed model is 48.1x faster than the exact solver and more accurate than all the existing competitive approximate approaches to MCS detection.", "review": " The authors proposed a novel method to find the Maximum Common Subgraph (MCS) of two graphs. I am familiar with the quadratic assignment problem (QAP) based graph matching and I am not very familiar with the MCS problem. The authors adopt Graph Matching Networks (GMN) for feature embedding, and then similarity matrix X can be generated by computing the similarities between the embeddings. The similarity matrix is then normalized using a similar way as the sinkhorn procedure in [1-3]. The Assignment matrix then can be given from X. Then a novel procedure, named Guided subgraph Extraction (GSE, which is considered as the main contribution of this paper), is used to get an MCS from assignment matrix. Here the authors may consider a simple baseline, which is to use QAP to give the assignment matrix, and then run GSE to obtain the MCS. Overall the paper is well written, and the experiment is good and solid. Some suggestions: The GCN based GMN might not be the best choice for graph embedding. The authors may consider stronger Graph Neural Networks such as DGCNN (used in [3]) or Message Passing Neural Network (used in [4] and [5]) as the graph embedding module in the future work. [1] Deep Learning of Graph Matching, CVPR18 [2] Learning Combinatorial Embedding Networks for Deep Graph Matching. ICCV19 [3] Deep Closest Point: Learning Representations for Point Cloud Registration. ICCV19 [4] Deep Graphical Feature Learning for the Feature Matching Problem, ICCV19 [5] Neural Message Passing for Quantum Chemistry, ICML17"}
{"id": "iclr2020_101", "title": "ADA+: A GENERIC FRAMEWORK WITH MORE ADAPTIVE EXPLICIT ADJUSTMENT FOR LEARNING RATE | OpenReview", "abstract": "Abstract:###Although adaptive algorithms have achieved significant success in training deep neural networks with faster training speed, they tend to have poor generalization performance compared to SGD with Momentum(SGDM). One of the state-of-the-art algorithms, PADAM, is proposed to close the generalization gap of adaptive methods while lacking an internal explanation. This work pro- poses a general framework, in which we use an explicit function ?(·) as an adjustment to the actual step size, and present a more adaptive specific form AdaPlus(Ada+). Based on this framework, we analyze various behaviors brought by different types of ?(·), such as a constant function in SGDM, a linear function in Adam, a concave function in Padam and a concave function with offset term in AdaPlus. Empirically, we conduct experiments on classic benchmarks both in CNN and RNN architectures and achieve better performance(even than SGDM).", "review": "Review:###This work proposes a modification to the ADAM optimizer by introducing an adjustment function, which consists of a square root function and an extra parameter delta. Although the modification is very simple and easy to implement, the theoretical analysis is weak and the empirical performances of the proposed method are similar to the previous adaptive methods. Here are my main concerns of the current paper: 1. The presentation is a little bit confusing in the motivation section. According to equation (2.1), the norm of is defined as a vector. However, it seems that the authors treat as a scalar when presenting the intuition of the proposed method in Figure 1. 2. I do not understand why Figure 1 says the proposed method is better than Padam. It seems to me that if Padam choose a specific p, it can recover the proposed function, or even better than the proposed method. Therefore, I do not think the intuition of the proposed method is correct. 3. For the convergence analysis, the authors only consider the convex setting, which I think is meaningless. Because the proposed method is designed for training neural networks, such convergence guarantee in convex setting is not enough. There exist some work such as [1] have proved the convergence guarantee of the adaptive algorithms including Padam in the nonconvex setting. 4. There is one missing baseline Yogi [2] in the current paper. 5. For experimental results, the performance of the proposed method is very similar to the Padam. Due to the close formulation of the proposed method and Padam, it seems to me that the proposed method is just a more careful hyperparameter tuning process. 6. In NMT experiments, why there is no Padam baseline? In addition, the authors should also report the test perplexity to validate the generalization performance of the proposed optimizer. 7. To fully evaluate the performance of the proposed method, the authors should at least conduct an experiment on the task of language model. Minor comments: There is an unknown citation in section 5.1. Reference: [1]. Zhou, Dongruo, et al. *On the convergence of adaptive gradient methods for nonconvex optimization.* arXiv preprint arXiv:1808.05671 (2018). [2]. Zaheer, Manzil, et al. *Adaptive methods for nonconvex optimization.* Advances in Neural Information Processing Systems. 2018."}
{"id": "iclr2020_102", "title": "UWGAN: UNDERWATER GAN FOR REAL-WORLD UNDERWATER COLOR RESTORATION AND DEHAZING | OpenReview", "abstract": "Abstract:###In real-world underwater environment, exploration of seabed resources, underwater archaeology, and underwater fishing rely on a variety of sensors, vision sensor is the most important one due to its high information content, non-intrusive, and passive nature. However, wavelength-dependent light attenuation and back-scattering result in color distortion and haze effect, which degrade the visibility of images. To address this problem, firstly, we proposed an unsupervised generative adversarial network (GAN) for generating realistic underwater images (color distortion and haze effect simulation) from in-air image and depth map pairs. Secondly, U-Net, which is trained efficiently using synthetic underwater dataset, is adopted for color restoration and de-hazing. Our model directly reconstructs underwater clear images using end-to-end autoencoder networks, while maintaining scene content structural similarity. The results obtained by our method were compared with existing methods qualitatively and quantitatively. Experimental results on open real-world underwater datasets demonstrate that the presented method performs well on different actual underwater scenes, and the processing speed can reach up to 125FPS on images running on one NVIDIA 1060 GPU.", "review": "Review:###In this article, the authors propose a generative adversarial network named UWGAN to generate realistic underwater images from the pairs of in-air images and depth images. Then, a U-Net was leveraged to enhance the results. However, the text suffers from too many language problems. The authors should consult professional proofreading services. As a courtesy towards referees, the quality of writing needs meticulous attention before a scientific paper should be submitted. Other comments: 1. The literature is limited. I found some novel works being done in the field that must be addressed and listed in the background and experiments. 2. The underwater imaging model presented in this paper derives from the Jaffe-McGlamery model, which is a common sense in this field. The authors use a generator to produce underwater images that only implements the common model by a neural network. Moreover, the statement of section 2.2 is not clear. Please rewrite this section. 3. The authors used U-Net without any improvement to enhance the results generated from UWGAN, which is the integration of existing models. 4. The authors claimed that their model is better than others, while there is no evidence to indicates that. For example, 1) in (page 5, line 4 from bottom), “It can be seen that our proposed method has achieved a higher score.”, can we observe this from the Table 1 and 2? 2) “The method we proposed has the fastest processing speed compared to other methods. Moreover, the method proposed in this paper has the fewest parameters compared to other deep-learning-based methods.”, it is suggested that a study about the parameters and FLOPs of the involved methods should be given. 5. Please carefully check the references. For example, “Hummel R. Image enhancement by histogram transformation[J]. Unknown, 1975.” lacks the journal name. 6. High-resolution figures should be given in the manuscript."}
{"id": "iclr2020_103", "title": "Meta-Learning Acquisition Functions for Transfer Learning in Bayesian Optimization | OpenReview", "abstract": "Abstract:###Transferring knowledge across tasks to improve data-efficiency is one of the open key challenges in the area of global optimization algorithms. Readily available algorithms are typically designed to be universal optimizers and, thus, often suboptimal for specific tasks. We propose a novel transfer learning method to obtain customized optimizers within the well-established framework of Bayesian optimization, allowing our algorithm to utilize the proven generalization capabilities of Gaussian processes. Using reinforcement learning to meta-train an acquisition function (AF) on a set of related tasks, the proposed method learns to extract implicit structural information and to exploit it for improved data-efficiency. We present experiments on a sim-to-real transfer task as well as on several simulated functions and two hyperparameter search problems. The results show that our algorithm (1) automatically identifies structural properties of objective functions from available source tasks or simulations, (2) performs favourably in settings with both scarse and abundant source data, and (3) falls back to the performance level of general AFs if no structure is present.", "review": "Review:###The authors present MetaBO, which uses reinforcement learning to meta-learn the acquisition function (AF) for Bayesian Optimization (BO) instead of using a standard constant AF. The authors shows that MetaBO enables transferring knowledge between tasks and increasing sample efficiency on new tasks. The paper is mostly clearly written and I am not aware of existing work on meta-learning the AF for BO. However, the approach is related to Chen et al, which is cited in the text but not used as a baseline. It is also not shown clearly enough how the performance of MetaBO depends on the number of training tasks and distance between training and test tasks. I therefore consider the paper as borderline. Major comments ============= 1. The presented approach is very similar to Chen et al, which is discussed in the related work section but not used as a baseline. Although Chen et al assumed that f(x) is differentiable, their approach can be easily generalized to non-differentiable functions by using RL as Chen et al discussed in the last paragraph of section 2.1. Chen et al does not depend on a GP and is therefore more scalable. The source code is publicly available (https://github.com/deepmind/learning-to-learn) and you can also adapt your implementation by removing the GP part. 2. Global Optimization Benchmark Functions: How does the performance of MetaBO depend on the number of training samples (number of training tasks times the budget T)? 3. Figure 3: How does MetaBO generalizes to functions that are translated and scaled at the same time? This can be visualized as a heatmap with the scaling and translation on the x and y axis, and using the color to show the number of steps to reach a certain reward. How does the generalization performance depend on the noise level, where the noise can be sampled from standard normal distribution? Why does EI perform better if the function is translated more? 4. Simulation-to-Real task: How does the generalization performance of MetaBO depend on the distance between training and source tasks (x-axis: distance; y-axis: steps to reach a certain reward)? You sampled test tasks 10%-200% around the true parameters. Test tasks can therefore have identical or similar parameters than training tasks. 5. Simulation-to-Real task: How does the performance depend on the number of training tasks (x-axis: # training tasks; y-axis: steps to reach a certain performance)? Minor comments ============= 6. Section 1, 2nd paragraph: The performance of BO also depends on the GP kernel and kernel hyper-parameters, not only the AF. Please mention this. Similarly, ‘no need to calibrate any hyperparameter’ in section 4 ignores GP hyper-parameters. Please clarify. 7. Section 2, 4th paragraph: A Neural Process (https://arxiv.org/abs/1807.01622) is another scalable alternative to a GP. Please cite. 8. Section 3, 2nd paragraph: Please cite standard AFs such as EI, PI, UCP. 9. Section 4, last paragraph before ‘Training procedure’. The state s_t is undefined at this point. This section misses a clear description of the state, reward, and transition function of the MDB. Does the state s_t take previous function evaluations into account (e.g. via a RNN state), or only mu and sigma at the current step t? Does the state include the time step as described in the text and in the section about the value network in the appendix but not in table 1. 10. Section 4, ‘the state corresponds to the entire functions’. It only depends on the first two moments (and the time step t?). 11. Section 4: replace ‘not to be available’ by ‘unavailable’. 12. Section 4: reference or describe ‘Sobol grid’. 13. Section 4: The approach to maximize the AF on grid points does not scale to high-dimensional search spaces. Please also clarify how global and local grid points were chosen. In particular, ‘local maximization’ is unclear. Also, ‘cheap approximation’ of the global maximum of f(x) is infeasible if the search space is high-dimensional. 14. Please move figure 3 above figure 4."}
{"id": "iclr2020_104", "title": "BOOSTING ENCODER-DECODER CNN FOR INVERSE PROBLEMS | OpenReview", "abstract": "Abstract:###Encoder-decoder convolutional neural networks (CNN) have been extensively used for various inverse problems. However, their prediction error for unseen test data is difficult to estimate a priori, since the neural networks are trained using only selected data and their architectures are largely considered blackboxes. This poses a fundamental challenge in improving the performance of neural networks. Recently, it was shown that Stein’s unbiased risk estimator (SURE) can be used as an unbiased estimator of the prediction error for denoising problems. However, the computation of the divergence term in SURE is difficult to implement in a neural network framework, and the condition to avoid trivial identity mapping is not well defined. In this paper, inspired by the finding that an encoder-decoder CNN can be expressed as a piecewise linear representation, we provide a close form expression of the unbiased estimator for the prediction error. The close form representation leads to a novel boosting scheme to prevent a neural network from converging to an identity mapping so that it can enhance the performance. Experimental results show that the proposed algorithm provides consistent improvement in various inverse problems.", "review": "Review:###1. Summary The authors address the problem of efficiently employing the SURE estimator as a network training regularizer. They show that for CNN autoencoders this can be efficiently computed. Their other contribution is a bagging/boosting technique which is proved to avoid trivial solutions. The proposed architecture, motivated by the theoretical statements, is shown to outperform classic and 2019 state of the art image reconstruction algorithms in MRI and EDX. 2. Decision and arguments Unfortunately this paper is outside my expertise so I can’t evaluate the novelty of the theoretical accomplishments. However taking that as a given, they well-motive the proposed architecture and achieve impressive experimental results. The experiments are well described. 3. Questions a) Why do Table 1 and Figure 3 provide different PSNR and SSIM values? b) Is there any way to measure accuracy to ground-truth with the EDX data? Or are the results just qualitative? c) With respect to Figure 2, and in general for autoencoders, the input and output have the same dimension. So how do you reconcile this with undersampled MRI and EDX data? I understand you train on fully sampled data—then how do you input undersampled data? Are the unknown samples set to zero?"}
{"id": "iclr2020_105", "title": "Natural- to formal-language generation using Tensor Product Representations | OpenReview", "abstract": "Abstract:###Generating formal-language represented by relational tuples, such as Lisp programs or mathematical expressions, from a natural-language input is an extremely challenging task because it requires to explicitly capture discrete symbolic structural information from the input to generate the output. Most state-of-the-art neural sequence models do not explicitly capture such structure information, and thus do not perform well on these tasks. In this paper, we propose a new encoder-decoder model based on Tensor Product Representations (TPRs) for Natural- to Formal-language generation, called TP-N2F. The encoder of TP-N2F employs TPR *binding* to encode natural-language symbolic structure in vector space and the decoder uses TPR *unbinding* to generate a sequence of relational tuples, each consisting of a relation (or operation) and a number of arguments, in symbolic space. TP-N2F considerably outperforms LSTM-based Seq2Seq models, creating a new state of the art results on two benchmarks: the MathQA dataset for math problem solving, and the AlgoList dataset for program synthesis. Ablation studies show that improvements are mainly attributed to the use of TPRs in both the encoder and decoder to explicitly capture relational structure information for symbolic reasoning.", "review": "Review:###The authors propose a binding-unbinding mechanism for translating natural language to formal language. The idea is good and novel. As far as I know, this is indeed the first work for handling this task using binding-unbinding mechanism. The experimental results also look promising in compared with the exsiting models. However, the designed specific neural network does not support the claimed binding-unbinding theory very well. Moerover, there seem to be some errors about the correctness of the theory (See the first point below). Firstly, in the last paragraph of Section 2, the authors claim that the role matrix would be invertible such that there exists a matrix such that the fillers would be recovered. However, is defined as a non-square matrix in the previous paragraph. How can a non-square matrix be invertible? Secondly, the design of the specific neural network cannot describe the theory behind proposed binding-unbinding mechanism properly. The authors try to interpret the design of the neural networks using the concepts in the proposed binding-unbinding theorybut are not convincible. In Section 2, the basics of binding-unbinding are introduced and many mathematical properties are required to make the binding-unbinding work. However, all the parameters/variables in the neural networks are freely designated and are not correlated to each other, thus they cannot work together to meet the requirements in the binding-unbinding mechanism. According to my understanding, at least there should be some direct connections between the parameters in the encoder and decoder. For example, is there any restriction on the parameters in encoder and decoder respectively to reflect the property as in Section 2. Lastly, in the encoder part, the role and filler are learned in an unsupervised without any evidence. The input for the decoder is an *assumed* TPR, thus the only evidence from the objective function are cut-off by the assumed TPR. Given that there are no other connections between encoder and decoder, the design of the encoder cannot learn role and filler properly. Other suggestions: The natural language to formal language problem is named semantic parsing in natural language processing field. In semantic parsing problem, langugae to programatic language is a typical task. I would recommend include some references in semantic parsing."}
{"id": "iclr2020_106", "title": "Weakly Supervised Disentanglement with Guarantees | OpenReview", "abstract": "Abstract:###Learning disentangled representations that correspond to factors of variation in real-world data is critical to interpretable and human-controllable machine learning. Recently, concerns about the viability of learning disentangled representations in a purely unsupervised manner has spurred a shift toward the incorporation of weak supervision. However, there is currently no formalism that identifies when and how weak supervision will guarantee disentanglement. To address this issue, we provide a theoretical framework—including a calculus of disentanglement— to assist in analyzing the disentanglement guarantees (or lack thereof) conferred by weak supervision when coupled with learning algorithms based on distribution matching. We empirically verify the guarantees and limitations of several weak supervision methods (restricted labeling, match-pairing, and rank-pairing), demonstrating the predictive power and usefulness of our theoretical framework.", "review": "Review:###The paper tries to bring some theoretical foundation to the weakly supervised disentanglement. Overall it is a good contribution, but the message of the paper is not clear. The authors propose two notions: consistency and restrictiveness, which they don*t imply each other. However, the experiment on real data shows that they are highly correlated. Up until the experiment section, the paper is well written (although a bit verbose). It seems that it is great but unfinished work. The paper is well written, but in my opinion, there is too much verbosity on page 4-5 on rather trivial definitions consistency and restrictiveness and a big box in the calculus of disentanglement that steals space from the main results. In my opinion, those sections can be reduced so that other theorem can be covered. In my opinion, the theorem nine should be part of the main text. I understand the definition of *Sufficiency for Disentanglement * but it is not clear why it is important. Sure, it is a strong definition that says for any (and not a subset) the algorithm ( ) should be able to match the distribution of the observation but why is it a big deal according to the next paragraph? I don*t see any proof that Eq.11 should be between [0,1]. Yes, g is optimal, and if you enter suboptimal values to it, one expects the nominator to be less than dominator. However, g a function that is optimal in expectation, which does not mean for every s value it nominator is less than the denominator. In fact, some of the values in fig 3 are small negatives. Fig 3 is not explained well: you are showing normalized consistency and restiveness. First of all, what is the dataset you tried this on? Second, why some values are negative?! These are supposed to be between [0,1]. Third, what is the take-home-message of this figure? the first two matrices from left show that the factors are consistent b/c they are almost diagonal. The third one from left shows that the algorithm you used is not restrictive? Then are you suggesting this as a metric of evaluation? I am not sure I understand the first figure from the right. Overall, the authors perform a significant amount of experiments, but they did a poor job in summarizing the results. Finally, the authors claim *...We believe this correlation between consistency and restrictiveness to have been a general source of confusion in the disentanglement literature, causing many to either observe or believe that restricted labeling or share pairing on (which only guarantees consistency) is sufficient for disentangling Si ...* Each of those methods should be analyzed separately to ensure that their algorithms do not induce restiveness. I just don*t see the natural connection between your figure 4 and this conclusion that you made. Minor: Where is the proof for Theorem 1? In the Supp, it starts with Theorem 8, I guess you meant Lemma 8? You need to clean up the Supp so that one can find the proof easily. I suggest restructuring the Supp to less and finally proof of Thorem 1."}
{"id": "iclr2020_107", "title": "Optimising Neural Network Architectures for Provable Adversarial Robustness | OpenReview", "abstract": "Abstract:###Existing Lipschitz-based provable defences to adversarial examples only cover the L2 threat model. We introduce the first bound that makes use of Lipschitz continuity to provide a more general guarantee for threat models based on any p-norm. Additionally, a new strategy is proposed for designing network architectures that exhibit superior provable adversarial robustness over conventional convolutional neural networks. Experiments are conducted to validate our theoretical contributions, show that the assumptions made during the design of our novel architecture hold in practice, and quantify the empirical robustness of several Lipschitz-based adversarial defence methods.", "review": "Review:###1. Contributions: A) Extension of robustness bound based on margin and Lipschitz constant of the network to arbitrary l_p norms. Significance: Low. B) High probability bound on adversarial robustness based on A) and McDiarmid*s inequality. Significance: Low. C) Lipschitz constant bound for one-versus-all networks (OVAs). Significance: Low. D) Experimental evaluation. Significance: Low 2. Detailed comments: Originality: Contribution A) is a simple extension of previously known bounds for the l_2 norm which is rather technical. As such It has questionable novelty and limited potential impact. The same goes for B) which is a simple application of a well-known concentration bound that holds for many statistical estimators based on averaging. Also C) is a well-known alternative for multiclass classification (one-vs-all approach) and somehow defeats the purpose of representation reuse from intermediate layers of the network to perform classification. In OVAs the prediction for each class depends on entirely independent values of hidden layers. As such it is not surprising that. a slightly tighter estimate of robustness can be obtained, as one does not have to worry about interactions between features from each independent sub-network. Quality: The paper is in general well written, but lacks any depth or clear important contribution. The propositions are rather technical and/or simple extensions. The proposal of OVA networks and the bound on the adversarial risk are two parallel ideas that could be explored and hopefully find a significant contribution. For example, what could the authors say about local Lipschitz constant estimates? are they easy/difficult to compute? what new bounds could be derived in term of such constants? For OVA networks can there be some intermediate reuse of hidden layers and still get some improvement in certified robustness? perhaps only the final layers require a split to see the empirical improvement, in this way reducing the burden of training one network from scratch for each different class (ImageNet has 1000 classes so OVAs could be quite expensive to train). Or maybe for OVAs the complexity of each subnetwork can be substaintially less (e.g., less layers) to obtain the same accuracy/robustness?? It looks from the experiments that this is true but some theoretical insights about why it is the case can yield a strong paper. Clarity: The paper could see some improvements in notation and some erroneous claims: 1. Authors claim that the robustness can not be assessed when f is non-convex. This is in general, false. There is a constrained maximization procedure. constrained maximization of convex functions is a hard problem. Not to be confused with certain instances of constrained minimization of convex functions that can be solved efficiently. The same in section 5.1. The true statement is that for SVM the problem becomes optimization of a linear function with some simple constraint. 2. In section 3 it is not clear if the loss l depends on the label y or not. sometimes it appears as an argument, and sometimes it disappears. see for example equation (4). 3. In proposition 2 the assumption is that l is monotonically decreasing and its range is [0, B). Does this mean that B is negative?? this makes the claims in this section seem weird. There is some problem here. Also McDiarmids does not require the Doob Martingale concept, which is not even defined or referenced. Significance: I find the significance low as the work is incremental and has technical rather than theoretical statements, and lacks a clear contribution. I vote for rejecting this submission."}
{"id": "iclr2020_108", "title": "Newton Residual Learning | OpenReview", "abstract": "Abstract:###A plethora of computer vision tasks, such as optical flow and image alignment, can be formulated as non-linear optimization problems. Before the resurgence of deep learning, the dominant family for solving such optimization problems was numerical optimization, e.g, Gauss-Newton (GN). More recently, several attempts were made to formulate learnable GN steps as cascade regression architectures. In this paper, we investigate recent machine learning architectures, such as deep neural networks with residual connections, under the above perspective. To this end, we first demonstrate how residual blocks (when considered as discretization of ODEs) can be viewed as GN steps. Then, we go a step further and propose a new residual block, that is reminiscent of Newton*s method in numerical optimization and exhibits faster convergence. We thoroughly evaluate the proposed Newton-ResNet by conducting experiments on image and speech classification and image generation, using 4 datasets. All the experiments demonstrate that Newton-ResNet requires less parameters to achieve the same performance with the original ResNet.", "review": " The paper proposes a modification to ResNet architecture, motivated by Gauss-Newton optimization method. They change the residual block to include a weighted product of residual input with itself. I did not understand how exactly this product operation is defined, it is not clearly explained. In figure 2b there is a sign ***, not defined in text, which looks like Hadamard product of input convolved with weight and input itself. I do not see a link between Gauss-Newton and this operation, though, and will let the authors to correct me. The proposed residual block is compared to the original ResNet on a number of image classification datasets. The authors also compare their quadratic residual block to a residual block without non-linearities. I propose reject mainly because experimental validation is not aligned with paper claims. The authors claim that their block aims to *accelerate convergence of ResNet*, meaning to decrease the number of residual blocks in the network, and test this on CIFAR-10 and CIFAR-100 with 18 to 34 layer networks, claiming that Newton-ResNet is achieving the same performance with fewer blocks. However, state-of-the-art results on these simple datasets can be achieved with even 10-16 layers. On the difficult ImageNet dataset they show no improvement over ResNet, since the proposed network has more parameters than ResNet-50. In figure 1 the authors visualize loss surface of ResNet with their modification with original ResNet. It is not evident from the figure why ResNet takes more steps to reach the minimum, and how this it related to the number of blocks. Specific details of this experiment are not provided. I would also disagree that Gauss-Newton was dominant method for solving optimization problems before deep learning, this is arguable. The authors also claim that the proposed residual block eliminates the need of nonlinearity, which is false, because the weighted product is a nonlinear quadratic operation. Another remark, CIFAR and ImageNet datasets are so well known that their description could be removed. Experiments with generative models are a nice addition, but do not support the claims. PyTorch or tensorflow code defining the proposed modification to residual block would be very helpful."}
{"id": "iclr2020_109", "title": "Scale-Equivariant Steerable Networks | OpenReview", "abstract": "Abstract:###The effectiveness of Convolutional Neural Networks (CNNs) has been substantially attributed to their built-in property of translation equivariance. However, CNNs do not have embedded mechanisms to handle other types of transformations. In this work, we pay attention to scale changes, which regularly appear in various tasks due to the changing distances between the objects and the camera. First, we introduce the general theory for building scale-equivariant convolutional networks with steerable filters. We develop scale-convolution and generalize other common blocks to be scale-equivariant. We demonstrate the computational efficiency and numerical stability of the proposed method. We compare the proposed models to the previously developed methods for scale equivariance and local scale invariance. We demonstrate state-of-the-art results on MNIST-scale dataset. Finally, we demonstrate that the proposed scale-equivariant convolutions show remarkable gains on STL-10 when used as drop-in replacements for non-equivariant convolutional layers.", "review": "Review:###The paper describes a method for integrating scale equivariance into convolutional networks using steerable filters. After developing the theory using continuous scale and translation space, a discretized implementation using a fixed set of steerable basis elements is described. Experiments are performed measuring the error from true equivariance, varying number of layers, image scale and scales in scale interactions. The method is evaluated using MNIST-scale and STL-10, with convincing results on MNIST-scale and bit less convincing but still good results on STL-10. Overall, I think this is a nice paper with generally good explanations and experiments probing the behavior. I would have liked to see more probing into the effects of number and distance between scales. Table 1 and corresponding text say that a significant advantage of the approach is that it can handle arbitrary scale values, but there was no explicit exploration of the effects of using this beyond one set of scales per experiment/dataset. What scale values can be sampled, which work best, and why? Also, while the MNIST-scale experiment seems convincing, I think the STL-10 is a bit less (but still OK): Although the method outperforms other methods and appropriate baseline models, it*s a little disappointing that pooling over scales (which I would would convert the equivariance to invariance) is best, and inter-scale interactions increase error. (Perhaps this is not too surprising in retrospect, as images may have limited scale variation from camera position in this dataset, but significant within-class viewpoint variation.) Even so, I still find the method concise and of interest, with the basics evaluated, even if some of its unique advantages may have been better explored. Additional Questions: * Inter-scale interaction could be elaborated a bit more. End of sec 4 says, *use convHH for each scale sequentially and .. sum*. I believe this is sequencing over scales in the kernel; explaining a bit better how this is implemented, including the shape of w in this case, would be helpful. * Which scales were chosen for the fixed basis? How large in spatial extent are the kernels in the basis elements, at each scale? * In the implementation, what is the value of V (sampled 2d conv kernel size)?"}
{"id": "iclr2020_110", "title": "Barcodes as summary of objective functions* topology | OpenReview", "abstract": "Abstract:###We apply canonical forms of gradient complexes (barcodes) to explore neural networks loss surfaces. We present an algorithm for calculations of the objective function*s barcodes of minima. Our experiments confirm two principal observations: (1) the barcodes of minima are located in a small lower part of the range of values of objective function and (2) increase of the neural network*s depth brings down the minima*s barcodes. This has natural implications for the neural network learning and the ability to generalize.", "review": " The paper aims to study the topology of loss surfaces of neural networks using tools from algebraic topology. From what I understood, the idea is to effectively (1) take a grid over the parameters of a function (say a parameters of a neural net), (2) evaluate the function at those points, (3) compute sub-levelset persistent homology and (4) study the resulting barcode (for 0/1-dim features) (i.e., the mentioned *canonical form* invariants). Some experiments are presented on extremely simple toy data. Overall, the paper is very hard to read, as different concepts and terminology appear all over the place without a precise definition (see comments below). Given the problems in the writing of the paper, my assessment is that this idea boils down to computing persistent homology of the sub-levelset filtration of the loss surface sampled at fixed parameter realizations. I do not think that this will be feasible to do, even for small-scale real-world neural networks, simply due to the difficulty of finding a suitable grid, let alone the vast number of function evaluations involved. The paper is also unclear in many parts. A selection is listed below: (1) What do you mean by gradient flow? One can define a gradient flow in a linear space X and for a function F: X->R, e.g., as a smooth curve R->X, such that x*(t) = -\nabla F(x(t)); is that what is meant? (2) What do you mean by *TDA package*? There are many TDA packages these days (maybe the CRAN TDA package?) (3) *It was tested in dimensions up to 16 ...* What is meant by dimension here? The dimensionality of the parameter space? (4) The author*s talk about the *minima*s barcode* - I have no idea what is meant by that either; the barcode is the result of sub-levelset persistent homology of a function -> it*s not associated to a minima. (5) Is Theorem 2.3. not just a restatement of a theorem from Barannikov *94? At least the proof in the appendix seems to be . (6) Right before Theorem 2.3., what does the notation F_sC_* mean? This needs to be introduced somewhere. From my perspective, the whole story revolves around how to compute persistence barcodes from the sub-levelset filtration of the loss surface, obtained from function values taken on a grid over the parameters. The paper devotes quite some time to the introduction of these concepts, but not in a very clear or understandable manner. The experiments are effectively done on toy data, which is fine, but the paper stops at that point. I do not buy the argument that *it is possible to apply it [the method] to large-scale modern neural networks*. Without a clear strategy to extend this, or at least some preliminary *larger*-scale results, the paper does not meet the ICLR threshold. The more theoretical part is too convoluted and, from my perspective, just a restatement of earlier results."}
{"id": "iclr2020_111", "title": "HUBERT Untangles BERT to Improve Transfer across NLP Tasks | OpenReview", "abstract": "Abstract:###We introduce HUBERT which combines the structured-representational power of Tensor-Product Representations (TPRs) and BERT, a pre-trained bidirectional transformer language model. We validate the effectiveness of our model on the GLUE benchmark and HANS dataset. We also show that there is shared structure between different NLP datasets which HUBERT, but not BERT, is able to learn and leverage. Extensive transfer-learning experiments are conducted to confirm this proposition.", "review": " This paper proposes an alternative way of reusing pretrained BERT for downstream tasks rather than the traditional method of fine-tuning the embeddings equivalent to the CLS token. For each bert embedded token, the proposed method aims at disentangling semantic information of the word from its structural role. Authors provide two ways to provide this disentagling using LSTM or transformer blocks. with several design choices such as: * a regularization term to encourages the roles matrix to be orthogonal and hence each role carry independent information * design the roles and symbols matrices so that the number of symbols is greater than the number of roles In evaluation authors design several experiments to show that: * Does transferring disentangled role & symbol embeddings improve transfer learning * the effectiveness of the TPR layer on performance? * Transfer beyond Glue tasks? While those experiments provide empirical gains of the design choices, authors don*t show enough study to attribute those empirical gains to the presented design choices: One large claim in the paper is that empirical gains in the ability of transfer between similar tasks MNLI and GLUE is because of disentangling the semantics from the role representations. We don*t know if the TPR layer really manages to do that, this could have been easily verified using for example clustering word senses of the same word. The empirical gains in transfer learning can be simply attributed to: - More params it seems adding an LSTM over bert embeddings already does some improvement, I would have loved to see this more exploited but it wasn*t. This aligns with some recent findings that BERT is undertrained (Liu et al. 2019) https://arxiv.org/abs/1907.11692 - Variance in the results (authors report only results of one single run not mean and std of several runs). - More budget given to hyper-parameter search for the models proposed in the paper. Hyper param budget isn*t also reported in the paper. - other factors, not the ones associated with the claims in the paper: for example what authors claim is an ablation study was comparing several different models together. It would have been more interesting to see for example the effect of making the # symbols = # roles or removing the orthogonality loss from the roles matrix. Conclusion: The paper introduces large claims and empirical results that correlate with, however the provided experiments are not done with enough control to attribute gains to the design choices provided in the paper."}
{"id": "iclr2020_112", "title": "Learning from Imperfect Annotations: An End-to-End Approach | OpenReview", "abstract": "Abstract:###Many machine learning systems today are trained on large amounts of human-annotated data. Annotation tasks that require a high level of competency make data acquisition expensive, while the resulting labels are often subjective, inconsistent, and may contain a variety of human biases. To improve data quality, practitioners often need to collect multiple annotations per example and aggregate them before training models. Such a multi-stage approach results in redundant annotations and may often produce imperfect ``ground truth** labels that limit the potential of training supervised machine learning models. We propose a new end-to-end framework that enables us to: (i) merge the aggregation step with model training, thus allowing deep learning systems to learn to predict ground truth estimates directly from the available data, and (ii) model difficulties of examples and learn representations of the annotators that allow us to estimate and take into account their competencies. Our approach is general and has many applications, including training more accurate models on crowdsourced data, ensemble learning, as well as classifier accuracy estimation from unlabeled data. We conduct an extensive experimental evaluation of our method on 5 crowdsourcing datasets of varied difficulty and show accuracy gains of up to 25% over the current state-of-the-art approaches for aggregating annotations, as well as significant reductions in the required annotation redundancy.", "review": "Review:###The submission addresses a problem with collecting ground truth: human annotations are noisy, a common approach is to collect many annotations and apply majority voting to elicit a single label. With this approach, the annotators* expertise and the difficulty of single data instances is ignored. What the authors propose is a framework which allows one to combine a direct graphical model of how human annotations are produced with model training. The graphical model introduces latent variables for the difficulty of an instance and the competence of an annotator as well as the (unobserved) true label. This way one can potentially benefit from the meta-information about the annotators (e.g., their demographics) and improve upon the majority-voting baseline of aggregating available annotations. Maybe most importantly, the proposed framework allows one to get the true label with fewer annotations (significantly reduces redundancy). The proposed model is intuitive, the learning of the hidden variables is done with EM, the presentation is easy to follow. The experimental part is done on five annotation tasks (image classification, NLP, bio-NLP) and compares the proposed model (LIA) with six prior approaches (e.g., majority vote, MMCE, Snorkel). The evaluation metric is accuracy (i.e., guessing the true label as obtained from experts). Overall, the new model achieves higher or comparable accuracy to that of MMCE which in turn outperforms all other methods. W.r.t. redundancy reduction, on some tasks LIA achieves much better accuracy with fewer annotations. For example, on the word similarity task the accuracy with only two annotations is higher than that of the majority baseline with ten. The submission is well-written and I enjoyed reading it. I am not an expert in this area, but as far as I am concerned the contributions are sufficient to accept it. I have not found any technical or methodological flaws. However, I have the following questions and concerns: 1. The analysis part is very short and while the accuracy numbers are impressive, I wonder if a different experiment is needed to fully demonstrate the claimed benefits of the model. For example, I am not sure which part shows empirically that annotators* features are indeed used and useful. 2. Related to the above point, maybe one could create a synthetic dataset where the true labels and true noise are added as if coming from two additional annotators and show that the model can identify them as highly competent / incompetent? 3. Section 3.1 mentions a fine-tuning procedure in the end but the experimental part does not specify how much of a gain it delivered. How does the model perform without this fine-tuning? 4. I have not followed this topic much but it seems to me that there should be more related work on modelling annotators* competence and item difficulty for crowd-sourced annotations. Isn*t, for example, work by Bachrach et al. 2012 [1] relevant? 5. How stable are results along the redundancy dimension? For example, the word similarity task has only 30 word pairs with ten ratings per item. How much, if at all, is accuracy with redundancy@2 affected by using different samples of two? Minor typos: - *can be can be* in Sec. 1 on page 2. - *a models* in Sec. 3.2. on page 5. - *a sources* in Sec. 3.3 on page 6. - *LIA-E* (should be *LIA*?) on page 7. [1] *How To Grade a Test Without Knowing the Answers — A Bayesian Graphical Model for Adaptive Crowdsourcing and Aptitude Testing* ICML*12."}
{"id": "iclr2020_113", "title": "Graph-based motion planning networks | OpenReview", "abstract": "Abstract:###Differentiable planning network architecture has shown to be powerful in solving transfer planning tasks while possesses a simple end-to-end training feature. Many great planning architectures that have been proposed later in literature are inspired by this design principle in which a recursive network architecture is applied to emulate backup operations of a value iteration algorithm. However existing frame-works can only learn and plan effectively on domains with a lattice structure, i.e. regular graphs embedded in a certain Euclidean space. In this paper, we propose a general planning network, called Graph-based Motion Planning Networks (GrMPN), that will be able to i) learn and plan on general irregular graphs, hence ii) render existing planning network architectures special cases. The proposed GrMPN framework is invariant to task graph permutation, i.e. graph isormophism. As a result, GrMPN possesses the generalization strength and data-efficiency ability. We demonstrate the performance of the proposed GrMPN method against other baselines on three domains ranging from 2D mazes (regular graph), path planning on irregular graphs, and motion planning (an irregular graph of robot configurations).", "review": "Review:###The paper proposes using graph neural networks for learning to plan on general graphs. The proposed method is able to learn from optimal plans and generalize to unseen graphs. Empirically it outperforms several existing baseline approaches on both regular graphs (2D lattices) and general graphs. This paper should be rejected because (1) the contribution is incremental, similar ideas has been explored in previous papers; (2) the experiments are too toy to demonstrate the practical use of the proposed method. ============================================================================================== Main argument The idea of this paper is not novel enough. As its predecessors, VIN[1] contributed the idea of embedding the value iteration algorithm into the neural architecture; and GVIN[2] extended the same idea to the general graph domain. This paper instead tries to improve GVIN by replacing the value iteration update with the graph neural network message passing update. The only contribution is showing empirically that graph neural networks are somehow more suitable than the previous parametrizations of update operators. Moreover, the experiment part of this paper is not convincing. Both synthetic domains I and II looks toy and domain III looks very similar to domain II (please let me know if I am wrong). Domain III is more practical but it doesn*t look obvious to me why using learning is better than using traditional shortest path algorithm such as Dijkstra? From my understanding Dijkstra would be more efficient and effective. This experiment is not a good example of a potential real-world application of the proposed method. ============================================================================================== Writing, soundness and organization of the paper The writing of this paper is acceptable, but the organization can be improved. Using more than 4 pages to introduce the background knowledge is way too much. The writers should focus more on their own work. The authors keep emphasizing the idea of invariant to graph isomorphism throughout the paper. It is a property of the proposed architecture but correct me if I am wrong, it doesn*t occur to me that VIN and GVIN lack such property. It also confuses me when the author says in the last paragraph in the first section, *These models are known to be invariant to graph isomorphism, therefore they are able to have a generalization ability to graphs of different sizes and structures*. I don*t really understand the rationale behind this. Can the authors explain this in the response? [1] Tamar, Aviv, et al. *Value iteration networks.* Advances in Neural Information Processing Systems. 2016. [2] Niu, Sufeng, et al. *Generalized value iteration networks: Life beyond lattices.* Thirty-Second AAAI Conference on Artificial Intelligence. 2018."}
{"id": "iclr2020_114", "title": "Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations | OpenReview", "abstract": "Abstract:###Clustering high-dimensional data, such as images or biological measurements, is a long-standing problem and has been studied extensively. Recently, Deep Clustering gained popularity due to the non-linearity of neural networks, which allows for flexibility in fitting the specific peculiarities of complex data. Here we introduce the Mixture-of-Experts Similarity Variational Autoencoder (MoE-Sim-VAE), a novel generative clustering model. The model can learn multi-modal distributions of high-dimensional data and use these to generate realistic data with high efficacy and efficiency. MoE-Sim-VAE is based on a Variational Autoencoder (VAE), where the decoder consists of a Mixture-of-Experts (MoE) architecture. This specific architecture allows for various modes of the data to be automatically learned by means of the experts. Additionally, we encourage the latent representation of our model to follow a Gaussian mixture distribution and to accurately represent the similarities between the data points. We assess the performance of our model on synthetic data, the MNIST benchmark data set, and a challenging real-world task of defining cell subpopulations from mass cytometry (CyTOF) measurements on hundreds of different datasets. MoE-Sim-VAE exhibits superior clustering performance on all these tasks in comparison to the baselines and we show that the MoE architecture in the decoder reduces the computational cost of sampling specific data modes with high fidelity.", "review": "Review:###Summary: The paper proposes to expand the VAE architecture with a mixture-of-experts latent representation, with a mixture-component-specific decoder that can specialize in a specific cluster. Importantly, the method can take advantage of a similarity matrix to help with the clustering. Overall, I recommend a weak accept. The method seems reasonable, and the paper is well-written, but the results are only marginally better than other methods, and there are several weaknesses with the proposed architecture and experimental setup. Positives: * The idea of a more expressive variational distribution seems good, although it is not novel. * The ability to have multiple decoder networks seems reasonable. * The ability to incorporate domain knowledge (in the form of a similarity matrix S) is a plus. * The experiments are thorough, although the method is generally only slightly better than competing methods. Negatives: * It*s not clear if the similarity matrix S is already solving the clustering problem - in which case, why do we need the rest of the model? For example, in your experiments you often used UMAP to cluster data. How does using UMAP by itself work? (Along these lines, it was not clear if your GMM experiments clustered data in the original space, or in the UMAP*d space - please clarify this). A good ablation would be to somehow remove the S matrix, to see if the model can accurately cluster samples. * There is little variance in the generated samples. * There is not a one-to-one mapping of clusters to labels, so it is hard to use this method to generate a specific type of data (for example, it is hard to generate a specific digit). This is a big difference from, say, a conditional sampler as learned by a GAN. This also arises in Fig. 3, where it is clear that latent cluster assignments do not match human-interpretable cluster assignments. I suppose this is to be expected, but taken with the previous point (little variance in generated samples) I think it seriously weakens the paper*s claim that this is an *accurate an efficient data generation method.* * The method does not do well when the number of clusters is large. Regular GMMs seem to outperform it. * I felt that this paper made excessive use of the appendix. The paper is not self-contained enough, effectively violating the length restrictions. Please make an effort to move key results back in to the main body of the paper. Experiments to run: An ablation regarding the similarity matrix S. Clarification of whether GMM experiments are run in data-space, or UMAP*d space. MIXAE features prominently in your related works, but is not compared to in your experiments. It sounds like a natural comparison. Please run this experiment, or explain why it is not a comparable method."}
{"id": "iclr2020_115", "title": "Visual Representation Learning with 3D View-Constrastive Inverse Graphics Networks | OpenReview", "abstract": "Abstract:###Predictive coding theories suggest that the brain learns by predicting observations at various levels of abstraction. One of the most basic prediction tasks is view prediction: how would a given scene look from an alternative viewpoint? Humans excel at this task. Our ability to imagine and fill in missing visual information is tightly coupled with perception: we feel as if we see the world in 3 dimensions, while in fact, information from only the front surface of the world hits our (2D) retinas. This paper explores the connection between view-predictive representation learning and its role in the development of 3D visual recognition. We propose inverse graphics networks, which take as input 2.5D video streams captured by a moving camera, and map to stable 3D feature maps of the scene, by disentangling the scene content from the motion of the camera. The model can also project its 3D feature maps to novel viewpoints, to predict and match against target views. We propose contrastive prediction losses that can handle stochasticity of the visual input and can scale view-predictive learning to more photorealistic scenes than those considered in previous works. We show that the proposed model learns 3D visual representations useful for (1) semi-supervised learning of 3D object detectors, and (2) unsupervised learning of 3D moving object detectors, by estimating motion of the inferred 3D feature maps in videos of dynamic scenes. To the best of our knowledge, this is the first work that empirically shows view prediction to be a useful and scalable self-supervised task beneficial to 3D object detection.", "review": "Review:###This paper deals with turning a 2.5D video representation into a 3D representation of an environment or a scene. The authors introduce self-supervised methods to pretrain the 2d-3d projection with a contrastive loss, which is the neural backbone for multiple other tasks. They then assess their approach on numerous tasks such as 3D-object detection, 3D-moving object detection, and 3D motion estimation. The authors also evaluate the transferability of the features in a challenging sim2real setting. It is dense paper with multiple modules (2d-3d, ergomotion, memory, etc.) and concept. Still, the authors make it accessible by concise paragraphs, highlighting key equations (The enum + eq 1 and 2 are quite useful), and well-designed sketch (Figure1). I had some difficulties digging into the visual head component for each task as I was not familiar with this topic. However, the authors always explain their choices in a few lines and refer to the related papers for technical details in a meaningful way. I am pretty convinced with the experiments, especially Sim2Real, in Tab1, where the baselines are clears and make sense. I appreciated the limitation section, which is transparent and honest, and clearly states the strength and weaknesses (such as image downscaling) of the approach. Besides, the code and the data should be released, which is always a positive point. Remarks: - Latent map update: running average is a simple and efficient mechanism, it also makes sense as you are dealing with big 3D tensors. Yet, have you tried other update mechanisms? - A natural follow-up to this paper is Contrastive Predictive Losses (which had several successes in pure vision setting[1]). Did you already try this approach? - In visual CPC papers [1] (or since the early days of visual representation learning!), data transformation has been applied to improve model performance. Would it make sense to apply it to I_{n+1}, D_{n+1} ? - Although the authors assess their approach with RGB-D, the models were still trained on 2.5D video. It would have been useful also to assess a pretraining on pure RGB-D data However, I have two (somehow related) concerns. First of all, the machine learning novelties are rather small, contrastive losses are now widespread, and the models are closed to Tung et al. as mentioned by the authors. However, I believe the paper to be a substantial contribution in vision, as they show the feasibility of their approach on large scale scenarios and over a highly diverse set of tasks. Again, the authors also release the code, making the paper a valuable baseline for the following work. On my side, I am impressed by the sim2real env. My second concern is the following, it is a high quality vision paper, and I am curious why the authors chose ICLR over CVPR. Besides, the tasks are vision-oriented, and 2.5D vision is not common in the ML community. Having said that, the proposed approach is pretty generic, can be applied to RGB-D (more common in ML), and require few expert knowledge in vision (only the Egomotion module). As a result, I would advocate for clear accept if we assess vision-based contribution for ICLR; otherwise, I would only recommend weak accept the paper is solely based on ML contributions (the paper is still sound, well-written, with numerous experiments and with a semi-generic architecture)"}
{"id": "iclr2020_116", "title": "Quantum Expectation-Maximization for Gaussian Mixture Models | OpenReview", "abstract": "Abstract:###The Expectation-Maximization (EM) algorithm is a fundamental tool in unsupervised machine learning. It is often used as an efficient way to solve Maximum Likelihood (ML) and Maximum A Posteriori estimation problems, especially for models with latent variables. It is also the algorithm of choice to fit mixture models: generative models that represent unlabelled points originating from different processes, as samples from multivariate distributions. In this work we define and use a quantum version of EM to fit a Gaussian Mixture Model. Given quantum access to a dataset of vectors of dimension , our algorithm has convergence and precision guarantees similar to the classical algorithm, but the runtime is only polylogarithmic in the number of elements in the training set, and is polynomial in other parameters - as the dimension of the feature space, and the number of components in the mixture. We generalize further the algorithm by fitting any mixture model of base distributions in the exponential family. We discuss the performance of the algorithm on datasets that are expected to be classified successfully by those algorithms, arguing that on those cases we can give strong guarantees on the runtime.", "review": "Review:###The authors present and analyze a quantum computing algorithm for learning GMMs. I think this paper cannot be accepted because it violates formatting guidelines. Also, I think it is not appropriate for ICLR since it assumes knowledge of quantum computing that most people at this conference would not have, and I as a reviewer do not possess, and hence cannot evaluate this paper. For example, I do not know bra-ket notation. If the ACs disagree, I am happy to revise my review for this paper and try to be more thorough."}
{"id": "iclr2020_117", "title": "Hierarchical Graph-to-Graph Translation for Molecules | OpenReview", "abstract": "Abstract:###The problem of accelerating drug discovery relies heavily on automatic tools to optimize precursor molecules to afford them with better biochemical properties. Our work in this paper substantially extends prior state-of-the-art on graph-to-graph translation methods for molecular optimization. In particular, we realize coherent multi-resolution representations by interweaving the encoding of substructure components with the atom-level encoding of the original molecular graph. Moreover, our graph decoder is fully autoregressive, and interleaves each step of adding a new substructure with the process of resolving its attachment to the emerging molecule. We evaluate our model on multiple molecular optimization tasks and show that our model significantly outperforms previous state-of-the-art baselines.", "review": "Review:###This paper developed a hierarchical graph-to-graph translation model to generate molecular graphs using chemical substructures as building blocks. In contrast to previous work, the proposed model is fully autoregressive and learns coherent multi-resolution representations. The experimental results show that the proposed method outperforms previous models. A few comments: 1.The novelty - The method seems to be almost the same as the previous junction tree based formulation. The paper includes a straightforward hierarchical extension and provides limited novelty with respect to deep learning. - Can the method be used for other types of graph generation? 2. Some minor wording issues - For instance, in the abstract, * In particular, we realize coherent multi-resolution representations ..* What does this mean? 3. The main claim : * ... our graph decoder is fully autoregressive..* why is this a merit? 4. The paper provided results from multiple molecular optimization tasks. The results and analysis seem comprehensive. The model was shown to significantly outperform baseline methods in discovering molecules with desired properties. The model runs faster during decoding and can perform conditional translation."}
{"id": "iclr2020_118", "title": "Neural-Guided Symbolic Regression with Asymptotic Constraints | OpenReview", "abstract": "Abstract:###Symbolic regression is a type of discrete optimization problem that involves searching expressions that fit given data points. In many cases, other mathematical constraints about the unknown expression not only provide more information beyond just values at some inputs, but also effectively constrain the search space. We identify the asymptotic constraints of leading polynomial powers as the function approaches 0 and infinity as useful constraints and create a system to use them for symbolic regression. The first part of the system is a conditional expression generating neural network which preferentially generates expressions with the desired leading powers, producing novel expressions outside the training domain. The second part, which we call Neural-Guided Monte Carlo Tree Search, uses the network during a search to find an expression that conforms to a set of data points and desired leading powers. Lastly, we provide an extensive experimental validation on thousands of target expressions showing the efficacy of our system compared to exiting methods for finding unknown functions outside of the training set.", "review": "Review:###Summary: The authors of this paper propose a novel approach for symbolic regression. The simulation results demonstrate that the proposed approach can find a better function g producing similar results as desired function f by providing the additional information – the leading power of function f. Paper strength: 1. The proposed NG-MCTS is elegant to solve the problem of symbolic regression. 2. Experiment results illustrate the superiority of the proposed approach Paper weakness: 1. I can follow most of the mathematics in the paper. But the most confusing part for me is why you feed the random partial sequence for training. Besides, how you do inference to generate a sequence of the production rules. 2. What is the final objective function? If I do not misunderstand, it could be the cross-entropy loss between the output of GRU and the next target production rule, RMSE and the error on leading power. Then how you optimize it? Please describe more details about this. 3. The authors of this paper introduce more information – leading power of desired function for symbolic regression but they incorporate the additional information by introducing a simple loss function term. How about the performance of baseline approaches with those kinds of information? 4. The whole systems seem like very complicate and it would be more interesting if the authors provide sufficient ablations to decompose the complex algorithm."}
{"id": "iclr2020_119", "title": "Abductive Commonsense Reasoning | OpenReview", "abstract": "Abstract:###Abductive reasoning is inference to the most plausible explanation. For example, if Jenny finds her house in a mess when she returns from work, and remembers that she left a window open, she can hypothesize that a thief broke into her house and caused the mess, as the most plausible explanation. While abduction has long been considered to be at the core of how people interpret and read between the lines in natural language (Hobbs et al., 1988), there has been relatively little research in support of abductive natural language inference and generation. We present the first study that investigates the viability of language-based abductive reasoning. We introduce a challenge dataset, ART, that consists of over 20k commonsense narrative contexts and 200k explanations. Based on this dataset, we conceptualize two new tasks – (i) Abductive NLI: a multiple-choice question answering task for choosing the more likely explanation, and (ii) Abductive NLG: a conditional generation task for explaining given observations in natural language. On Abductive NLI, the best model achieves 68.9% accuracy, well below human performance of 91.4%. On Abductive NLG, the current best language generators struggle even more, as they lack reasoning capabilities that are trivial for humans. Our analysis leads to new insights into the types of reasoning that deep pre-trained language models fail to perform—despite their strong performance on the related but more narrowly defined task of entailment NLI—pointing to interesting avenues for future research.", "review": "Review:###Summary: the paper purposes a dataset of abductive language inference and generation. The dataset is generated by human, while the testing set is adversarially selected using BERT. The paper experiments the popular deep learning models on the dataset and observe shortcoming of deep learning on this task. Comments: overall, the problem on abductive inference and abductive generation in language in very interesting and important. This dataset seems valuable. And the paper is simple and well-written. Concerns: I find the claim on deep networks kind of irresponsible. 1. The dataset is adversarially filtered using BERT and GPT, which gives deep learning model a huge disadvantage. After all, the paper says BERT scores 88% before the dataset is attacked. 2. The human score of 91.4% is based on majority vote, which should be compared with an ensemble of deep learning prediction. To compare the author should use the average score of human. 3. The ground truth is selected by human. On a high level, the main difficulty of abduction is to search in the exponentially large space of hypothesis. Formulating the abduction task as a (binary) classification problem is less interesting. The generative task is a better option. Decision: despite the seeming unfair comparison, this task is novel. I vote for weak accept."}
{"id": "iclr2020_120", "title": "Impact of the latent space on the ability of GANs to fit the distribution | OpenReview", "abstract": "Abstract:###The goal of generative models is to model the underlying data distribution of a sample based dataset. Our intuition is that an accurate model should in principle also include the sample based dataset as part of its induced probability distribution. To investigate this, we look at fully trained generative models using the Generative Adversarial Networks (GAN) framework and analyze the resulting generator on its ability to memorize the dataset. Further, we show that the size of the initial latent space is paramount to allow for an accurate reconstruction of the training data. This gives us a link to compression theory, where Autoencoders (AE) are used to lower bound the reconstruction capabilities of our generative model. Here, we observe similar results to the perception-distortion tradeoff (Blau & Michaeli (2018)). Given a small latent space, the AE produces low quality and the GAN produces high quality outputs from a perceptual viewpoint. In contrast, the distortion error is smaller for the AE. By increasing the dimensionality of the latent space the distortion decreases for both models, but the perceptual quality only increases for the AE.", "review": " Summary: The paper explores the influence of the dimensionality of the latent space to the quality of the learned distributions for autoencoders (AE) and GANs (more precisely Wasserstein GANs). In particular, the paper looks at the ability of the learned AE or GAN to reconstruct the training images, the visual quality of the images as the dimension of the latent space increases, and the ability to reconstruct images not in the training set (structured in some ways). Evaluation: While the general flavor of question the paper studies is undoubtedly interesting, I found the paper severely lacking both in terms of the quality of writing (in particular, I was at confused about the goal of various sections/experiments), as well as the significance of the results the authors observe (and how they are reported -- I found them to be oversold). Regarding the quality of results: * The paper primarily talks about the ability of AEs and GANs to *reconstruct* images, either in the training set, or in the test set, or in some different dataset altogether (e.g. shifted images, different image dataset). This is a problematic thing on multiple levels: first, the goal of a GAN or AE is to fit a distribution -- merely having a data point in its domain says nothing about the *probability* of that point; second, the way these *spans* are tested is via running a gradient descent search for a pre-image for the data point. The authors never comment or explore whether the problem may *not* be that these data points are not in the image of the GAN, but rather that the optimization procedure doesn*t succeed. (And indeed, increasing the dimensionality of the latent space may act as *overparametrization* for this gradient descent procedure, making it more likely to succeed.) Finally, there are some fairly arbitrary choices in the entire experimental setup: why WGANs vs another architecture -- are the GAN results sensitive to architecture choices? why AEs and not VAEs (and with what variational posterior) -- how sensitive are the observations here to choosing the most vanilla variant of autoencoders? These are all questions that invariably linger after reading the paper. Regarding the quality of writing: * There are various sloppy sentences in crucial parts of the paper. I will only list a few: -- *Once a suitable AE for a dataset is found, the decoder part of is used as the generative model of choice.* -- this seems to suggest a semi-synthetic setup where an AE is trained to use as a generator of a data set for which a GAN is fit. I never saw this setup in Section 5 -- although this would be a good way to test *relative* representational power of GANs and AEs. -- *In principle, getting from an AE to a GAN is just a rearrangement of the NNs* in Section 5.1 -- I wasn*t sure what this is supposed to mean, and this is a critical part of that section. -- *The AE network acts as a lower bound for the GAN algorithm, therefore validating our intuition that the AE complexity lower-bounds the GAN* in Section 5.1 -- also very sloppy, and I*m not sure what it means -- I guess the authors mean the reconstruction performance of AE is a lower bound on the GAN reconstruction. Not sure what this has to do with *complexity*. * Various sections are meandering, and I wasn*t sure what the goal is. Just a few examples: section 3 spends a lot of time talking about known theoretical results wrt. to invertibility of random-like neural nets. It wasn*t clear to me how this relates to the results in Section 5, especially since the authors never leverage/talk about these theory results again. (Instead, they study empirical invertibility via gradient-descent based procedures.) Similarly, interpolating by polynomials is talked about in (2), seemingly without any point."}
{"id": "iclr2020_121", "title": "Learning Video Representations using Contrastive Bidirectional Transformer | OpenReview", "abstract": "Abstract:###This paper proposes a self-supervised learning approach for video features that results in significantly improved performance on downstream tasks (such as video classification, captioning and segmentation) compared to existing methods. Our method extends the BERT model for text sequences to the case of sequences of real-valued feature vectors, by replacing the softmax loss with noise contrastive estimation (NCE). We also show how to learn representations from sequences of visual features and sequences of words derived from ASR (automatic speech recognition), and show that such cross-modal training (when possible) helps even more.", "review": " This is one of those papers where the number of experiments conducted to produce the results is beyond the capabilities of *almost all* research groups. From the paper: *we use 32 Cloud TPUs. The model is trained for 2 million iterations, which takes around 2 days.* However, with that being said, it*s a good paper of general interest to the community. The paper focuses on self-supervised learning in video, and combines two contributions. The first is using a noise contrastive estimation loss (2016) which can be used for any visual dataset. The second is a cross-modal (BERT) model that requires language and vision. A few modifications over other BERT flavours are introduced. The cross-modal BERT is not tested alone, however when added to the NCE loss function, seems to suit a range of downstream tasks from classification to anticipation and captioning. NCE alone seems to clearly produce better results over published results, however these are not compared like-to-like, as published results are used for this comparison. The paper is full of technical details to reproduce the results. This makes the main novelty is actually in showing that this approach works. However, the approach is technically sound and up to my knowledge has not been attempted before."}
{"id": "iclr2020_122", "title": "Fantastic Generalization Measures and Where to Find Them | OpenReview", "abstract": "Abstract:###Generalization of deep networks has been intensely researched in recent years, resulting in a number of theoretical bounds and empirically motivated measures. However, most papers proposing such measures only study a small set of models, leaving open the question of whether these measures are truly useful in practice. We present the first large scale study of generalization bounds and measures in deep networks. We train over two thousand CIFAR-10 networks with systematic changes in important hyper-parameters. We attempt to uncover potential causal relationships between each measure and generalization, by using rank correlation coefficient and its modified forms. We analyze the results and show that some of the studied measures are very promising for further research.", "review": "Review:###The paper aims at providing a better understanding of generalization for Deep Learning models. The idea is really interesting for the ML community as, despite their broad use, the astounding property of deep neural networks to generalize that well is still not well understood. The idea is not to show new theoretical bounds for generalization gaps but stress the results of an empirical study comparing the already existing measures. The authors choose 7 common hyperparameters related to optimization and analyze the correlation between the generalization gaps effectively observed and the ones predicted by different measures (VC dim, cross-entropy, canonical ordering …). The writing of the paper is clear and easily understandable. Besides, I believe that the study is relevant to ICLR conference. However, I believe the level of the paper is marginally below the threshold of acceptance and therefore would recommend to reject it. The paper is solely empirical but I believe that the empirical section is a bit weak, or at least some important points remain unclear. If I appreciate the extent efforts made in trying to evaluate different measures of generalization gaps, I do not believe that the findings are conclusive enough. 1) First, all this empirical result are based on one-dataset (CIFAR-10) only thus limiting the impact of the study. Indeed, a given measure might very correlate with generalization gap on this specific dataset but not on others. 2) Specifically, we see that on this specific dataset, all training accuracies are already quite good (cf : Figure 1, distribution of the training losses). Consequently, authors are more correlating the chosen measures with the test error rather than with the generalization gaps. On other more complicated datasets where the training loss is higher, the VC dimension might consequently have way better results. Similarly, in Section 6, the authors say that the results «confirm the widely known empirical observation that over-parametrization improves generalization in deep learning. » In this specific case, no reference was given to support the claim. I would agree with the claim « over-paramatrization improves test accuracy (reduces test error) » but the link between over-parametrization and generalization is less clear. 3) In Section 4, the authors say « drawing conclusion from changing one or two hyper-parameters » can be a pitfall as « the hyper-parameter could be the true cause of both change in the measure and change in the generalization ». I totally agree with the authors here. Consequently, I do not understand why the correlations were measured by only changing one hyper-parameter at a time instead of sampling randomly in Theta. 4) It is still not clear to me how the authors explain why some measures are more correlated with generalization gaps than others. Are some bounds tighter than others ? This empirical study was only applied to convolutional neural networks and consequently one may wonder that for example the VC dim bounds computed in the specific case of neural networks are too loose. However, this measure could be efficient for type of models. I would like the authors to clear the following points : - How do you ensure that the empirical study clearly correlated measures predictions with generalization gaps and not simply with test errors (or accuracies) ? (point 2) - Could you please also answer Point 4 ? - Finally, how would you explain the fact that the canonical order performs so well compare to many other measures and that it is a really tough-to-beat baseline ?"}
{"id": "iclr2020_123", "title": "Label Cleaning with Likelihood Ratio Test | OpenReview", "abstract": "Abstract:###To collect large scale annotated data, it is inevitable to introduce label noise, i.e., incorrect class labels. A major challenge is to develop robust deep learning models that achieve high test performance despite training set label noise. We introduce a novel approach that directly cleans labels in order to train a high quality model. Our method leverages statistical principles to correct data labels and has a theoretical guarantee of the correctness. In particular, we use a likelihood ratio test(LRT) to flip the labels of training data. We prove that our LRT label correction algorithm is guaranteed to flip the label so it is consistent with the true Bayesian optimal decision rule with high probability. We incorporate our label correction algorithm into the training of deep neural networks and train models that achieve superior testing performance on multiple public datasets.", "review": " This paper proposes a label correction approach based on a likelihood ratio test, for robust training of deep neural networks against label noise. First, this paper introduces the LRT-Correction procedure, which is the main component of the proposed label correction approach. LRT-Correction uses the current model prediction to run a likelihood ratio test and flip labels when they are rejected. The decision is made by comparing the likelihood test results with a predefined value Delta. Then they introduce the full algorithm, AdaCorr, where the LRT-Correction procedure serves as an inner loop for the label correction. Lastly, there are experiments done on four datasets to conclude the superior performance of the proposed AdaCorr in contrast to several existing methods. Overall, this paper proposes a new label correction approach based on a likelihood ratio test. Standard experiments show that the proposed AdaCorr is superior to several existing methods. The following questions are expected to be addressed during rebuttal: 1. The LRT-Correction procedure introduces additional computation costs. What is the difference in computation costs between Standard and the proposed label correction approach? Is the extra computation cost significant? 2. The LRT-Correction procedure is introduced based on a binary setting. The main theorem (Theorem 1) also only supports the binary setting. There is a statement in Corollary 1 that “LRT-Correction can be generalized to multiclass classification tasks, by flipping \tilde{y} to be the best prediction of f when the null hypothesis is rejected. Theorem 1 can be generalized to multiclass classification tasks, by considering all pairs of class values.” How exactly did you do for that? Please provide more details. 3. This paper uses the ablation study to demonstrate that the proposed AdaCorr is robust to several important hyper-parameters, e.g. the number of epochs m for the burn-in stage, the predefined value Delta for likelihood ratio test. How did you exactly choose the optimal value for these hyper-parameters? These is a statement “We choose m=20 in this data set (CIFAR10) and “similarly” in other datasets.” Did you use the same m(=20) for all datasets? Does this also hold for the hyper-parameter Delta ? 4. The experiments are too standard. Any results on real-world datasets, e.g. Clothing1M [1]? Minor comments: 1. The summarization of the existing related work is not consistent throughout the paper. In Introduction section, this paper believes that the existing methods mainly follow two directions, i.e. probabilistic reasoning and data selecting; while in Related Work section, they are classified into three categories. Please clarify your arguments. 2. Page 5: In Corollary 1, “LRT-Correctioncan” -> “LRT-Correction can” 3. Page 7: In Table 2, MINIST -> MNIST. 4. Page 7: In Experiment Setup, please provide more training details, e.g. learning rate. [1] Xiao, Tong, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. *Learning from massive noisy labeled data for image classification.* In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2691-2699. 2015."}
{"id": "iclr2020_124", "title": "The Blessing of Dimensionality: An Empirical Study of Generalization | OpenReview", "abstract": "Abstract:###The power of neural networks lies in their ability to generalize to unseen data, yet the underlying reasons for this phenomenon remain elusive. Numerous rigorous attempts have been made to explain generalization, but available bounds are still quite loose, and analysis does not always lead to true understanding. The goal of this work is to make generalization more intuitive. Using visualization methods, we discuss the mystery of generalization, the geometry of loss landscapes, and how the curse (or, rather, the blessing) of dimensionality causes optimizers to settle into minima that generalize well.", "review": "Review:###The paper aims to provide an intuitive explanation for why neural networks generalize despite over-parameterization. They argue that neural networks generalize well because flat local minima generalize well since they are insensitive to small perturbations in the data, and they are wide so they tend to attract SGD more than sharp local minima. Overall, the paper is well-written and contains nice visualizations. However, I don*t think the paper provides an explanation to why neural networks generalize for a few reasons: - First, I don*t think the methodology used in the paper to identify bad local minima is a good sampling strategy of local minima in general. It could be the case that adding the second objective in Eq 2 will tend to generate sharp local minima (since by construction it works well for a subset of the data and works poorly on another subset). In other words, the local minima identified by solving Eq 2 might be sharp by construction. Of course, these sharp minima do not generalize by construction as well. Using this as an explanation is not justifiable since there might be other bad minima that are flat but are never identified by solving Eq 2. - The explanation provided here does not explain why linear models do not generalize well (the ones that have been tried in Figure 2). Do linear models have sharp minima? It would be good to measure the curvature of the loss for both the optimal solution of the linear model and the local minima of neural networks. - The explanation that flat minima are wide and, hence, tend to attract SGD is incorrect since we don*t know how many of the local minima are wide. What if the majority of local minima are sharp? There is no analysis in the paper of the ratio between the number of flat minima and sharp minima. - Regarding the discussion about high dimensionality, it is true that flat minima will be several order of magnitudes larger than sharp minima but there is also the generalization penalty of having higher dimensions. Are the generalization bounds that explain the value of flat minima dimension-independent? I don*t think they are since the dimension often shows up indirectly in the bounds (such as in the condition number which tends to increase linearly with dimension in random matrices). In summary, the paper is nice to read but I don*t think it answers the question of why neural networks generalize in practice."}
{"id": "iclr2020_125", "title": "On Universal Equivariant Set Networks | OpenReview", "abstract": "Abstract:###Using deep neural networks that are either invariant or equivariant to permutations in order to learn functions on unordered sets has become prevalent. The most popular, basic models are DeepSets (Zaheer et al. 2017) and PointNet (Qi et al. 2017). While known to be universal for approximating invariant functions, DeepSets and PointNet are not known to be universal when approximating equivariant set functions. On the other hand, several recent equivariant set architectures have been proven equivariant universal (Sannai et al. 2019, Keriven and Peyre 2019), however these models either use layers that are not permutation equivariant (in the standard sense) and/or use higher order tensor variables which are less practical. There is, therefore, a gap in understanding the universality of popular equivariant set models versus theoretical ones. In this paper we close this gap by proving that: (i) PointNet is not equivariant universal; and (ii) adding a single linear transmission layer makes PointNet universal. We call this architecture PointNetST and argue it is the simplest permutation equivariant universal model known to date. Another consequence is that DeepSets is universal, and also PointNetSeg, a popular point cloud segmentation network (used e.g., in Qi et al. 2017) is universal. The key theoretical tool used to prove the above results is an explicit characterization of all permutation equivariant polynomial layers. Lastly, we provide numerical experiments validating the theoretical results and comparing different permutation equivariant models.", "review": "Review:###TLDR: The function these deep set networks can approximate is too limited to call these networks universal equivariant set networks. Authors should scope the paper to the specific function family these networks can approximate. No baseline comparison with GraphNets. The paper proposes theoretical analysis on a set of networks that process features independently through MLPs + global aggregation operations. However, the function of interest is limited to a small family of affine equivariant transformations. A more general function is where is the set of index of neighbors within the set . It is trivial to show that this function is permutation equivariant. Then, can the function family the authors used in the paper approximate this function? No. Can the proposed permutation equivariant function represent all function the authors used in the paper? Yes. 1) If , then the proposed function becomes MLP. 2) If and , then this is , the global aggregation function. Also, this is the actual function that a lot of people are interested in. Let me go over few more examples. 3) If adjacency on a graph and , then this is a graph neural network *convolution* (it is not a convolution) Example adjacency . 4) If where is the color, is the pixel coordinate and pixel neighbors within some kernel size, to be the block diagonal matrix only for the first three dimensions and 0 for the rest, then this is the 2D convolution. Again, the above function is a more general permutation equivariant function that can represent: a graph neural network layer, a convolution, MLP, global pooling and is one of the most widely used functions in the ML community, not MLP + global aggregation. Regarding the experiment metrics and plots: On the Knapsack test, the metric of interest is not the accuracy of individual prediction. Rather, whether the network has successfully predicted the optimal solution, or how close the prediction is to the solution. For example: success rate within the epsilon radius of the optimal solution while satisfying all the constraints. Fail otherwise. If these networks can truly solve these problems, authors should report the success rate while varying the threshold, not individual accuracy of the items which can be arbitrarily high by violating constraints. Also, the authors should compare with few more graphnet + transmission layer (GraphNetST) baselines with the graph layers: and the same single transmission layer in PointNetST. PointNet is a specialization of graphnets and GraphNetST should be added as a baseline with reasonable adjacency. Also experiment figures are extremely compact. Try using log scale or other lines to make the gaps wider. Minor I am quite confused with the name PointNetST. Authors claim adding one layer of DeepSet layer to a PointNet becomes PointNetST, but I see this as a special DeepSet with a single transmission layer. The convention is B -> B*, not A + B -> A*. In this case, A: PointNet, B: DeepSet Lemma 3 is too trivial. The paper is not very self contained. Spare few lines of equations to define what are DeepSets and PointNetSeg in the paper and point out the difference since these networks are used throughout the paper extensively without proper mathematical definition. P.2 power sum multi-symmetric polynomials. *For a vector and a multi-index ...* I think it was moved out of the next paragraph since the same is defined again as again in the next sentence. Also, try using the consistent dimension for x throughout the paper, it confuses the reader."}
{"id": "iclr2020_126", "title": "Certified Robustness for Top-k Predictions against Adversarial Perturbations via Randomized Smoothing | OpenReview", "abstract": "Abstract:###It is well-known that classifiers are vulnerable to adversarial perturbations. To defend against adversarial perturbations, various certified robustness results have been derived. However, existing certified robustnesses are limited to top-1 predictions. In many real-world applications, top- predictions are more relevant. In this work, we aim to derive certified robustness for top- predictions. In particular, our certified robustness is based on randomized smoothing, which turns any classifier to a new classifier via adding noise to an input example. We adopt randomized smoothing because it is scalable to large-scale neural networks and applicable to any classifier. We derive a tight robustness in norm for top- predictions when using randomized smoothing with Gaussian noise. We find that generalizing the certified robustness from top-1 to top- predictions faces significant technical challenges. We also empirically evaluate our method on CIFAR10 and ImageNet. For example, our method can obtain an ImageNet classifier with a certified top-5 accuracy of 62.8\\% when the -norms of the adversarial perturbations are less than 0.5 (=127/255). Our code is publicly available at: url{https://github.com/jjy1994/Certify_Topk}.", "review": "Review:###Summary: This paper studies the certifiable bounds for adversarial perturbations in ell_2 radius for top-k predictions instead of top-1 predictions. The paper obtains a certifiable radius of ell_2 perturbations in the case of top-k predictions (Theorem 1) and shows that the bounds are tight (Theorem 2). The result thus generalizes the results obtained in Cohen et al. (2019) by setting k=1. Since Theorem 1 requires lower and upper bounds, the paper proposes two methods for calculating the bounds on multinomial probabilities. Experimental evidence suggests that one indeed obtains a better certifiable radius for the top-k radius vs. the top-1 radius. My evaluation of the paper is positive: The theoretical results (Theorem 1 and 2) are new and study practical use cases of these models. The experimental results (Figure 1) support the claim that there is a non-trivial difference between the certified radii of top-1 and top-k predictions. However, the level of technical novelty is relatively low. The proof of Theorem 1 follows the similar procedure for top-1 predictions and the methods proposed for estimating probabilities (BinoCP and SinuEM) are standard procedures. Other comments: 1. What is the trend of top-k-clean-accuracy and top-k-adversarial-accuracy as a function of k? Is this trend similar across different radii? 2. What is the value of k in Figure 3 and Figure 4?"}
{"id": "iclr2020_127", "title": "Defective Convolutional Layers Learn Robust CNNs | OpenReview", "abstract": "Abstract:###Robustness of convolutional neural networks has recently been highlighted by the adversarial examples, i.e., inputs added with well-designed perturbations which are imperceptible to humans but can cause the network to give incorrect outputs. Recent research suggests that the noises in adversarial examples break the textural structure, which eventually leads to wrong predictions by convolutional neural networks. To help a convolutional neural network make predictions relying less on textural information, we propose defective convolutional layers which contain defective neurons whose activations are set to be a constant function. As the defective neurons contain no information and are far different from the standard neurons in its spatial neighborhood, the textural features cannot be accurately extracted and the model has to seek for other features for classification, such as the shape. We first show that predictions made by the defective CNN are less dependent on textural information, but more on shape information, and further find that adversarial examples generated by the defective CNN appear to have semantic shapes. Experimental results demonstrate the defective CNN has higher defense ability than the standard CNN against various types of attack. In particular, it achieves state-of-the-art performance against transfer-based attacks without applying any adversarial training.", "review": "Review:###This paper presents a technique for adversarial defense, employing what the authors refer to as *defective convolution layers*. It attempts to use such layers to provide more adversarially robust models. The paper is easy to follow and well written, but the experimentation is lacking, and so the contribution is limited. Defective Convolution layers make use of a random fixed masking matrix, to fix some number of neurons to constant values, effectively removing the incoming weights of these neurons from the optimization problem. The outgoing weights of the masked neurons are effectively additional bias terms for the layer above. Applying this technique to a fully connected layer would be equivalent to training with a fully connected layer with a smaller size. The paper presents the accuracy of such models on a variety of black box attacks. They focus on black box for two reasons, neither of which are particularly convincing. They claim that the white box attacks are semantically meaningful and so could fool a human, but no human evaluation is presented and the examples which are illustrated do not demonstrate this property. They also claim to focus on black box attacks because is it more practical in a real world setting. However the model they present achieves significantly lower test accuracy on clean data than a standard network, so the practical deployment of such a model seems unlikely in its current implementation. The paper presents thorough ablation studies on the architectural choices that go into this model. This is a positive quality of the work. However they do not compare standard networks with similar test accuracy to their defective models. As they note in the paper and in the appendix, there is a correlation between test accuracy and adversarial robustness. Based on the findings which they present, it is unclear if the effect of their *Defective Convolution Layer* is simply to reduce the test accuracy and thereby increase the adversarial robustness. This issue must be addressed for the work to contribute to the adversarial literature in a meaningful way."}
{"id": "iclr2020_128", "title": "The Role of Embedding Complexity in Domain-invariant Representations | OpenReview", "abstract": "Abstract:###Unsupervised domain adaptation aims to generalize the hypothesis trained in a source domain to an unlabeled target domain. One popular approach to this problem is to learn domain-invariant embeddings for both domains. In this work, we study, theoretically and empirically, the effect of the embedding complexity on generalization to the target domain. In particular, this complexity affects an upper bound on the target risk; this is reflected in experiments, too. Next, we specify our theoretical framework to multilayer neural networks. As a result, we develop a strategy that mitigates sensitivity to the embedding complexity, and empirically achieves performance on par with or better than the best layer-dependent complexity tradeoff.", "review": "Review:###This paper studies the impact of embedding complexity on domain-invariant representations. By incorporating embedding complexity into the previous upper bound explicitly, the authors demonstrate the limitations of previous theories and algorithms. Based on their theoretical findings, the authors propose to control the embedding complexity with implicit regularization. Specifically, aligning source and target feature distributions in multiple layers controls both embedding complexity and domain discrepancy. The proposed algorithm can achieve similar performance as DANN with manual selection of embedding depth. By noting that the hypothesis space can be decomposed in to the feature extractor and the classifier, the authors propose to address the domain discrepancy separately. D_HDeltaH is termed latent divergence, which the algorithm attempts to minimize. D_GDeltaG is treated as embedding complexity, which is the intrinsic property of the feature extractor. Thus, domain-invariant representations should seek a proper tradeoff between those two terms. The paper is well-written and the contributions are stated clearly. The exploration on the layer division is really insightful. However, I have several concerns: 1. The proposed upper bound is insightful, but it has several limitations. Compared to the version applied to the feature space in equation (3), the proposed upper bound is looser. The embedding complexity terms includes two encoders, which are deep neural networks in practice, thus it can be excessively large. As the authors point out, in equation (3), the embedding complexity is not addressed explicitly, but it is implicit in the adaptability lambda in a more reasonable way. Previous works [1], [2], [3] have already taken them into consideration. Proposition 5 is a direct application of proposition 1 in [1]. 2. On the claim of implicit regularization. By applying domain adversarial training to multiple layers, the authors claim that the encoder in higher layers is implicitly restricted. However, they do not validate this regularization effect. Is the embedding complexity controlled? Theoretical analysis or experimental results would be helpful. 3. The proposed MDM method seems to be incremental. [4] has probed into the effect of multi-layer adaptation strategy. Besides, applying domain adversarial training to many layers leads to more computational cost and may slow down training significantly. [1]Fredrik D Johansson, Rajesh Ranganath, and David Sontag. Support and invertibility in domain- invariant representations. arXiv preprint arXiv:1903.03448, 2019. [2]Han Zhao, Remi Tachet des Combes, Kun Zhang, and Geoffrey J Gordon. On learning invariant representation for domain adaptation. arXiv preprint arXiv:1901.09453, 2019. [3] Hong Liu, Mingsheng Long, Jianmin Wang, and Michael Jordan. Transferable adversarial training: A general approach to adapting deep classifiers. In International Conference on Machine Learning, pp. 4013–4022, 2019. [4] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features with deep adaptation networks. In Proceedings of the 32nd International Conference on International Conference on Machine Learning, volume 37, pp. 97–105, 2015."}
{"id": "iclr2020_129", "title": "Towards Stable and comprehensive Domain Alignment: Max-Margin Domain-Adversarial Training | OpenReview", "abstract": "Abstract:###Domain adaptation tackles the problem of transferring knowledge from a label-rich source domain to an unlabeled or label-scarce target domain. Recently domain-adversarial training (DAT) has shown promising capacity to learn a domain-invariant feature space by reversing the gradient propagation of a domain classifier. However, DAT is still vulnerable in several aspects including (1) training instability due to the overwhelming discriminative ability of the domain classifier in adversarial training, (2) restrictive feature-level alignment, and (3) lack of interpretability or systematic explanation of the learned feature space. In this paper, we propose a novel Max-margin Domain-Adversarial Training (MDAT) by designing an Adversarial Reconstruction Network (ARN). The proposed MDAT stabilizes the gradient reversing in ARN by replacing the domain classifier with a reconstruction network, and in this manner ARN conducts both feature-level and pixel-level domain alignment without involving extra network structures. Furthermore, ARN demonstrates strong robustness to a wide range of hyper-parameters settings, greatly alleviating the task of model selection. Extensive empirical results validate that our approach outperforms other state-of-the-art domain alignment methods. Additionally, the reconstructed target samples are visualized to interpret the domain-invariant feature space which conforms with our intuition.", "review": " This paper proposed a new method for unsupervised domain adaptation. Different from a conventional domain classifier based adaptation, they propose to utilize the loss of autoencoder to extract domain-invariant features. They trained reconstruction network to reconstruct source examples well whereas making reconstruction loss of the target examples large with some margin. Their goal is to stabilize the training of adversarial training for domain adaptation, incorporate pixel-level information, and give interpretable learned feature space. They performed experiments on digits datasets and WiFi Gesture Recognition datasets. Through experiments, they have shown that their method shows better performance than baseline methods and their method is not parameter-sensitive, is stable and provides interpretable adaptation results. I think their method is interesting and motivation is important. However, their experimental results are not convincing enough. First, they did not compare their method with recent state-of-the-art methods. For example, there are classifier*s discrepancy based adversarial learning method, Saito, Kuniaki, et al. *Maximum classifier discrepancy for unsupervised domain adaptation.*. In addition, they did not compare with *A dirt-t approach to unsupervised domain adaptation*, which they cited in the paper. I think their method is for stable and interpretable adversarial learning. So, it does not have to outperform other methods in accuracy. However, they need to show some superiority over these representative adversarial methods. Second, their experiment is only on digits and WIFI datasets. Is the method effective for object recognition datasets, such as Office or OfficeHome? This is an important question to be addressed because the two datasets are benchmark domain adaptation dataset and the behavior on this dataset will show how this method is applicable to various datasets. I would say that the method does not have to outperform state-of-the art methods for these datasets, but they need to show how the method works on this dataset with respect to stability and interpretability. In addition, this method seems to have clear connection with *Zhao, Junbo, Michael Mathieu, and Yann LeCun. *Energy-based generative adversarial network.* arXiv preprint arXiv:1609.03126 (2016)*. They need to add this paper to a reference and explain some connections. To sum up, due to the two questions listed above, I think this paper is marginally below the acceptance threshold. Please respond to the questions."}
{"id": "iclr2020_130", "title": "MLModelScope: A Distributed Platform for ML Model Evaluation and Benchmarking at Scale | OpenReview", "abstract": "Abstract:###Machine Learning (ML) and Deep Learning (DL) innovations are being introduced at such a rapid pace that researchers are hard-pressed to analyze and study them. The complicated procedures for evaluating innovations, along with the lack of standard and efficient ways of specifying and provisioning ML/DL evaluation, is a major *pain point* for the community. This paper proposes MLModelScope, an open-source, framework/hardware agnostic, extensible and customizable design that enables repeatable, fair, and scalable model evaluation and benchmarking. We implement the distributed design with support for all major frameworks and hardware, and equip it with web, command-line, and library interfaces. To demonstrate MLModelScope*s capabilities we perform parallel evaluation and show how subtle changes to model evaluation pipeline affects the accuracy and HW/SW stack choices affect performance.", "review": "Review:###This paper studies the question of model evaluation and reproducibility in machine learning research, specifically deep learning research, and designs and tests an extensive system for evaluating and comparing models. Users specify their evaluation parameters through a text file, and this is used by the runtime, which can be interfaced through a UI or the command line, in order to carry out the evaluation. Several experiments shed interesting light on various aspects of model, framework, and hardware performance. Hypothetically, if I were to design a new model and wish to evaluate its performance relative to existing SotA models, I would potentially use this system to run all of the models, including my own. That would mean that I need to *upload* or otherwise integrate my model into this system, and it was unclear from my reading of the paper how easy such a process would be. Similarly, I would wish to maintain similar training and evaluation conditions for my model, e.g., the same pre and post-processing, and that would involve *extracting* those steps from the system for use during training. I would also like to understand whether or not this is feasible and easy given the system*s design. In section 3.1, the authors write *The hardware details are not present in the manifest, but are user-provided options when performing the evaluation.* An example of how this operates would be useful in the paper. As far as experiments go, I*m not sure what the main takeaway is from section 4.1. To me, the takeaway that pre-processing is important and existing models are sensitive to pre-processing is not a new finding. The results from Table 1 could certainly be obtained without the use of the proposed system, and though there would be some scaffolding involved, I don*t think that the coding would not be particularly difficult. Is the takeaway that the proposed system makes it easier or faster to evaluate the effects of different types of pre-processing? Wouldn*t this be most interesting at training time? I find the experiments in sections 4.2 and 4.3 interesting. In section 4.2, I*m not sure if figure 9 includes enough information or description to conclude that *GPU instances in general are more cost-efficient than CPU instances for batched inference*, and some more detail here would be useful. Generally, I believe that the work is well-motivated and timely, the authors seem to have done a good job in citing related work (though admittedly I don*t know much about this area), and the results are supportive of the claims of the system*s usefulness."}
{"id": "iclr2020_131", "title": "Policy Message Passing: A New Algorithm for Probabilistic Graph Inference | OpenReview", "abstract": "Abstract:###A general graph-structured neural network architecture operates on graphs through two core components: (1) complex enough message functions; (2) a fixed information aggregation process. In this paper, we present the Policy Message Passing algorithm, which takes a probabilistic perspective and reformulates the whole information aggregation as stochastic sequential processes. The algorithm works on a much larger search space, utilizes reasoning history to perform inference, and is robust to noisy edges. We apply our algorithm to multiple complex graph reasoning and prediction tasks and show that our algorithm consistently outperforms state-of-the-art graph-structured models by a significant margin.", "review": "Review:###The paper presents a policy message passing algorithm for graph neural network where both node & edge energy functions and the message passing procedure are treated in a unified manner as outputs of neural nets. In particular, the message passing procedures for individual edges are individualized (i.e. not homogeneous) while a common memory pool is also shared among these procedures. It also aims at doing message passing in the distribution level instead of singling out a particular message passing rule. The idea is new (to my understanding) and interesting. Empirical results demonstrate the usefulness of the proposed methods. On the other hand, the main concerns are: *Only a rough idea is introduced and discussed, then followed by some nice practical results. In the middle, there is lacking of concrete execution of the proposed idea. The devil could lie in the details. *many details are missing. Throughout the paper, it is not clear at all how the message functions and the inference procedure functions (or called reasoning agents) are realized in practice. Also I do not see the practical implementation would be made publicly accessible."}
{"id": "iclr2020_132", "title": "LSTOD: Latent Spatial-Temporal Origin-Destination prediction model and its applications in ride-sharing platforms | OpenReview", "abstract": "Abstract:###Origin-Destination (OD) flow data is an important instrument in transportation studies. Precise prediction of customer demands from each original location to a destination given a series of previous snapshots helps ride-sharing platforms to better understand their market mechanism. However, most existing prediction methods ignore the network structure of OD flow data and fail to utilize the topological dependencies among related OD pairs. In this paper, we propose a latent spatial-temporal origin-destination (LSTOD) model, with a novel convolutional neural network (CNN) filter to learn the spatial features of OD pairs from a graph perspective and an attention structure to capture their long-term periodicity. Experiments on a real customer request dataset with available OD information from a ride-sharing platform demonstrate the advantage of LSTOD in achieving at least 6.5% improvement in prediction accuracy over the second best model.", "review": "Review:###Summary: This paper proposes a latent spatial-temporal origin-destination model to address the OD flow prediction problem. The main contributions are summarized as follows: 1. The authors propose a purely convolutional framework to learn both short-term and long-term spatio-temporal features simultaneously from dynamic origin-destination flow data. 2. The authors propose a novel SACN architecture to capture the relevance of OD flows by modeling each OD flow map as an adjacency matrix. 3. The authors design a periodically shift attention mechanism to model the long-term periodicity. 4. The results demonstrate that the proposed model outperforms state-of-the-art methods in OD flow prediction, with 6.5% to 15.0% improvement of testing RMSE. However, my major concerns are as follows: 1. On Page 4, the authors explain the reasons why they use TGCNN instead of RNN-based architectures to capture the temporal representation. However, it would be more convincing if quantitative analysis or empirical results are provided. 2. On Page 4, readers might be confused with the symbols in the formulation (3) that are not defined clearly. 3. On Page 7, the experiment only considers one metric, i.e., RMSE. The efficiency comparisons between the proposed model and the baselines are missing."}
{"id": "iclr2020_133", "title": "Making Sense of Reinforcement Learning and Probabilistic Inference | OpenReview", "abstract": "Abstract:###Reinforcement learning (RL) combines a control problem with statistical estimation: the system dynamics are not known to the agent, but can be learned through experience. A recent line of research casts *RL as inference* and suggests a particular framework to generalize the RL problem as probabilistic inference. Our paper surfaces key shortcomings in that approach, and clarifies the sense in which RL can be coherently cast as an inference problem. In particular, an RL agent must consider the effects of its actions upon future rewards and observations: the exploration-exploitation tradeoff. In all but the most simple settings, the resulting inference is computationally intractable so that practical RL algorithms must resort to approximation. We show that the popular *RL as inference* approximation can perform poorly in even the simplest settings. Despite this, we demonstrate that with a small modification the RL as inference framework can provably perform well, and we connect the resulting algorithm with Thompson sampling and the recently proposed K-learning algorithm.", "review": "Review:###The paper at hand presents an alternative view on reinforcement learning as probabilistic inference (or equivalently maximum entropy reinforcement learning). With respect to other formulations of this view (e.g. Levine, 2018; I am referring to the references of the paper here), the paper identifies a shortcoming in the disregard of the agent’s epistemic uncertainty (which seems to refer to the uncertainty with respect to the underlying MDP). It is argued, that algorithms based on the prevailing probabilistic formulation (e.g. soft Q-learning) suffer from suboptimal exploration. The paper thus compares maximum entropy RL to K-learning (O’Donoghue, 2018), which is taken to address the issue of suboptimal exploration due to its temperature scheduling and its inclusion of state-action pair counts in the reward signal. As its technical contribution, the paper re-interprets K-learning via the latent variable denoting optimality employed in Levine (2018) and introduces a theorem bounding the distance between the policies of Thompson sampling and K-learning. Empirical validation of the claims is provided via experiments on an engineered bandit problem and the tabular MDP (i.e. DeepSea from Osband et al., 2017), as well as via soft Q-learning results on the recently suggested bsuite (Osband et al., 2019). I consider this paper a weak reject. This is in light of me finding it very hard to follow the papers main claims and arguments, even though it positions itself as communicating connections (“making sense”) in prior work, rather than presenting a novel algorithm. While this is in part due to the complicated issue and math being discussed (and the paper probably catering to a very narrow audience), the paper in its current state does seem to hinder understanding as well. On the positive side, I do appreciate the intention of the paper, namely to connect RL as probabilistic inference, Thompson sampling and K-learning. In my opinion, this can be taken as a valuable addition to the current understanding of these approaches. Also, I like the experiments as they are specifically constructed to support the claims of the paper. On the negative side, vague language, missing assumptions and lax notation seem to hinder the understanding of the paper to a considerable extend: e.g. it is stated, that “we connect the resulting algorithm with […] K-learning”. However, I do not recognize a new algorithm being provided. Instead the paper argues in favor of K-learning. The assumptions that come with K-learning are not mentioned. The restriction of K-learning to tabular RL is taken to be understood implicitly (whereas RL as probabilistic inference seems applicable with function approximation also, which is not mentioned in the comparison). The paper always talks of shortcomings (plural) of RL as probabilistic inference, but only provides one argument (suboptimal exploration) with respect to this. RL as probabilistic inference is introduced in a different form as in prior literature (i.e. Equation 6), while the derivation in the Appendix spanning the differences in notation being hard to follow due to (maybe minor?) notational issues (e.g. x and y seem to have replaced s’ and a; further down there is a reference to Equation 7, however probably it is meant to be 8 and even that with some leap in notation). The paper would benefit from better proof-reading, where mistakes in a very dense argumentation make it hard to follow (e.g. I do not understand the sentence “The K-learning expectation (7) is with respect to the posterior over Q[…] to give a parametric approximation to the probability of optimality.”) Literature wise, the paper draws heavily from two unpublished papers (Levine, 2018; O’Donoghue, 2018). While this makes it harder to arrive at a high confidence level with respect to the paper’s claims, I would not argue this to be critical. I would consider raising my score, if the authors would improve the accessibility of the paper by polishing the argumentation and notation. Confidence: low. It is very likely, that I have misunderstood key arguments and derivations. Also, I did not attempt to follow all of the technical derivations. ====== post rebuttal comment: I changed the score of my review in light of the rebuttal. The changes made to the paper overall address my concerns. I do consider the additional explanations and re-phrasings as well as the improved notation a nice improvement of the paper. While I did not read all of the appendix, Section 5.1 is much more readable and understandable in the new version. In light of this paper probably being published, I share some typos/inconsistencies I still noticed: p. 4: the solving for -> solving for the p. 7: s_{h+1} -> s* (in Table 3) ? p. 7: table -> Table; tables -> Tables p. 7: (Fix position of K:) pi_h(s)^K -> pi_h^K(s) ((also in Appendix)) p. 9: (2x) soft-Q learning -> soft Q-learning; Q Networks -> Q-Networks; Soft Q -> soft Q-learning"}
{"id": "iclr2020_134", "title": "How the Softmax Activation Hinders the Detection of Adversarial and Out-of-Distribution Examples in Neural Networks | OpenReview", "abstract": "Abstract:###Despite having excellent performances for a wide variety of tasks, modern neural networks are unable to provide a prediction with a reliable confidence estimate which would allow to detect misclassifications. This limitation is at the heart of what is known as an adversarial example, where the network provides a wrong prediction associated with a strong confidence to a slightly modified image. Moreover, this overconfidence issue has also been observed for out-of-distribution data. We show through several experiments that the softmax activation, usually placed as the last layer of modern neural networks, is partly responsible for this behaviour. We give qualitative insights about its impact on the MNIST dataset, showing that relevant information present in the logits is lost once the softmax function is applied. The same observation is made through quantitative analysis, as we show that two out-of-distribution and adversarial example detectors obtain competitive results when using logit values as inputs, but provide considerably lower performances if they use softmax probabilities instead: from 98.0% average AUROC to 56.8% in some settings. These results provide evidence that the softmax activation hinders the detection of adversarial and out-of-distribution examples, as it masks a significant part of the relevant information present in the logits.", "review": " This simple paper shows that the normalization of softmax causes a loss of information compared to using the unnormalized logits when trying to do OOD and adversarial example detection. The main reason for this is of course the normalization used by the softmax. The paper is mostly empirical following this specific observation, and uses a number of examples on MNIST and CIFAR to show the improvement in performance by using unnormalized logits instead of softmax. While interesting, it is to be noted that methods such as ODIN and temperature scaling specifically include a temperature to exactly overcome this same issue with softmax. The lack of comparison to such baselines makes this paper quite incomplete, especially as it is an empirical paper itself."}
{"id": "iclr2020_135", "title": "Testing Robustness Against Unforeseen Adversaries | OpenReview", "abstract": "Abstract:###Most existing defenses against adversarial attacks only consider robustness to L_p-bounded distortions. In reality, the specific attack is rarely known in advance and adversaries are free to modify images in ways which lie outside any fixed distortion model; for example, adversarial rotations lie outside the set of L_p-bounded distortions. In this work, we advocate measuring robustness against a much broader range of unforeseen attacks, attacks whose precise form is unknown during defense design. We propose several new attacks and a methodology for evaluating a defense against a diverse range of unforeseen distortions. First, we construct novel adversarial JPEG, Fog, Gabor, and Snow distortions to simulate more diverse adversaries. We then introduce UAR, a summary metric that measures the robustness of a defense against a given distortion. Using UAR to assess robustness against existing and novel attacks, we perform an extensive study of adversarial robustness. We find that evaluation against existing L_p attacks yields redundant information which does not generalize to other attacks; we instead recommend evaluating against our significantly more diverse set of attacks. We further find that adversarial training against either one or multiple distortions fails to confer robustness to attacks with other distortion types. These results underscore the need to evaluate and study robustness against unforeseen distortions.", "review": "Review:###Summary: This paper proposes to examine robustness against more unforeseen attacks (than Lp attacks) such as Elastic, Fog and Snow etc. The Unforeseen Attack Robustness (UAT) evaluation metric is further proposed to produce a consistent score over a range of test attacks and perturbations sizes. They also demonstrate the overfitting issue of joint adversarial training on 2 different norms of attacks. -------------- My concerns: 1. It is hard to tell which type of robustness is evaluating in this paper: adversarial or common corruptions? Adversarial robustness often refer to robustness to small imperceptible adversarial perturbations, however, many of the perturbations illustrated in this paper are neither small nor imperceptible. I would expect none of existing models can survive any of those large perturbations. On the other hand, the new attacks proposed in this paper are basically common corruptions, which has been comprehensively studied in [1] on 15 types of corruptions including the proposed Elastic, fog and snow. I don*t think adversarially perturb those common corruptions can contribute more for robustness evaluation. There are also other corruptions such as translations and rotations [2]. 2. The real contribution of this paper is not clear to me. What is the focus of this paper: the robustness of different DNNs, training strategies or defense techniques? Why only one type of defense (eg. adversarial training) was considered (why not a few latest works in adversarial training: [3][4])? My concern comes from this *one-defense* setting. Since all models are trained by one fixed defense, it is hard to draw any concrete conclusions on other defenses, or defense in general. In the abstract, the authors claimed that *These results underscore the need to evaluate and study robustness against unforeseen distortions*, which seems quite obvious. It is always beneficial to include more corruptions, but why the suggested Elastic, Fog, Snow in particular? So far, defense techniques still suffer from low robustness to even small perturbations (the strongest defense, adversarial training, only has 43% - 65% robustness against PGD perturbation L_{infty} < 8/255). Then the question is, why evaluating against so many attacks is necessary and important, considering they even fail the easiest Lp corruptions? By the way, in Table 1, why the ATA4 is 66.9, which seems too high, even higher than state-of-the-art robustness? 3. The proposed epsilon_{min} and epsilon_{max} calibration is a bit confusing. *The minimum distortion size epsilon_{min} is the largest epsilon for which the adversarial validation accuracy against an adversarially trained model is comparable to that of a model trained and evaluated on unattacked data.* -- what this means, exactly? The proposed UAR score is also not clear: 1) what is the different between Acc and ATA? 2) *ATA is the best adversarial accuracy on the validation set that can be achieved by adversarially training a specific architecture* -- how this best accuracy was obtained?? 3) the validation set is also confusing, is it part of the training data held out for validation? *To make UAR roughly comparable between distortions, we evaluate at epsilon increasing geometrically from epsilon_{min} to epsilon_{max} by factors of 2 and take the subset of epsilon whose ATA values have minimum `1-distance to the ATA values of the L1 attack at geometrically increasing * -- I have no idea what the authors are referring to here. 4. The presentation can be improved. For example, shorten the ticks used in those matrix plots (Figure 6, 7, etc). It can also benefit from a summary table somewhere about all those distortion types and sizes. It also helps if the authors can explain the epsilon_{min}, epsilon_{max} and UAR score with a more concrete example. [1] Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations [2] The limitations of adversarial training and the blind-spot attack. ICLR 2019 [3] Theoretically principled trade-off between robustness and accuracy. ICML, 2019. [4] On the Convergence and Robustness of Adversarial Training. ICML, 2019."}
{"id": "iclr2020_136", "title": "Fault Tolerant Reinforcement Learning via A Markov Game of Control and Stopping | OpenReview", "abstract": "Abstract:###Recently, there has been a surge in interest in safe and robust techniques within reinforcement learning (RL). Current notions of risk in RL fail to capture the potential for systemic failures such as abrupt stoppages from system failures or surpassing of safety thresholds and the appropriate responsive controls in such instances. We propose a novel approach to fault-tolerance within RL in which the controller learns a policy can cope with adversarial attacks and random stoppages that lead to failures of the system subcomponents. The results of the paper also cover fault-tolerant (FT) control so that the controller learns to avoid states that carry risk of system failures. By demonstrating that the class of problems is represented by a variant of SGs, we prove the existence of a solution which is a unique fixed point equilibrium of the game and characterise the optimal controller behaviour. We then introduce a value function approximation algorithm that converges to the solution through simulation in unknown environments.", "review": "Review:###The paper sets up the problem of training fault tolerant RL agents as a stochastic game between a stoppage inducing adversary, and an agent that tries the minimize the penalty from stoppage. This should drive an agent to avoid states where there is a high cost for stoppage. I think training RL agents to be fault tolerant is an important problem, however I am not convinced that the current framework proposed in this paper is the right approach to tackle the problem of catastrophic failures. Are abrupt stoppages the right framework to train RL agents to be fault tolerant? I think a key desired feature for FT systems that this formulation might miss is that of graceful degradation that we might want from our RL agents. I am not sure if the current level abstraction with the optimal stopping criteria for an adversary is sophisticated enough to capture this aspect. I am not an expert in this area, but I found the setup a fairly straight forward extension of minimax two player games, and value iteration via the Bellman operator. I am not convinced that the theoretical contributions here are extremely novel, and justify acceptance on their own. If the current work could be backed with some illustrative/meaningful demonstrations demonstrating the proposed approach, I think it would make the paper stronger. How does this work relate to approaches such as CVaR where the agents are trained to avoid high risk (catastrophic) states? Minor comments/typos: Equation 3 — Did you mix up min and max? Stopping time is maximizing the reward? “which the agent has concern the process at a catastrophic system state.” — sentence seems to go wrong “the transition kernel P ·” — weird symbol after P P^a_{sds} — what is ‘d’? I am not sure if it has been defined before? Notation in 5 is confusing. What is the minimum over? “Additionally, Theorem 1 establishes the existence of a given by J ? , the computation of which, is the subject of the next section.” — grammar goes wrong"}
{"id": "iclr2020_137", "title": "Deep Nonlinear Stochastic Optimal Control for Systems with Multiplicative Uncertainties | OpenReview", "abstract": "Abstract:###We present a deep recurrent neural network architecture to solve a class of stochastic optimal control problems described by fully nonlinear Hamilton Jacobi Bellman partial differential equations. Such PDEs arise when one considers stochastic dynamics characterized by uncertainties that are additive and control multiplicative. Stochastic models with the aforementioned characteristics have been used in computational neuroscience, biology, finance and aerospace systems and provide a more accurate representation of actuation than models with additive uncertainty. Previous literature has established the inadequacy of the linear HJB theory and instead rely on a non-linear Feynman-Kac lemma resulting in a second order forward-backward stochastic differential equations representation. However, the proposed solutions that use this representation suffer from compounding errors and computational complexity leading to lack of scalability. In this paper, we propose a deep learning based algorithm that leverages the second order Forward-Backward SDE representation and LSTM based recurrent neural networks to not only solve such Stochastic Optimal Control problems but also overcome the problems faced by previous approaches and scales well to high dimensional systems. The resulting control algorithm is tested on non-linear systems in robotics and biomechanics to demonstrate feasibility and out-performance against previous methods.", "review": "Review:###**Summary:** The paper contains a method tailored to a certain kind of SOC problems involving multiplicative noise. The central idea is to use a recurrent network to transform the observations into a representation that can be used with solvers specifically tailored towards that class of problems. **Decision:** I recommend to accept the paper for publication. **Arguments for decision:** The paper clearly adresses an important problem and poroposes a method capable of solving it. The method appears to be theoretically founded and the experimental validation seems solid. The relevance of the method is there as the problem class is prevalent in practical applications. The venue is a good fit as well, as the focus is the representation of a control problem in a way that allows more efficient solutions. **Feedback for improvement:** - The type setting could be improved at times. E.g. below equation (1). - I feel that the term *exploration* is overloaded. While it serves as an explicit mean to reduced the sample complexity of methods in RL, it appears to be about avoiding premature convergence in this work. I am too unfamiliar with the relevant SOC literature to judge how well the term fits, but coming from a ML background I stumbled over this expression. - Some of the experimental details, e.g. the exact choice of time discretisation, don*t appear motivated well. - The paper needs to respect [1, 2] in the related work and show relations. From the perspective of learning state representations for optimal control, both works are relevant. - Is it necessary to start the discussion from the continuous case? While I appreciate the elegance of starting out with a continuous problem and then discretising at the last step, it felt like a barrier to understanding in my case, as my understanding of continuous optimal control is limited–and I feel the audience of ICLR might have the same problem. **References:** [1] Watter, Manuel, et al. *Embed to control: A locally linear latent dynamics model for control from raw images.* *Advances in neural information processing systems*. 2015. [2] Banijamali, Ershad, et al. *Robust locally-linear controllable embedding.* *arXiv preprint arXiv:1710.05373*(2017)."}
{"id": "iclr2020_138", "title": "Domain-Agnostic Few-Shot Classification by Learning Disparate Modulators | OpenReview", "abstract": "Abstract:###Although few-shot learning research has advanced rapidly with the help of meta-learning, its practical usefulness is still limited because most of the researches assumed that all meta-training and meta-testing examples came from a single domain. We propose a simple but effective way for few-shot classification in which a task distribution spans multiple domains including previously unseen ones during meta-training. The key idea is to build a pool of embedding models which have their own metric spaces and to learn to select the best one for a particular task through multi-domain meta-learning. This simplifies task-specific adaptation over a complex task distribution as a simple selection problem rather than modifying the model with a number of parameters at meta-testing time. Inspired by common multi-task learning techniques, we let all models in the pool share a base network and add a separate modulator to each model to refine the base network in its own way. This architecture allows the pool to maintain representational diversity and each model to have domain-invariant representation as well. Experiments show that our selection scheme outperforms other few-shot classification algorithms when target tasks could come from many different domains. They also reveal that aggregating outputs from all constituent models is effective for tasks from unseen domains showing the effectiveness of our framework.", "review": "Review:######Summary### This paper aims to tackle few-shot classification with many different domains. The idea is to build a pool of embedding models, which are based on the same base network. The models are diversed by their own modulators. The high-level intuition is to let the model pool capture good domain-invariant features by the shared parameters and domain-specific features by the selection network, which is desirable to represent the complex cross-domain task distribution, without a significant increase in the number of parameters. The model includes the embedding networks f_E and a selection network f_S. The overall training process of the proposed method is: (1) Train the base embedding network f_E with the aggregated dataset from multiple domains. (2)Sample one domain to optimize the corresponding embedding model. (3) Build a selection network through cross-domain episodic training. The task representation is calculated by average the embedding vectors from the base network, which resembles the prototype. Then the model with the highest accuracy on the query set is selected to compute the labels for the target domain. For the embedding model f_E, the paper provides two architectures, one with convolution 1x1 and the other with Channel-wise transform, denoted as DoS and DoS-CH, respectively. Instead of selecting the model which has the highest accuracy, the averaging model generates an output by averaging class prediction probabilities of all constituent models, denoted by DoA and DoA-CH, respectively. The baseline used in this paper includes Fine-tune, Simple-Avg, ProtoNet, FEAT, ProtoMAML. The paper performs extensive experiments on a batch of datasets, including Aircraft, CIFAR100, DTD, GTSRB, ImageNet12, Omniglot, SVHN, UCF101, and Flowers. The 5-way 1 shot and 5 way 5 shot experimental results demonstrate that the proposed DoS and DoS-CH can outperform other baselines in the *see domains* setting. However, the results on *unseen domains* experiments are worse than the averaging baselines (DoA, DoA-Ch). The paper also surveys the few-shot classification results on a varying number of source domains to show that DoA and DoA-ch are robust to deal with different settings. ### Novelty ### The paper composes of two sub-network, with one baseline network to extract the commonsense knowledge for different datasets and another selection network to select the best model from the model pools for each specific domain. The idea of leveraging multiple modulators for domain-agnostic image recognition is interesting and heuristic, thus the proposed framework shows some novelty. ###Clarity### Overall, the paper is readable and logically clear. The images are well-presented and well-explained by the captions and the text. While this paper misses many prior works both in the track of domain-agnostic learning and few-shot learning. I would recommend the authors to the following materials: a). Domain Agnostic Learning with Disentangled Representations, ICML 2019. https://arxiv.org/pdf/1904.12347.pdf b). Generalizing from a Few Examples: A Survey on Few-Shot Learning, https://arxiv.org/pdf/1904.05046.pdf There also exist some grammar mistakes and typoes in the paper. It will be better to revise and polish the paper. ###Pros### 1) The paper proposes a framework that includes two parts, i.e. the base network and the selection network. The idea is to make deep models more robust to different image domains, which is interesting and heuristic. 2) The paper provides extensive experiments on a wide range of datasets. The experiments demonstrate the effectiveness of the proposed method. 3) The paper is applicable to many practical scenarios since the data from the real-world application is complicated. ###Cons### 1) The critical component of the proposed method is the selection network. However, the experiments on the *novel domains* show that the proposed DoS and DoS-Ch works worse than just averaging the outputs of the models in the model pool. 2) The paper should explain the details about the baseline experiments. Since the proposed models have much more parameters than the baselines, what*s the effect of the auxiliary parameters? Is the comparison between the baselines and the proposed method fair? 3) The presentation of the proposed paper should be polished. Many critical techniques used in this paper are not well-explained, such as the ProtoNet. There exist some grammar mistakes and typos that need a revision. Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs."}
{"id": "iclr2020_139", "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding | OpenReview", "abstract": "Abstract:###Human observers can learn to recognize new categories of objects from a handful of examples, yet doing so with machine perception remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable, as suggested by recent perceptual evidence. We therefore revisit and improve Contrastive Predictive Coding, a recently-proposed unsupervised learning framework, and arrive at a representation which enables generalization from small amounts of labeled data. When provided with only 1% of ImageNet labels (i.e. 13 per class), this model retains a strong classification performance, 73% Top-5 accuracy, outperforming supervised networks by 28% (a 65% relative improvement) and state-of-the-art semi-supervised methods by 14%. We also find this representation to serve as a useful substrate for object detection on the PASCAL-VOC 2007 dataset, approaching the performance of representations trained with a fully annotated ImageNet dataset.", "review": " This paper improves Contrastive Predictive Coding method and reaches a good performance in several downstream tasks. However, the novelty and technical contributions are limited. Strengths: + The experimental results seem good. The reimplemented CPC v2 performs much better than the original version. And the performance of down-stream tasks is comparable or better than the state-of-the-art methods. + The paper is well written. The paper structure is clear and figures are well illustrated. + Figure 3 shows clearly the performance improvements of a series of incremental modifications to the original CPC methods. Weaknesses: - The novelty and technical contributions are limited. This paper only proposes some minor improvements based on the original CPC method and use a deeper network to get better performance. The proposed method lacks of important insights for the research community. - The capacity of network architecture is crucial for self-supervised learning. But in Table 1,2,3, the network architecture of the proposed method is deeper than that in the comparison methods, which is unfair for the comparison methods. Meanwhile, the network architectures of many compared methods are not listed in the tables, which may be misleading. For example, Unsupervised Data Augmentation (Xie et al., 2019) in table 2 and Instance Discrimination (Wu et al., 2018) in table 3 use ResNet50, which is much more shallow than ResNet-161 in this paper. - In section 2.1, the paper doesn*t describe clearly what*s the input of masked convolutional network and how to calculate ."}
{"id": "iclr2020_140", "title": "Classification-Based Anomaly Detection for General Data | OpenReview", "abstract": "Abstract:###Anomaly detection, finding patterns that substantially deviate from those seen previously, is one of the fundamental problems of artificial intelligence. Recently, classification-based methods were shown to achieve superior results on this task. In this work, we present a unifying view and propose an open-set method to relax current generalization assumptions. Furthermore, we extend the applicability of transformation-based methods to non-image data using random affine transformations. Our method is shown to obtain state-of-the-art accuracy and is applicable to broad data types. The strong performance of our method is extensively validated on multiple datasets from different domains.", "review": "Review:###This paper proposes a novel approach to classification-based anomaly detection for general data. Classification-based anomaly detection uses auxiliary tasks (transformations) to train a model to extract useful features from the data. This approach is well-known in image data, where auxiliary tasks such as classification of rotated or flipped images have been demonstrated to work effectively. The paper generalizes to the task by using the affine transformation y = Wx+b. A novel distance-based classification is also devised to learn the model in such as way that it generalizes to unseen data. This is achieved by modeling the each auxiliary task subspace by a sphere and by using the distance to the center for the calculation of the loss function. The anomaly score then becomes the product of the probabilities that the transformed samples are in their respective subspaces. The paper provides comparison to SOT methods for both Cifar10 and 4 non-image datasets. The proposed method substantially outperforms SOT on all datasets. A section is devoted to explore the benefits of this approach on adversarial attacks using PGD. It is shown that random transformations (implemented with the affine transformation and a random matrix) do increase the robustness of the models by 50%. Another section is devoted to studying the effect of contamination (anomaly data in the training set). The approach is shown to degrade more gracefully than DAGMM on KDDCUP99. Finally, a section studies the effect of the number of tasks on the performance, showing that after a certain number of task (which is probably problem-dependent), the accuracy stabilizes. PROS: * A general and novel approach to anomaly detection with SOT results. * The method allows for any type of classifier to be used. The authors note that deep models perform well on the large datasets (KDDCUP) while shallower models are sufficient for smaller datasets. * The paper is relatively well written and easy to follow, the math is clearly laid out. CONS: * The lack of a pseudo-code algorithm makes it hard to understand and reproduce the method * Figure 1 (left) has inverted colors (DAGMM should be blue - higher error). * Figure 1 (right) - it is unclear what the scale of the x-axis is since there is only 1 label. Also the tick marks seem spaced logarithmically, which, if i understand correctly, is wrong. * The paragraph *Number of operations* should be renamed *Number of tasks* to be consistent. Also the sentence *From 16 ...* should be clarified, as it seems to contrast accuracy and results, which are the same entity. The concept of *stability of results* is not explained clearly. It would suffice to say: *From 16 tasks and larger, the accuracy remains stable*. * In section 6, the paragraph *Generating many tasks* should be named *Number of tasks*, to be consistent with the corresponding paragraph in section 5.2. Also the first sentence should be: *As illustrated in Figure 1 (right), increasing the number of tasks does result in improved performance but the trend is not linear and beyond a certain threshold, no improvements are made. And again the concept of *stability* is somewhat misleading here. The sentence *...it mainly improves the stability of the results* is wrong. The stability is not improved, it is just that the performance trend is stable. * The study on the number of tasks should be carried on several datasets. Only one dataset is too few to make any claims on the accuracy trends as the number of task is increased. * The authors should coin an acronym to name their methods. Overall this paper provides a novel approach to classification-based semi-supervised anomaly detection of general data. The results are very encouraging, beating SOT methods by a good margin on standard benchmarks."}
{"id": "iclr2020_141", "title": "Stochastic Latent Residual Video Prediction | OpenReview", "abstract": "Abstract:###Video prediction is a challenging task: models have to account for the inherent uncertainty of the future. Most works in the literature are based on stochastic image-autoregressive recurrent networks, raising several performance and applicability issues. An alternative is to use fully latent temporal models which untie frame synthesis and dynamics. However, no such model for video prediction has been proposed in the literature yet, due to design and training difficulties. In this paper, we overcome these difficulties by introducing a novel stochastic temporal model. It is based on residual updates of a latent state, motivated by discretization schemes of differential equations. This first-order principle naturally models video dynamics as it allows our simpler, lightweight, interpretable, latent model to outperform prior state-of-the-art methods on challenging datasets.", "review": "Review:###Contributions: this submission proposes a video pixel generation framework with the goal to decouple visual appearance and dynamics. The latent dynamics are modeled with a latent residual dynamics model. Empirical evaluations on moving MNIST show that the proposed residual dynamics model outperform MLP or GRU. On more challenging KTH and BAIR datasets, the proposed method achieves on par or better quantitative performance with previous methods, and have nice qualitative results on content *swap* and dynamics interpolation. Assessment: - To my knowledge, the proposed model is novel for video generation. - The proposed method is supported with strong quantitative results and qualitative analysis, ablation on Moving MNIST shows that the proposed latent residual dynamics model outperforms MLP and GRU baselines. - The authors might be interested in related work on video generation with decoupled appearance and dynamics models, such as [1]. It would also be interesting to see evaluation on more challenging datasets, such as Human3.6M. - Question: how does the proposed inference framework make sure to decouple appearance with dynamics? Can y_i not encode the appearance information? [1] Minderer et al., Unsupervised Learning of Object Structure and Dynamics from Videos. NeurIPS 2019. ----------------------------- Post rebuttal: Thank you for your answers to my questions and the updated manuscript. My questions have been addressed and the additional results further confirm the performance of the proposed method. Therefore I recommend weak accept of the submission."}
{"id": "iclr2020_142", "title": "UW-NET: AN INCEPTION-ATTENTION NETWORK FOR UNDERWATER IMAGE CLASSIFICATION | OpenReview", "abstract": "Abstract:###The classification of images taken in special imaging environments except air is the first challenge in extending the applications of deep learning. We report on an UW-Net (Underwater Network), a new convolutional neural network (CNN) based network for underwater image classification. In this model, we simulate the visual correlation of background attention with image understanding for special environments, such as fog and underwater by constructing an inception-attention (I-A) module. The experimental results demonstrate that the proposed UW-Net achieves an accuracy of 99.3% on underwater image classification, which is significantly better than other image classification networks, such as AlexNet, InceptionV3, ResNet and Se-ResNet. Moreover, we demonstrate the proposed IA module can be used to boost the performance of the existing object recognition networks. By substituting the inception module with the I-A module, the Inception-ResnetV2 network achieves a 10.7% top1 error rate and a 0% top5 error rate on the subset of ILSVRC-2012, which further illustrates the function of the background attention in the image classifications.", "review": " This paper proposed an underwater image classification network. The current manuscript missed some very important information (see below). Besides, the experimental results are also weak. The paper mentioned *complex distortions existed in underwater images (e.g., low contrast, blurring, non-uniform brightness, non-uniform color casting and noises)* many times. But when the paper introduced the UW-Net structure, it does not explain how the network over-comes these difficulties. The UW-Net structure only considers the factors of the background and attention. Thus I think the proposed network structure is not convincing. For the experimental part, I am afraid the results are also weak. For example, please notice that many network structures have proposed to improve the classification. I think authors should compare more existing works to demonstrate the superiority of the proposed one."}
{"id": "iclr2020_143", "title": "Adversarial Paritial Multi-label Learning | OpenReview", "abstract": "Abstract:###Partial multi-label learning (PML), which tackles the problem of learning multi-label prediction models from instances with overcomplete noisy annotations, has recently started gaining attention from the research community. In this paper, we propose a novel adversarial learning model, PML-GAN, under a generalized encoder-decoder framework for partial multi-label learning. The PML-GAN model uses a disambiguation network to identify noisy labels and uses a multi-label prediction network to map the training instances to the disambiguated label vectors, while deploying a generative adversarial network as an inverse mapping from label vectors to data samples in the input feature space. The learning of the overall model corresponds to a minimax adversarial game, which enhances the correspondence of input features with the output labels. Extensive experiments are conducted on multiple datasets, while the proposed model demonstrates the state-of-the-art performance for partial multi-label learning.", "review": "Review:###The motivation of this paper is to handle noise candidate multi-labels by co-training two networks. The work trains the disambiguation network, which learns to predict the probability of each class label being the additive irrelevant label and then is used to get the disambiguated label confidence vector, and the prediction network, which learns to predict the probability of the disambiguated labels. The additional adversarial loss and generation loss is aimed to enhance the label disambiguation by learning the mapping from the disambiguated labels to the input features. Pros: By minimizing the disagreement between the confidence of being the disambiguated label and the predicted probability of each label, the partial multi-label learning gets better results. The experiments in this paper are complete and thorough. The authors have tested the model in many datasets and designed the ablation study to verify the effect of each loss. And the proposed model achieved the state-of-art results. Cons: However, the effect of the adversarial loss and the generation loss is doubtful because: 1) The mapping from the labels to the input features is hard to learn since the label space does not contain the complete information of input features. It is doubtful that the generation is helpful. 2) The results of the ablation study do not show consistently significant improvements. 3) In the appendix, the variant of PML-GAN, which considers an auxiliary classification loss on the generated data, has little improvement compared to PML-GAN. The author claims that it is because of sufficient training data. What if considering the insufficient training data and testing the variant of PML-GAN? I think it can somehow verify the effect of the generation. About the writing of this paper, the motivation of the work is not clearly defined. Although we can get what the work was done, we cannot get why the work did this."}
{"id": "iclr2020_144", "title": "WHITE NOISE ANALYSIS OF NEURAL NETWORKS | OpenReview", "abstract": "Abstract:###A white noise analysis of modern deep neural networks is presented to characterize their biases at the whole network level or the single neuron level. Our analysis is based on two popular and related methods in psychophysics and neurophysiology namely classification images and spike triggered analysis. These methods have been widely used to understand the underlying mechanisms of sensory systems in humans and monkeys. We leverage them to investigate the inherent biases of deep neural networks and to obtain a first-order approximation of their functionality. We emphasize on convolutional neural networks (CNNs) since they are currently the state of the art methods in computer vision and are a decent model of human visual processing. In addition, we study multi-layer perceptrons, logistic regression, and recurrent neural networks. Experiments over four classic datasets, MNIST, Fashion-MNIST, CIFAR-10, and ImageNet, show that the computed bias maps resemble the target classes and when used for classification lead to an over two-fold performance than the chance level. Further, we show that classification images can be used to attack a black-box classifier and to detect adversarial patch attacks. Finally, we utilize spike triggered averaging to derive the filters of CNNs and explore how the behavior of network changes when neurons in different layers are modulated. Our effort illustrates a successful example of borrowing from neuroscience to study ANNs and highlights the importance of cross-fertilization and synergy across machine learning, deep learning, and computational neuroscience.", "review": "Review:###Summary: This paper introduces two tools from spike analysis to understand the bias neural networks have. The first tool is classification images and the second, spike-triggered analysis. The broad goal of the paper is to add more tooling to add interpretability and robustness to a neural network. Classification images can be summarized as: Produce a stimulus from the sum of a signal image and a noise image. Then average the response over many trials to determine what template the network used. Spike-triggered analysis can be understood as measuring a neuron*s response to time-varying stimuli (ie: neuron responding to a line) which scopes out the receptive field. The paper is well written. Experimentation section is thorough. The related work is well discussed. The overall techniques are interesting and can help the community think about interpretability by using tools from related disciplines. Decision: Accept Reasoning: Interpretability is a very important problem and the authors present ways of thinking about it from the lens of computational neuroscience. This paper has the potential to inspire future research in this direction."}
{"id": "iclr2020_145", "title": "MLModelScope: A Distributed Platform for ML Model Evaluation and Benchmarking at Scale | OpenReview", "abstract": "Abstract:###Machine Learning (ML) and Deep Learning (DL) innovations are being introduced at such a rapid pace that researchers are hard-pressed to analyze and study them. The complicated procedures for evaluating innovations, along with the lack of standard and efficient ways of specifying and provisioning ML/DL evaluation, is a major *pain point* for the community. This paper proposes MLModelScope, an open-source, framework/hardware agnostic, extensible and customizable design that enables repeatable, fair, and scalable model evaluation and benchmarking. We implement the distributed design with support for all major frameworks and hardware, and equip it with web, command-line, and library interfaces. To demonstrate MLModelScope*s capabilities we perform parallel evaluation and show how subtle changes to model evaluation pipeline affects the accuracy and HW/SW stack choices affect performance.", "review": "Review:###The paper presents a unified approach to specify, evaluate and benchmark different ML methods. With the main goal of enforcing repeatability and faireness when testing different methods, authors propose an open source runtime on which 1) specify the model, 2) describe the workflow and 3) evaluate the benchmark of several ML algorithms and frameworks. The core of the work is the definition of the so-called *model evaluation manifest* which consists of a formatted collection of descriptive information where both hardware/software and framework versions are specified, along with the set of tasks to be carried on as well as the data sources to test the methods against. Once the manifest has been created, the desired hw/sw configuration is deployed on Amazon and the specified models are benchmarked. This benchmarking offers several insights on the evaluation of a given ML model, by stressing out the importance of aspects that can severely bias the final outcome of the model (e.g., pre-processing tasks, different hardware configurations or normalization of the data). To describe the workflow, authors use an image classifier on a given hardware as a running example, and play with different preprocessing methods to measure their impact on the final accuracy of the model. Some details are not well specified/clear in the work: 1) Data exploitation. There is the possibility of testing different methods on own datasets. Given that the deployment is run on Amazon instances, what are the requirements (e.g., data must be on S3 and so on). 2) The manifest can be injected with python scripts that, running in a container, perform the desired operations (preprocessing). It is stated that *parameters are passed by reference*. So if you pass a *mutable* object (*env*, I guess) you need to bind it to the outher scope. How this is accomplished? (globals?) Instead, if you pass an *immutable* object (*data*, I guess), you cannot rebind the outer reference nor mutate the object. So, what*s the meaning of *passing by reference*? 3) Privacy and anonymity. When performing debugging, system, framework and model level profiling information are collected on a tracing server. Is this server part of the platform?"}
{"id": "iclr2020_146", "title": "MUSE: Multi-Scale Attention Model for Sequence to Sequence Learning | OpenReview", "abstract": "Abstract:###Transformers have achieved state-of-the-art results on a variety of natural language processing tasks. Despite good performance, Transformers are still weak in long sentence modeling where the global attention map is too dispersed to capture valuable information. In such case, the local/token features that are also significant to sequence modeling are omitted to some extent. To address this problem, we propose a Multi-scale attention model (MUSE) by concatenating attention networks with convolutional networks and position-wise feed-forward networks to explicitly capture local and token features. Considering the parameter size and computation efficiency, we re-use the feed-forward layer in the original Transformer and adopt a lightweight dynamic convolution as implementation. Experimental results show that the proposed model achieves substantial performance improvements over Transformer, especially on long sentences, and pushes the state-of-the-art from 35.6 to 36.2 on IWSLT 2014 German to English translation task, from 30.6 to 31.3 on IWSLT 2015 English to Vietnamese translation task. We also reach the state-of-art performance on WMT 2014 English to French translation dataset, with a BLEU score of 43.2.", "review": "Review:###The proposes a transformer layer that performs self-attention, feed-forward and dynamic-convolution all in parallel. The dynamic convolution consists of several different kernel sizes, which are then combined by learned weights. Experiments are done on three NMT tasks. In terms of novelty, there is very little. The model essentially combines DynamicConv from Wu et.al with self-attention. More importantly, it*s not clear why that*s better. The only direct comparison to Wu et.al. is the EN-FR translation task, but the proposed model doesn*t not outperform DynamicConv, even though having more parameters. Another novelty in the paper is placing feedforward in parallel to self-attention, which seems to improve the performance. This is probably why the model is more stable compared to Transformer because it has essentially 2x less depth, while maintaining the same capacity. As I read the paper, it felt bit rushed. For example, page 2 top says *language modeling*, but I can*t find any language modeling experiment. Figure 3 is never mentioned in the text. Also, the equation numbers in *weight tying* paragraph doesn*t match, and Eq8 is missing an operator (probably ReLU). In page6, the authors say *DynamicConv diverges*, but I can*t find that result. Also it said *The small version of MUSE still beats Transformer by a large margin.*, but where is that result? From table 3, it looks like they match in performance. Other comments: - The authors claim their architecture allow the feed-forward module better access to token features, but I think that*s only true for the first layer. Besides, Transformer has skip-connection that can directly feed token features to all layers. - Why W_in is tied to W^V? Why not W^K for example? - Why alpha is initialized to 1/n as if they are probabilities? When you pass alpha through softmax, any constant offset will have no effect on the output. - What are the kernel sizes used? - Type: *... dataset, We ...* - Figure 4 caption says *dynamically*, but my understanding is that the kernel weights are just learned (adaptive) and doesn*t dynamically change with inputs."}
{"id": "iclr2020_147", "title": "LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS | OpenReview", "abstract": "Abstract:###In this paper, we discuss the fundamental problem of representation learning from a new perspective. It has been observed in many supervised/unsupervised DNNs that the final layer of the network often provides an informative representation for many tasks, even though the network has been trained to perform a particular task. The common ingredient in all previous studies is a low-level feature representation for items, for example, RGB values of images in the image context. In the present work, we assume that no meaningful representation of the items is given. Instead, we are provided with the answers to some triplet comparisons of the following form: Is item A more similar to item B or item C? We provide a fast algorithm based on DNNs that constructs a Euclidean representation for the items, using solely the answers to the above-mentioned triplet comparisons. This problem has been studied in a sub-community of machine learning by the name *Ordinal Embedding*. Previous approaches to the problem are painfully slow and cannot scale to larger datasets. We demonstrate that our proposed approach is significantly faster than available methods, and can scale to real-world large datasets. Thereby, we also draw attention to the less explored idea of using neural networks to directly, approximately solve non-convex, NP-hard optimization problems that arise naturally in unsupervised learning problems.", "review": "Review:###The paper proposes to use the triplet loss as a convex relaxation of the ordinal embedding problem. The loss is solved using feed-forward neural network with the input to the network being the ids of the items encoded in binary codes. The benefit of using a deep network is to exploit its optimization capability and the parallelism on GPUs. The experiments presented in the paper include a set of simulation experiments and a real-world task. I am giving a score of 3. This work is an interesting application of deep learning, but it gives little insight as to why deep networks are able to solve the problem and how to solve ordinal embedding itself. To elaborate, the problem is known to be NP-hard in the worst case, while the data sets used in the paper seem to have certain nice properties. It would be interesting to see how deep networks do for the hard cases. It would also be interesting to see if additional assumptions, such as the existence of clusters or separation between clusters, make ordinal embedding simpler and thus tractable. Another approach is to assume the solution to have low surrogate loss (4), and any convex solver with sufficiently large number of points is able to find such a solution. Then the question becomes how deep networks solve the particular convex optimization problem. Thinking along these directions would bring more insight and impact to both the ordinal embedding problem and optimization in deep networks. one quick question: equations (3) and (4) --> isn*t this the same as using the hinge loss to bound the zero-one loss?"}
{"id": "iclr2020_148", "title": "GRAPH ANALYSIS AND GRAPH POOLING IN THE SPATIAL DOMAIN | OpenReview", "abstract": "Abstract:###The spatial convolution layer which is widely used in the Graph Neural Networks (GNNs) aggregates the feature vector of each node with the feature vectors of its neighboring nodes. The GNN is not aware of the locations of the nodes in the global structure of the graph and when the local structures corresponding to different nodes are similar to each other, the convolution layer maps all those nodes to similar or same feature vectors in the continuous feature space. Therefore, the GNN cannot distinguish two graphs if their difference is not in their local structures. In addition, when the nodes are not labeled/attributed the convolution layers can fail to distinguish even different local structures. In this paper, we propose an effective solution to address this problem of the GNNs. The proposed approach leverages a spatial representation of the graph which makes the neural network aware of the differences between the nodes and also their locations in the graph. The spatial representation which is equivalent to a point-cloud representation of the graph is obtained by a graph embedding method. Using the proposed approach, the local feature extractor of the GNN distinguishes similar local structures in different locations of the graph and the GNN infers the topological structure of the graph from the spatial distribution of the locally extracted feature vectors. Moreover, the spatial representation is utilized to simplify the graph down-sampling problem. A new graph pooling method is proposed and it is shown that the proposed pooling method achieves competitive or better results in comparison with the state-of-the-art methods.", "review": " ============ After rebuttal ============ I thank the authors for carefully discussing the points of my review. I have upgraded the score to marginal acceptance (6). ============ Original review ============ In this paper, the authors identify a shortcoming of existing GNN architectures for graph classification tasks -- specifically, the fact that, in the featureless regime, the graph convolutional layers rely on propagating very rudimentary structural information, making it hard (or impossible) to distinguish graphs with similar local structure. To fix the problem, the authors propose to augment the input feature space with graph-structural embeddings (computed by an algorithm like DeepWalk), and processing those in parallel with any other input features available. On existing real-world datasets, as well as synthetic datasets carefully constructed to illustrate this phenomenon, the proposed pipeline matches or exceeds the version without the structural embedding inputs. Further, the authors note that the structural embeddings could be used to propose a novel graph pooling method -- one which attempts to preserve as diverse structural feature sets as possible. It is shown that this method is competitive to other differentiable pooling methods, like DiffPool and Graph U-Nets. Lastly, the authors demonstrate that the addition of pooling layers does not help baseline GNNs on the synthetically constructed tasks, as the fundamental issue of handling similar local structures is still not addressed. I believe that the paper clearly exposes and proposes a nice idea which could hold great potential, and which can be useful to graph representation learning practitioners. I am particularly happy with the design of the synthetic experiments. However, I find that, in its current form, the manuscript is narrowly below the bar for a venue like ICLR. Comments: * The observation that existing GNN layers may struggle with distinguishing featureless graphs is not particularly novel. It*s largely the centerpiece of the (already cited) work of Xu et al. (ICLR 2019), and I believe that its relevance and relation to the authors* work should be better stressed in the related work section. * The usage of DeepWalk to encode structural information (and even to be used as initial features for a GNN) is, ultimately, also not necessarily a novel idea. At least, it*s something that should be clear to any expert GNN practitioner already: if useful features are missing from the graph, a method like DeepWalk (if applicable; see below!) could be a go-to method for obtaining such features. In its current form, I don*t see that the authors are proposing anything substantially architecturally novel, and their contribution is primarily on the data/feature engineering side. * The above point is not necessarily problematic, but if the aim is to stress the importance of the architectural novelties of the proposed GNN-ESR model more, and not just the added features, I would recommend the authors to perform a few ablations: e.g. seeing how well would processing a concatenation of E and F in the same GNN layer perform. * Many of the standard graph classification datasets are known to be noisy and unreliable (see e.g. Luzhnica, Day and Liò (ICML GraphReasoning Workshop 2019). This means that it is a must to report error bars of the cross-validation experiments. It*s hard to say that many of the improvements depicted here are statistically significant otherwise. * I have concerns about the computational complexity, or even feasibility, of using DeepWalk-like methods in the general case, e.g. for node classification. Namely, if such layers are to be applied in inductive settings (with nodes gradually added to graphs), one would require re-running DeepWalk every time a new node is added. The authors should comment on this adequately, and perhaps discuss the feasibility of other unsupervised embedding techniques for obtaining the e-vectors -- such as VGAE (Kipf and Welling, NIPS BDL 2016), GraphSAGE (Hamilton et al., NIPS 2017), Graph2Gauss (Bojchevski and Günnemann, ICLR 2018) or DGI (Veli?kovi? et al., ICLR 2019). * While I find the proposed pooling method interesting (and more grounded in the graph*s structural features than other proposed works), I find that there are many potential limitations to be discussed. For example, the fact that we start from a random first index means that we cannot rely on the obtained pooling to always be the same -- could this cause undesirable variance at test time? Furthermore, the downsampling from A^3 is a sure-fire way to obtain dense graphs after the first pooling -- potentially severely limiting the applicability of the method for large graphs. In my opinion, the authors should appropriately comment on these and perform ablations against pooling with A and A^2 (as was done in Graph U-Nets). It should also be interesting to note that there exist other structurally-informed pooling methods; see e.g. the Clique pooling method from Luzhnica, Day and Liò (ICLR RLGM 2019). Given all of the above, I recommend (marginal) rejection, but am open to improving my score if the authors appropriately address the aforementioned comments. Some minor comments and thoughts: * The paper has several typos and grammar issues, and a typo pass is highly encouraged to aid clarity; * The *attention mechanism* of Equation (5--6) seems to be nonparametric? If so, it might be interesting to compare with a version that features learnable queries, e.g. using the Transformer-style attention. * In Equation (3), should the first min actually be a max? As we*re maximising the overall minimum distances between topological features. * I*m not sure that the paper is anywhere clear on what*s the exact GNN layer being used. Could this be clarified and made more explicit? It*s critical to reproducibility. * I find it curious that the authors needed to resort to using batch normalisation---I usually found it to either have no meaningful effect on the results on the graph classification benchmarks, or it made results worse. Can the authors comment on this decision? * The idea to concatenate output of all convolutional layers is heavily resembling Jumping Knowledge networks (Xu et al., ICML 2018), and I believe they should be appropriately cited."}
{"id": "iclr2020_149", "title": "Non-Autoregressive Dialog State Tracking | OpenReview", "abstract": "Abstract:###Recent efforts in Dialogue State Tracking (DST) for task-oriented dialogues have progressed toward open-vocabulary or generation-based approaches where the models can generate slot value candidates from the dialogue history itself. These approaches have shown good performance gain, especially in complicated dialogue domains with dynamic slot values. However, they fall short in two aspects: (1) they do not allow models to explicitly learn signals across domains and slots to detect potential dependencies among (domain, slot) pairs; and (2) existing models follow auto-regressive approaches which incur high time cost when the dialogue evolves over multiple domains and multiple turns. In this paper, we propose a novel framework of Non-Autoregressive Dialog State Tracking (NADST) which can factor in potential dependencies among domains and slots to optimize the models towards better prediction of dialogue states as a complete set rather than separate slots. In particular, the non-autoregressive nature of our method not only enables decoding in parallel to significantly reduce the latency of DST for real-time dialogue response generation, but also detect dependencies among slots at token level in addition to slot and domain level. Our empirical results show that our model achieves the state-of-the-art joint accuracy across all domains on the MultiWOZ 2.1 corpus, and the latency of our model is an order of magnitude lower than the previous state of the art as the dialogue history extends over time.", "review": "Review:###The authors build on recent work for non-autoregressive encoder-decoder models in the context of machine translation (most significantly [Gu, et al., ICLR18]) and adapt this to dialogue state tracking. Specifically, as in [Gu, et al, ICLR18], they use a fertility decoder modified for DST to be on a per-slot basis which is input to a second decoder to generate the (open-vocabulary) tokens representing dialogue state. An interesting aspect of the resulting model as formulated is that the latent space also takes into account interdependencies between generated slot values, which leads to a direct structured prediction-like loss of joint accuracy. Additionally, as slot values have a smaller (and likely peakier) combinatorial space, NAT models actually are more applicable to DST than MT. The resulting model achieves state-of-the-art empirical results on the MultiWOZ dataset while incurring decoding times that are an order of magnitude faster. Once one understands this conceptual modification of modifying the NAT string encoder-decoder to a more structured NAT encoder-decoder (which in DST is more of a normalized string), they apply all of the state-of-the-art techniques to build a DST system: gating of all potential (domain, slot) pairs as an auxiliary prediction (e.g., [Wu et al., ACL19]), de-lexicalizing defined value types [Lei, et al., ACL18], content encoder, and domain-slot encoder (with pretty standard hyper-parameters, etc.). The significant addition is the fertility decoder and the associated NAT state decoder. Thus, from a conceptual level, this isn’t a huge leap and something many researchers *could* have done (i.e., I think many people, including myself, to have expected this paper to come out) — thus, it is more of a ‘done first’ paper than a ‘done unexpectedly’ paper. However, it is done well and the results are convincing and interesting. Given the impressive performance, I expect others to continue building on this work and potentially even influencing people to combine encoder-decoder models with a more structured prediction approach to DST. Thus, I would prefer to see it accepted if possible. That being said, I do have a few questions regarding this work — but these are more questions that might be able to be addressed than actual criticisms per se. First, in Table 5, why without the delexicalized dialogue history does the performance drop from 49.04% to 39.45%? This does not make sense to me as the model is much more complex than TRADE; however, TRADE does not do delexicalization yet achieves 45.6% joint accuracy. Meanwhile, with such complex model, I would expect the model can learn from raw data without delexicalization. Moreover, the proposed method use both previous predictions and *previous system actions* to do delexicalization. Also, the NATMT models don’t do delexicalization (although they have significantly more data). I think the authors should do an ablation study that do not use previous system actions, because this is extra information compared with TRADE — even if delexicalizing. Secondly, another worthy baseline would be an autoregressive decoder (with other blocks of the model remain the same). I’d assume that the performance is slight higher. It is interesting to see the gap, because this gap is the cost to speed up decoder using fertility — even if it is a bit counter-intuitive. If there is no improvement in this setting, then structured prediction in general may make more sense. Honestly, I think more would be interested in the second point than the first. In any case, nice paper — well-written, well-motivated, interesting empirical results. The only reason I am recommending ‘weak accept’ is that the novelty is fairly straightforward and the strength is in the execution."}
{"id": "iclr2020_150", "title": "Improving Differentially Private Models with Active Learning | OpenReview", "abstract": "Abstract:###Broad adoption of machine learning techniques has increased privacy concerns for models trained on sensitive data such as medical records. Existing techniques for training differentially private (DP) models give rigorous privacy guarantees, but applying these techniques to neural networks can severely degrade model performance. This performance reduction is an obstacle to deploying private models in the real world. In this work, we improve the performance of DP models by fine-tuning them through active learning on public data. We introduce two new techniques - DiversePublic and NearPrivate - for doing this fine-tuning in a privacy-aware way. For the MNIST and SVHN datasets, these techniques improve state-of-the-art accuracy for DP models while retaining privacy guarantees.", "review": "Review:###This paper considers the problem of how to improve the performance of an existing DP classifier, with the help of the labelled public data. The paper considers the following process: 1. find a public dataset containing relevant samples; 2. carefully select a subset of public samples that can improve the performance of the classifier; 3 fine-tune the existing classifier on the selected samples. Two different techniques from active learning are utilized in order to select representative samples from a public dataset and fine-tune a DP classifier. This paper also conducts some experiments on the MNIST and SVHN datasets and demonstrates improvement compared with the benchmark. I vote for rejecting for this paper, because of the following two concerns: 1. I do not think this paper has made a lot of contribution to either differential privacy or active learning. It just *borrows* some fine-tuning techniques from active learning and apply it in DP. There is almost no theoretical contribution made by this paper. Besides, from the experimental perspective, neither can I see an obvious improvement compared with the benchmarks. 2. I do not think the privacy analysis of the NearPrivate algorithm (Algorithm 2) is correct. The paper uses private argmax algorithm and claims that it satisfies -DP. However, this is only true when . Generally, it should satisfy -DP. So if we look at the experimental setting of MNIST, roughly thousand times less noise is added! Since this amount of noise is non-trivial at all, I can not judge the effectiveness of the algorithm. ------------------------------------------------------------------------------------------ Thanks for the authors* classification. I missed the part that each private sample was only assigned to one public sample. Now I can confirm the correctness of the algorithm and increase my score accordingly."}
{"id": "iclr2020_151", "title": "Semi-supervised Autoencoding Projective Dependency Parsing | OpenReview", "abstract": "Abstract:###We describe two end-to-end autoencoding models for semi-supervised graph-based dependency parsing. The first model is a Local Autoencoding Parser (LAP) encoding the input using continuous latent variables in a sequential manner; The second model is a Global Autoencoding Parser (GAP) encoding the input into dependency trees as latent variables, with exact inference. Both models consist of two parts: an encoder enhanced by deep neural networks (DNN) that can utilize the contextual information to encode the input into latent variables, and a decoder which is a generative model able to reconstruct the input. Both LAP and GAP admit a unified structure with different loss functions for labeled and unlabeled data with shared parameters. We conducted experiments on WSJ and UD dependency parsing data sets, showing that our models can exploit the unlabeled data to boost the performance given a limited amount of labeled data.", "review": "Review:###The paper proposes two semi-supervised (projective dependency) parsing models, which are first-order graph-based (only head-modiffier scores are used). Both models, local and global, are auto-encoders. The latent variable in the local model is to represent words, thus the model generates trees sequentially. The latent variable in the global model is to present a whole tree, thus the model can consider all the dependency scores at once. One of the key contributions is that, thanks to the first order property, the authors can make use of available dynamic programming algorithms to compute expectations tractably. The experiments show that both the models work across several languages. I would accept the paper because: - it*s well written, with clear motivations - the idea of using dynamic programming to compute the global model tractably is thoughtful and innovative. The math details seem correct to me (though I didn*t check them carefully) - the experimental results do support that the semi-supervised learning does work, and are consistent across several languages. I would reject the paper because: - technically speaking, tractability is very cool, but in this case the global model has to sacrifice the expressiveness of higher-order parsing. It*s very unclear to me if that*s worth or not. The experimental results do not say anything. - I don*t know why the authors split 10% - 90% for labelled and unlabelled sentences. The gain (L+U vs L) seems not impressive to me. It is also unclear to me where the gain is from. I suggest to adjust this ratio (e.g. the portion of labeled sentences if from 1% to 100%) Typos: - Algorithm 1 and the text should be consistent with each other for the use of the symbol of the encoder*s parameters. Question: - The two models are graph-based, thus they not necessarily projective. I see why the projectivity is needed for the variant of inside-outside algorithm. But I*m wondering if there are any dynamic programming algorithms for graph-based non-projective parsing?"}
{"id": "iclr2020_152", "title": "Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets | OpenReview", "abstract": "Abstract:###Adaptive gradient algorithms perform gradient-based updates using the history of gradients and are ubiquitous in training deep neural networks. While adaptive gradient methods theory is well understood for minimization problems, the underlying factors driving their empirical success in min-max problems such as GANs remain unclear. In this paper, we aim at bridging this gap from both theoretical and empirical perspectives. Theoretically, we develop an algorithm (Optimistic Stochastic Gradient, OSG) for solving a class of non-convex non-concave min-max problem and establish complexity for finding -first-order stationary point, in which only one stochastic first-order oracle is invoked in each iteration. An adaptive variant of the proposed algorithm (Optimistic Adagrad, OAdagrad) is also analyzed, revealing an emph{improved} adaptive complexity ~footnote{Here compresses a logarithmic factor of .}, where characterizes the growth rate of the cumulative stochastic gradient and . To the best of our knowledge, this is the first work for establishing adaptive complexity in non-convex non-concave min-max optimization. Empirically, our experiments show that indeed adaptive gradient algorithms outperform their non-adaptive counterparts in GAN training. Moreover, this observation can be explained by the slow growth rate of the cumulative stochastic gradient, as observed empirically.", "review": "Review:###This paper proposes two methods named OSG and OAdagrad for solving stochastic non-convex non-concave min-max problems. In theoretical analyses, a convergence rate of and a much better rate are provided for OSG and OAdagrad, respectively, to find the -accurate first-order stationary point. Finally, the superior performance of the proposed method is empirically verified on training generative adversarial networks (GANs). Clarity: The paper is well organized and easy to read. Quality: The work is of good quality and is technically sound. However, I did not verify the proof in detail. Significance: A stochastic non-convex non-concave minimax problem studied in this paper is recently considered as an important class of the optimization problems because important machine learning problems such as GANs fall into this class and most past papers studied convex-concave min-max problems instead. A few studies [Iusem+(2017), Lin+(2018)] proposed optimization algorithms for this problem and derived convergence rates and , respectively. On the other hand, proposed methods have several preferable properties compared to these methods. For instance, OSG exhibits a comparable convergence rate to [Iusem+(2017)] with a fewer per-iteration complexity and OAdagrad exhibits a much faster convergence rate depending on the parameter that is an order of the growth of the cumulative stochastic gradients norm. In addition, the order of is shown to be slow in general and certainly faster convergence rates are also confirmed in experiments on training GANs. Thus, experimental results seem consistent with the theory. Since a derived convergence rate of OAdagrad is potentially much faster than those of existing methods, I think the OAdagrad is one of the promising methods for training GANs."}
{"id": "iclr2020_153", "title": "Auto Completion of User Interface Layout Design Using Transformer-Based Tree Decoders | OpenReview", "abstract": "Abstract:###It has been of increasing interest in the field to develop automatic machineries to facilitate the design process. In this paper, we focus on assisting graphical user interface (UI) layout design, a crucial task in app development. Given a partial layout, which a designer has entered, our model learns to complete the layout by predicting the remaining UI elements with a correct position and dimension as well as the hierarchical structures. Such automation will significantly ease the effort of UI designers and developers. While we focus on interface layout prediction, our model can be generally applicable for other layout prediction problems that involve tree structures and 2-dimensional placements. Particularly, we design two versions of Transformer-based tree decoders: Pointer and Recursive Transformer, and experiment with these models on a public dataset. We also propose several metrics for measuring the accuracy of tree prediction and ground these metrics in the domain of user experience. These contribute a new task and methods to deep learning research.", "review": "Review:###This paper proposes an autocompletion model for UI layout based on adaptations of Transformers for tree structures and evaluates the models based on a few metrics on a public UI dataset. I like the area of research the authors are looking into and I think it*s an important application. However, the paper doesn*t answer key questions about both the application and the models: 1) There is no clear rationale on why we need a new model based on Transformers for this task. What was wrong with LSTMs/GRUs as they*ve been used extensively for recursive problems including operations on trees? Similarly, I*d have expected baselines that included those models in the evaluation section showing the differences in performance between the newly proposed Transformer model for trees and previously used methods. 2) The evaluation metrics used while borrowed from the language or IR fields doesn*t seem to translate to UI design. UI layout is about visual and functional representation of an application so if one is seeking to evaluate different models, they need to relate to those."}
{"id": "iclr2020_154", "title": "Unsupervised Out-of-Distribution Detection with Batch Normalization | OpenReview", "abstract": "Abstract:###Likelihood from a generative model is a natural statistic for detecting out-of-distribution (OoD) samples. However, generative models have been shown to assign higher likelihood to OoD samples compared to ones from the training distribution, preventing simple threshold-based detection rules. We demonstrate that OoD detection fails even when using more sophisticated statistics based on the likelihoods of individual samples. To address these issues, we propose a new method that leverages batch normalization. We argue that batch normalization for generative models challenges the traditional emph{i.i.d.} data assumption and changes the corresponding maximum likelihood objective. Based on this insight, we propose to exploit in-batch dependencies for OoD detection. Empirical results suggest that this leads to more robust detection for high-dimensional images.", "review": "Review:###The paper makes the observation that likelihood models trained with batch norm assign much lower likelihoods to *training batches* of OoD data (batch norm statistics computed over over minibatch) than evaluation batches of OoD data (batch norm statistics over entire training set). One issue with comparing this method to most other OoD detection works is that it considers OoD detection on *batches* of (all OoD data) or (all in-distribution data). As soon as the problem is changed to *classify between OoD batches* and not single samples, there are a large number of possible statistical tests one can perform to perform OoD (T-test between likelihoods of each batch) and the problem becomes *much* easier. In some ways, this makes things more well-defined (hard to compare distributions when one of them is just a single sample from an arbitrary distribution). However, that brings me to a big concern I have with the evaluation protocol. The batch size used for train/evaluation is rather large (64, this detail is hidden in the Appendix and I would have appreciated the number put in the main experiments section). If you take a likelihood model and evaluate on 64 samples from SVHN, you are all but guaranteed to sample a sample with *exceedingly* low likelihood, which dominates the mean statistic, making it possible to separate SVHN batch from CIFAR10 batches. I suspect that OoD datasets have plenty of these *extremely low likelihood* examples that will drag the mean likelihood down a lot. This is consistent with your batch normalization experiments: in training mode, the likelihood is computed from mean activations over a batch of OoD samples, several of which probably contribute to the low likelihoods. In evaluation mode, likelihoods for each OoD sample are evaluated independently, which results in a similar observation to prior work showing that CIFAR10 likelihoods are inaccurate for SVHN. In other words, I think there is a mistake made here: it is the phenomenon that *batch likelihoods*, not *batch norm*, that is responsible for this method working well. One experiment that is missing from your paper (and would prove my hypothesis wrong) would be if you adapted the OoD criteria to compare the *mean* likelihoods in the evaluation mode, and show that for OoD datasets, the difference between batches still remains small. Nits: - *...such as learning a mixture of Gaussians*, I believe this toy example was on univariate gaussians, not mixtures. - Choi et al. 2018 and should also be included in the citation that *CIFAR10 gives higher likelihood estimates to SVHN than CIFAR10 ones* (this was a concurrent discovery between the two papers) - Choi et al. 2018 is not the right citation for *we evaluate the area under the ROC curve (AUC) and average precision (AP)* for each binary classification task, a more appropriate one would be Hendryks and Gimpel 2017. - No doubt the authors realized already but Page 7 has some *related work still needs some work, but there....* which should be deleted. - It took me awhile to find the batch size used in training and evaluation mode (64), which was on page 16."}
{"id": "iclr2020_155", "title": "Switched linear projections and inactive state sensitivity for deep neural network interpretability | OpenReview", "abstract": "Abstract:###We introduce switched linear projections for expressing the activity of a neuron in a ReLU-based deep neural network in terms of a single linear projection in the input space. The method works by isolating the active subnetwork, a series of linear transformations, that completely determine the entire computation of the deep network for a given input instance. We also propose that for interpretability it is more instructive and meaningful to focus on the patterns that deactive the neurons in the network, which are ignored by the exisiting methods that implicitly track only the active aspect of the network*s computation. We introduce a novel interpretability method for the inactive state sensitivity (Insens). Comparison against existing methods shows that Insens is more robust (in the presence of noise), more complete (in terms of patterns that affect the computation) and a very effective interpretability method for deep neural networks", "review": " The work basically introduced a new way of looking at interpretability; instead of focusing on the source of activations in the network for a given input image, focus on the source of stability (non-active) neurons (in a ReLU network). The work starts by proving (although it is trivial) that in a ReLU (more generally any piece-wise linear) network, for a given input image, there is a locally linear relationship between a given neuron*s activation and the image: v= w^T x + b. As the authors correctly mention, focusing on *w* as the sensitivity analysis is basically the vanilla gradient method. The contribution, however, is focusing on the projection of bias and the introduced notion of *centre*. With this provided notion, one can focus on the deactivated neurons in the network and how each input pixel is responsible for it. In other words, unlike previous work that focuses on the activation map, the authors correctly refer to the deactivated neurons as another source of the network*s prediction. I*m have reasons for both accepting and rejecting this work. The work provides a new perspective and asks a very interesting question. The introduced method, although quite simple and trivial, is useful and the authors do a very good job of making valid and reasonable claims about their work*s contribution and how it connects to the existing literature. The main drawback of the paper, however, is whether the contributions are enough for this venue. The paper does not convince me that the introduced method would result in better interpretability of deep networks compared to what is already there. Another minor (or for some people in the field major) issue is the experimental setup. All of the experiments are focused on subjective examples and no objective measure of the introduced method is provided (and the field has many of those objective measures). Providing a few examples of the method in comparison with other methods is not sufficient. Anyhow, the experiment where they prove the usefulness of the method by adding background noise is interesting. I would personally suggest the authors to expand this experiment to testing the method*s sanity using the sanity measures provided in previous work: https://arxiv.org/abs/1810.03292 The claims made about the results on smallNORB can be controversial as the authors interpret their method*s flipping of importance to be the reality of what*s happening in the network and the other method*s focus on the edges as false; this is not clear to be true. My score would be subject to change if better experimental results are provided (and the other way round). A few suggestions and questions: * One very important issue with the method is that it considers all of the inactive neurons. We know that a substantial percentage of inactive neurons are just dead neurons the stability of which does not matter. How would the method address the issue? * There definitely needs to be an objective measure of the introduced method*s performance compared to previous work. * The work seems very related to DeepLIFT while there is no mentioning of it. * I*m not a fan, but adding results on a SOTA ImageNET paper always helps with making the experiments section crisper. * The authors claim that even small perturbations will change the activation pattern. This is not a small claim and is definitely in need of more evidence."}
{"id": "iclr2020_156", "title": "Convergence Behaviour of Some Gradient-Based Methods on Bilinear Zero-Sum Games | OpenReview", "abstract": "Abstract:###Min-max formulations have attracted great attention in the ML community due to the rise of deep generative models and adversarial methods, and understanding the dynamics of (stochastic) gradient algorithms for solving such formulations has been a grand challenge. As a first step, we restrict to bilinear zero-sum games and give a systematic analysis of popular gradient updates, for both simultaneous and alternating versions. We provide exact conditions for their convergence and find the optimal parameter setup and convergence rates. In particular, our results offer formal evidence that alternating updates converge *better* than simultaneous ones.", "review": "Review:###Summary: The paper presents exact conditions for the convergence of several gradient based methods for solving bilinear games. In particular, the methods under study are Gradient Descent(GD), Extragradient (EG), Optimizatic Gradient descent (OGD) and Momentum methods. For these methods, the authors provide convergence rates (with optimal parameter setup) for both alternating (Gauss-Seidel) and simultaneous (Jacobi) updates. Comments: The paper is well written and the main contributions are clear. I find the theoretical results of the paper interesting and promising, however i believe that the proposed analysis will be difficult to extend beyond bilinear games to more practical scenarios as the authors claim. In particular, the proposed analysis is based on understanding the bilinear game dynamics using spectral analysis. This approach is not novel. It is well known for bilinear games (see for example [1] and the references therein) and is not easy to extent to general games. The authors provide necessary and sufficient conditions under witch all previously mentioned algorithms (GD, EG, OGD,Momentum) converge for bilinear games. The convergence analysis (Theorems 3.1 -3.4) is easy to follow and seems correct. Main issue: The authors mentioned in their abstract that *... and understanding the dynamics of (stochastic ) gradient algorithms for solving ...*. In addition in their figures 4 and 5 they compare stochastic methods. However there is no convergence analysis on stochastic variants of the proposed methods. Note that in [2], it was shown that stochastic variants can prevent the convergence of standard game optimization methods, while their deterministic version converges. If the goal of the last experiment is to show that the proposed methods and analysis can be extended to more interesting general settings then the algorithms analyzed in the paper should be used in the numerical evaluation and not their stochastic variants. The authors should be more clear (from the abstract) on what algorithms they study. The paper clearly focuses on deterministic (full gradient) methods. To conclude I liked the paper and the theoretical analysis seems correct however i am not convinced that the results could be of interest for the ICLR community. It focuses only on simple bilinear zero-sum games and on deterministic methods for solving them. Minor Comments: page 2, bellow eq 2.2 biliner---> bilinear page 3 bellow eq. 2.6: Cesari---> Cesaro In caption of figure 2: replace the x-axis and y-axis with horizontal axis and vertical axis respectively. References: [1] Gidel, Gauthier, Reyhane Askari Hemmat, Mohammad Pezeshki, Remi Lepriol, Gabriel Huang, Simon Lacoste-Julien, and Ioannis Mitliagkas. *Negative momentum for improved game dynamics.* arXiv preprint arXiv:1807.04740 (2018). [2] Chavdarova, Tatjana, Gauthier Gidel, François Fleuret, and Simon Lacoste-Julien. *Reducing noise in gan training with variance reduced extragradient.* arXiv preprint arXiv:1904.08598 (2019). ========= after rebuttal ============= I would like to thank the authors for the reply. After reading their response and the comments of the other reviewers I decide to update my score to *weak accept*."}
{"id": "iclr2020_157", "title": "V4D: 4D Covolutional Neural Networks for Video-level Representations Learning | OpenReview", "abstract": "Abstract:###Most existing 3D CNN structures for video representation learning are clip-based methods, and do not consider video-level temporal evolution of spatio-temporal features. In this paper, we propose Video-level 4D Convolutional Neural Networks, namely V4D, to model the evolution of long-range spatio-temporal representation with 4D convolutions, as well as preserving 3D spatio-temporal representations with residual connections. We further introduce the training and inference methods for the proposed V4D. Extensive experiments are conducted on three video recognition benchmarks, where V4D achieves excellent results, surpassing recent 3D CNNs by a large margin.", "review": "Review:###[Summary] The paper presents a video classification framework that employs 4D convolution to capture longer term temporal structure than the popular 3D convolution schemes. This is achieved by treating the compositional space of local 3D video snippets as an individual dimension where an individual convolution is applied. The 4D convolution is integrated in resnet blocks and implemented via first applying 3D convolution to regular spatio-temporal video volumes and then the compositional space convolution, to leverage existing 3D operators. Empirical evaluation on three benchmarks against other baselines suggested the advantage of the proposed method. [Decision] Overall, the paper addresses an important problem in computer vision (video action recognition) with an interesting. I found the motivation and solution are reasonable (despite some questions pending more elaboration), and results also look promising, thus give it a weak accept (conditional on the answers though). [Comments] At the conceptual level, the idea of jointly modeling local video events is not novel, and can date back to at least ten years ago in the paper “Learning realistic human actions from movies”, where the temporal pyramid matching was combined with the bag-of-visual-words framework to capture long-term temporal structure. The problem with this strategy is that the rigid composition only works for actions that can be split into consecutive temporal parts with prefixed duration and anchor points in time, which is clearly challenged by many works later when more complicated video events are studied. It seems to me that the proposed framework also falls in this category, with a treatment from deep learning. It is definitely worth some discussion on this path. That said, I would like to see more analysis on the behavior of the proposed method under various interesting cases not tested yet. Despite the claim that the proposed method can capture long-term video patterns, the static compositional nature seems to work best for activities with well-defined local events and clear temporal boundaries. These assumptions hold mostly true for the three datasets used in the experiment, and also are suggested by results in table 2(e), where 3 parts are necessary to achieve optimal results. How does the proposed method perform in more complicated tasks such as - action detection or localization (e.g., in benchmarks JHMDB or UCF101-24). - complex video event modeling (e.g., recognizing activities in extended video of TRECVID). Will it still be more favorable than other concerning baselines? Besides, on the computation side, it would be complexity, an explicit comparison of complexity makes it easier to evaluate the performance when compared to other state-of-the-art methods. [Area to improve] Better literature review to reflect the relevant previous video action recognitions, especially those on video compositional models. Proof reading - The word in the title should be “Convolutional”, right?"}
{"id": "iclr2020_158", "title": "Adversarial Imitation Attack | OpenReview", "abstract": "Abstract:###Deep learning models are known to be vulnerable to adversarial examples. A practical adversarial attack should require as little as possible knowledge of attacked models T. Current substitute attacks need pre-trained models to generate adversarial examples and their attack success rates heavily rely on the transferability of adversarial examples. Current score-based and decision-based attacks require lots of queries for the T. In this study, we propose a novel adversarial imitation attack. First, it produces a replica of the T by a two-player game like the generative adversarial networks (GANs). The objective of the generative model G is to generate examples which lead D returning different outputs with T. The objective of the discriminative model D is to output the same labels with T under the same inputs. Then, the adversarial examples generated by D are utilized to fool the T. Compared with the current substitute attacks, imitation attack can use less training data to produce a replica of T and improve the transferability of adversarial examples. Experiments demonstrate that our imitation attack requires less training data than the black-box substitute attacks, but achieves an attack success rate close to the white-box attack on unseen data with no query.", "review": "Review:###The authors propose to use a generative adversarial network to train a substitute that replicates (imitates) a learned model under attack. They then show that the adversarial examples for the substitute can be effectively used to attack the learned model. The idea is straightforward. The proposed approach leads to better success rates of attacking than other substitute-training approaches that require more training examples. Promising experimental results against decision and score-based attach schemes also demonstrate the effectiveness of the proposed approach. My main concern is that the comparison to other approaches seem unfair, because (1) Substitute models typically use all training data and query the learned model once per training example; the proposed approach uses fewer training data but needs 1800 queries per example. So it is not surprising that the proposed approach can be better than substitute models. As the authors focus on *practical* scenarios, the number of queries should be fixed as a constraint for all approaches to be fair. (2) Having said (1), there is not enough detail in this paper to understand the similarity and difference between the proposed approach and decision-based ones (which is claimed to have similar query complexity during training). The authors mix the results with score-based (requiring more information), making the results more confusing to the readers. The authors are encouraged to present more detailed discussions on the comparison with decision-based competitors. Also, (3) One thing that is missing from the experiments is how well the replica clones the original model. All the information in the experiments are somewhat *indirect* (success rate, test accuracy, etc.) to answer this question, but there is no direct evidence. Is a well-cloned replica really a better substitute to construct adversarial examples? For instance, for replica with different *cloning accuracy* (rather than test accuracy), is a better replica also a better substitute? The paper fails to answer this question that best matches its design motivation. The above are my main concerns. A minor one is (4) The abstract that directly uses notations like T, G and D is horribly hard to read."}
{"id": "iclr2020_159", "title": "Exploring by Exploiting Bad Models in Model-Based Reinforcement Learning | OpenReview", "abstract": "Abstract:###Exploration for reinforcement learning (RL) is well-studied for model-free methods but a relatively unexplored topic for model-based methods. In this work, we investigate several exploration techniques injected into the two stages of model-based RL: (1) during optimization: adding transition-space and action-space noise when optimizing a policy using learned dynamics, and (2) after optimization: injecting action-space noise when executing an optimized policy on the real environment. When given a good deterministic dynamics model, like the ground-truth simulation, exploration can significantly improve performance. However, using randomly initialized neural networks to model environment dynamics can _implicitly_ induce exploration in model-based RL, reducing the need for explicit exploratory techniques. Surprisingly, we show that in the case of a local optimizer, using a learned model with this implicit exploration can actually _outperform_ using the ground-truth model without exploration, while adding exploration to the ground-truth model reduces the performance gap. However, the learned models are highly local, in that they perform well _only_ for the task for which it is optimized, and fail to generalize to new targets.", "review": " This paper investigates the effect of adding noise to action selection and transition dynamics estimation in model-based reinforcement learning (MBRL) for improved exploration. The authors use the iterative Linear Quadratic Regulator (iLQR) algorithm with their methods for inducing noise and evaluate the performance on a robotic arm simulator. I think the paper explores an interesting problem which does not receive a lot of attention (exploration in MBRL). However, given that it is an empirical paper, I don*t find the empirical evaluation thorough enough and some of the results found seem questionable. Further, the paper is completely focused on iLQR on the robotic arm task, so it*s not at all clear whether the results carry through to other algorithms or other environments. Some more detailed comments: - Intro: *combines two extremely data inefficient techniques*: are you referring to model-free versus model-based? they*re not really *combined* in RL, and *extremely data inefficient* seems rather strong. - Intro: *In model free methods, exploration is usually done by adding noise to the action suggested by the optimized policy.*: not really, most of the interesting exploratory algorithms use more sophisticated techniques like intrinsic rewards, pseudo-counts, and many even have theoretical guarantees (see all the PAC-MDP work). - Sec 2.1: There*s a whole line of related work missing here. For example: * *Near-Optimal Reinforcement Learning in Polynomial Time*, Kearns & Singh, 2002 * *R-max – A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning*, Brafman & Tennenholtz, 2002 * *PAC Optimal Exploration in Continuous Space Markov Decision Processes*, Pazis & Parr, 2013 * *Unifying Count-Based Exploration and Intrinsic Motivation*, Bellemare et al., 2016 * *Benchmarking Bonus-Based Exploration Methods on the Arcade Learning Environment*, Taiga et al., 2019 - Algorithm 3 line 6: Where does hat{pi} come from? - Section 3: *aim of reinforcement learning is to find a stochastic policy* it doesn*t have to be stochastic - Section 4.1, Action-maximum-entropy: You use , what does this mean? Why does Q have two a_t subscripts? - Section 4.2, Transition-fixed-covariance: *we set *: this seems quite high. is there a reason why? did you try other values? - Equation (4): Is this a standard cost function or is it only specific to this work? If the former, add reference, if the latter, explain the choice. - Section 5: It*s not clear what you mean by *inverse kinematics* - Figure 1: It*s not clear where the targets are. It might help if you draw a trajectory from the initial to target position on each, otherwise it*s hard to tell them apart. - Figure 2 (a): What*s the difference between the three plots? - Figure 2 (a): The labels on the legend don*t match anything that was previously introduced, so it*s not clear what*s what. - Section 5: What*s MPC? - Table 1: There are a number of different hyperparameter choices for the different exploration methods you evaluate. How did you pick them? Did you do a sweep? Are the results averaged over multiple runs? How many? - Section 5.2: What is *finite-difference approximation*? - Section 5.2: *We run the same set of experiments as above, but only with inner-loop exploration*: why? - Table 2: The algorithm with no exploration does *much* worse when using the ground truth dynamics compared with using the model in table 1. This is totally counter-intuitive: shouldn*t the results be strictly better given that there*s zero model approximation error? I*m not sure I buy the justification you gave in section 5.3. There is a >2x degradation when using ground truth without exploration for the 0.10m target, which is significantly worse than the degradation for the harder 0.45m target. If exploration really is the cause, I would expect to see the decrease in performance correlate with the difficulty. - Section 5.3.1: This section seems somewhat unrelated to the main thesis of the paper. If you do want to investigate this problem, it calls for a much more thorough investigation. Right now the investigation provided is somewhat superficial, which detracts from the paper. - Section 5.3.1: *Thus, we conclude that... for a particular target*: Is this really that surprising? Ground truth dynamics are agnostic to the target, whereas the learned models build their estimates from collected trajectories, which are gathered using the current best policy, which is aiming to minimize the cost to the specific target. One minor comment: - Section 5: *We generate targets three distances 0.1m, 0.45, 0.8m away from the initial...* would read better as *We use targets at three different distances from the initial end-effector: 0.1m, 0.45m, 0.8m*."}
{"id": "iclr2020_160", "title": "On the Weaknesses of Reinforcement Learning for Neural Machine Translation | OpenReview", "abstract": "Abstract:###Reinforcement learning (RL) is frequently used to increase performance in text generation tasks, including machine translation (MT), notably through the use of Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN). However, little is known about what and how these methods learn in the context of MT. We prove that one of the most common RL methods for MT does not optimize the expected reward, as well as show that other methods take an infeasibly long time to converge. In fact, our results suggest that RL practices in MT are likely to improve performance only where the pre-trained parameters are already close to yielding the correct translation. Our findings further suggest that observed gains may be due to effects unrelated to the training signal, concretely, changes in the shape of the distribution curve.", "review": " This work carefully studies RL for neural machine translation and draws several conclusions: 1. One of the most common RL methods for MT does not optimize the expected reward, as well as show that other methods take an infeasibly long time to converge. 2. RL practices in MT are likely to improve performance only where the pre-trained parameters are already close to yielding the correct translation. 3. Observed gains may be due to effects unrelated to the training signal, concretely, changes in the shape of the distribution curve. I have questions about several technical claims, which lead to my doubt about the technical correctness of the paper: 1. [updated after checking authors* response] *it may well converge to values that are not local maxima of R, making it theoretically ill-founded.* Previously I had concerns about the convergence of REINFORCE with deep NNs as its policy. After checking the references provided by the authors, the local convergence can indeed be guaranteed. 2. *reducing a constant baseline from r, so as to make the expected reward zero, disallows learning.* This conflicts with my intuition. Where is the experiment supporting this claim? 3. *MRT succeeds in pushing ybest to be the highest ranked token if it was initially second, but struggles where it was initially ranked third or below.* Why ybest in third position cannot be boosted while second can be boosted? Figure 5 only shows two tokens, which cannot lead to any meaningful statistical conclusions. I*d like to see the statistic numbers: a. how many ybest in second are boosted to first in training? How many are not? b. how many ybest in third are boosted to first or second in training? How many are not? How about other positions?"}
{"id": "iclr2020_161", "title": "One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation | OpenReview", "abstract": "Abstract:###Recent advances in the sparse neural network literature have made it possible to prune many large feed forward and convolutional networks with only a small quantity of data. Yet, these same techniques often falter when applied to the problem of recovering sparse recurrent networks. These failures are quantitative: when pruned with recent techniques, RNNs typically obtain worse performance than they do under a simple random pruning scheme. The failures are also qualitative: the distribution of active weights in a pruned LSTM or GRU network tend to be concentrated in specific neurons and gates, and not well dispersed across the entire architecture. We seek to rectify both the quantitative and qualitative issues with recurrent network pruning by introducing a new recurrent pruning objective derived from the spectrum of the recurrent Jacobian. Our objective is data efficient (requiring only 64 data points to prune the network), easy to implement, and produces 95 % sparse GRUs that significantly improve on existing baselines. We evaluate on sequential MNIST, Billion Words, and Wikitext.", "review": "Review:###OVERALL: I should first say that this is reasonably far outside my wheelhouse. I have never worked on RNNs or pruning. I also have no familiarity with the data sets used. All these things being said, I can follow the derivations, and the idea seems reasonable and well-motivated, and pruning is interesting for both scientific and practical reasons, and this technique seems to help a substantial amount, so I*m inclined to vote for acceptance, with the understanding that perhaps better informed reviewers will in the future point out something I have missed. DETAILED NOTES: > overparameterized networks require more storage capacity and are computationally more expensive than their pruned counterparts I*m with you on the storage capacity, but do any of these pruned networks actually run faster than their non-pruned counterparts? I thought you had to work really hard to prune to some kind of block-sparse representation to realize any speed gains. This question is not rhetorical - I know very little about this topic. > Work for the present volume began by asking the question... I like this paragraph for motivation, but perhaps *volume* is slightly overwrought? > For our pruning objective, we simply take the K weights with largest sensitivity scores, as those represent the parameters which most affect the Jacobian objective near the initialization. Is there some notion of redundancy, where certain sets of parameters affect the jacobian in the same way, so that all but 1 element of the set could be pruned, or something? In line 13 of algorithm 1, why do we need to sort if in the next step we just mask out everything with sensitivity less than D tilde k? Maybe this is a dumb question, but if you*re pruning at initialization, why not just initialize a smaller network in such a way that you wouldn*t choose to prune any of its parameters? Am I misunderstanding what you*re doing? > In the prequel, we postulated t The prequel? Fig 1 is interesting, but it raises the question of whether you could recover a simpler algorithm by just modifying random pruning so that it evenly distributes *prunes* across gate and Type. In fig 2, why are all the singular values less than 1? It*s not obvious to me why that should be true, unless you enforce it w/ the initialization."}
{"id": "iclr2020_162", "title": "Reject Illegal Inputs: Scaling Generative Classifiers with Supervised Deep Infomax | OpenReview", "abstract": "Abstract:###Deep Infomax~(DIM) is an unsupervised representation learning framework by maximizing the mutual information between the inputs and the outputs of an encoder, while probabilistic constraints are imposed on the outputs. In this paper, we propose Supervised Deep InfoMax~(SDIM), which introduces supervised probabilistic constraints to the encoder outputs. The supervised probabilistic constraints are equivalent to a generative classifier on high-level data representations, where class conditional log-likelihoods of samples can be evaluated. Unlike other works building generative classifiers with conditional generative models, SDIMs scale on complex datasets, and can achieve comparable performance with discriminative counterparts. With SDIM, we could perform emph{classification with rejection}. Instead of always reporting a class label, SDIM only makes predictions when test samples* largest logits surpass some pre-chosen thresholds, otherwise they will be deemed as out of the data distributions, and be rejected. Our experiments show that SDIM with rejection policy can effectively reject illegal inputs including out-of-distribution samples and adversarial examples.", "review": "Review:###The paper proposed a supervised method for robust classification. By utilizing mutual information constrain on the encoder, supervised probabilistic constraint on the class conditional probability, and introducing a margin to the maximum likelihood, the proposed method can manage a high classification accuracy and detect the out of distribution data. The motivation is good, the writing is OK. The structure of the paper needs to be refined. The experiments are not very strong. Several concerns are listed: 1. It will be better if the author can show a structure map for Encoder, classification, class condition embedding, and mutual information evaluation networks, to clarify their relationships. 2. In Table 2, what is the accuracy for the rejection (outlier detection)? 3. The choose of the threshold for rejection is tricky. It will be better if the author can provide some rules for choosing the threshold which can be generalized to a new dataset. 4. The overall loss functions have three components, what is the contribution of different components to the final performance experimentally? 5. The architecture for encoder is large. Is it proper for Mnist which is a relatively simple dataset? 6. The comparison talked in the paragraph “Is Fully Generative Model Necessary for Generative Classification”, are these accuracies obtained from a comparable network size? It only makes sense if they are obtained using a comparable parameter size."}
{"id": "iclr2020_163", "title": "Asynchronous Stochastic Subgradient Methods for General Nonsmooth Nonconvex Optimization | OpenReview", "abstract": "Abstract:###Asynchronous distributed methods are a popular way to reduce the communication and synchronization costs of large-scale optimization. Yet, for all their success, little is known about their convergence guarantees in the challenging case of general non-smooth, non-convex objectives, beyond cases where closed-form proximal operator solutions are available. This is all the more surprising since these objectives are the ones appearing in the training of deep neural networks. In this paper, we introduce the first convergence analysis covering asynchronous methods in the case of general non-smooth, non-convex objectives. Our analysis applies to stochastic sub-gradient descent methods both with and without block variable partitioning, and both with and without momentum. It is phrased in the context of a general probabilistic model of asynchronous scheduling accurately adapted to modern hardware properties. We validate our analysis experimentally in the context of training deep neural network architectures. We show their overall successful asymptotic convergence as well as exploring how momentum, synchronization, and partitioning all affect performance.", "review": "Review:###This paper analyzes convergence of asynchronous methods on general non-smooth and non-convex functions (typically arising from deep leaning). Stochastic sub-gradient asynchronous methods are of particular challenging when coupled with complicated hardware behavior of modern NUMA architecture. To validate the analysis, and study the impact of momentum, variable partitioning, numerical experiments on deep learning training are given. A major concern to me is that the assumptions made in the analyze may be too restrictive. For example, Assumption 3.1.1 regarding the unbiasedness of the stochastic sub-gradients may not hold for since the Clarke sub-differentiation is not additive. The Clarke sub-differentiation of |x| and -|x| are not included in their average which is zero, therefore the Assumption 3.1.1 is not true for tame functions (as cited in the paper) . If this assumption were not true, all the proof arguments based on Martingale differences may not be follow to prove Theorem 3.1. There are a few typos which make the paper hard to understand. Is the momentum variable u_i^kc in Algorithm 1 a central variable, as x_i? It seems to be yes since the update needs a lock. But why there is an kc on its index, which is not the case for x_i? In terms of numerical results, the system specification is not so clear to me why it includes a diverse asynchronous system settings. Even though Figure 1 shows convergence in terms cross-entropy loss, which is not directly related to validate Theorem 3.1 since it is not clear what is going on with x_t. The paper would be of great interest if the global shared memory asynchronous model is made more precise in the main body of the paper. At a first glace, it is not clear what is the benefit (or what insights) using this model to analyze these algorithms, compared to simplistic models in literature."}
{"id": "iclr2020_164", "title": "Knowledge Consistency between Neural Networks and Beyond | OpenReview", "abstract": "Abstract:###This paper aims to analyze knowledge consistency between pre-trained deep neural networks. We propose a generic definition for knowledge consistency between neural networks at different fuzziness levels. A task-agnostic method is designed to disentangle feature components, which represent the consistent knowledge, from raw intermediate-layer features of each neural network. As a generic tool, our method can be broadly used for different applications. In preliminary experiments, we have used knowledge consistency as a tool to diagnose knowledge representations of neural networks. Knowledge consistency provides new insights to explain the success of existing deep-learning techniques, such as knowledge distillation and network compression. More crucially, knowledge consistency can also be used to refine pre-trained networks and boost performance.", "review": " The submission proposes a method for extracting *knowledge consistency* in neural networks and using that toward analyzing different aspects of them, eg understanding the representations, explaining knowledge distillation, and analyzing network compression. What*s defined as consistent knowledge is essentially the stable parts of representations of different networks trained for the same task (stable-->consistent). I found the submission insightful, well executed, and backed up by many experimental results. Strengths: A) the proposed concept, extracting the *consistency* among representation of different networks, is interested and using that towards understanding what*s going on under-the-hood of neural networks makes sense. I*m not aware of a similar existing method and didn*t see one among the citations, so my current assumption is that this proposal is pretty novel. B) the extracted representational consistency quantity appears to be meaningful and strong, as authors were able to use it toward explaining several existing phenomena (e.g. knowledge distillation, network compression, etc). C) Authors perform extensive experimental studies on various aspects of neural networks in relation to the proposed representational consistency quantity. I applaud the efforts made by authors. Improvements/questions: D) The submission uses a few different phrases in close connection or interchangeably, eg *fuzziness*, *order*, *linear/nonlinear transformation*, *easy/hard to guess/estimation* (see the last 2 paragraphs of page 2). While I understand what the authors are trying to convey, those concepts are not in principle necessarily the same, so some clarification/unification would make the presentation more solid. For instance: nonlinear<--> higher order<-->fuzzy<-->hard to guess. Also guess<-->estimate. E) Sec 3 is the most important part of the paper and the technical meat. However, I found it harder to follow compared to the rest of the paper. It probably deserves more than 1 page. Authors could consider smoothing the presentation and adding details even at the cost of slightly extending the length. F) I think section 4.2 could walk the reader through more details to make sure the basics are understood, as this is the first time results of the method are being presented. E.g. fig 3 could be analyzed further and the color maps should be defined. G) I found the post hoc explanation in the last paragraph of page 6 somewhat dubious. H) In the feature refinement experiment (paragraph 2 of sec 4.3), is the process done in a recursive manner (first layer 1, then layer 2, and so on)? Or as a one time process? The former seems stronger. I) The consistency quantity is based on comparing different networks trained for the *same task*. Have you considered doing the same among different networks trained for *different tasks*? Would that say something about similarities among tasks and multi-task learning in a fashion similar to the analysis of Taskonomy 2018? J) I did not understand *and Beyond* in the title. I*d consider a more directly descriptive title. K) Seems like authors define *knowledge* and *visual concept* to be the invariant part of a feature (see paragraph 3 of intro). I don*t see a particular problem with that, but a direct statement like *invariant features* would have resonated with me just fine, while whether we can call that *knowledge* is a matter of (unnecessary) semantics in my opinion. ===== Post rebuttal comments: Thanks to authors for the detailed response and additions. I read through the comments and skimmed the revised PDF. Overall I preserve my positive rating toward this paper. The updates did improve the paper, but I would recommend the authors to use the camera ready opportunity to further address the original comments especially with respect to the delivery. I find the added experiments in multi task learning interesting especially given the time constraints of rebuttal. However the adopted setting is not the strongest case as what is defined to be *two tasks* is basically a superclass classification of a fine grained classifier. More concrete datasets and tasks specifically targeting challenging multi task learning are available now (see the full discussion in the original review). I would recommend taking those into consideration either for more experimentation or calibrating the conclusion made out of the current experiment. I recommend acceptance."}
{"id": "iclr2020_165", "title": "Min-Max Optimization without Gradients: Convergence and Applications to Adversarial ML | OpenReview", "abstract": "Abstract:###In this paper, we study the problem of constrained robust (min-max) optimization ina black-box setting, where the desired optimizer cannot access the gradients of the objective function but may query its values. We present a principled optimization framework, integrating a zeroth-order (ZO) gradient estimator with an alternating projected stochastic gradient descent-ascent method, where the former only requires a small number of function queries and the later needs just one-step descent/ascent update. We show that the proposed framework, referred to as ZO-Min-Max, has a sub-linear convergence rate under mild conditions and scales gracefully with problem size. From an application side, we explore a promising connection between black-box min-max optimization and black-box evasion and poisoning attacks in adversarial machine learning (ML). Our empirical evaluations on these use cases demonstrate the effectiveness of our approach and its scalability to dimensions that prohibit using recent black-box solvers.", "review": "Review:###The authors aim to propose new algorithms for min-max optimization problem when the gradients are not available and establish sublinear convergence of the algorithm. I don*t think this paper can be accepted for ICLR for the following reasons: 1. For Setting (a) (One-sided black-box), the theory can be established by the same analysis for ZO optimization by optimizing y. Even by a proximal step for y, the analysis is essentially the same as ZO where an estimation of the gradient for x is conducted. 2. The assumptions A1 and A2 are hardly satisfied in ML applications, where the objective is essentially smooth. The authors should at least analyze the case where a sub/super-gradients is available. 3. Also, for most ML problems we have today, I don*t find many applications where the gradients are not available, and I thus feel that it is not interesting to consider ZO optimizations."}
{"id": "iclr2020_166", "title": "Refining the variational posterior through iterative optimization | OpenReview", "abstract": "Abstract:###Variational inference (VI) is a popular approach for approximate Bayesian inference that is particularly promising for highly parameterized models such as deep neural networks. A key challenge of variational inference is to approximate the posterior over model parameters with a distribution that is simpler and tractable yet sufficiently expressive. In this work, we propose a method for training highly flexible variational distributions by starting with a coarse approximation and iteratively refining it. Each refinement step makes cheap, local adjustments and only requires optimization of simple variational families. We demonstrate theoretically that our method always improves a bound on the approximation (the Evidence Lower BOund) and observe this empirically across a variety of benchmark tasks. In experiments, our method consistently outperforms recent variational inference methods for deep learning in terms of log-likelihood and the ELBO. We see that the gains are further amplified on larger scale models, significantly outperforming standard VI and deep ensembles on residual networks on CIFAR10.", "review": "Review:###Summary. This paper describes a method for training flexible variational posterior distributions, which consists in making iterative locale refinements to an initial mean-field approximation, using auxiliary variables. The focus is on Gaussian latent variables, and the method is applied to Bayesian neural nets to perform variational inference (VI) over the weights. Empirical results show improvements upon the performance of the mean-field approach and some other baselines, on classification and regression tasks. Main comments. Overall, this paper is well written and easy to follow. It tackles an important topic in VI and proposes an interesting idea to improve the flexibility of the approximate distribution. I have the following comments/questions. - On the guarantee of improvement. I still have some doubts regarding the inequality “ELBO_aux >= ELBO_init”. Can you please elaborate more on this and provide a detailed formal proof? Figure 2 shows that ELBO_aux can go below ELBO_init. - The focus of the paper is on Gaussian variables and a configuration where some key distributions, q(a_1) and q(w|a_1), are accessible in closed from. The generalization of the proposed method beyond these settings should be discussed and explored in experiments. - Important baselines are missing in the experiments. I would recommend including at least the other VI techniques relying on auxiliary variables to build flexible variational families [1,2]. This would help to better assess the impact/importance of the proposed method. [1] Ranganath, Rajesh, Dustin Tran, and David Blei. *Hierarchical variational models.* ICML. (2016). [2] Maaløe, Lars, et al. *Auxiliary deep generative models.* ICML (2016)."}
{"id": "iclr2020_167", "title": "On Iterative Neural Network Pruning, Reinitialization, and the Similarity of Masks | OpenReview", "abstract": "Abstract:###We examine how recently documented, fundamental phenomena in deep learn-ing models subject to pruning are affected by changes in the pruning procedure. Specifically, we analyze differences in the connectivity structure and learning dynamics of pruned models found through a set of common iterative pruning techniques, to address questions of uniqueness of trainable, high-sparsity sub-networks, and their dependence on the chosen pruning method. In convolutional layers, we document the emergence of structure induced by magnitude-based un-structured pruning in conjunction with weight rewinding that resembles the effects of structured pruning. We also show empirical evidence that weight stability can be automatically achieved through apposite pruning techniques.", "review": "Review:###There are major problems with this paper. It is concerned with the examination of pruning experiments for a LeNet on the MNIST dataset. I fail to see how anything useful can be derived from this, as MNIST is a completely trivial dataset and LeNet is a very old, small architecture which does not at all resemble the massive overparameterised models that we care about. From a narrative perspective, I am not sure what the key point is, what should the reader take home? What should they take account of when performing network pruning? In terms of presentation, some of the figures are unreadable (figure 4). Figure 15 looks like noise. The writing is good however, if a bit grandiloquent. I dislike writing short reviews, but I fear this paper falls too far short of ICLR standard. Pros: - Well written Cons: - Experiments are weak - Unclear narrative; what*s the one key message? I have to give this paper a reject as the experiments conducted are far too weak, and there is little evidence anything found here will, say, generalise to a ResNet/DenseNet on ImageNet."}
{"id": "iclr2020_168", "title": "The asymptotic spectrum of the Hessian of DNN throughout training | OpenReview", "abstract": "Abstract:###The dynamics of DNNs during gradient descent is described by the so-called Neural Tangent Kernel (NTK). In this article, we show that the NTK allows one to gain precise insight into the Hessian of the cost of DNNs: we obtain a full characterization of the asymptotics of the spectrum of the Hessian, at initialization and during training.", "review": "Review:###This paper uses the Neural Tangent Kernel (NTK) to presents an asymptotic analysis of the evolution of Hessian of the loss (w.r.t. model parameters) throughout training. The authors leverage the Neural Tangent Kernel to analyze the evolution of the Hessian of the loss w.r.t the model parameters. Specifically the authors show that as the width of the neural networks tend to infinity, the Hessian can be decomposed into two components: (1) one that reflects the initialization of the model parameters and reduces as training progresses; and (2) one that captures the principal directions of the data. Technical Soundness: The paper appears to be technically sound, though I did not go through the 14 pages of proofs. Potential Impact: I found the focus of the paper to be quite narrow which I think will negatively affect the impact that this paper could have. The authors take the reader on a notational taxing voyage through the decomposition of the Hessian to finally arrive at a result that is rather intuitive if not entirely obvious. While I learned something about the evolution of the Hessian (in the limit of the infinitely wide NN), I can not say that this paper will have significant impact on how I think about NN training. To address this issue of significance and impact: What novel conclusions about Neural Network learning dynamics can you draw from your analysis? What are the implications for generalization or for future training algorithms? Clarity: Beyond the possibly unduly heavy notation, the paper is rather clear and well written. There is a minor typo in the first equation of Sec. 2.3 (i -> j)."}
{"id": "iclr2020_169", "title": "RefNet: Automatic Essay Scoring by Pairwise Comparison | OpenReview", "abstract": "Abstract:###Automatic Essay Scoring (AES) has been an active research area as it can greatly reduce the workload of teachers and prevents subjectivity bias . Most recent AES solutions apply deep neural network (DNN)-based models with regression, where the neural neural-based encoder learns an essay representation that helps differentiate among the essays and the corresponding essay score is inferred by a regressor. Such DNN approach usually requires a lot of expert-rated essays as training data in order to learn a good essay representation for accurate scoring. However, such data is usually expensive and thus is sparse. Inspired by the observation that human usually scores an essay by comparing it with some references, we propose a Siamese framework called Referee Network (RefNet) which allows the model to compare the quality of two essays by capturing the relative features that can differentiate the essay pair. The proposed framework can be applied as an extension to regression models as it can capture additional relative features on top of internal information. Moreover, it intrinsically augment the data by pairing thus is ideal for handling data sparsity. Experiment shows that our framework can significantly improve the existing regression models and achieve acceptable performance even when the training data is greatly reduced.", "review": "Review:#### Summary # This paper works on essay scoring. Instead of treating it as a regression problem, the authors proposed to model it by pairwise comparison (i.e., who should be scored higher). They then introduced a way to infer the final score by comparing the test essay to multiple reference essays. The experiments result demonstrated the improvement against the baseline, especially when the training data is limited. # Strength # S1. The paper is in general well-written and can be easily followed. The ablation study is well-designed to show improvement. S2. The proposed idea seems to be novel for essay scoring and works well in both the limited and sufficient data cases, which may become the essential building block for future work. # Weakness or comments # W1. Beyond the issue of insufficient data, I feel that in general, essay scoring should not be treated as a regression problem. Indeed, the score may not follows a good metric: the difference between score 6 and 4 is not equal to score 4 and 2. This is similar to the problem of facial age estimation: the degree of appearance change between ages 1 and 3 is different from ages 61 and 63. The essay scoring problem might naturally be better modeled as an *ordinal regression problem* (or ranking) or classification problem, rather than regression. What the authors proposed can indeed be seen as ordinal regression by pairwise ranking, and the authors should thus discuss the literature. The followings are examples. Z. Cao et al., *Learning to rank: from pairwise approach to listwise approach,* ICML 2009 L. Lin et al., *Ordinal regression by extended binary classification,* NIPS 2007 W2. The related work can be much strengthened. The authors could discuss problems where learning pairwise ranking/similarity is beneficial, for example: F. Sung et al., *Learning to Compare: Relation Network for Few-Shot Learning,* CVPR 2018 D. Parikh et al., *Relative attributes,* ICCV 2011 The final decision is made by aggregating the scores from references (based on the pairwise comparison or similarity), which is essentially a case of non-parametric models, which are known to work well in insufficient data. The authors thus may discuss techniques like support vector machines (regression), Parzen-window methods, etc. For example, Changpinyo et al. showed that with limited data, support vector regressions work much better than MLP for regression. Changpinyo et al., *Predicting visual exemplars of unseen classes for zero-shot learning,* ICCV 2017 W3. The method is not learned end-to-end directly for score prediction. The authors should discuss about this. W4. [About testing/ evaluation] The authors do not include formulas for evaluation. Also, how many reference (known) essays the authors consider in making a decision for a test essay (e.g., Eq. (3)). Since there are multiple prompts, during testing, do the authors only select those known essays that are from the same prompt of the test essay to aggregate the score? Is this a fair setting comparing to other baselines (i.e., by knowing the prompt where the test essay is from)? W5. The explanation of Sect. 4.2 is not clear. How can a model overfit when using even more data? I think it might relate to imbalanced training, where there are more pairs with large score differences than those that have small score differences. # For rebuttal # 1. Please discuss W1-W5. 2. Besides using transfer learning (i.e., pre-train and fine-tune), the authors should compare to multi-task learning: i.e., directly learning the two objectives --- pairwise comparison and regression --- together. 3. What is 10x in Table 3? Also, do the existing methods also use BERT for text embeddings? If not, the comparison might be unfair."}
{"id": "iclr2020_170", "title": "A Mutual Information Maximization Perspective of Language Representation Learning | OpenReview", "abstract": "Abstract:###We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).", "review": "Review:###This paper first gives a concise yet precise summary of maximizing one of variational lower bounds of mutual information, InfoNCE, then it provides an alternative view to explain case by case why word embedding Skip-gram, BERT, XLNet work in practice can be viewed by InfoNCE framework, thus we have a good understand for these methods. Moreover it introduces a self-learning method that maximizes the mutual information between a global sentence representation and n-grams in the sentence based on deep InfoMax framework instead. Experiments show that it is better then BERT and BERT-NCE. It*s known that InfoNCE increases bias but reduce variance, the same is true for deep InfoMax. Do you observe this in your experiments? If so, please provide. The paper is well-written and easy to follow. The originality is relative low though, since it is mainly an application of deep InfoMax to language modeling, not inventing a new algorithm and applying to language modeling. In equations 1 and 2, should a, b be written in capital? Since they represent random variables."}
{"id": "iclr2020_171", "title": "Extreme Value k-means Clustering | OpenReview", "abstract": "Abstract:###Clustering is the central task in unsupervised learning and data mining. k-means is one of the most widely used clustering algorithms. Unfortunately, it is generally non-trivial to extend k-means to cluster data points beyond Gaussian distribution, particularly, the clusters with non-convex shapes (Beliakov & King, 2006). To this end, we, for the first time, introduce Extreme Value Theory (EVT) to improve the clustering ability of k-means. Particularly, the Euclidean space was transformed into a novel probability space denoted as extreme value space by EVT. We thus propose a novel algorithm called Extreme Value k-means (EV k-means), including GEV k-means and GPD k-means. In addition, we also introduce the tricks to accelerate Euclidean distance computation in improving the computational efficiency of classical k-means. Furthermore, our EV k-means is extended to an online version, i.e., online Extreme Value k-means, in utilizing the Mini Batch k-means to cluster streaming data. Extensive experiments are conducted to validate our EV k-means and online EV k-means on synthetic datasets and real datasets. Experimental results show that our algorithms significantly outperform competitors in most cases.", "review": " This paper utilizes EVT to improve k-means, which aims to address the problem of clustering data of nonconvex shape. GEV k-means and GPD k-means are proposed as two kind of Extreme Value k-means. A method for accelerating Euclidean distance computation has also been proposed to solve the bottleneck of k-means. The proposed idea is novel and the paper is well written. Experimental reaultts are also good to me. There are two concerns which I need the authors to address: 1. Since the authors claim that they propose to speed up the computation of the Euclidean distances in k-means. However, there is no time cost comparison in the experiments. 2. Some other variants of k-means should be added for experimental comparison."}
{"id": "iclr2020_172", "title": "Discriminability Distillation in Group Representation Learning | OpenReview", "abstract": "Abstract:###Learning group representation is a commonly concerned issue in tasks where the basic unit is a group, set or sequence. The computer vision community tries to tackle it by aggregating the elements in a group based on an indicator either defined by human such as the quality or saliency of an element, or generated by a black box such as the attention score or output of a RNN. This article provides a more essential and explicable view. We claim the most significant indicator to show whether the group representation can be benefited from an element is not the quality, or an inexplicable score, but the \textit{discrimiability}. Our key insight is to explicitly design the \textit{discrimiability} using embedded class centroids on a proxy set, and show the discrimiability distribution \textit{w.r.t.} the element space can be distilled by a light-weight auxiliary distillation network. This processing is called \textit{discriminability distillation learning} (DDL). We show the proposed DDL can be flexibly plugged into many group based recognition tasks without influencing the training procedure of the original tasks. Comprehensive experiments on set-to-set face recognition and action recognition valid the advantage of DDL on both accuracy and efficiency, and it pushes forward the state-of-the-art results on these tasks by an impressive margin.", "review": "Review:###This paper proposed a post-processing method for improving group-based recognition tasks. Several manually designed features based on the pretrained networks are supervised trained on a light-weighted network like the teacher-student module. This paper should be rejected because (1) the novelty of the algorithm is limited: only using the well-known intra-class distance and inter-class distance as features. Besides, the superiority of such features should be better explained; (2) Similarly, the discriminability of testing image is too complicated, such as Eq. (9, 10). Why these params are designed in such ways? Explain them; (3) Why don’t you learn the discriminability by directly operating on the extracted pretrained features? The used two distances certainly are not the best choices compared with learning methods. You should compare the performance with your methods of training under the manually-designed features. (4) The written is poor, such as Figure 6: “table ??”; Table 2: “avgerage”."}
{"id": "iclr2020_173", "title": "Neural Linear Bandits: Overcoming Catastrophic Forgetting through Likelihood Matching | OpenReview", "abstract": "Abstract:###We study neural-linear bandits for solving problems where both exploration and representation learning play an important role. Neural-linear bandits leverage the representation power of deep neural networks and combine it with efficient exploration mechanisms, designed for linear contextual bandits, on top of the last hidden layer. Since the representation is being optimized during learning, information regarding exploration with *old* features is lost. Here, we propose the first limited memory neural-linear bandit that is resilient to this catastrophic forgetting phenomenon. We perform simulations on a variety of real-world problems, including regression, classification, and sentiment analysis, and observe that our algorithm achieves superior performance and shows resilience to catastrophic forgetting.", "review": "Review:###This paper adapts Bayesian linear regression to the setting of a limited memory replay buffer. The idea is to calibrate the prior mean and variance when the neural representation of context is updated. Overall the paper is well written and explained clearly. Some experiments are provided to show that the proposed method is able to achieve a performance competitive to Bayesian linear regression with infinite memory. The result of this paper is interesting. But I am not sure if the current experimental results are convincing enough to justify the significance of the proposed method. 1. The results in Section 4.2 seem to be following the setting in Riquelme 2018. These datasets are all in a supervised learning setting. It is a bit disappointing that the proposed method is not tested on RL datasets. 2. No other baseline is provided in the experiments for comparison. a. There are other methods in the literature to overcome catastrophic forgetting of neural networks, e.g regularizing the update of the network. How would that be compared to the proposed method? b. What about other methods, like [1]? 3. Most of the experiment details are missing. For example, how is the reward defined in section 4.3? What is the overhead in computation in practice, especially for the SDP? Other comments: 1. Why would solving a SDP require only O(g^{0.5}) in section 3.1? 2. In the discussion in section 3, even if equation (5) and (6) can be exactly solved, how does the heavy tailed problem mentioned in section 2 been solved? [1] Elmachtoub, Adam N., et al. *A practical method for solving contextual bandit problems using decision trees.* arXiv preprint arXiv:1706.04687 (2017)."}
{"id": "iclr2020_174", "title": "Ternary MobileNets via Per-Layer Hybrid Filter Banks | OpenReview", "abstract": "Abstract:###MobileNets family of computer vision neural networks have fueled tremendous progress in the design and organization of resource-efficient architectures in recent years. New applications with stringent real-time requirements in highly constrained devices require further compression of MobileNets-like already computeefficient networks. Model quantization is a widely used technique to compress and accelerate neural network inference and prior works have quantized MobileNets to 4 ? 6 bits albeit with a modest to significant drop in accuracy. While quantization to sub-byte values (i.e. precision ? 8 bits) has been valuable, even further quantization of MobileNets to binary or ternary values is necessary to realize significant energy savings and possibly runtime speedups on specialized hardware, such as ASICs and FPGAs. Under the key observation that convolutional filters at each layer of a deep neural network may respond differently to ternary quantization, we propose a novel quantization method that generates per-layer hybrid filter banks consisting of full-precision and ternary weight filters for MobileNets. The layer-wise hybrid filter banks essentially combine the strengths of full-precision and ternary weight filters to derive a compact, energy-efficient architecture for MobileNets. Using this proposed quantization method, we quantized a substantial portion of weight filters of MobileNets to ternary values resulting in 27.98% savings in energy, and a 51.07% reduction in the model size, while achieving comparable accuracy and no degradation in throughput on specialized hardware in comparison to the baseline full-precision MobileNets.", "review": "Review:###This paper proposes a novel quantization method towards the MobileNets architecture, with the consideration of balance between accuracy and computation costs. Specifically, the paper proposes a layer-wise hybrid filter banks which only quantizes a fraction of convolutional filters to ternary values while remaining the rest as full-precision filters. The method is tested empirically on ImageNet dataset. This paper is generally well written with good clarity. Several concerns are as follows: 1. This paper is incremental in nature, with a natural generalization of (Tschannen et al.(2018)). But it is still an interesting contribution. For this kind of paper, I would like to see a more complete set of empirical results. However, The experiments only perform comparison on ImageNet dataset. Though this dataset has a reasonable size, however, as in many cases, different datasets can bias the performance of different models. To strengthen the results, could the experiments be conducted on multiple datasets as in (Tschannen et al.(2018))? 2. The proposed method is only designed for MobileNets. Is it possible to apply the hybrid filter banks to other neural network architecture?"}
{"id": "iclr2020_175", "title": "Generalized Transformation-based Gradient | OpenReview", "abstract": "Abstract:###The reparameterization trick has become one of the most useful tools in the field of variational inference. However, the reparameterization trick is based on the standardization transformation which restricts the scope of application of this method to distributions that have tractable inverse cumulative distribution functions or are expressible as deterministic transformations of such distributions. In this paper, we generalized the reparameterization trick by allowing a general transformation. Unlike other similar works, we develop the generalized transformation-based gradient model formally and rigorously. We discover that the proposed model is a special case of control variate indicating that the proposed model can combine the advantages of CV and generalized reparameterization. Based on the proposed gradient model, we propose a new polynomial-based gradient estimator which has better theoretical performance than the reparameterization trick under certain condition and can be applied to a larger class of variational distributions. In studies of synthetic and real data, we show that our proposed gradient estimator has a significantly lower gradient variance than other state-of-the-art methods thus enabling a faster inference procedure.", "review": "Review:###I am having a hard time validating the claims of contributions of this paper over the contributions of Ruiz et al.,(2016). Ruiz et al already propose the generalized method which allows the reparametrizated density to be dependent on the latent variable. They already derive the score function method and the standard reparameterization as special cases of their generalizaed reparameterization. So both the remarks on Page 3 area already known from previous works. The way this paper is structured and contributions are listed, it seems the authors completely overlook several contributions of Ruiz et al, while touting some of them as their own. e.g. Above Section 3 (last para of Section 2), the authors talk about the cases of Gamma, Beta etc which are hard to handle using the standard parameterization, but the authors fail to mention that the work of Ruiz et al is able to handle these perfectly well. The notations in the beginning of Section 3 is confusing to me. \rho is a transformation function, yet the authors say *let w() be the the density function of \rho*, because somehow \rho is suddenly a random variable ? How did that come about ? Similarly, some of the background material is misleading -- e.g. the authors say that the entropy term is *ignored* and they *focus on the simplified ELBO*, while that is not the case in VI. The gradient of entropy is more amenable, it is not ignored. The background is very poorly written and misleading. The possible connection to transport theory might be interesting, but the authors merely allude to it, listing the velocity field and picking the condition for unbiasedness out of thin air without any explanation or background. Simiarly the connection to control variate is only alluded to in less than 10 sentences, and in my humble opinion does not provide enough clarity for this to be a contribution. I suggest the authors clearly discuss the contributions over and above those of Ruiz et al (the obviation of calculation of full fledged jacobians) and discuss their work more vis-a-viz Ruiz et al, fix the background so that it is more faithful to what is known in the literature (how is the velocity field introduction different ?), provide the details of the method /before/ directly writing down the theorem for it (Thm 3.1), and clarify the connections to control variate in more detail rather than just merely alluding to it (may be also provide more detail in the background about it). The authors have listed the connection to control variates as a contribution but in the draft body it is merely a mention. Here is the direct quote *We can see that the GTRANS model is a special case of the control variate method with a complex differential structure*. In its current form the paper is not ready for publication IMHO."}
{"id": "iclr2020_176", "title": "StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding | OpenReview", "abstract": "Abstract:###Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman, we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks. The StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.", "review": "Review:###This paper introduces two new tasks for large scale language model pretraining: trigram word unscrambling and contextual sentence ordering. Using these tasks to pretrain on top of masked language modelling shows improvements when the resulting model is finetuned on downstream tasks. The proposed tasks are simple to implement, and particularly the sentence ordering task is an improvement over the original BERT next sentence task, which is widely regarded as too simple to drive learning good representations. For this reason, I recommend acceptance of this paper. Some minor quibbles: 1) Structure in language usually means syntactic structure. How does unscrambling word trigrams help uncover syntactic structure? The references to Elman 1990 also don*t serve to clarify anything, I suggest that they are removed. 2) Some prior work on word ordering (e.g. [1] and older papers cited therein) is missing. 3) The permutation objective seems very similar to the XLNet objective. Could the authors elaborate more on this in the paper? 4) Did the authors try with other n-gram shuffling orders? 5) The sentence ordering task has been used previously (e.g. [2]). 6) Table 1 overhangs the right margin. References: [1] Discriminative Syntax-Based Word Ordering for Text Generation, Zhang and Clark 2015 [2] Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning, Jernite et al. 2017"}
{"id": "iclr2020_177", "title": "On Predictive Information Sub-optimality of RNNs | OpenReview", "abstract": "Abstract:###Certain biological neurons demonstrate a remarkable capability to optimally compress the history of sensory inputs while being maximally informative about the future. In this work, we investigate if the same can be said of artificial neurons in recurrent neural networks (RNNs) trained with maximum likelihood. In experiments on two datasets, restorative Brownian motion and a hand-drawn sketch dataset, we find that RNNs are sub-optimal in the information plane. Instead of optimally compressing past information, they extract additional information that is not relevant for predicting the future. Overcoming this limitation may require alternative training procedures and architectures, or objectives beyond maximum likelihood estimation.", "review": "Review:###Summary: The paper investigate how optimal recurrent neural networks (RNNs) are at storing past information such that it is useful for predicting the future. The authors estimated optimality in terms of mutual information between the past and the future. If the RNN was able to retain MI between the past and the future, it then has kept optimal information from the past for predicting the future. The experiments suggest that RNNs are not optimal in terms of prediction of the future. It also suggest that this is due to the maximum likelihood training objective. Comments for the paper: 1. Overall, the paper is a very interesting read and it explores and analyzes RNN under a different light. It answers a fundamental question about RNN training. 2. There are a few things that would be nice to clarify. At the end of P3, the authors mentioned that the stochastic RNNs are trained either by a). deterministically during training and noise added during test or b) noise added during training and test. It is not very clear to me how the authors trained stochastic RNNs deterministically during training. It would be nice if this can be clarified. 3. I am also curious how this compares to the training methods for example used in https://papers.nips.cc/paper/7248-z-forcing-training-stochastic-recurrent-networks.pdf. It seems that this would also help with retaining RNN optimality in terms of predicting the future. it would be interesting to include a comparison to this method for example. Overall an interesting paper. However, I think a few things could be improved and I would be willing to rise the score if the authors could addressed the above points."}
{"id": "iclr2020_178", "title": "Classification Logit Two-sample Testing by Neural Networks | OpenReview", "abstract": "Abstract:###The recent success of generative adversarial networks and variational learning suggests training a classifier network may work well in addressing the classical two-sample problem. Network-based tests have the computational advantage that the algorithm scales to large samples. This paper proposes to use the difference of the logit of a trained neural network classifier evaluated on the two finite samples as the test statistic. Theoretically, we prove the testing power to differentiate two smooth densities given that the network is sufficiently parametrized, by comparing the learned logit function to the log ratio of the densities, the latter maximizing the population training objective. When the two densities lie on or near to low-dimensional manifolds embedded in possibly high-dimensional space, the needed network complexity is reduced to only depending on the intrinsic manifold geometry. In experiments, the method demonstrates better performance than previous network-based tests which use the classification accuracy as the test statistic, and compares favorably to certain kernel maximum mean discrepancy (MMD) tests on synthetic and hand-written digits datasets.", "review": " # ==== Summary of the paper ==== Given two samples (two sets) and , two-sample testing seeks to decide whether using only the samples and . This is a long-standing problem in statistics and machine learning. Many papers have addressed this problem. Among these, some approaches that are relevant here are: 1. Maximum Mean Discrepancy (MMD, Gretton et al., 2012), 2. Classifier two-sample test (net-acc, Lopez-Paz & Oquab, 2016) which sees two-sample testing as a binary classification problem. This paper proposes a new test statistic for two-sample testing. It is closely related to net-acc. The proposal is: 1. Train a classifier network with the softmax loss (or Bernoulli log likelihood) 2. Consider the two outputs in the last layer before the softmax (i.e., the logits). Call these u(x) and v(x) (corresponding to the two classes). Define . 3. Define the test statistic to be (see Eq 1) = empirical mean of f wrt X - empirical mean of f wrt Y. The paper contributes a number of interesting results: 1. Theorem 4.1. Briefly it states that (only the main message), if p!=q, then there is a neural network architecture such that the resulting test will eventually decide that p!=q (i.e., reject the null hypothesis H0: p=q), as the sample size n goes to infinity. 2. An extension of Theorem 4.1 to the case where the distributions p,q are supported on a manifold. The paper concludes with simulation studies on toy low-dimensional problems (Section 5.1), a 2d problem on a sphere (manifold), and a real problem with MNIST (distinguishing real digits from a mixture of real and generated digits). # ==== Review ==== The paper is well written overall. One main message from the paper is that the new statistic is good for detecting the difference in the tails of p and q. Under this setting, the paper shows with a 1d toy example that the new test has higher power than the MMD with a Gaussian kernel (Section 3). I think this is an interesting point and should be emphasized more. The proposed approach is most closely related to net-acc (Lopez-Paz & Oquab, 2016) in the sense that both see the two-sample problem as a binary classification problem, and define a test statistic based on a trained neural network classifier. The main difference is that net-acc uses the classification loss as the statistic, making it less sensitive to small changes between p and q, compared to what is proposed in the present paper. The presented theoretical result (Theorem 4.1) is interesting. It is rare to see this kind of *approximability* result in the context of two-sample testing i.e., by this, I refer to the statement *there exists a network with O(..) many parameters that can detect the differences.* Having said that, the proof is largely based on the work of Yarotsky, 2018. Still, I feel that the present paper has put together nice theoretical results. I also would like to praise this paper for its honesty. I appreciate that the paper comments on when the proposed method works well (i.e., difference in the tail), and when it does not (e.g., mean shift, or variance difference in Figure 2), without overselling it. The paper also directly mentions that the comparison in Figure 1 is unfair to MMD, which is true. I reassure the authors that doing this only had positive impact on the evaluation, and encourage the authors to keep this practice. Well done. Having said that, I do have some concerns. I hope that the following points can be addressed. # ==== Major comments, questions ==== 1. The work *empirical density* is used a few times. Do you mean empirical measures? 2. According to Section 2.3, the null distribution is simulated by permutation. Do you need to retrain the network for each permutation? If so, this would be quite expensive. Also in the 3rd line, *... recompute the test statistics for times, typically a few tens.*, I don*t think that is enough. I expect at least a few hundreds. 3. Major comment: As noted in the paper that the proposed approach is unstable (to train) when the sample size is small. The proposed approach is good when the difference is in the tail area. I really think this point should be studied more. It would be nice to have more theory to justify this point. Alternatively, one can also consider a simulation study with large n (to make training more stable), and consider p,q that differ in the tail areas. Also consider D (input dimension) much larger than 1. This will make the paper stronger. 4. Why is there no loss of generality to consider the unit ball as the domain (in Section 4.2)? 5. The setting of the 2d manifold experiment is unclear. Write the definitions of p and q. It is unclear what *density departure* means here. Also, why is the proposed net-logit good for this problem? 6. Corollary 4.2: This is a *semi* asymptotic statement, isn*t it? Is it correct that the power expression on the right hand side is only approximate (for large n)? 7. Section 5.1: for gmmd-ad, why did you choose to maximize the kernel MMD (for choosing the right Gaussian bandwidth)? Isn*t it more principled to maximize the test power as done in Sutherland et al., 2016? See Gretton et al., 2012b (in your citation list) for an empirical result that compares the two ways for choosing the kernel bandwidth. They showed that maximizing the test power gives better test power. Note: Gretton et al., 2012b considers a weighted sum of MMDs which is slightly different. But still, it is worth trying maximizing the test power. 8. Section 5.2: is it correct that gmmd uses a Gaussian kernel on the raw pixels of MNIST? If so, the poor test power of gmmd, gmmd-ad is not surprising. This is to do with a wrong kernel rather than the statistic itself. More interestingly, why does net-logit (proposed) have slightly higher test power than net-acc in this case? I understand that both use the same network. Willing to reconsider my evaluation after checking the authors* responses. # ==== Minor. Did not affect the score ==== * In the intro: *... often restricted to data of small dimensionality and/or small sample size, or certain specific classes of densities ....*. This sentence is a bit vague. MMD is a metric if used with a characteristic kernel. * Intro, 2nd paragraph: *... distinguish the model density q produced by a generative network ...* GAN models in general do not have a tractable density function available. For flow-based models, yes. But the cited papers in this sentence do not study flow-based models (normalizing flows). * I would move section 1.1 (related works) to be after Section 3. Better get into the main section quickly. But just a suggestion. It is up to the authors. * Section 1.1: *...Compared to kernel methods, neural networks are algorithmically more efficient ...*, I would avoid saying this since kernel methods cover a broad class of models/algorithms, depending on the kernel. For a finite-dimensional kernel (say, defined as the inner product of the outputs of a network), the MMD can be computed in O(n). * Section 2.2: (yj, 0) -> (yj, 1) * Throughout the paper: gaussian -> Gaussian. * Section 3: eq 2, write p and q. Also in Figure 1, put a legend for p and q in the first subfigure (p=blue, q=red). * After eq 2: *difference of the densities* -> *mixing proportion*. * Section 4.2: *The above analysis is for general p and q...*. -> *...for general compactly supported ."}
{"id": "iclr2020_179", "title": "MxPool: Multiplex Pooling for Hierarchical Graph Representation Learning | OpenReview", "abstract": "Abstract:###Graphs are known to have complicated structures and have myriad applications. How to utilize deep learning methods for graph classification tasks has attracted considerable research attention in the past few years. Two properties of graph data have imposed significant challenges on existing graph learning techniques. (1) Diversity: each graph has a variable size of unordered nodes and diverse node/edge types. (2) Complexity: graphs have not only node/edge features but also complex topological features. These two properties motivate us to use multiplex structure to learn graph features in a diverse way. In this paper, we propose a simple but effective approach, MxPool, which concurrently uses multiple graph convolution networks and graph pooling networks to build hierarchical learning structure for graph representation learning tasks. Our experiments on numerous graph classification benchmarks show that our MxPool has marked superiority over other state-of-the-art graph representation learning methods. For example, MxPool achieves 92.1% accuracy on the D&D dataset while the second best method DiffPool only achieves 80.64% accuracy.", "review": " This paper introduces a new hierarchical graph representation learning method for graph classification. It builds upon the diffpool method. Specifically, the authors propose the multiplex convolution and the multiplex pooling operation. The multiplex convolution learns multiple graph convolution operations (potentially with different parameters) and merges them using a neural network. Similarly, the multiplex pooling learns multiple assignment matrices utilizing diffpool and merges them using a neural network. The effectiveness of the proposed model is demonstrated with experiments in several standard benchmark datasets for graph classification. Strengths The overall architecture of the proposed method can be regarded as an extension of diffpool method by using multiple convolution and pooling operations. An analysis on how the number of convolution and pooling operations can affect the performance of the model shows the effectiveness of the proposed multiplex convolution/pooling operations. The effectiveness of the proposed model is further verified by the experimental results on 5 standard benchmark datasets. Weakness The novelty of the paper is limited. The major contribution of this paper is to utilize multiple convolution/pooling operations and merge them with neural networks. It is not clear how issues mentioned in the motivational examples shown in Figure 1 can be solved by the proposed model. As discussed in Figure 1(a) and Figure 1(b), small graphs may prefer small embedding while large graphs may prefer large embedding. However, it is unclear how the proposed model achieves this goal as the combination of the embeddings from different convolution operations is through a neural network that is shared by all graphs. Similarly, for Figure 1(c) and Figure 1(d), it is not clear how the proposed model can focus more on one type of information than the other. It would be helpful if there are some empirical results to demonstrate this. Recommendations It would be better if the authors can provide some analysis on time complexity of the proposed model. The major concern of the efficiency is the introducing of fully connected graph after each pooling as mentioned in Section 3.3. Minor comments In section 3.3, “is a GCN different” -> “is a GCN variant”? In Section 3.3, should there be a softmax function after Eq.(5)? In section 4.2, “in ter90o8ims” -> “in terms”?"}
{"id": "iclr2020_180", "title": "Curriculum Loss: Robust Learning and Generalization against Label Corruption | OpenReview", "abstract": "Abstract:###Deep neural networks (DNNs) have great expressive power, which can even memorize samples with wrong labels. It is vitally important to reiterate robustness and generalization in DNNs against label corruption. To this end, this paper studies the 0-1 loss, which has a monotonic relationship between empirical adversary (reweighted) risk (Hu et al. 2018). Although the 0-1 loss is robust to outliers, it is also difficult to optimize. To efficiently optimize the 0-1 loss while keeping its robust properties, we propose a very simple and efficient loss, i.e. curriculum loss (CL). Our CL is a tighter upper bound of the 0-1 loss compared with conventional summation based surrogate losses. Moreover, CL can adaptively select samples for stagewise training. As a result, our loss can be deemed as a novel perspective of curriculum sample selection strategy, which bridges a connection between curriculum learning and robust learning. Experimental results on noisy MNIST, CIFAR10 and CIFAR100 dataset validate the robustness of the proposed loss.", "review": "Review:###After rebuttal, I think the authors made a valid argument to address my concerns on evaluation. So, I*d like to increase my score as weak accept! ===== Summary: To handle noisy labels, this paper proposed a curriculum loss that corresponds to the upper bound of 0-1 loss. Using synthetic noisy labels on MNIST and CIFAR, the authors verified that the proposed method can significantly improve the robustness against noisy labels. Detailed comments: Overall, the paper is well-written and the ideas are novel. However, experiments are a little weak due to weak baselines and experimental setups (see suggestions for more details). I will consider raising my score according to the rebuttal. Suggestions: 1. Could the authors consider more baselines like D2L [Ma* 18] and Reweight [Ren* 18] 2. Similar to [Lee* 19], could the authors evaluate the performance of the proposed methods on more realistic noisy labels such as semantic noisy labels and open-set noisy labels? [Lee* 19] robust inference via generative classifiers for handling noisy labels, In ICML, 2019. [Ma* 18] Dimensionality-Driven Learning with Noisy Labels, In ICML, 2018. [Ren* 18] Learning to Reweight Examples for Robust Deep Learning, In ICML, 2018."}
{"id": "iclr2020_181", "title": "RGTI:Response generation via templates integration for End to End dialog | OpenReview", "abstract": "Abstract:###End-to-end models have achieved considerable success in task-oriented dialogue area, but suffer from the challenges of (a) poor semantic control, and (b) little interaction with auxiliary information. In this paper, we propose a novel yet simple end-to-end model for response generation via mixed templates, which can address above challenges. In our model, we retrieval candidate responses which contain abundant syntactic and sequence information by dialogue semantic information related to dialogue history. Then, we exploit candidate response attention to get templates which should be mentioned in response. Our model can integrate multi template information to guide the decoder module how to generate response better. We show that our proposed model learns useful templates information, which improves the performance of *how to say* and *what to say* in response generation. Experiments on the large-scale Multiwoz dataset demonstrate the effectiveness of our proposed model, which attain the state-of-the-art performance.", "review": "Review:###The basic idea of integrating templates for dialog generation is interesting. However, the implementation is confusing, yet another architecture, with clear motivations, intuitions, or even clarity. Worst of all, the experimental results are missing in table 2, the only placeholder for results in the paper. Further, I would appreciate human evaluation, rather than using BLEU as the metric. The latter is known to be inappropriate for dialog modeling evaluation. Section 2.1 needs more explanation. (Strong Reject)"}
{"id": "iclr2020_182", "title": "Dynamically Balanced Value Estimates for Actor-Critic Methods | OpenReview", "abstract": "Abstract:###Reinforcement learning in an actor-critic setting relies on accurate value estimates of the critic. However, the combination of function approximation, temporal difference (TD) learning and off-policy training can lead to an overestimating value function. A solution is to use Clipped Double Q-learning (CDQ), which is used in the TD3 algorithm and computes the minimum of two critics in the TD-target. We show that CDQ induces an underestimation bias and propose a new algorithm that accounts for this by using a weighted average of the target from CDQ and the target coming from a single critic. The weighting parameter is adjusted during training such that the value estimates match the actual discounted return on the most recent episodes and by that it balances over- and underestimation. Empirically, we obtain more accurate value estimates and demonstrate state of the art results on several OpenAI gym tasks.", "review": "Review:###This paper proposes the balanced clipped double estimator that uses a weighted averaged of the target from CDQ (leading to underestimations) and the target from a single critic (causing overestimations), to better estimate the Q value. The weight is adjusted by a simple heuristic during the training. Experimental results show that the BTD3 algorithm outperforms TD3, SAC, DDPG on six MuJoCo environments. How to get accurate function approximations is a very important problem. The idea that using a weight to balance the target value from CDQ and the target value from the single critic is welled motivated and interesting. Figure 1 gives a good motivation example to show how BCDQ reduces the estimator error of Q value. I appreciate that the authors plot the detail of beta in the training in Figure 2. The experimental results are somewhat weak as there are no results on Swimmer and HumanoidStandup. And I do not think BTD3 significantly outperforms other baselines in Ant, Walker2d, and Reacher. The paper would be more convincing with the result of BTD3 with fixed beta in Figure 1. Questions: 1. Are there any statistics of the estimation errors of TD3, DDPG, and BTD3 in all environments?"}
{"id": "iclr2020_183", "title": "Hierarchical Graph Matching Networks for Deep Graph Similarity Learning | OpenReview", "abstract": "Abstract:###While the celebrated graph neural networks yields effective representations for individual nodes of a graph, there has been relatively less success in extending to deep graph similarity learning. Recent work has considered either global-level graph-graph interactions or low-level node-node interactions, ignoring the rich cross-level interactions between parts of a graph and a whole graph. In this paper, we propose a Hierarchical Graph Matching Network (HGMN) for computing the graph similarity between any pair of graph-structured objects. Our model jointly learns graph representations and a graph matching metric function for computing graph similarity in an end-to-end fashion. The proposed HGMN model consists of a multi-perspective node-graph matching network for effectively learning cross-level interactions between parts of a graph and a whole graph, and a siamese graph neural network for learning global-level interactions between two graphs. Our comprehensive experiments demonstrate that our proposed HGMN consistently outperforms state-of-the-art graph matching networks baselines for both classification and regression tasks.", "review": "Review:###In this paper, a hierarchical graph matching network, which considers both graph-graph interaction and node-graph interaction, is proposed. Specifically, the graph-graph interaction is modeled through graph level embeddings learned by GCN with pooling layers. While node-graph interaction is modeled using node embedding learned by GCN and attentive graph embedding aggregated from node embedding. Some concerns about the paper are as follows: 1) The novelty of the paper is incremental. The major contribution of the paper lies in the propose of multi-perspective matching function , which is somewhat similar to the Neural Tensor Networks proposed in [1] and utilized in [2] Although, in [2], the Neural Tensor Network is used to measure the similarity between graph-level embeddings. 2) Some of the technical details of the paper is not clearly presented or well explained. a. In Eq. (7), attentive graph-level embeddings are calculated using weighted average of its node embeddings. However, it is not clear which node from the other graph should be used to calculate the weights (alpha_{I,j}, _x0008_eta_{i,j}). Furthermore, it is also not clear why the attention score should solely base on a single node from the other graph rather than the information of the entire graph. b. In Eq. (10), it would be better if the authors could provide more motivations about using Bi-LSTM aggregator. Especially, the embeddings to be aggregated are unordered. What are the two directions in Bi-LSTM in this case? What is the benefit of using Bi-LSTM as aggregator compared with LSTM aggregator or other aggregators? Some other questions to be clarified: 1) Why different similarity score functions are adopted for the classification task and the regression task? 2) For the classification task, the mean squared error loss is adopted. Why not using other more commonly used loss for classification task? Suggestions: It would be better if the authors could empirically show the effectiveness of the Bi-LSTM aggregator. It would be helpful if the authors could conduct some investigation on how the number of perspectives affect the performance of the model. [1] Reasoning With Neural Tensor Networks for Knowledge Base Completion [2] SimGNN: A Neural Network Approach to Fast Graph Similarity Computation"}
{"id": "iclr2020_184", "title": "Compressive Transformers for Long-Range Sequence Modelling | OpenReview", "abstract": "Abstract:###We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.", "review": " This paper proposes a way to compress past hidden states for modeling long sequences. Attention is used to query the compressed representation. The authors introduce several methods for compression such as convolution, pooling etc. The outcome is a versatile model that enables long-range sequence modeling, achieving strong results on not only language model tasks but also RL and speech. For testing and evaluating the modeling of really long context sequence modeling, the authors introduce PG-19, a new benchmark based on Project Gutenberg narratives. The idea is a simple and straightforward one. The choices of compression functions are intuitive and natural. The probably more interesting part of this paper is the training schemes designed to train the memory compression network. Results are very strong and there is a pretty diverse set of experiments. That said, it seems like a huge amount of resources were spent on this work alone. It also seems like these models are not trivial to train (or get them to work). It would be interesting to find out how much resources were spent (in terms of preliminary experiments) to getting these models to start working decently. There are also no reports of parameter counts, which might make the experiments unfair. Achieving SOTA is one thing, which could be attributed to large resource pools and maybe larger parameter sizes of models. Overall, I am voting for a weak accept. While this paper is more incremental and novelty may be slightly lacking, I think the breadth of experiments and competitive results warrants an acceptance. Several issues and questions for the authors: 1) Why are the results on PG-19 not reported in a Table format? Why are there no results of the base Transformer on PG-19? I think this is really necessary and should be reported. 2) The authors mention that this memory compression architecture enables long sequence modeling. However, is there an intended way of use for long-text that is not necessarily framed as a LM problem? For instance, results on NarrativeQA benchmark would be nice. UPDATE: I have read the author response and other reviewer*s comments. I am happy with the efforts made by the authors and I am raising my score to 8 (accept)."}
{"id": "iclr2020_185", "title": "Fast Neural Network Adaptation via Parameters Remapping | OpenReview", "abstract": "Abstract:###Deep neural networks achieve remarkable performance in many computer vision tasks. However, for most semantic segmentation (seg) and object detection (det) tasks, the backbone of the network directly reuses the network manually designed for classification tasks. Utilizing a network pre-trained on ImageNet as the backbone has been a popular practice for seg/det challenges. However, because of the gap between different tasks, adapting the network directly to the target task could bring performance promotion. Some recent neural architecture search (NAS) methods search for the backbone of seg/det networks. ImageNet pre-training of the search space representation or the searched network bears huge computational cost. In this paper, we propose a fast neural network adaptation method FNA, which can adapt the manually designed network on ImageNet to the new seg/det tasks efficiently. We adopt differentiable NAS to adapt the architecture of the network. We first expand the manually designed network to a super network which is the representation of the search space. Then we successively conduct the adaptation on the architecture-level and parameter-level. Our designed parameters-remapping paradigm accelerates the adaptation process. Our experiments include both seg and det tasks. We conduct adaptation on the MobileNetV2 network. FNA demonstrates performance promotion compared with both manually and NAS designed networks. The total computational cost of FNA is much less than many SOTA seg/det NAS methods, 1737x less than DPC, 6.8x less than Auto-DeepLab and 7.4x less than DetNAS.", "review": "Review:###In this paper, the authors take a MobileNet v2 trained for ImageNet classification, and adapt it either (i) semantic segmentation on Cityscapes, or (ii) object detection on COCO. They do this by first expanding the network into a *supernet* and copy weights in an ad-hoc manner, then, they perform DARTS-style architecture search before fine-tuning for the task at hand. There is no TLDR for this paper, and I must admit, on reading the abstract and introduction I wasn*t entirely sure what this paper was doing at first. Perhaps I was being slow. From a narrative perspective, one of the main selling points is not needing to perform any expensive ImageNet pre-training; however, a pre-trained MobileNetv2 is being utilised. While this was off-the-shelf, it still incurred an initial training cost, so it isn*t really fair in e.g. Table 4 to put pre-training cost as zero. On a related note, the authors write that this network is used for its *generality*. I*d argue that MobileNetv2 is a highly engineered network specialised for mobile computation; a standard ResNet-50 would be more general really. I would like to see a comparison to a random search, as there are several papers (https://arxiv.org/abs/1902.07638, https://arxiv.org/abs/1902.08142) indicating that this is a very strong baseline. As mentioned earlier, the choices for remapping weights seem very ad-hoc. I can*t really tell what*s going on in Table 5 (why is PR in the NE and PA row?) so the ablation study of how effective this weight mapping is lost on me. The stuff in Table 6 is pretty interesting however, if convoluted. I find the odd choices of hyperparameters (tau as 45, gamma as 10, lambda as 9e-3) rather alarming. How important are these? Would this technique work under any other circumstances? Error bars would be a welcome inclusion, particularly in Table 3 where you have 0.1% separating FNA and MNasnet-92. I appreciate that this can be expensive however. Pros: - Some promising results - Good figures Cons: - Ad-hoc design choices - Not a fair comparison regarding pre-training. - Very specific to one network choice - Lack of error bars or comparison to random search. I am giving this paper a weak reject, as there is insufficient experimental evidence that the technique works, or generalises beyond Mobilenetv2. I am also concerned about the ad-hoc hyperparmaters or weight-mapping. A comprehensive ablation study, along with error bars, and another choice of seed network would do much to strengthen this paper."}
{"id": "iclr2020_186", "title": "Physics-as-Inverse-Graphics: Unsupervised Physical Parameter Estimation from Video | OpenReview", "abstract": "Abstract:###We propose a model that is able to perform physical parameter estimation of systems from video, where the differential equations governing the scene dynamics are known, but labeled states or objects are not available. Existing physical scene understanding methods require either object state supervision, or do not integrate with differentiable physics to learn interpretable system parameters and states. We address this problem through a \textit{physics-as-inverse-graphics} approach that brings together vision-as-inverse-graphics and differentiable physics engines, where objects and explicit state and velocity representations are discovered by the model. This framework allows us to perform long term extrapolative video prediction, as well as vision-based model-predictive control. Our approach significantly outperforms related unsupervised methods in long-term future frame prediction of systems with interacting objects (such as ball-spring or 3-body gravitational systems), due to its ability to build dynamics into the model as an inductive bias. We further show the value of this tight vision-physics integration by demonstrating data-efficient learning of vision-actuated model-based control for a pendulum system. We also show that the controller*s interpretability provides unique capabilities in goal-driven control and physical reasoning for zero-data adaptation.", "review": "Review:###The paper proposes to integrate model-based physical simulation and data-driven (deep) learning. In a nutshell, one deep network predicts the state variables of the physics simulation (such as objet location, shape and velocity) from an image. A second network does the inverse task, to render images given the state variables (and a background image). In this way, one can go from a video frame to a physical system state, modify the state with physics simulation, and then go back from the modified state to a video frame. Together with a differentiable physics engine, through which one can back-propagate, this makes it possible to use the un-annotated video itself as supervision. At the same time, the two neural networks can be seen as an auto-encoder, in which the latent state is explicitly constrained to correspond to the desired physical state variables. The topic of the paper is hot: a proper integration of physical models with data-driven deep learning is, arguably, one of the big short- to mid-term themes of machine learning research. The way it is done in the present paper intuitively makes sense. The approach is fairly obvious at the conceptual level; but in the details poses a number of technical challenges especially for the decoder, which are nicely analysed and resolved. Some minor design choices are not well justified and at first sight appear a bit l*art pour l*art. While it is a sensible, pragmatic choice to first predict object masks, then extract their location ands and velocities in a second step; I do not quite see why one would have to do the latter with neural networks. it would seem that once the masks have been found, their location can be chosen as something like the (perfectly differentiable) mask-weighted centroid and does not need a multi-layer network; and similarly that deriving velocity from locations in adjacent frames can be hard-coded and does not need a 3-layer network. The experiments are still at an early *toy* level, with synthetic videos where high-contrast, homogeneous objects move in front of a uniform or blurry background. The baselines are sensible and ablation studies are done with care. Still, it would have been nice to also run the method on some real video. To my understanding, this would be easily possible at least for future frame prediction, all one has to do is either annotate the objects in the target frame or measure success by comparing the predicted and true frames at the image level. It is also not clear whether the videos were synthesised with the same physics engine also use inside the system - which would be slightly questionable, in the sense that the learnable pipeline is then a-priori matched to the biases in the data. One comment on the presentation: while the paper is generally well-written and easy to follow, the wording could at times be more careful. There is a slight tendency to identify the particular (simple) physical systems of the paper with physics as a whole. E.g., not all physics simulation must have objects - for instance, fluid dynamics or radiative transfer do not have individual objects, but are nevertheless relevant in the context of visual data. Similarly, even for defined objects, position and velocity are not always a sufficient state, for instance objects might deform, or have different elastic properties when colliding. Overall, I find the work interesting and well-executed. It is a natural step to take towards the important goal of integrated data-driven and physical models, including the associated theme of self-supervision via physical constraints. On the negative side the paper does make a slightly rushed and unfinished impression by not showing any, even qualitative, experiments on real video. Most people - rightly - use simple toy-like datasets for development and analysis. But showing only those gives me the impression that the paper was written too early, just to be the first and to make the deadline. Or that moving to real video poses a much greater challenge than expected - but then this should be stated and discussed."}
{"id": "iclr2020_187", "title": "Truth or backpropaganda? An empirical investigation of deep learning theory | OpenReview", "abstract": "Abstract:###We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike. We study the prevalence of local minima in loss landscapes, whether small-norm parameter vectors generalize better (and whether this explains the advantages of weight decay), whether wide-network theories (like the neural tangent kernel) describe the behaviors of classifiers, and whether the rank of weight matrices can be linked to generalization and robustness in real-world networks.", "review": "Review:###The authors look at empirical properties of deep neural networks and discuss their connection to past theoretical work on the following issues: * Local minima: they give an example of setting where bad local minima (far from the global minimum) are obtained. More specifically, they show such minima can be obtained by initializing with large random biases for MLPs with ReLU activation. They also provide a theoretical result that can be used to find a small set of such minima. I believe this is a useful incremental step towards a better understanding of local minima in deep learning, although it is not clear how many practical implications this has. One question that would ideally be answered is: in practical settings, to what degree does bad initialization cause bad performance specifically due to bad minima? (as opposed to, say, slow convergence or bad generalization performance). * Weight decay: the authors penalize the size of the norm of the weights as it diverges from a constant, as opposed to when it diverges from 0 as is normally done for weight decay. They show that this works as well or better than normal weight decay in a number of settings. This seem to put into question the belief sometimes held that solutions with smaller norms will generalize better. * Kernel theory: the authors try to reproduce some of the empirical properties predicted in the Neural Tangent Kernel paper (Jacot et al., 2018) in particular by using more realistic architectures. The results, however, do not appear very conclusive. This might be the weakest part of the paper, as it is hard to draw anything conclusive from their empirical results. * Rank: The authors challenge the common belief that low rank provides better generalization and more robustness towards adversarial attacks. When enforcing a low or high rank weight matrices during training on ResNet-18 trained on CIFAR-10, the two settings have similar performance and are similarly robust to adversarial attacks, showing at least one counter example. I think overall this is a useful although somewhat incremental paper, that makes progress in the understanding of the behavior of neural networks in practice, and can help guide further theoretical work and the development of new and improved training techniques and initialization regimes for deep learning. Other comments/notes: * minor: the order of the last 2 sub topics covered (rank and NTK) is flipped in the introduction, compared to the abstract and the order of the chapters * in the table confidence intervals are given, it would be nice to have more details on how they are computed, (e.g. +- 1.96 * std error) * how is the constant mu in the norm-bias chosen?"}
{"id": "iclr2020_188", "title": "Adapting Behaviour for Learning Progress | OpenReview", "abstract": "Abstract:###Determining what experience to generate to best facilitate learning (i.e. exploration) is one of the distinguishing features and open challenges in reinforcement learning. The advent of distributed agents that interact with parallel instances of the environment has enabled larger scale and greater flexibility, but has not removed the need to tune or tailor exploration to the task, because the ideal data for the learning algorithm necessarily depends on its process of learning. We propose to dynamically adapt the data generation by using a non-stationary multi-armed bandit to optimize a proxy of the learning progress. The data distribution is controlled via modulating multiple parameters of the policy (such as stochasticity, consistency or optimism) without significant overhead. The adaptation speed of the bandit can be increased by exploiting the factored modulation structure. We demonstrate on a suite of Atari 2600 games how this unified approach produces results comparable to per-task tuning at a fraction of the cost.", "review": "Review:###This in an interesting paper as it tries to alleviate the burden of hyper-parameters tuning for exploration strategies Deep Reinforcement learning. The paper proposes an adaptive behaviour in order to shape the data generation process for effective learning. The paper considers a behaviour policy that is parametrized by a set of variables z called modulations: for example the Boltzmann softmax temperature, the probability epsilon for epsilon-greedy, per-action biases, .. The author frame the modulations search into a non-stationary multi-armed bandit problem and proposes to adapt the modulations according to a proxy to the learning progress. The author provides thorough experimental results. Comments: - All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL. - The proposed proxy is simply the empirical episodic return. It is not well explained in the paper how this proxy correlates with the Learning progress criteria. - The proxy seems to encourage selecting modulations that lead to generate most rewarding trajectories. How this proxy incentives the agent to explore poorly-understood regions? In other terms, how this proxy help to tradeoff between exploration and exploitation ? - The modulation adaptation problem is framed into non-stationary multi-armed bandit problem but the authors present a heuristic to solve it instead of using provably efficient bandit algorithm such as exponential weight methods (Besbes et al 2014) or Thompson sampling (Raj & Kalyani 2017) cited in the paper. - The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me. They estimate a certain probability at time step t by empirical frequency based on data from previous time steps. But as the parameters change during the learning, the f_t’(z) at time t’ < t is not distributed as f_t(z). This introduces a biases in the estimate. - I appreciate the thorough empirical results and ablation studies in the main paper and the appendix. They are really interesting. - I am confused what is the fixed reference in Figure 6. It is not explained in the main paper. Is it a baseline with the best hyperprameters in hindsight? - From the plots of learning curves in appendix, the proposed methods doesn’t seem to show a huge boost of performance comparing to the uniform bandit. Could you show aggregated comparison between the proposed method and uniform bandit similarly to what is done in Figure 4 ?"}
{"id": "iclr2020_189", "title": "Learning with Social Influence through Interior Policy Differentiation | OpenReview", "abstract": "Abstract:###Animals develop novel skills not only through the interaction with the environment but also from the influence of the others. In this work we model the social influence into the scheme of reinforcement learning, enabling the agents to learn both from the environment and from their peers. Specifically, we first define a metric to measure the distance between policies then quantitatively derive the definition of uniqueness. Unlike previous precarious joint optimization approaches, the social uniqueness motivation in our work is imposed as a constraint to encourage the agent to learn a policy different from the existing agents while still solve the primal task. The resulting algorithm, namely Interior Policy Differentiation (IPD), brings about performance improvement as well as a collection of policies that solve a given task with distinct behaviors", "review": "Review:###This paper proposes a new way to incentivize diverse policy learning in RL agents: the key idea is that each agent receives an implicit negative reward (in the form of an early episode termination signal) from previous agents when an episode begins to resemble prior agents too much (as measured by the total variational distance measure between the two policy outputs). Results on three Mujoco tasks are mixed: when PPO is combined with the proposed objective for training diverse policies, it results in very strong performance boosts on Hopper and HalfCheetah, but falls significantly short of standard PPO on Walker 2D. I would have liked to see a deeper analysis of what makes the approach work in some environments and not in others. Experimental comparisons in the paper are only against alternative approaches to optimize the same diversity objective as the proposed approach (with weighted sum of rewards (WSR) or task novel bisection(TNB)). Given that this notion of diversity is itself being claimed as a contribution, I would expect to see comparisons against prior methods, such as in DIAYN. There are other methods that have been proposed before in similar spirit to induce diversity in the policies learned. Aside from the evolutionary approaches covered in related work, within RL too, there have been methods such as the max-entropy method proposed in Eysenbach et al, *Diversity is All You Need...*. These methods, evolutionary and RL, could be compared against to make a more convincing experimental case for the proposed approach. The experimental setting is also not fully clear to me: throughout experiments, are the diversity methods being evaluated for the average performance over all the policies learned in sequence to be different from prior policies? Or only the performance of the last policy? Related, I would be curious to know, if K policies are trained, the reward vs the training order k of the K policies. This is close to, but not identical to the study in Fig 4, to my understanding. Aside from the above points being unclear, the paper in general could overall be better presented. While I am not an expert in this area, I would still expect to be able to understand and evaluate the paper better than I did. - Sec 3.1 makes a big deal of metric distance, but never quite explains how this is key to the method. - The exact baselines used in experiments are unhelpfully labeled *TNB* (with no nearby expansion) and *weighted sum of rewards (WSR)*, with further description moved to appendix. In general, there are a few too many references to appendices. - The results in Fig 2 are difficult to assess for diversity, and this is also true for the video in the authors* comment. - There is an odd leap in the paper above Eq 7, where it claims that *social uniqueness motivates people in passive ways*, which therefore suggests that *it plays more like a constraint than an additional target*. - Sec 5.1 at one point points to Table 1 for *detailed comparison on task related rewards* but says nothing about any important conclusions from the table. - There are grammar errors throughout."}
{"id": "iclr2020_190", "title": "Alternating Recurrent Dialog Model with Large-Scale Pre-Trained Language Models | OpenReview", "abstract": "Abstract:###Existing dialog system models require extensive human annotations and are difficult to generalize to different tasks. The recent success of large pre-trained language models such as BERT and GPT-2 have suggested the effectiveness of incorporating language priors in down-stream NLP tasks. However, how much pre-trained language models can help dialog response generation is still under exploration. In this paper, we propose a simple, general, and effective framework: Alternating Recurrent Dialog Model (ARDM). ARDM models each speaker separately and takes advantage of the large pre-trained language model. It requires no supervision from human annotations such as belief states or dialog acts to achieve effective conversations. ARDM outperforms or is on par with state-of-the-art methods on two popular task-oriented dialog datasets: CamRest676 and MultiWOZ. Moreover, we can generalize ARDM to more challenging, non-collaborative tasks such as persuasion. In persuasion tasks, ARDM is capable of generating human-like responses to persuade people to donate to a charity.", "review": " This paper explores methods to incorporate a pretrained language model into a dialog system. The authors propose Alternating Recurrent Dialog Model (ARDM) where the pretrained language model is used to initialize both the user LM and the system LM. The authors also present a memory module to augment this dialog system where each memory slot contains a <key, value> pair derived from hidden states for each dialog turn. I think the writing of the model section could be improved. A few clarification questions for the authors: - How is the hidden state for the dialog turn t (i.e., h_t) derived from hidden states of tokens in turn t? Supposed there are N tokens in turn t, which Transformer hidden state is used as h_t? - How is h_t used to compute p(w_i | w_{<i}, u_{<t}, s_{<t})? If I understand correctly, the authors use t to index dialog turns, but then they seem to use the same symbol to denote token indices in the same section. So the exact model, although appears to be simple, is very confusing to me. In terms of experiment results, the authors show that their approach improves over the basic GPT-2 and is competitive with baseline methods that rely on more supervision.A few clarification questions regarding the experiments: - Did the authors tune the GPT-2 model on each dataset, similar to ARDM as well? Or are the GPT-2 results shown in Table 1 after fine-tuning? In summary, this paper is a plug-and-play extension of the GPT-2 pretrained model that could be explained more clearly."}
{"id": "iclr2020_191", "title": "SDGM: Sparse Bayesian Classifier Based on a Discriminative Gaussian Mixture Model | OpenReview", "abstract": "Abstract:###In probabilistic classification, a discriminative model based on Gaussian mixture exhibits flexible fitting capability. Nevertheless, it is difficult to determine the number of components. We propose a sparse classifier based on a discriminative Gaussian mixture model (GMM), which is named sparse discriminative Gaussian mixture (SDGM). In the SDGM, a GMM-based discriminative model is trained by sparse Bayesian learning. This learning algorithm improves the generalization capability by obtaining a sparse solution and automatically determines the number of components by removing redundant components. The SDGM can be embedded into neural networks (NNs) such as convolutional NNs and can be trained in an end-to-end manner. Experimental results indicated that the proposed method prevented overfitting by obtaining sparsity. Furthermore, we demonstrated that the proposed method outperformed a fully connected layer with the softmax function in certain cases when it was used as the last layer of a deep NN.", "review": " The paper proposes a discriminative Gaussian mixture model with a sparsity prior over the decoding weight. They can automatically learn the number of components with the sparsity prior and learn Gaussian-structured feature space. 1. I think the model is just ARD prior over discriminative GMM which is not that novel. DGMM models have been for a while [1,2]. Adding ARD sparsity prior over the decoding weight is also a classic routine. It*s also well known that ARD can do feature selection and removal. [1] Discriminative gaussian mixture models for speaker verification [2] Discriminative Gaussian mixture models: A comparison with kernel classifiers 2. I don*t think differentiating between discriminative GMM and generative GMM would make such a big deal. DGMM is basically Gaussian mixtures existing for each class. Any skill applied to GMM can be applied to DGMM. There are many works for component number selection for GMM with non-parametric Bayesian methods. For example, Dirichlet Process Mixture Model can automatically learn the number of components without predefining. 3. Only comparing SDGM with LR, SVM and RVM is quite weak, not mentioning that the performance is not that dominatingly better. SDGM is GMM+LR. So SDGM should be better than LR if the data has structures. What SVM you compare with? Do you use nonlinear kernels which can learn better nonlinear feature space? Overall, I think the contribution of the paper is a bit incremental. I vote for a rejection."}
{"id": "iclr2020_192", "title": "Low-Resource Knowledge-Grounded Dialogue Generation | OpenReview", "abstract": "Abstract:###Responding with knowledge has been recognized as an important capability for an intelligent conversational agent. Yet knowledge-grounded dialogues, as training data for learning such a response generation model, are difficult to obtain. Motivated by the challenge in practice, we consider knowledge-grounded dialogue generation under a natural assumption that only limited training examples are available. In such a low-resource setting, we devise a disentangled response decoder in order to isolate parameters that depend on knowledge-grounded dialogues from the entire generation model. By this means, the major part of the model can be learned from a large number of ungrounded dialogues and unstructured documents, while the remaining small parameters can be well fitted using the limited training examples. Evaluation results on two benchmarks indicate that with only training data, our model can achieve the state-of-the-art performance and generalize well on out-of-domain knowledge.", "review": "Review:###This paper studies knowledge-grounded dialogue response generation in the low-resource setting. More precisely, it proposes a disentangled decoder consisting of three components: language model, context processor, and document reader. Disentangled decoder architecture provides a flexibility to train (or pre-train) different components on different data, making it convenient for low-resource setting. Overall, it is a sound idea for low-resource setting with generally positive experimental results, but limited in novelty in terms of the proposed architecture (similar to [1*, 2*]) and disentangling language and knowledge idea (similar to [3*]), lacks comparison with baselines for low-resource setting, and lacks discussion/reference of a few closely related works. Here are some of my questions and concerns for the paper: Great to have some ablations in terms of pre-training to see its effect on different components. However, it would be quite useful to also have ablations over components by completely dropping a component (like LM) while training decoding manager. It would also be useful to see some statistics/discussion on the effect of different components in the actual generation of responses. It might also be useful to include qualitative examples of the generated responses annotated with predictions of different components for each generated word. In Figure-2 (c) and (d), some ablation results are reported for when pre-training is removed for each of the three components independently. It is interesting, though, to see that removing pre-training does not hurt (might even improve) the performance (esp. for Test Unseen) much for FULL training data case. Is there a particular reason for this observation? Also, it makes me curious how the proposed model would perform without pre-training any of the components. Would it already outperform the baselines discussed in the paper? If so, are these baselines strong enough (SOTA or close to SOTA) to help draw a meaningful conclusion from comparison with them? For example, how would fine-tuning a pre-trained MASS [4*] perform and compare as a baseline? Can authors comment on this? The proposed approach is very similar in architecture to [1*, 2*, 3*], which are not discussed/referenced in the paper. Except for pre-training, the only difference from [2] is that copying and generation distributions are softly combined into separate distribution each, independently. Inducing a single output distribution is done instead by deciding which source to use by an MLP layer on decoder state as in Eq. 11. So, I think the authors need to better isolate what the core contribution of this paper is: disentangled decoder or pre-training strategy? If it is the proposed disentangled decoder architecture, then authors should compare with similar architectures [2, 3] in the low-resource setting by initializing encoder and decoder from pre-trained weights on the same Reddit corpus. If it is the pre-training strategy, then it should be compared with various pre-training strategies for sequence generation (e.g., [4*]) proposed recently. For example, it would be useful to include a comparison with fine-tuning a pre-trained MASS [4*] with the same amount of WoW training data (changing from 1/16 to 1/1). Presentation of the paper can be improved by 1) changing the name of “document reader” (maybe to “knowledge processor” similar to context) as it essentially attends on the document rather than reading, 2) using abstraction in the technical section to help simplify notation and make it more interpretable. REFERENCES: [1*] Get To The Point: Summarization with Pointer-Generator Networks, See et al. [2*] DeepCopy: Grounded Response Generation with Hierarchical Pointer Networks, Yavuz et al. [3*] Disentangling Language and Knowledge in Task-Oriented Dialogs, Raghu et al. [4*] MASS: Masked Sequence to Sequence Pre-training for Language Generation, Song et al."}
{"id": "iclr2020_193", "title": "Ergodic Inference: Accelerate Convergence by Optimisation | OpenReview", "abstract": "Abstract:###Statistical inference methods are fundamentally important in machine learning. Most state-of-the-art inference algorithms are variants of Markov chain Monte Carlo (MCMC) or variational inference (VI). However, both methods struggle with limitations in practice: MCMC methods can be computationally demanding; VI methods may have large bias. In this work, we aim to improve upon MCMC and VI by a novel hybrid method based on the idea of reducing simulation bias of finite-length MCMC chains using gradient-based optimisation. The proposed method can generate low-biased samples by increasing the length of MCMC simulation and optimising the MCMC hyper-parameters, which offers attractive balance between approximation bias and computational efficiency. We show that our method produces promising results on popular benchmarks when compared to recent hybrid methods of MCMC and VI.", "review": " The presented method is very useful to deep learning in the era of uncertainty modelling, which requires the use of Bayesian inference arguments. It*s a valuable improvement upon variational inference, it*s novel, and the derivations are correct. The presentation is elaborate and covers all expected aspects. The literature review is up to date. The experimental results are diverse enough and convincing. The authors have considered both proof of concept experiments and deep learning architectures. The comparisons are valid."}
{"id": "iclr2020_194", "title": "Continual Learning via Principal Components Projection | OpenReview", "abstract": "Abstract:###Continual learning in neural networks (NN) often suffers from catastrophic forgetting. That is, when learning a sequence of tasks on an NN, the learning of a new task will cause weight changes that may destroy the learned knowledge embedded in the weights for previous tasks. Without solving this problem, it is difficult to use an NN to perform continual or lifelong learning. Although researchers have attempted to solve the problem in many ways, it remains to be challenging. In this paper, we propose a new approach, called principal components projection (PCP). The idea is that in learning a new task, if we can ensure that the gradient updates will only occur in the orthogonal directions to the input vectors of the previous tasks, then the weight updates for learning the new task will not affect the previous tasks. We propose to compute the principal components of the input vectors and use them to transform the input and to project the gradient updates for learning each new task. PCP does not need to store any sampled data from previous tasks or to generate pseudo data of previous tasks and use them to help learn a new task. Empirical evaluation shows that the proposed method PCP markedly outperforms the state-of-the-art baseline methods.", "review": "Review:###This paper introduces Principal Components Projection, a method that computes the principal components of input vectors, using them to train on a transformed input space and to project gradient updates. Experiments show improved results over OWM (the method that this paper builds on) and EWC. If I understand correctly (which I think may not be the case), the principal component vectors are computed after the first forward/backward pass of each task, for the inputs to each layer (C_l^k). These principal components are then fixed, the orthogonal projection matrix P_l^k is then found, and then normal training is iterated until convergence using this C_l^k and P_l^k. Questions: - Seeing as (especially for the first task), weights are initialised randomly, why does this method provide reasonable principal components for layers after the first layer? - I also do not understand why the dxd projection matrix P, which is orthogonal to all previous basis matrices C, has the property span(P^i) subset span(P^j) for i < j. Surely as more basis matrices are found, then the orthogonal space restricts in size. - I also do not understand Equation 1. What is grad{W}? If it is, as defined 2 pages later, *the backpropagation with respect to X_{k+1}* [or X_k here], then is Equation 1 saying that only one gradient step is used per task? The experiments seem reasonable, except that there are no standard deviations on the results. However, as far as I*m aware, these experimental protocols (dataset and model size) are not used in other papers: it would be nice to see experiments which match previous papers* protocols, for example with MNIST and CIFAR-10 at least (other papers use smaller model sizes). As it is currently, I am unable to understand the paper despite spending some time trying to understand it. I am therefore giving the paper a weak reject. Hopefully the authors can answer my questions. Finally, some minor specific suggestions for improving the writing: - Immediately after Equation 12, there is grad{P^j} instead of grad{W^j}{P^{k-2}} - The paragraph before Equation 13 uses *t* instead of *k* sometimes for task index - Use ` not * for open quotation marks"}
{"id": "iclr2020_195", "title": "Top-down training for neural networks | OpenReview", "abstract": "Abstract:###Vanishing gradients pose a challenge when training deep neural networks, resulting in the top layers (closer to the output) in the network learning faster when compared with lower layers closer to the input. Interpreting the top layers as a classifier and the lower layers a feature extractor, one can hypothesize that unwanted network convergence may occur when the classifier has overfit with respect to the feature extractor. This can lead to the feature extractor being under-trained, possibly failing to learn much about the patterns in the input data. To address this we propose a good classifier hypothesis: given a fixed classifier that partitions the space well, the feature extractor can be further trained to fit that classifier and learn the data patterns well. This alleviates the problem of under-training the feature extractor and enables the network to learn patterns in the data with small partial derivatives. We verify this hypothesis empirically and propose a novel top-down training method. We train all layers jointly, obtaining a good classifier from the top layers, which are then frozen. Following re-initialization, we retrain the bottom layers with respect to the frozen classifier. Applying this approach to a set of speech recognition experiments using the Wall Street Journal and noisy CHiME-4 datasets we observe substantial accuracy gains. When combined with dropout, our method enables connectionist temporal classification (CTC) models to outperform joint CTC-attention models, which have more capacity and flexibility.", "review": "Review:###========================= Update review After reading the authors response I would like to keep my score as is. I still see many unclear statements, and most importantly I feel that more analysis of the proposed method should have been done here. ========================= This paper proposed a Top-Down method for neural networks training based on the good classifier hypothesis. In other words, after obtaining a classifier that performs well on the test set, keep fine-tuning / re-learning the data representation. The authors provide character error rate results for the task of Automatic Speech Recognition using WSJ and CHiME-4 datasets. Although being an interesting research idea, several issues in this paper make it not yet ready for publication at ICLR. First, the paper is poorly written; there are many claims the authors are making without providing experiments/proofs/citations. For example: *...since the feature extractor learns more slowly, then potentially the classifier may overfit the feature extractor before the feature extractor is able to learn much about the underlying pattern of the data...*. Or: *...We suggest that the reason for this is that when all layers are trained on the noisy dataset jointly, the middle layers overfit the bottom-most layers much faster than the bottom-most layers are able to learn input features...* Next, since there is no theoretical/mathematical explanation of the proposed approach, I expect the authors to run an analysis on the results to better understand the effect of using such an approach. For instance, under which settings this method is most efficient? In what layer should I start the fine-tuning? Is it better to reinitialize the bottom layers or fine-tune them? Does the proposed approach applicable to different domains? i.e. vision/nlp/other speech/signal processing tasks? Does the proposed approach applicable to different models or only for the proposed one? Lastly, although it is not the main point in this paper since all results are reported on ASR, did the authors tried to compute WERs too? That way, people can compare results with other ASR models. The baseline seems relatively weak, at least in Table 1. Minor comments: The complexity of the algorithm is written to be O(n). However, this assumes training the model takes O(1) or did I miss something? Can the authors provide more details/insights regarding the delta differences in Table 1? Did the authors use the same initializations? Did the authors try different ones?"}
{"id": "iclr2020_196", "title": "Farkas layers: don*t shift the data, fix the geometry | OpenReview", "abstract": "Abstract:###Successfully training deep neural networks often requires either {batch normalization}, appropriate {weight initialization}, both of which come with their own challenges. We propose an alternative, geometrically motivated method for training. Using elementary results from linear programming, we introduce Farkas layers: a method that ensures at least one neuron is active at a given layer. Focusing on residual networks with ReLU activation, we empirically demonstrate a significant improvement in training capacity in the absence of batch normalization or methods of initialization across a broad range of network sizes on benchmark datasets.", "review": "Review:###The authors propose a new `normalization* approach called Farkas layer for improving the training of neural networks. The main idea is to augment each layer with an extra hidden unit so that at least one hidden unit in each layer will be active. This is achieved by making the extra hidden unit dependent on the rest of the units in the layer, so that it will become active if the rest are inactive, and they name it after Farkas* lemma in linear programming. This avoids the gradient becoming zero when all the units in a layer are dead. The empirical results show that this normalization method is effective, and improves the training of deep ResNets when no batch normalization is used. The accuracies on CIFAR10 and CIFAR100 are improved with the use of Farkas layers. Unfortunately it still cannot beat or replace batch normalization. When batch normalization is used, the benefit of using this Farkas layer becomes marginal (Tables 1 and 2). I am also not completely satisfied with the authors* explanation on why Farkas* layers work. The authors motivate the design of the layer with dead hidden units, but in the experiments they do not show if any layer actually becomes completely `dead* (or gradient becomes very small) when Farkas* layer is not used. There could be other reasons why the layer helps, other than keeping some units in a layer active. Overall I think the idea is novel and interesting, but the improvement is not big enough to replace existing normalization methods that makes this paper slightly below the acceptance threshold in my opinion."}
{"id": "iclr2020_197", "title": "CEB Improves Model Robustness | OpenReview", "abstract": "Abstract:###We demonstrate that the Conditional Entropy Bottleneck (CEB) can improve model robustness. CEB is an easy strategy to implement and works in tandem with data augmentation procedures. We report results of a large scale adversarial robustness study on CIFAR-10, as well as the IMAGENET-C Common Corruptions Benchmark.", "review": "Review:###TITLE CEB Improves Model Robustness REVIEW SUMMARY As a thorough empirical evaluation of a new method for training deep neural networks, this paper is a useful contribution. PAPER SUMMARY The paper presents an empirical study of the robustness of neural network classifiers trained using a conditional entropy bottleneck regularization. CLARITY The paper is well written and easy to follow. Results are presented in a way that gives the reader a good overview, but figure quality / legibility could be improved. ORIGINALITY The CEB method is an original (but minor) modification of a well known method. SIGNIFICANCE The primary contribution of the paper is a thorough experimental evaluation of the particular regularization method presented in the paper. While this is certainly useful, it would have been even better with more comparison with other regularization methods. The paper clearly demonstrates the benefits of CEB, which is of interest in itself. FURTHER COMMENTS Consider making a more clear distinction between generalization and robustness. *are are* Double word. *all machine-learned systems...highly vunerable* Too strong statement. *VIB* Abbreviation not defined in the text. Figure 1, 2 and 3 are a bit hard to read (especially the dotted lines)"}
{"id": "iclr2020_198", "title": "Capsules with Inverted Dot-Product Attention Routing | OpenReview", "abstract": "Abstract:###We introduce a new routing algorithm for capsule networks, in which a child capsule is routed to a parent based only on agreement between the parent*s state and the child*s vote. Unlike previously proposed routing algorithms, the parent*s ability to reconstruct the child is not explicitly taken into account to update the routing probabilities. This simplifies the routing procedure and improves performance on benchmark datasets such as CIFAR-10 and CIFAR-100. The new mechanism 1) designs routing via inverted dot-product attention; 2) imposes Layer Normalization as normalization; and 3) replaces sequential iterative routing with concurrent iterative routing. Besides outperforming existing capsule networks, our model performs at-par with a powerful CNN (ResNet-18), using less than 25% of the parameters. On a different task of recognizing digits from overlayed digit images, the proposed capsule model performs favorably against CNNs given the same number of layers and neurons per layer. We believe that our work raises the possibility of applying capsule networks to complex real-world tasks.", "review": "Review:###In this paper, the authors propose a simple and effective routing algorithm for capsule networks. The paper is well written. A nice analysis of the proposed routing algorithm is provided. Experiments of varying the routing iterations demonstrate the stableability of proposed routing algorithm compared to others. Here are some issues: 1. Would the authors release the code for reproducing the results in the paper? It will be helpful for future research in this area. 2. In Fig.5, it would be better to give some brief explanations about why CasNet (Matrix) occupies much more memory while possessing less parameters."}
{"id": "iclr2020_199", "title": "Measuring Numerical Common Sense: Is A Word Embedding Approach Effective? | OpenReview", "abstract": "Abstract:###Numerical common sense (e.g., ``a person with a height of 2m is very tall**) is essential when deploying artificial intelligence (AI) systems in society. To predict ranges of small and large values for a given target noun and unit, previous studies have implemented a rule-based method that processed numeric values appearing in a natural language by using template matching. To obtain numerical knowledge, crawled textual data from web pages are frequently used as the input in the above method. Although this is an important task, few studies have addressed the availability of numerical common sense extracted from corresponding textual information. To this end, we first used a crowdsourcing service to obtain sufficient data for a subjective agreement on numerical common sense. Second, to examine whether common sense is attributed to current word embedding, we examined the performance of a regressor trained on the obtained data. In comparison with humans, the performance of an automatic relevance determination regression model was good, particularly when the unit was yen (a maximum correlation coefficient of 0.57). Although all the regression approach with word embedding does not predict values with high correlation coefficients, this word-embedding method could potentially contribute to construct numerical common sense for AI deployment.", "review": "Review:###This paper attempts to study if learned word embeddings for common objects contain information about *numerical common sense*. The hypothesis is that certain numerical information may co-occur with the words for certain objects/measurement units within their context windows. To verify this hypotheses, the authors have created a dataset through a crowd-sourcing service which represents *numerical common sense*. Using this dataset, the authors examine the predict abilities of regressors trained on learned word embeddings and the aforementioned crowd-sourced dataset. The hypothesis is that if the regressors demonstrate good accuracy, then the word embeddings contained information relevant to *numerical common sense*. To the best of my knowledge, this is the first paper that attempts to analyze learned word embeddings in the context of numerical common sense. This paper should be rejected because (1) the NCS datasets are too small to represent *numerical common sense* (2) the NCS datasets contain faulty data points and (3) the results from the experiments conducted are not sufficient to accept or refute the hypotheses. Main argument The first question that we must ask is - are the NCS-50x1 and NCS-60x3 datasets reliable for experiments on *numerical common sense*. No, because of two flaws: (1) The number of samples in the dataset is too small to represent *numerical common sense*. Consider the histogram for object *dog* in Figure 2. If the largest data point in this plot was absent, the average of the distribution would be smaller by several orders of magnitude. Perhaps there are other objects in the dataset which are missing samples from the tail end of the distribution that could have large effects on the mean of the collected dataset. (2) Some data points in the dataset don*t make sense to me. For example, Fig 2 represents the *small* dataset, yet I see samples like 400m long dogs, 40m long cats, 150m long monitors and 20m long mice? Also, it is not clear how the confidence scores of the participants were taken into account when training the regressors or if they were used at all. If the NCS dataset does not represent *numerical common sense*, it invalidates all experimental results from the paper. My second issue with the paper is that it is not possible to conclude if the experimental results support or refute the hypothesis (ignoring the issue with the dataset): 1. In tables 2 and 3, the correlation coefficients were quite low and and the MAEs were pretty large. In Table 3, rows 1 and 2, even though the correlation is 0.57 and 0.48, the MAE is 100 million yen and 7.5 million yen respectively which is quite large. To me this suggests that just because the correlation is larger we cannot conclude mean that the model is performing well. 2. It is unclear why the correlation coefficient was chosen to decide that ARD is the superior model in experiment 1. The MAE for random forests with concatenated feature vectors was an order of magnitude smaller than that of the ARD model. 3. Why are the correlation coefficients missing for the unit-only experiment in Table 2? The LS model shows very good MAE relative to the other models and perhaps the correlation should have been measured for that as well? In fact, if the correlation coefficients for this case is comparable to the case with concatenated features, it would mean that the word embedding for the object is not helping at all! Moreover, I find it surprising that the LS model with concatenated features performs worse than the unit-only features. We cannot conclude if paper*s interpretation about the results is correct unless this missing information is provided. 4. It is hard to judge what a correlation coefficient of 0.57 means. Why didn*t you provide a scatter plot of the predictions vs targets as well? It often happens that even noisy plots demonstrate good correlations. 5. The paper should have additional ablation studies - for example, what would happen in the concatenated feature vector experiment if you trained the regressors using randomly initialized word embeddings instead of the trained word embeddings? Do you get the same performance as learned word embeddings?"}
{"id": "iclr2020_200", "title": "PAC-Bayesian Neural Network Bounds | OpenReview", "abstract": "Abstract:###Bayesian neural networks, which both use the negative log-likelihood loss function and average their predictions using a learned posterior over the parameters, have been used successfully across many scientific fields, partly due to their ability to `effortlessly* extract desired representations from many large-scale datasets. However, generalization bounds for this setting is still missing. In this paper, we present a new PAC-Bayesian generalization bound for the negative log-likelihood loss which utilizes the emph{Herbst Argument} for the log-Sobolev inequality to bound the moment generating function of the learners risk.", "review": " This paper suggests a PAC-Bayesian bound for negative log-likelihood loss function. Many PAC-Bayesian bounds are provided for bounded loss functions but as authors point out, Alquier et al. (2016) and Germain et al. (2016) extend them to unbounded loss functions. I have two major concerns regarding this paper: 1- Technical contribution: Since Alquier et al. (2016) has already introduced PAC-Bayesian bounds for the hinge-loss, I think the technical contributions of this paper is not significant enough for the publication. Moreover, the particular format of the bound in Theorem 2 is problematic since the right hand side depends on the data-distribution. When presenting the generalization bound, we really want the right hand side to be independent of the distribution (given the training set) and that is the whole point of calculating the generalization bounds. In particular, I don*t see why inequality (1) is any better than inequality (2). 2- Experiments: The main issue with the correlation analysis done in Section 6 is that authors only change depth of the networks and then check the correlation of the generalization bound to the test error. The problem is that in all those networks deeper ones generalize better so it is not clear that the correlation is due to a direct relationship to generalization or a direct relationship to depth. For example, if we take 1/depth as a measure, it would correlate very well with generalization in all these experiments but 1/depth is definitely not the right complexity measure or generalization bound. To improve the evaluation, I suggest varying more hyperparameters to avoid the above issue. *************************** After rebuttals: Unfortunately, my concerns are not addressed adequately by authors. Therefore, my evaluation remains the same."}
{"id": "iclr2020_201", "title": "Mix-review: Alleviate Forgetting in the Pretrain-Finetune Framework for Neural Language Generation Models | OpenReview", "abstract": "Abstract:###In this work, we study how the large-scale pretrain-finetune framework changes the behavior of a neural language generator. We focus on the transformer encoder-decoder model for the open-domain dialogue response generation task. We find that after standard fine-tuning, the model forgets important language generation skills acquired during large-scale pre-training. We demonstrate the forgetting phenomenon through a detailed behavior analysis from the perspectives of context sensitivity and knowledge transfer. Adopting the concept of data mixing, we propose an intuitive fine-tuning strategy named *mix-review**. We find that mix-review effectively regularize the fine-tuning process, and the forgetting problem is largely alleviated. Finally, we discuss interesting behavior of the resulting dialogue model and its implications.", "review": "Review:###This paper studies the forgetting problem in the pretrain-finetune framework, specifically in the dialogue response generation task. It proposes a mix-review strategy to alleviate the forgetting issue. The paper makes three major claims: (1) In the finetuning stage, the model forgets part of the language generation skills acquired during pretraining. (2) The proposed mix-review strategy effectively alleviates the forgetting problem. (3) The proposed method performs better in terms of context-sensitivity and knowledge transfer. Although the forgetting problem pointed out by this paper is interesting and worth studying, the proposed method (1) lacks novelty and (2) does not perform well empirically. The writing of the paper also needs to be improved. The proposed mix-review strategy is very straightforward and lacks novelty. To prevent catastrophic forgetting, the simplest way is to sample some data from the historical task and jointly train with the new task, which is exactly the proposed method. It is not surprising using this strategy alleviates the forgetting problem, but the question is whether it can make the model generalize better. Empirically, it does not perform better compared with the commonly-used weight decay regularizer. In Table 2, we can see the proposed method didn’t improve much in terms of either perplexity or human evaluation. The two analyses are also not convincing to show mix-review performs better. For example, in terms of context-sensitivity, the increase in perplexity after distort the context does not necessarily mean the model’s generation is more context-related. Overall, the contribution made by this paper is not yet enough for an ICLR publication."}
{"id": "iclr2020_202", "title": "Recognizing Plans by Learning Embeddings from Observed Action Distributions | OpenReview", "abstract": "Abstract:###Plan recognition aims to look for target plans to best explain the observed actions based on plan libraries and/or domain models. Despite the success of previous approaches on plan recognition, they mostly rely on correct action observations. Recent advances in visual activity recognition have the potential of enabling applications such as automated video surveillance. Effective approaches for such problems would require the ability to recognize the plans of agents from video information. Traditional plan recognition algorithms rely on access to detailed planning domain models. One recent promising direction involves learning approximate (or shallow) domain models directly from the observed activity sequences. Such plan recognition approaches expect observed action sequences as inputs. However, visual inference results are often noisy and uncertain, typically represented as a distribution over possible actions. In this work, we develop a visual plan recognition framework that recognizes plans with an approximate domain model learned from uncertain visual data.", "review": "Review:###The paper proposes an action affinity method for plan recognition from a sequence of videos. The method attempts to learn embeddings for distribution of actions. The author proposes a loss function by combing: 1) average KL divergence between log likelihood of an action at certain time step and those at future time steps, and 2) hierarchical softmax loss. Experimental results on a real world dataset and a synthetic dataset are conducted to show that the proposed method has a better sample and computation efficiency. I think the general direction of the work seems interesting. My main concern exists in novelty/significance. The loss of minimizing KL divergence seems to be very intuitive, and the other part of loss using hierarchical softmax loss does not have justification. It simply mentions that using such a loss results in better performance due to previous work. As a result, from a technical perspective, I think the work seems somewhat incremental."}
{"id": "iclr2020_203", "title": "PAC Confidence Sets for Deep Neural Networks via Calibrated Prediction | OpenReview", "abstract": "Abstract:###We propose an algorithm combining calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees---i.e., the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, and on a dynamics model the half-cheetah reinforcement learning problem.", "review": "Review:###The paper proposes an algorithm combining calibrated prediction and generalization to construct confidence sets for deep neural networks with PAC guarantees. The main novelty is that existing approaches do not come with PAC guarantees Following (Platt et al., 1999) and (Guo et al., 2017), the calibration of the learned model is controlled by its temperature. In particular, the proposed approach exploits another (small) training dataset to learn a temperature that gives the best calibration. An efficient algorithm for constructing confidence sets that are *small in size* is also proposed. The framework is introduced for the classification, regression and reinforcement learning tasks. I vote for acceptance for the following reasons: - The paper is well-written, the motivation and comparison to related work is also clear. - The paper is very solid theoretically and experimentally. A theoretical analysis is provided, and practical implementations are proposed to deal with scalability issues. VC generalization bounds are studied in detail. Concerning experiments that are not about Reinforcement learning, the paper proposes different strategies to learn a ResNet architecture for ImageNet. One might argue that the different results might be only valid for the chosen architecture and dataset. Although the study for that architecture and (large scale) dataset is exhaustive, it might be interesting to study if the reported results are also valid on (smaller?) datasets and other architectures."}
{"id": "iclr2020_204", "title": "Three-Head Neural Network Architecture for AlphaZero Learning | OpenReview", "abstract": "Abstract:###The search-based reinforcement learning algorithm AlphaZero has been used as a general method for mastering two-player games Go, chess and Shogi. One crucial ingredient in AlphaZero (and its predecessor AlphaGo Zero) is the two-head network architecture that outputs two estimates --- policy and value --- for one input game state. The merit of such an architecture is that letting policy and value learning share the same representation substantially improved generalization of the neural net. A three-head network architecture has been recently proposed that can learn a third action-value head on a fixed dataset the same as for two-head net. Also, using the action-value head in Monte Carlo tree search (MCTS) improved the search efficiency. However, effectiveness of the three-head network has not been investigated in an AlphaZero style learning paradigm. In this paper, using the game of Hex as a test domain, we conduct an empirical study of the three-head network architecture in AlpahZero learning. We show that the architecture is also advantageous at the zero-style iterative learning. Specifically, we find that three-head network can induce the following benefits: (1) learning can become faster as search takes advantage of the additional action-value head; (2) better prediction results than two-head architecture can be achieved when using additional action-value learning as an auxiliary task.", "review": " The paper proposed to use three-head network for AlphaZero-like training. The three-head network is used to predict policy, value and q function after an action is taken. While three-head network is presented by a prior work [1] and is learned via supervised learning on a fixed dataset, this paper mainly applies it to AlphaZero training for the game of Hex 9x9 and shows preliminary results. While the idea is interesting, there are many issues in the experiments and the conclusion is quite indecisive. So I feel that the paper is not ready for publication yet and thus vote for rejection. Why we need expansion threshold n_th to be 10? If you keep visiting the same node without expansion, won’t the same node back-propagate the same value (or q) 10 times before expansion? If that’s the case, what’s the difference if we just back-propagate once? Note that if n_th = 0 then prediction of q(s, a) is no-longer necessary (except that predicts q(s, a) becomes an aux task during training, as mentioned in the caption of Fig. 3). Fig. 2 shows that 3HNN trains faster than 2HNN. However, it looks like 2HNN and 3HNN show drastically different training curves, and are probably operating at different regions. In the text, the authors also acknowledge that one iteration of 2HNN is 5-6 times slower than 3HNN, since 2HNN builds a much deeper search tree. This bring about a question: is the performance difference due to unfavorable hyper-parameters on 2HNN (or other factors)? The paper doesn’t answer that. The text claims that when n_th = 0, 3HNN performs better than 2HNN, however, the figure shows that 2HNN has lower or comparable MSE than 3HNN. The prediction accuracy is better, though. When n_th = 1, Fig. 4 shows that the 2HNN is doing comparable or better in terms of MSE and Prediction Accuracy than 3HNN (compared to perfect play). This somehow defeats the purpose of using the third head of q(s, a) that only helps when n_th > 0. In Table 2, do you have standard derivation? Note that AlphaZero training is not that stable and the performance (in particular the initial performance since the performance might take off earlier or later) against a known bot can vary a lot, the difference between 56% and 63% can be purely due to noise. Also, how is the resulting model compared against MoHex-3HNN [1] and MoHex-CNN [2]? Note that MoHex-3HNN [1] shows 82.4% over MoHex 2.0 on 13x13, but is trained supervisedly, and Table 2 shows slightly better performance. So I am curious their performance comparison. Minor: The term “iteration” seems to be defined twice with different meanings. It is defined as one MCTS rollout (see Appendix A) and also defined (in Fig 1) as one full synchronization of self-play and training (AlphaGo Zero setting). This causes a lot of confusions. I believe each dot in Fig. 2 is “iteration” in the AlphaGo Zero sense. Finally, although many hardware information is revealed in the appendix, maybe it would be better if the authors could reveal more details about their AlphaZero-style training, e.g., how long does it take for each move and for each self-play game? How long does it take to wait until all self-play agent returns all games? Is there any synchronization overhead? This could give the audience some idea about the computational bottleneck. From the current number, it seems that 60 self-play processes are run on 56 cores, and each AlphaGo iteration takes approximate 5 hours (read from Fig. 2) with 200 games per self-play process. Assuming there is no synchronization overhead and 1 core per self-play process, this yields 200 games/5 hours per core, which is 1.5 min (or 90s) per game. Since each game has 9x9 = 81 moves, this means that it costs ~1.1 sec per move. Is that correct? [1] Chao Gao, Martin Muller, and Ryan Hayward. Three-head neural network architecture for Monte Carlo tree search. In IJCAI, pp. 3762–3768, 2018. [2] Chao Gao, Ryan B Hayward, and Martin Muller. Move prediction using deep convolutional neural networks in Hex. IEEE Transactions on Games, 2017. =====Post Rebuttal===== I really appreciated that the authors have made substantial efforts in improving the paper and adding more experiments. However, the additional change makes the story a bit more convoluted. After substantial parameter tuning on the 2HNN side, It seems that 3HNN is only slightly better than 2HNN (Fig. 5 in the revision, > 50% winrate, but it is not clear how much ELO it is better). Unfortunately, after tuning, 2HNN actually shows comparable performance in terms of speed (updated Fig. 3 and 4, middle columns), which somehow tarnishes the claims of the paper that 3HNN is better than 2HNN. The final performance against 10000-rollouts MoHex2.0 is 89.7% (2HNN) versus 91.6% (3HNN), so the performance is slightly better with 3HNN. This number is much better than previous works e.g., PGS-EXIT (Thomas et al. 2019). This indeed shows that the paper does a good job in terms of engineering and performance push (agreed with R1). In my opinion, the paper can be better rewritten as a paper that shows strong performance in Hex, compared to previous works, plus many ablation analysis. I keep the score."}
{"id": "iclr2020_205", "title": "A Copula approach for hyperparameter transfer learning | OpenReview", "abstract": "Abstract:###Bayesian optimization (BO) is a popular methodology to tune the hyperparameters of expensive black-box functions. Despite its success, standard BO focuses on a single task at a time and is not designed to leverage information from related functions, such as tuning performance metrics of the same algorithm across multiple datasets. In this work, we introduce a novel approach to achieve transfer learning across different datasets as well as different metrics. The main idea is to regress the mapping from hyperparameter to metric quantiles with a semi-parametric Gaussian Copula distribution, which provides robustness against different scales or outliers that can occur in different tasks. We introduce two methods to leverage this estimation: a Thompson sampling strategy as well as a Gaussian Copula process using such quantile estimate as a prior. We show that these strategies can combine the estimation of multiple metrics such as runtime and accuracy, steering the optimization toward cheaper hyperparameters for the same level of accuracy. Experiments on an extensive set of hyperparameter tuning tasks demonstrate significant improvements over state-of-the-art methods.", "review": "Review:###Summary: This paper proposes a new Bayesian optimization (BO) based hyperparameter searching method that can transfer across different datasets and different metrics. The method is to build a regression model based on Gaussian Copula distribution, which maps from hyperparameter to metric quantiles. The paper shows that by leveraging this estimation using some specific (sampling) strategies, it is able to improve over other BO methods. The high-level idea of this paper seems sound to me -- that improves standard BO to generalize across datasets and metrics by learning a mapping between the space of hyperparameters and metrics. While I am not an expert in this area, the derivation looks sound to me, and the evaluation results of this paper are comprehensive to show that CGP seems to outperform a number of methods."}
{"id": "iclr2020_206", "title": "Learning to Reach Goals Without Reinforcement Learning | OpenReview", "abstract": "Abstract:###Imitation learning algorithms provide a simple and straightforward approach for training control policies via standard supervised learning methods. By maximizing the likelihood of good actions provided by an expert demonstrator, supervised imitation learning can produce effective policies without the algorithmic complexities and optimization challenges of reinforcement learning, at the cost of requiring an expert demonstrator -- typically a person -- to provide the demonstrations. In this paper, we ask: can we use imitation learning to train effective policies without any expert demonstrations? The key observation that makes this possible is that, in the multi-task setting, trajectories that are generated by a suboptimal policy can still serve as optimal examples for other tasks. In particular, in the setting where the tasks correspond to different goals, every trajectory is a successful demonstration for the state that it actually reaches. Informed by this observation, we propose a very simple algorithm for learning behaviors without any demonstrations, user-provided reward functions, or complex reinforcement learning methods. Our method simply maximizes the likelihood of actions the agent actually took in its own previous rollouts, conditioned on the goal being the state that it actually reached. Although related variants of this approach have been proposed previously in imitation learning settings with example demonstrations, we present the first instance of this approach as a method for learning goal-reaching policies entirely from scratch. We present a theoretical result linking self-supervised imitation learning and reinforcement learning, and empirical results showing that it performs competitively with more complex reinforcement learning methods on a range of challenging goal reaching problems.", "review": "Review:###This work presents the goal-conditioned supervised learning algorithm (GCSL), which learns goal conditioned policies using only behavioral-cloning of the agent*s own actions. The intuition behind the algorithm is the goal of an observed trajectory can be identified after the fact, by simply looking at the states reached during that trajectory. GCSL treats each executed action as a sample from the expert policy conditioned on each of the states reached after that action is taken. Given a distribution over goal states, GCSL alternates between executing its current goal-conditioned policy on randomly selected goals, and learning to imitate the generated actions conditioned on the states they actually reached. Experimental results demonstrate superior performance against a base (non-goal conditioned) RL algorithm (TRPO), and against another approach to learning goal-conditioned polices (TD3-HER), on a relatively diverse set of control problems. A major issue is that the proof of the main theoretical result appears to be wrong. As there don*t appear to be any constraints placed on the policy pi_old, it would seem that the surrogate loss would collapse to 0 for any policy pi if pi_old is such that the target goal is never reached (the probability of any trajectory t reaching g is 0 under pi_old(t|g)). It seems to be the case that the quality of the GCSL loss depends on the relationship between pi_old and the goal distribution p(g). The fact that the theoretical results are incorrect does not mean that the algorithm, or the general approach do not have value, but it does highlight the fact that this approach may only be effective for a specific class of problems similar to the experimental domains. While not a flaw in the work itself, it should be made clear in the text that the notion of optimality for the learning tasks considered in this work (i.e. achieving the goal by the end of episode), avoids one of the apparent limitations of the algorithm. A randomly generated trajectory is itself optimal for any state that it reaches, if we define optimality as simply reaching a state. Such a trajectory may not be the most efficient way of reaching that state however, so the relabelling process would seem to be prone to learning policies that achieve the conditioned goals, but not doing so in an efficient manner. It isn*t clear how well this approach would work for tasks where the efficiency, in terms of the time required to reach the objective, is a key part of the evaluation. Again, this is not a flaw in the work itself, and it is possible that the algorithm will be effective in such tasks, perhaps because the likelihood of an action resulting in a given state is higher if that action brings us closer to this state. It might be useful to conduct some additional experiments where evaluation is based on the time required to solve a task, rather than just the accuracy of the final state."}
{"id": "iclr2020_207", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations | OpenReview", "abstract": "Abstract:###Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations, longer training times, and unexpected model degradation. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large.", "review": "Review:###This paper proposes a new pre-trained BERT-like model called ALBERT. The contributions are mainly 3-fold: factorized embedding parameterization, cross-layer parameter sharing, and intern-sentence coherence loss. The first two address the issue of model size and memory consumption in BERT; the third corresponds to a new auxiliary task in pre-train, sentence-order prediction (SOP), replacing the next sentence prediction (NSP) task in BERT. These modifications lead to a much leaner model and improved performance. As a result, ALBERT pushes the state of the art on GLUE, RACE, and SQuAD while having fewer parameters than BERT-large. This is a well-written paper which is easy to follow even for readers without deep background knowledge. The proposed method is meaningful and effective. Its empirical results are impressive. Other comments: - Section 4.9. Why use the all-share condition for state-of-the-art ALBERT results (as indicated in Table 2)? Judging from Table 4 and 5, shouldn*t the non-shared condition give better results? The number of parameters would be larger, of course. - I like the justification/motivation given for replacing NSP with SOP. I wonder if the authors have tried other objectives (but didn*t work out). Such negative results are valuable to practitioners. - Typo in Sec. 4.1: x1,1, x1,2 should be x2,1, x2,2."}
{"id": "iclr2020_208", "title": "Neural Embeddings for Nearest Neighbor Search Under Edit Distance | OpenReview", "abstract": "Abstract:###The edit distance between two sequences is an important metric with many applications. The drawback, however, is the high computational cost of many basic problems involving this notion, such as the nearest neighbor search. A natural approach to overcoming this issue is to embed the sequences into a vector space such that the geometric distance in the target space approximates the edit distance in the original space. However, the known edit distance embedding algorithms, such as Chakraborty et al.(2016), construct embeddings that are data-independent, i.e., do not exploit any structure of embedded sets of strings. In this paper we propose an alternative approach, which learns the embedding function according to the data distribution. Our experiments show that the new algorithm has much better empirical performance than prior data-independent methods.", "review": "Review:###In this paper, the authors propose an approach for learning embeddings for strings using a three-phase approach. Each phase uses a different neural network to learn embeddings for strings such that the distance between strings in embedding space approximates the edit distance between the strings. A modest set of empirical results suggests that the proposed approach outperforms hand-crafted embeddings. I found the presentation in this paper very disjointed and difficult to follow. While I believe (my interpretation of) the basic idea of this paper is interesting, I believe the current presentation significantly hinders readers from following the authors’ intentions. Comments The description of how the network structure and weights are “initialized” across the different phases is not clear. Different notation (f_1, f_2, f_3) is used for each network, but in reality, this is just the same network. However, the writing makes this very difficult to notice. The authors introduce the CGK and CGK’ embedding algorithms, and then proceed to prove various properties about them. However, it is not clear to me how these theoretical properties are used by the neural network. From what I can tell, CGK’ is an alternative to CGK which reduces the output size relative to CGK (from 3n to at most 2n) while still ensuring exact reconstruction of the input. (I did not verify the proof in detail.) The authors then claim that this is helpful in the current context because it ensures the network parameters can be easily optimized. It is not clear to me what this means. (I guess that somehow using “CGK’ distance” makes training the model easier than using “CGK distance”.) Additionally, the experiments do not verify this claim empirically. So it is unclear whether using “CGK’ distance” helps in the context of learning embeddings. It is really unclear to me whether the neural network outputs a continuous or a binary vector. In particular, Equations 5 - 8 all suggest that Hamming loss is defined on the outputs of the various neural networks (f_1, f_2, and f_3). The paper also refers to bits in the output of f_3. Later on, though, the paper mentions that the neural embedded strings are continuous vectors. While this could just be typos or inconsistent notation, considering that other parts of the paper do rely on binary representations, this makes the presentation very confusing. It is unclear to me whether the can be (approximately) reconstructed from the embeddings. It seems that Theorem 1 suggests that the binary outputs of CGK’ can be decoded, but I cannot tell whether that extends to the embeddings. It is unclear to me how positives and negatives are sampled for training in Phase 2, and also whether that impacts training. The experimental results should include some measure of variance based on different train and/or test splits. It seems as though the three phases could be rolled into a single multi-task learning problem in which the network is trained during a single phase. Typos, etc. The references are not consistently formatted."}
{"id": "iclr2020_209", "title": "Increasing batch size through instance repetition improves generalization | OpenReview", "abstract": "Abstract:###Large-batch SGD is important for scaling training of deep neural networks. However, without fine-tuning hyperparameter schedules, the generalization of the model may be hampered. We propose to use batch augmentation: replicating instances of samples within the same batch with different data augmentations. Batch augmentation acts as a regularizer and an accelerator, increasing both generalization and performance scaling for a fixed budget of optimization steps. We analyze the effect of batch augmentation on gradient variance and show that it empirically improves convergence for a wide variety of networks and datasets. Our results show that batch augmentation reduces the number of necessary SGD updates to achieve the same accuracy as the state-of-the-art. Overall, this simple yet effective method enables faster training and better generalization by allowing more computational resources to be used concurrently.", "review": "Review:###The paper proposes a very simple strategy to reduce the variance of a batch of training data. It applies data augmentation operations to expand a training example, so that the mini-batch solely consists of the variants of this example. Since the correlations among examples within the batch is stronger than those in a batch with different examples, the gradient has less variance. While it can reduce the gradient variance during training, which leads to faster convergence, it also has better generalization performance. This is confirmed by empirical studies on difference datasets with a variety of neural networks, such lstm, transformer and convnet. Overall, this seems to be a simple yet effective method to improve the convergence and generalization at the same time, which is orthogonal to existing approaches. It might be better to try this method and improve the state-of-the-art on at least one dateset."}
{"id": "iclr2020_210", "title": "Strategies for Pre-training Graph Neural Networks | OpenReview", "abstract": "Abstract:###Many domains in machine learning have datasets with a large number of related but different tasks. Those domains are challenging because task-specific labels are often scarce and test examples can be distributionally different from examples seen during training. An effective solution to these challenges is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective for improving many language and vision domains, pre-training on graph datasets remains an open question. Here, we develop a strategy for pre-training Graph Neural Networks (GNNs). Crucial to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs. We systematically study different pre-training strategies on multiple datasets and find that when ad-hoc strategies are applied, pre-trained GNNs often exhibit negative transfer and perform worse than non-pre-trained GNNs on many downstream tasks. In contrast, our proposed strategy is effective and avoids negative transfer across downstream tasks, leading up to 11.7% absolute improvements in ROC-AUC over non-pre-trained models and achieving state of the art performance.", "review": "Review:###The authors introduce strategies for pre-training graph neural networks. Pre-training is done at the node level as well as at the graph level. They evaluate their approaches on two domains, biology and chemistry on a number of downstream tasks. They find that not all pre-training strategies work well and can in fact lead to negative transfer. However, they find that pre-training in general helps over non pre-training. Overall, this paper was well written with useful illustrations and clear motivations. The authors evaluate their models over a number of datasets. Experimental construction and analysis also seems sound. I would have liked to see a bit more analysis as to why some pre-training strategies work over others. However, the authors mention that this is in their planned future work. Also, in figure 4, the authors mention that their pre-trained models tend to converge faster. However, this does not take into account the time already spent on pre-training. Perhaps the authors can include some results as to the total time taken as well as amortized total time over a number of different downstream tasks."}
{"id": "iclr2020_211", "title": "Noisy $ell^{0}$-Sparse Subspace Clustering on Dimensionality Reduced Data | OpenReview", "abstract": "Abstract:###High-dimensional data often lie in or close to low-dimensional subspaces. Sparse subspace clustering methods with sparsity induced by L0-norm, such as L0-Sparse Subspace Clustering (L0-SSC), are demonstrated to be more effective than its L1 counterpart such as Sparse Subspace Clustering (SSC). However, these L0-norm based subspace clustering methods are restricted to clean data that lie exactly in subspaces. Real data often suffer from noise and they may lie close to subspaces. We propose noisy L0-SSC to handle noisy data so as to improve the robustness. We show that the optimal solution to the optimization problem of noisy L0-SSC achieves subspace detection property (SDP), a key element with which data from different subspaces are separated, under deterministic and randomized models. Our results provide theoretical guarantee on the correctness of noisy L0-SSC in terms of SDP on noisy data. We further propose Noisy-DR-L0-SSC which provably recovers the subspaces on dimensionality reduced data. Noisy-DR-L0-SSC first projects the data onto a lower dimensional space by linear transformation, then performs noisy L0-SSC on the dimensionality reduced data so as to improve the efficiency. The experimental results demonstrate the effectiveness of noisy L0-SSC and Noisy-DR-L0-SSC.", "review": " This paper studies the L0-Sparse Subspace Clustering (L0-SSC) of high dimensional noisy data. The goal of L0-SSC is to cluster the data according to their low-dimensional subspace structure by minimizing the number of the nonzero elements of a matrix Z such that X = XZ for data matrix X. Compared to the existing work of L0-SSC, this paper proposes noisy L0-SSC to handle noisy data. Moreover, it proposes Noisy-DR-L0-SSC to improve the efficiency of noisy L0-SSC by projecting the high-dimensional data onto a low-dimensional space by random projection and then performing noisy L0-SSC on the obtained low-dimensional data. The correctness of both noisy L0-SSC and Noisy-DR-L0-SSC is proved by showing that the subspace detection property holds. The idea of using random projection to improve the efficiency of noisy L0-SSC seems interesting, and the experimental results support the correctness of noisy L0-SSC and Noisy-DR-L0-SSC by demonstrating that the proposed methods achieve the promising result. Some questions are as below: 1 Please include mean and deviation in the results of Noisy-DR-L0-SSC in Table 1 and Table 2. Because random projection is used for dimension reduction, mean and deviation are expected in the results to avoid performance fluctuation due to randomness. 2 Can the authors show the performance of Noisy-DR-L0-SSC with different p-values (i.e. different dimensions after random projection)? 3 Can one try sparse random projection in the framework of Noisy-DR-L0-SSC so as to further improve its efficiency? For example, does the correctness of Noisy-DR-L0-SSC still hold if we use OSNAP (Jelani Nelson and Huy L. Nguyen. OSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings. FOCS 2013)? If does, the contribution would be more sufficient. 4 L0 minimization problem is an NP-hard problem, so in general case, the Proximal Gradient Descent (PGD) in Section 5 only renders an approximate solution to the optimization problem of noisy L0-SSC and Noisy-DR-L0-SSC. It could be better if the authors can provide a bound between the approximate solution by PGD and the optimal solution so that there is a guarantee that the gap between the approximate solution and the optimal solution is bounded. 5 Since the correctness of noisy L0-SSC and Noisy-DR-L0-SSC are proved in terms of the subspace detection property, it could be better if a measure directly related to the subspace detection property is used to measure the performance of the proposed methods. 6 In the randomized analysis of noisy L0-SSC, the assumption is adopted, i.e., data in subspace are i.i.d. isotropic samples. In Remark 5, this assumption is regarded to equivalent to the assumption of data are i.i.d. samples uniformly distributed on the unit sphere up to a scaling factor. The scaling factor, i.e. square root of d_k, can be easily removed in the current analysis of noisy L0-SSC so that almost the same correctness of noisy L0-SSC can still be obtained under the assumption that data are i.i.d. samples uniformly distributed on the unit sphere. The assumption of the uniform distribution on the unit sphere has been widely used, e.g. in L1-SSC Soltanolkotabi & Cands (2012) and noisy L1-SSC Wang & Xu (2013). Slightly reformatting the current results of L0-SSC under randomized models and presenting the correctness of L0-SSC under the assumption of the uniform distribution on the unit sphere can make a comparison to existing works easier. 7 C_{p,p_0} in equation (31) is defined in the appendix (equation (55)). Please define it before it is used."}
{"id": "iclr2020_212", "title": "Defending Against Physically Realizable Attacks on Image Classification | OpenReview", "abstract": "Abstract:###We study the problem of defending deep neural network approaches for image classification from physically realizable attacks. First, we demonstrate that the two most scalable and effective methods for learning robust models, adversarial training with PGD attacks and randomized smoothing, exhibit very limited effectiveness against three of the highest profile physical attacks. Next, we propose a new abstract adversarial model, rectangular occlusion attacks, in which an adversary places a small adversarially crafted rectangle in an image, and develop two approaches for efficiently computing the resulting adversarial examples. Finally, we demonstrate that adversarial training using our new attack yields image classification models that exhibit high robustness against the physically realizable attacks we study, offering the first effective generic defense against such attacks.", "review": " [Note: I gave a 3 for thoroughness even though I only read the paper once, because I believe that I carefully considered the paper while reading it.] This paper argues that threat models such as L-inf are limited when considering physically realizable attacks, and provides evidence for this by showing that L-inf adversarial training is insufficient to fully confer robustness against physically realizable attacks in the literature such as the adversarial glasses attack. The paper then proposes an alternate threat model based on contiguous rectangular regions, and shows that adversarial training against this model does far better. Overall this is a strong paper with thorough experiments, and was for the most part carefully written (although see some quibbles below). I particular liked the paper*s carefully distinction between physically *realizable* and physically *realized* attacks, and the admission that not all realizable attacks would fall under the threat model (while still justifying why the model is interesting). I am on the border of weak accept and strong accept. The main two points keeping me from strong accept are discussed below (and seem perhaps addressable by the authors): 1. The assertion that rectangular occlusions might be a fruitful model for realizable attacks is only lightly tested, since the attacks considered in this paper all fall into the rectangular occlusions threat model. Since one of the key points in the paper is that training in the L-inf threat model could leave vulnerabilities to non-Linf attacks, we might expect the same with rectangular occlusions. A more convincing demonstration would be to test robustness to physically realizable attacks that fall outside the rectangular model (perhaps even skewed or rotated rectangles, although a less synthetic example would be even better). A more minor point is that it would be nice to test the robustness of models trained on rectangular occlusions against L-inf adversarial attacks. This is relevant to knowing whether training on occlusions is giving up on adversarial robustness or actually helps against both (I actually think it*s plausible that you would do well on L-inf, but it seems worth testing either way). 2. The high-level claimed take-away that adversarial training does not help against physically realizable attacks seems false in light of Figure 2, which shows that L-inf adversarial training substantially improves robustness relative to the baseline. A more accurate take-away would be that on some datasets adversarial training helps but still leaves a gap, while on others it does not help at all and perhaps hurts. I would prefer more careful wording as a reader who only goes through the introduction might not see Figure 2. On #1, I should stress that the paper would still be interesting regardless of the outcome of the two experiments in #1; I just think that for thoroughness it would be nice to include them. The existing experiments are already thorough so this would be going above and beyond, but that is why it might help raise my score from weak to strong accept. EDITED TO ADD: For #2, I would also be happy with an argument from the authors as to why the current language appropriately describes the experiments. I do not wish to dictate to the authors what their take-aways are, but more to open up for discussion a point that seemed slightly sloppy to me. Minor comments: Please avoid subjective intensifiers: *We then use an extensive experimental evaluation to demonstrate that our proposed approach is far more robust against physical attacks on deep neural networks than adversarial training and randomized smoothing methods that leverage lp-based attack models.* Both *extensive* and *far* are unnecessary. Make sure to use citep vs citet correctly. First sentence of 2.1 is too verbose. Overall the prose in that paragraph is turgid, due to too many action phrases being turned into nouns. E.g. *The focus is on*, *The typical goal is* both indicate *action* and could be profitably turned into verbs. See Williams and Bizup*s book on Style. *since our ultimate goal is to defend against physical attacks, untargeted attacks that aim to maximize error are the most useful* This seems weak; I don*t understand why an attack being physical should go in line with it being untargeted; oftentimes an attacker will have a specific targeted goal. *another advantage of the ROA attack is that it is, in principle,easier to compute than, say,l?-bounded attacks* This seems incongruous with the subsequent text, which admits that ROA if implemented naively would be slower than L-inf attacks."}
{"id": "iclr2020_213", "title": "Invariance vs Robustness of Neural Networks | OpenReview", "abstract": "Abstract:###Neural networks achieve human-level accuracy on many standard datasets used in image classification. The next step is to achieve better generalization to natural (or non-adversarial) perturbations as well as known pixel-wise adversarial perturbations of inputs. Previous work has studied generalization to natural geometric transformations (e.g., rotations) as invariance, and generalization to adversarial perturbations as robustness. In this paper, we examine the interplay between invariance and robustness. We empirically study the following two cases:(a) change in adversarial robustness as we improve only the invariance using equivariant models and training augmentation, (b) change in invariance as we improve only the adversarial robustness using adversarial training. We observe that the rotation invariance of equivariant models (StdCNNs and GCNNs) improves by training augmentation with progressively larger rotations but while doing so, their adversarial robustness does not improve, or worse, it can even drop significantly on datasets such as MNIST. As a plausible explanation for this phenomenon we observe that the average perturbation distance of the test points to the decision boundary decreases as the model learns larger and larger rotations. On the other hand, we take adversarially trained LeNet and ResNet models which have good ell_infty adversarial robustness on MNIST and CIFAR-10, and observe that adversarially training them with progressively larger norms keeps their rotation invariance essentially unchanged. In fact, the difference between test accuracy on unrotated test data and on randomly rotated test data upto \theta , for all \theta in [0, 180], remains essentially unchanged after adversarial training . As a plausible explanation for the observed phenomenon we show empirically that the principal components of adversarial perturbations and perturbations given by small rotations are nearly orthogonal", "review": " This paper analyzes the behaviour of DNN trained with rotated images and adversarial examples. Namely, the paper analyzes the relationship between training with rotated images and the robustness to adversarial perturbations, and vice-versa. The paper has several technical issues that need to be resolved before drawing any conclusions: 1) “invariance”: this term is not used in the correct way. The fact that the network has the same accuracy when before and after rotation does not mean that the output layer is invariant to rotation. Note invariance in the output layer is a more stringent criterion as it requires that the images get labeled in the same way. The same accuracy can be achieved with completely different labelings of the images. What this paper is evaluation is robustness to rotation vs robustness to adversarial perturbations. 2) It is unclear that Figure 3 is saying that adversarial training does not affect the rotation invariance because there is a general drop of accuracy. The analysis could have been done by evaluating how many images are labelled differently after the rotation, and all the plots will be aligned at 0 degrees. 3) Finding out the robustness to adversarial perturbations is an NP-hard problem. So, for all tested cases in the paper, there could be a perturbation that damaged the model much more than the ones found, which could change the conclusions of the analysis. 4) The networks compared in the two experiments are different networks. There could be a network dependency. Also, I find the paper poorly written (eg. in the abstract: *Neural networks achieve human-level accuracy on many standard datasets used in image classification.” -> what does it mean “human-level accuracy”?; *The next step is to achieve better generalization to natural (or non-adversarial) perturbations” -> why is this the next step?)."}
{"id": "iclr2020_214", "title": "Collapsed amortized variational inference for switching nonlinear dynamical systems | OpenReview", "abstract": "Abstract:###We propose an efficient inference method for switching nonlinear dynamical systems. The key idea is to learn an inference network which can be used as a proposal distribution for the continuous latent variables, while performing exact marginalization of the discrete latent variables. This allows us to use the reparameterization trick, and apply end-to-end training with SGD. We show that this method can successfully segment time series data (including videos) into meaningful *regimes*, due to the use of piece-wise nonlinear dynamics.", "review": "Review:###In this paper, the authors consider the problem of learning model parameters of a switching nonlinear dynamical system from a dataset. They propose a new variational inference algorithm for this model-learning problem that marginalizes all discrete random variables in the model using the forward-backward algorithm and, in so doing, converts the model to one with a differentiable density, so that the gradient of the variational objective can be estimated with the low-variance reparameterization estimator. The authors also point out an issue in choosing a variational objective; the standard ELBO objective is not suitable for their learning problem, because it leads to a model that does not use discrete random variables meaningfully. To overcome this issue, they suggest a new improved objective and a learning procedure, which encourage the learned model to use discrete variables for capturing different modes of dynamics. The proposed variational inference algorithm was applied to three datasets, and in all these cases, it showed promising results. I found the main idea and technique of the paper simple and nice. I am reasonably positive about the paper. The main text of the paper is written well, but the experimental result section seems to be rushed and needs to be polished slightly. I gave weak accept, but if the authors give a convincing answer for my question below, I may raise my score. I presume that the objective L(theta,phi) in (11) is optimized by a version of gradient ascent. Here is my question related to this: [Q] Why is H(O) in p5 differentiable with respect to theta and phi? I am asking this question because the distribution O is defined in terms of arg max, which is not a differentiable operator. Furthermore, the definition of O uses p(s_t|z,x), which uses the model parameters theta. Oh, by the way, I think that the definitions of H and L_CE should include the expectation with respect to q_theta(z|x). Some minor comments are added below. * formula (1), p2: p(x1|s1) should be replaced by p(x1|z1)p(z1|s1) * p3: There are no sub-figures labeled with (a) and (b) in Figure 2. I suggest to put (a) and (b) in front of the captions of the two diagrams in Figure 2. A similar comment applies to Figure 3, because the main text refers to something called Figure 3(a) and Figure 3(b). Also, the paper uses fig. 2(b) sometimes, and Figure 2(b) in other times. Using one convention consistently might help some readers. * p3: Cat(s_t | S(f_s(...)) ===> Cat(s_t | S(f_s(...))) * p3: SDLS ===> SLDS * p3: log p(x) <= L(...) ===> log p(x) >= L(...) * p4: I found the phrase *so they need to perform multiple forward-backward (FB) passes* vague. The algorithm in the paper uses FB twice, and *multiple* in the quoted phrase might mean 2, 3 or more. This makes it less clear whether the algorithm has any benefit over the existing approaches. * p6: This measures compliments ===> This measure complements * p6: within some small temporal around ... where noted ===> within some range around ... as noted * p6: are is constant ===> are constant * p6: The ground truth discrete states ===> The ground truth discrete state * p7: The resulting of ===> The result of"}
{"id": "iclr2020_215", "title": "Batch Normalization is a Cause of Adversarial Vulnerability | OpenReview", "abstract": "Abstract:###Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "review": "Review:###This paper identifies an important weakness of batch normalization: it increases adversarial vulnerability. It is very well written and the claims are theoretically sound. In the experiments, the authors demonstrated a significant difference in robustness between networks with or without batch normalization layers, in varies settings against both random input noise and adversarial noise. This weakness of batch norm was explained due to the *decision boundary tilting* effect caused by the normalization. Overall, this paper has done solid work to reveal an interesting phenomenon. If it is true, this finding will impact almost all DNN models. My concern is that this phenomenon is just another effect of *gradient masking * (as pointed out by Athalye, et al.). Batch norm is a well-known technique to avoid overfitting, without batch norm the network can be easily trained to be saturated with almost zero gradients, demonstrating a false signal of *robustness* to noise. The random noise and real-world corruption experiments are definitely helpful to clear this doubt, but only partially. My concern remains because of two obvious signs of gradient masking: 1. The accuracy on PGD-li (epsilon=0.031) attacks are suspiciously too high (20% - 40% Table 3/4). For this level of attack, the acc should be nearly zero. This is likely caused by the gradient masking effect, considering the cifar-10 networks were trained for longer time with larger learning rate (150 epochs, fixed lr 0.01). Training on MNIST is much easier to get zero gradients. 2. The weight decay discussion is not helpful at all, on the contrary, it confirms my concern on the gradient masking effect. In Table 8, the robustness was increased ~40% by just using large weight decay. This is not the *real robustness*, and can be easily evaded by adaptive attack (see Athalye*s paper). With the above two concerns in mind, I doubt the phenomenon revealed in this paper is just *one can easily train a saturated model without batch norm* or equivalently *it*s hard to train a saturated model with batch norm*. It is hard to say if this is a bad thing for batch norm. I am quite surprised that the authors ignore this completely. Here are a few things that can be done to rule out the possibility of gradient masking. The masked gradient can be identified by: 1) One-step attacks perform better than iterative attacks; 2) Unbounded attacks do not reach 100% success., etc (see Section 3.1 of Athalye*s paper). 1. Including FGSM in the experiments and show the same trends as PGD-li. 2. Show two networks have similar gradient norms. 3. Apply cw-l2 attack, and show batch norm has forced large perturbation. Two other suggestions: 1. Summarize the different angles/steps taken to verify the phenomenon, somewhere before the experiments. 2. Cannot see why the input dimension discussion contribute to explanations of the batch norm weakness. ============ My rating stays the same after rebuttal. My original concerns are like the other reviewers: why BN, not other techniques such as structure of DNNs MLP vs CNN vs ResNet, activation functions, weight decay, learning rates, softmax etc. My initial suspect was that it is caused by gradient masking likely caused by the l2 weight regularization, so asked the authors to look at the gradient norms and run some testes to rule this out. Yes, the weight norm is directly related to the Lipschitz continuity of the function represented by the network, but it often becomes more complicated on complex nonlinear neural networks. According to the new experiment results, the vulnerability is indeed not an effect of gradient masking, thanks for the clarification. However, the new results also indicate that the finding is susceptible to both weight decay and learning rate: in Figure 16 (a): *Un PGD* < *BN PGD* before learning rate decay, andFigure 17 (a) vs (b), doubling the weight decay penalty to 1e-3 also increases the vulnerability of BN. Overall, I believe the phenomenon exists, but the reasons behind requires more explanations, at least not just the batch norm."}
{"id": "iclr2020_216", "title": "DeepSphere: a graph-based spherical CNN | OpenReview", "abstract": "Abstract:###Designing a convolution for a spherical neural network requires a delicate tradeoff between efficiency and rotation equivariance. DeepSphere, a method based on a graph representation of the discretized sphere, strikes a controllable balance between these two desiderata. This contribution is twofold. First, we study both theoretically and empirically how equivariance is affected by the underlying graph with respect to the number of pixels and neighbors. Second, we evaluate DeepSphere on relevant problems. Experiments show state-of-the-art performance and demonstrates the efficiency and flexibility of this formulation. Perhaps surprisingly, comparison with previous work suggests that anisotropic filters might be an unnecessary price to pay.", "review": "Review:###In this paper, CNNs specialized for spherical data are studied. The proposed architecture is a combination of existing frameworks based on the discretization of a sphere as a graph. As a main result, the paper shows a convergence result, which is related to the rotation equivalence on a sphere. The experiments show the proposed model achieves a good tradeoff between the prediction performance and the computational cost. Although the theoretical result is not strong enough, the empirical results show the proposed approach is promising. Therefore I vote for acceptance. The paper is overall clearly written. It is nice that the authors try to mitigate from overclaiming of the analysis. As a non-expert of spherical CNN, I don*t understand clearly the gap between the result Theorem 3.1 and showing the rotation equivalence. It would be nice to add some counterexample (i.e., in what situation the proposed approach does not have rotational equivalence while Theorem 3.1 holds)."}
{"id": "iclr2020_217", "title": "One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation | OpenReview", "abstract": "Abstract:###Recent advances in the sparse neural network literature have made it possible to prune many large feed forward and convolutional networks with only a small quantity of data. Yet, these same techniques often falter when applied to the problem of recovering sparse recurrent networks. These failures are quantitative: when pruned with recent techniques, RNNs typically obtain worse performance than they do under a simple random pruning scheme. The failures are also qualitative: the distribution of active weights in a pruned LSTM or GRU network tend to be concentrated in specific neurons and gates, and not well dispersed across the entire architecture. We seek to rectify both the quantitative and qualitative issues with recurrent network pruning by introducing a new recurrent pruning objective derived from the spectrum of the recurrent Jacobian. Our objective is data efficient (requiring only 64 data points to prune the network), easy to implement, and produces 95 % sparse GRUs that significantly improve on existing baselines. We evaluate on sequential MNIST, Billion Words, and Wikitext.", "review": "Review:###Notes: -RNN network pruning has proven to be challenging using the techniques often used with other network types. -One issue is that the performance of an LSTM/GRU can hinge on a few activated gates and can lead to more concentration of influence than would be seen in a feedforward network without parameter sharing between layers. -New objective uses 64 points to prune the network (I assume this is just the size of the minibatch). -Result is a 95% sparse GRU cell. -New idea is based on keeping weights that propagate information through many time steps. -Encourages *singular values of the temporal jacobian with respect to network weights to be non-degenerate* (I suppose this means that the gradient flowing through time will contain multiple directions of variation) -Introduction does a good job of introducing the key ideas. -J_t is a temporal jacobian of size N x N (N is number units) at time step t. -Chi is the spectral norm of this temporal jacobian. I*m a bit confused by this, because my understanding is that the spectral norm is the largest singular value, but equation 3 looks like a sum over singular values, making it more like a frobenius norm? -This jacobian isn*t tractable, so paper approximates it using a first-order taylor expansion. So basically the pruning just amounts to taking parameters with the largest gradient? -Section 2.4 is confusing and seems to come out of nowhere. Is this suggesting that the technique isn*t just pruning but adding a new normalization scheme? On second reading, this is a normalization scheme effecting which parameters to prune. The motivation for why the gradients are normalized like this is still confusing. If you*re willing to make a linear assumption, it seems like it*s enough to consider the gradient on the parameter multiplied by the magnitude of the parameter to see the overall effect of removing it? -The results look good, but sequential mnist is a bit of a toy task. I*d also like to see a more fine-grained analysis showing the tradeoff between the number of units removed and the performance. -The paper claims that L2 pruning requires more data, but it*s unclear if this really matters since the whole dataset was used to train both methods initially. -On Table 2 the results of the technique don*t seem that much better than *Random*. Review: This paper presented a fast pruning algorithm for RNNs, which uses the norm of the gradient as a guide to pruning. I*m borderline on this paper. The idea of using the gradient is good, but the explanation of some aspects like the normalization is confusing and felt random. Additionally the results, while better than some other pruning techniques on RNNs, don*t seem to be that much better than random."}
{"id": "iclr2020_218", "title": "Towards Modular Algorithm Induction | OpenReview", "abstract": "Abstract:###We present a modular neural network architecture MAIN that learns algorithms given a set of input-output examples. MAIN consists of a neural controller that interacts with a variable-length input tape and learns to compose modules together with their corresponding argument choices. Unlike previous approaches, MAIN uses a general domain-agnostic mechanism for selection of modules and their arguments. It uses a general input tape layout together with a parallel history tape to indicate most recently used locations. Finally, it uses a memoryless controller with a length-invariant self-attention based input tape encoding to allow for random access to tape locations. The MAIN architecture is trained end-to-end using reinforcement learning from a set of input-output examples. We evaluate MAIN on five algorithmic tasks and show that it can learn policies that generalizes perfectly to inputs of much longer lengths than the ones used for training.", "review": "Review:###This paper proposes a new variation of modular neural networks. Specifically, they proposed a new architecture for the neural controller that selects which module to use given the current state and history computations, and evaluated the architectures for five symbolic tasks. I*m leaning towards rejection because: 1. The story is confusing. Instead of framing algorithm induction as a program synthesis task, the paper told the story from the modular networks*s view. This is quite confusing as most of work in this line of research usually assumes the modules are neural networks and are learned, while this paper uses user provided modules. This should then be framed as doing program synthesis with pre-specified primitive functions. This work hence is quite close to NPI [1]. A RL version of NPI is also recently proposed [2]. 2. Lack of novelty. Continuing from the previous point, it seems it is not easy to distinguish the main idea of this work and the work of NPI. There are two differences. There*s an architectural difference about how one deals with history. NPI uses a LSTM and this work uses the last time step*s computation configuration as input to a FNN. It is unclear why one is better than the other? Such modification is also lack of motivation, unexplained in the paper. Another difference is about how one designs the tasks/environments. This work uses a working tape while NPI uses programming traces. 3. Lack of baseline comparisons. In the experiment section, no previous work are evaluated and compared to the proposed method, leaving one unknown how well the proposed method compares with previously known work. That being said, there are still some merits in the paper. The currently known work for training NPI with RL [2] does not support argument predictions, while this work does. I do not know if this is due to the interface/environment that*s designed for the tasks in the paper, but I recommend the authors to rewrite the story and clarify these differences more clearly. Also how general is this environment? For harder programming tasks, can one still uses tape? How scalable is this? Minor point - How do you use FNN to produce all actions of the controller? Do you use a different heads for different action output? Or do you use an autoregressive architecture? Have you tried both and compared? [1] Neural Programmer-Interpreters. [2] Learning Compositional Neural Programs with Recursive Tree Search and Planning."}
{"id": "iclr2020_219", "title": "The Dual Information Bottleneck | OpenReview", "abstract": "Abstract:###The Information-Bottleneck (IB) framework suggests a general characterization of optimal representations in learning, and deep learning in particular. It is based on the optimal trade off between the representation complexity and accuracy, both of which are quantified by mutual information. The problem is solved by alternating projections between the encoder and decoder of the representation, which can be performed locally at each representation level. The framework, however, has practical drawbacks, in that mutual information is notoriously difficult to handle at high dimension, and only has closed form solutions in special cases. Further, because it aims to extract representations which are minimal sufficient statistics of the data with respect to the desired label, it does not necessarily optimize the actual prediction of unseen labels. Here we present a formal dual problem to the IB which has several interesting properties. By switching the order in the KL-divergence between the representation decoder and data, the optimal decoder becomes the geometric rather than the arithmetic mean of the input points. While providing a good approximation to the original IB, it also preserves the form of exponential families, and optimizes the mutual information on the predicted label rather than the desired one. We also analyze the critical points of the dualIB and discuss their importance for the quality of this approach.", "review": "Review:###This paper introduces a variant of the Information Bottleneck (IB) framework, which consists in permuting the conditional probabilities of y given x and y given hat{x} in a Kullback-Liebler divergence involved in the IB optimization criterion. Interestingly, this change only results in changing an arithmetic mean into a geometric mean in the algorithmic resolution. Good properties of the exponential families (existence of non-trivial minimal sufficient statistics) are preserved, and an analysis of the new critical points/information plane induced is carried out. The paper is globally well written and clear, and the maths are rigorously introduced and treated. Two minor comments would concern the definition of the Mutual Information (MI), which could be recalled to help the unfamiliar reader and improve self-containedness, and the notation of the expected value (langle \rangle_p), unusual in Machine Learning where mathbb{E} is often preferred. Another point that could be enhanced is the intuition behind the IB and dualIB criteria: a small comment on their meaning/relevance as well as the rationale/implication of the probabilities permutation would be valuable addition. Exhibiting a link with variational autoencoders (VAEs), and expliciting the differences in a VAE framework between the two criteria could also represent an interesting parallel, especially for machine learning oriented readers. One of the main drawback of the present paper is however the lack of convincing experiments. Graphs showing the good behavior of the introduced framework are ok, but the clear interest of using dualIB rather than IB could be emphasized more. In particular, it does not seem that the issues about IB raised in the abstract (curse of dimensionality, no closed form solution) are solved with dualIB. Furthermore, dualIB optimizes the MI of the predicted labels, which is claimed to be beneficial contrary to the MI on the actual labels. However, no empirical demonstration of this superiority is produced, which is a bit disappointing."}
{"id": "iclr2020_220", "title": "Deep Multivariate Mixture of Gaussians for Object Detection under Occlusion | OpenReview", "abstract": "Abstract:###In this paper, we consider the problem of detecting object under occlusion. Most object detectors formulate bounding box regression as a unimodal task (i.e., regressing a single set of bounding box coordinates independently). However, we observe that the bounding box borders of an occluded object can have multiple plausible configurations. Also, the occluded bounding box borders have correlations with visible ones. Motivated by these two observations, we propose a deep multivariate mixture of Gaussians model for bounding box regression under occlusion. The mixture components potentially learn different configurations of an occluded part, and the covariances between variates help to learn the relationship between the occluded parts and the visible ones. Quantitatively, our model improves the AP of the baselines by 3.9% and 1.2% on CrowdHuman and MS-COCO respectively with almost no computational or memory overhead. Qualitatively, our model enjoys explainability since we can interpret the resulting bounding boxes via the covariance matrices and the mixture components.", "review": " Summary: The authors propose to use mixture models for bounding box detection models in order to robustify against occlusion. In practice, they try models which use Gaussian, Multivariate Gaussian, Mixture of Gaussian and Mixture of MVN models and find improved performance compared to deterministic models of bounding boxes in a variety of experiments. The prediction task here is given a region of interest to predict a bounding box consisting of 4 points for the corners of the box. The authors demonstrate usefulness on a variety of datasets. Comments: First, the proposed method has very little technical depth or complexity and is extremely straightforward. This is actually an advantage of the paper, as it makes it a useful trick for a variety of systems that perform this task, but also increases the burden of empirical evidence for its usefulness. Given that there is no particular technical novelty here apart from the application domain of mixture density networks, the paper has to convince based on sheer empirical evidence. Pros: The model makes sense, as images are fraught with ambiguity and when not having access to a generative model to resolve ambiguities through posterior inference the best a predictor can do is regress to these ambiguities, i.e. the different bounding boxes one might expect at object intersections. Suggestion: it would be great if the authors could come up with a toy dataset with ambiguous but known bounding boxes (i.e. overlapping objects with particular poses) to study how well the proposed model recovers those structures. Real-world datasets are great, but we have little understanding of what effect exactly is helping the classification here and an additional toy setting would increase the clarity of the modeling ideas. Cons: The authors handle their Gaussian Mixture Models (GMMs) unconventionally. In particular, they consider predictions to be the expectation of the GMM, i.e. the expected mean. Let*s consider a toy system with two bounding boxes which do not overlap and constitute two modes. The mean over the two boxes could easily cover an area of an image that has no support under either bounding box. As such, predictions with such multimodal and multivariate models have to be made by enumeration and/or sampling from the predictive distribution p(x|I), such that for example K bounding boxes are sampled from a model -for instance the mean of each component for simplicity or multiple samples from each component- and evaluation is performed over each of the K boxes and then the metrics are averaged. In contrast, the authors average the bounding box and then calculate their metrics. Respectfully, I believe this is statistically unsound use of the model and undermines the empirical value of the experiments. On experiments: it is unclear whether multivariate models and multimodality matter and what their effects are. The authors find that different datasets behave differently, which makes sense if more or less occlusion is present, but unfortunately the problem mentioned above undermines the results here. Finally, the authors state that during testing they ignore the covariance structures, which also seems ill-advised as the model is reduced to a deterministic one in that case. It would be interesting to present results contrasting this to samples from the model. Overall: the authors appear to not sufficiently utilize the structure offered by their potentially multivariate and multimodal observation model appropriately. Instead, they effectively use it as a regularizer for training and just utilize its expectation during testing. Minor comment: The authors should call their model a *mixture density network* (Bishop 1994) and cite the relevant paper, as this is a well-described technique for density estimation and the authors apply it to the task at hand. Decision: Given the simplicity of the model and the potentially broad usecases within the object detection field, I was expecting a more thorough empirical analysis here. The idea is simple but appealing. It, however, requires significantly more empirical depth and analysis to be considered for publication. A good start would be to mitigate the concerns I expressed for the evaluation of the model."}
{"id": "iclr2020_221", "title": "Program Guided Agent | OpenReview", "abstract": "Abstract:###Developing agents that can learn to follow natural language instructions has been an emerging research direction. While being accessible and flexible, natural language instructions can sometimes be ambiguous even to humans. To address this, we propose to utilize programs, structured in a formal language, as a precise and expressive way to specify tasks. We then devise a modular framework that learns to perform a task specified by a program – as different circumstances give rise to diverse ways to accomplish the task, our framework can perceive which circumstance it is currently under, and instruct a multitask policy accordingly to fulfill each subtask of the overall task. Experimental results on a 2D Minecraft environment not only demonstrate that the proposed framework learns to reliably accomplish program instructions and achieves zero-shot generalization to more complex instructions but also verify the efficiency of the proposed modulation mechanism for learning the multitask policy. We also conduct an analysis comparing various models which learn from programs and natural language instructions in an end-to-end fashion.", "review": "Review:###Update: I thank the reviewers for their extensive rebuttal and revision of the paper addressing all of my concerns. I have increased my score. Summary This paper investigates an important direction: How can RL agents make use of high-level instructions and task decompositions formalized as programs? The authors propose a model for a program guided agent that, conditioned on a program, interprets the program, executes it to query a perception module and subsequently proposes subgoals to a low-level action module. The method outperforms LSTM and Transformer baselines on a Minecraft-like task and generalizes to programs larger than the one seen during training. Strengths Contribution in the important direction of training RL agents with instructions and prior knowledge, here in the form of programs Clearly written paper with good illustrations of the model Good performance on generalization task of acting in environments where the programmatic instructions are longer than those seen during training Weaknesses One of the contributions of the paper is a modulation mechanism (Section 4.3) on the state features that incorporates a goal-conditioned policy. However, a very related approach has been proposed by Bahdanau, Dzmitry, et al. *Learning to Understand Goal Specifications by Modelling Reward.* ICLR 2019. They introduced FILM layers that modulate the layers in a ConvNet conditioned on a goal representation. This should be discussed and compared to in the paper. I am surprised there is no comparison to other work that conditions on programs or hierarchical RL approaches. For example, the authors mention various works in Section 2, but fail to compare to them or at least explain why a comparison would not be possible. Another point of criticism is that the authors do not use an existing environment, but instead a Minecraft-inspired one similar to Andreas et al, Oh et al. and Sohn et al. This makes a comparison to prior work hard and I would like to understand in what way previous environments were inadequate for the research carried out here. One aspect that I found most interesting in this paper is that the authors also let annotators map the given programs into natural language form. However, there is no discussion of these results. Similarly, there are interesting qualitative analyses in the appendix of the paper that I only stumbled upon by chance. I believe these should be referenced and a short summary should be integrated into the main part of the paper. I would particularly like to see a discussion of limitations already in the main part of the paper. Minor Comments p1: I like the motivation of cooking recipes for work on program conditioned learning. There is in fact a paper (probably multiple) from the NLP community that I think could be cited here. The one that comes to my mind is: Malmaud, Jonathan, et al. *Cooking with semantics.* Proceedings of the ACL 2014 Workshop on Semantic Parsing. 2014. p1: I agree with the argument that programs might be favored over natural language to specify goals as they are unambiguous. However, I think this can also be seen as a drawback. Natural language allows us to very efficiently share information, maybe sometimes information that is only disambiguated through observations in the environment. Another advantage is that natural language for instructing learning agents (like people) is abundant on the web, while programs are not. p2: *that leverages grammar* -> *that leverages a grammar* p2: *we propose to utilize an precise* -> *we propose to utilize a precise* p2: For learning from video demonstrations, an important prior work is Aytar, Yusuf, et al. *Playing hard exploration games by watching youtube.* Advances in Neural Information Processing Systems. 2018. p3: A deep learning program synthesis work prior to the ones mentioned here is Bošnjak, Matko, et al. *Programming with a differentiable forth interpreter.* Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017. p5: Would it make sense to also compare to a purely hand-crafted programmatic policy? I am missing a justification why learning is strictly necessary in the environment considered in this work. p6 Section 4.4.1: I believe the explanation of the perception module would benefit from a concrete example. Questions to Authors"}
{"id": "iclr2020_222", "title": "Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations | OpenReview", "abstract": "Abstract:###Clustering high-dimensional data, such as images or biological measurements, is a long-standing problem and has been studied extensively. Recently, Deep Clustering gained popularity due to the non-linearity of neural networks, which allows for flexibility in fitting the specific peculiarities of complex data. Here we introduce the Mixture-of-Experts Similarity Variational Autoencoder (MoE-Sim-VAE), a novel generative clustering model. The model can learn multi-modal distributions of high-dimensional data and use these to generate realistic data with high efficacy and efficiency. MoE-Sim-VAE is based on a Variational Autoencoder (VAE), where the decoder consists of a Mixture-of-Experts (MoE) architecture. This specific architecture allows for various modes of the data to be automatically learned by means of the experts. Additionally, we encourage the latent representation of our model to follow a Gaussian mixture distribution and to accurately represent the similarities between the data points. We assess the performance of our model on synthetic data, the MNIST benchmark data set, and a challenging real-world task of defining cell subpopulations from mass cytometry (CyTOF) measurements on hundreds of different datasets. MoE-Sim-VAE exhibits superior clustering performance on all these tasks in comparison to the baselines and we show that the MoE architecture in the decoder reduces the computational cost of sampling specific data modes with high fidelity.", "review": "Review:###The proposed method of mixture-of-experts variational autoencoders is valuable and insightful. On the other hand the work could be improved and clarified at some points: - in the abstract it is claimed that the method works for high-dimensional data.However, it should be better explained why this is the case. The method is largely based on density estimation with a mixture of Gaussians which is known to have limitations in higher dimensions (see e.g. classical textbooks like Bishop 1995) - the similarity matrix and the similarity values should be carefully defined. Is there also an underlying similarity function assumed? - a main shortcoming is that there is no discussion or experimental comparison with methods like spectral clustering and kernel spectral clustering. Given that the paper and the proposed method relates to similarity-based representations it would be important to know how it compares to such methods. Though e.g. in Table 1 the authors compare with about 10 other methods it would be more relevant that among some of these would have been spectral clustering and kernel spectral clustering, because of the similarity-based representations. - in section 4.1 the MNIST data are taken with k=10. Though it is nicely explained and illustrated on this data set, it is possibly somewhat misleading as an example. The reason is that this is a classification problem with 10 classes, therefore the choice k=10 is obvious. It would be more important to consider benchmark problems for clustering, instead of classification, for which the choice of k is also an important model selection issue and for which k is unknown (how should k be selected then?). - is each cluster always be assumed to be a Gaussian (which seems to be a strong assumption in general, and possibly not always realistic)? Could other components be used in the mixture?"}
{"id": "iclr2020_223", "title": "On the Linguistic Capacity of Real-time Counter Automata | OpenReview", "abstract": "Abstract:###While counter machines have received little attention in theoretical computer science since the 1960s, they have recently achieved a newfound relevance to the field of natural language processing (NLP). Recent work has suggested that some strong-performing recurrent neural networks utilize their memory as counters. Thus, one potential way to understand the sucess of these networks is to revisit the theory of counter computation. Therefore, we choose to study the abilities of real-time counter machines as formal grammars. We first show that several variants of the counter machine converge to express the same class of formal languages. We also prove that counter languages are closed under complement, union, intersection, and many other common set operations. Next, we show that counter machines cannot evaluate boolean expressions, even though they can weakly validate their syntax. This has implications for the interpretability and evaluation of neural network systems: successfully matching syntactic patterns does not guarantee that a counter-like model accurately represents underlying semantic structures. Finally, we consider the question of whether counter languages are semilinear. This work makes general contributions to the theory of formal languages that are of particular interest for the interpretability of recurrent neural networks.", "review": "Review:###The paper proof properties of counter machines that have in recent work be suggest that LSTMs can model those. The papers starts to mentions related work that relates automaton*s and counter machines with LSTMs. These related work papers do some correlational experiments partly restricted in size, layers and architecture. They provide mostly empirical evidence that some behaviour is related to performance seen in LSTMs, GRU, etc. and similar to those in counter automata. The paper makes then the point to take the counter machine as a simplified formal model of the LSTM. However, I would read Merrill 2019 that counter machines could be model via LSTMs but are not limited or they are not an upper bound what they can compute. The authors does some proves on counter automata and hopes to gain insights into the properties of LSTMs used for NLP or semantic analysis and this would provide insights for the use in NLP. It seems to me that the paper claims that counter automatons are an upper bound for the computation power of LSTMs. In the way I read this seems at least not well formulated or too strong. I would not follow the conclusion that *A general take-away from our results is that just because a counter machine (or LSTM) is sensitive to surface patterns in linguistic data does not mean it can build correct semantic representations*. The argumentation is flawed as counter machines are not an upper limit of expressiveness of LSTMs nor do they describe well what they do. That one can use LSTMs to compute languages that counter automata can do too means not that they could do more. The property of Counter automata are useful for instance to build phrase structures meaning they can be use to express scope and keep track of. However, deeper layered networks are widely used to put structure over the scopes (arguments) to connected them in a higher order fashion. There are many paper which show empirical how to build semantic or syntactic structures using LSTMs - also in already quite well in seq2seq fashion. The more theoretical part looks fine to me and could be of value to readers. Nevertheless, the authors could considere to revise their claims as they are not well supported by the evidence provided in the paper nor pervious literature cited."}
{"id": "iclr2020_224", "title": "Multi-scale Attributed Node Embedding | OpenReview", "abstract": "Abstract:###We present network embedding algorithms that capture information about a node from the local distribution over node attributes around it, as observed over random walks following an approach similar to Skip-gram. Observations from neighborhoods of different sizes are either pooled (AE) or encoded distinctly in a multi-scale approach (MUSAE). Capturing attribute-neighborhood relationships over multiple scales is useful for a diverse range of applications, including latent feature identification across disconnected networks with similar attributes. We prove theoretically that matrices of node-feature pointwise mutual information are implicitly factorized by the embeddings. Experiments show that our algorithms are robust, computationally efficient and outperform comparable models on social, web and citation network datasets.", "review": "Review:###This paper proposed an attributed network embedding method by leveraging a node’s local distribution over attributes. The neighborhood attribute distribution of a node is considered in both a pooled and a multi-scale way. The multi-scale embedding approach considers the neighborhood nodes with different distance to the interested node distinctly, providing more flexibilities to the model. Then, the paper proved theoretically that the proposed embedding methods, both the pooled and multi-scale versions, can be equivalently written the factorization of a node-feature pointwise mutual information matrix. The proposed embedding methods are standard. The key contribution of this paper comes from the theoretical part, which establishes the equivalence between the proposed embedding methods and matrix factorization. It looks interesting, although there are several similar works before, as mentioned in the paper. I don’t know how different your work is from the Qiu’s paper. The experimental results are not convincing. The node classification is a very standard task in the performance evaluation of network embedding, but you put the results into the appendix. I examine the results anyway, and I found the performance gain is very limited, and on some datasets, the proposed methods even perform inferiorly."}
{"id": "iclr2020_225", "title": "Learning De-biased Representations with Biased Representations | OpenReview", "abstract": "Abstract:###Many machine learning algorithms are trained and evaluated by splitting data from a single source into training and test sets. While such focus on in-distribution learning scenarios has led interesting advances, it has not been able to tell if models are relying on dataset biases as shortcuts for successful prediction (e.g., using snow cues for recognising snowmobiles). Such biased models fail to generalise when the bias shifts to a different class. The cross-bias generalisation problem has been addressed by de-biasing training data through augmentation or re-sampling, which are often prohibitive due to the data collection cost (e.g., collecting images of snowmobile on a desert) and the difficulty of quantifying or expressing biases in the first place. In this work, we propose a novel framework to train a de-biased representation by encouraging it to be different from a set of representations that are biased by design. This tactic is feasible in many scenarios where it is much easier to define a set of biased representations than to define and quantify bias. Our experiments and analyses show that our method discourages models from taking bias shortcuts, resulting in improved performances on de-biased test data.", "review": "Review:###This manuscript discusses the problem of bias shortcut employed by many machine learning algorithms (due to dataset problems or underlying effects of any algorithmic bias within an application). The authors argue that models tend to underutilize their capacities to extract non-bias signals when bias shortcuts provide enough cues for recognition. This is an interesting and important aspect of machine learning models neglected by many recent developments. The only problem is that the paper seems to be a bit immature as the exemplar application is too naive for illustrating the idea. The authors’ idea is to assume that ‘there is a family of feature extractors, such that features learned by any of these extractors would correspond to pure bias. Then in order to learn unbiased features, the goal is to construct a feature extractor that is **as different as this family* as possible’ in practice, their claim is texture and color are biases; one should learn shape instead of texture and color. So the family of feature extractors are the ones with small receptive field that can only capture texture and color. Therefore, what they eventually achieved is the unbiased feature extractor only learns the shape of object and avoids learning any texture and color. So, the problem is, in practice, it is very hard to define the family of biased feature extractors. It really depends on the dataset and the goal. Texture and color, in general, are still important cues for object recognition, removing this information is NOT equivalent to removing bias. Just as a suggestion, the background scene might be a better definition of bias. However, with the proposal in this paper, it would be unclear how to define the family of feature extractors for describing background. Therefore, the solution given for this important problem seems to be too ad-hoc and not generalizable. The second example (that does not have an experiments on) is action recognition; the family of biased feature extractors is 2D-frame-wise CNNs (object recognition). The authors claim that objects are biases for action recognition systems, but again a large part of action recognition is indeed object recognition. Many actions are defined based interaction of humans with objects (e.g., opening bottle or pouring water from bottle). Some objects may be instroducing bias in the task, but not all. Again, the proposed solution in this paper cannot disentangle this. The authors need to survey previous texture-shape disentanglement works and then compare with those methods."}
{"id": "iclr2020_226", "title": "Picking Winning Tickets Before Training by Preserving Gradient Flow | OpenReview", "abstract": "Abstract:###Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time. Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels.", "review": "Review:###This paper proposes a novel one-shot-pruning algorithm which improves the training of sparse networks by maximizing the norm of the gradient at initialization. The utility of training sparse neural networks and shortcomings of dense-to-sparse algorithms like Pruning, LotteryTicket are nicely motivated at introduction. The pruning criterion is motivated by the first order approximation of the change in the gradient norm when a single connection is removed, though the results show that removing many connections together with GraSP increases the total gradient norm therefore allowing the loss to decrease faster. Experiments suggest employing such pruning algorithm improves final performance over two baselines: random and SNIP. Though I find the proposed method intriguing and well motivated, experiments section of the paper misses some important sparse training baselines and needs some improvement. I am willing to increase my score given my concerns/questions below are addressed. (1) The paper doesn*t mention some important prior work on the topic. Since the paper focuses on end-to-end sparse training, the following sparse training methods needs to be considered and compared with: - Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science [Mocanu, 2018] - Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization [Mostafa, 2019] - Deep Rewiring: Training very sparse deep networks [Bellec, 2017] - There is also few recent work submitted to ICLR2020: https://openreview.net/forum?id=SJlbGJrtDB, https://openreview.net/forum?id=ryg7vA4tPB, https://openreview.net/forum?id=ByeSYa4KPS (2) Pruning baselines can be improved. I am not convinced that they represent the best achievable sparse training results. I would recommend method proposed by `Prune or Not to Prune` as a strong baseline. You can also check `State of Sparsity` paper to obtain some competitive pruning results. (3) It*s great that the authors are aware of the importance of having experiments on larger datasets. Though, I found the results reported on Imagenet to be limited. Is there a reason why Imagenet-2012 results are missing pruning baselines? I think having other reported pruning results here along with performance of other sparse training methods (SET, DSR) would be useful. Most of these numbers should be readily available in the papers mentioned above, but I guess it is always better to run them using the same settings. (4) To demonstrate the usefulness of the pruning criteria proposed, it would be nice to do some simple ablations. Some suggestions: (1) Remove weights that would *decrease* the gradient norm most (2) Do random pruning while preserving exact per layer sparsity fractions. (3) sweep over batch size used to calculate the importance scores and evaluate final accuracies or the initial gradient norm. The second experiment would help identifying whether the gains are due to better allocation of sparsities across layers or due to increased gradient norm. Looking at Figure-4 and seeing the per layer sparsities are different, It is not clear to me which one is the underlying reason for improved performance. (5) (Page 8 / Table 5) Do you aggregate all accuracies in Table 5 using different batch sizes and initialization methods? If, so I am not sure what the intended message here is, since it is difficult to infer how these hyper-parameters affect the result. Do you sweep different batch sizes for estimating importance of units, too? It would be nice to see whether the two batch sizes interact with each other and/or how increased batch size affects the quality of pruned networks. Some minor comments: (a) (Page 1) I found the motivation very intriguing. Though the statement `Recently, F&C (2019) shed light on this...` seems a bit off, given that LT can*t find solutions as well as the pruning solution in most practical (larger datasets and architectures) settings. Therefore I would be better to pose this as an `open problem`. (b) (end of page-1) `However, connection sensitivity is sub-optimal as a criterion because the gradient of each weight might change dramatically after pruning due to complicated interactions between weights`. I think this is still the case for GraSP. Since the criterion it uses assumes independence (i.e. what if we remove a single weight?). It would be nice to see some ablations on this. Does `K=number of weights removed` affect the norm of the sparsified networks? (c) (Figure 1) I find the comparative illustration between SNIP and GraSP very useful. Though, the architecture presented seems a bit artificial (i.e. I am not aware of any architecture with single hidden layer and a single output unit). I think the same motivation can be made by removing the top unit (therefore having 6-4-1 units) and removing all incoming connections for the output unit until a single connection remains. Then SNIP would remove that single connection whereas GraSP would remove one of the connections in the previous layer. (d) (Section 2.1) `In contrast, Hessian based algorithms...` Though it is a structured pruning algorithm It might be nice to include the following work, https://arxiv.org/abs/1611.06440. (e) (Section 2.1) Previous work needs following citations: [Bellec, 2017], [Mocanu, 2018] and [Mostafa, 2019] (f) (Section 2.2) Why the initial dynamics affect the final performance? One explanation given in the paper is through recent work on NTK and this is great. Though training settings used at `Lee et al (2019a)` and in the paper are a bit different. Usage of MSE, small datasets, etc… So it might be nice to point out differences. (g) (Section 3) At , `n`->`N` (h) (Page 4) `Preserving the loss value motivated several…` -> `motivated by several…` I think it is better to use existing terminology whenever available.I think using `One-shot pruning` instead of `Foresight pruning` would be a better choice and would prevent confusion. (j) (Page 5) `However, it has been observed that different weights are highly coupled …` This has been observed much earlier, too: like in Hassibi, 1993. (k) (Page 7) Last sentence `and thus hopefully..`: needs to be fixed. (l) (Page 8) The whole page needs some proof-reading. Some of them: (a) `SNIP and GraSP. We present...` probably connect with comma (b) `aims for preserving` -> `aims to preserve` (c) `In contrast, SNIP are more` `are`->`is` (d) `for ablation study` -> `as an ablation study`... (m) Is there a specific reason why VGG networks are preferred for experiments? I don*t think they are relevant to any practical application anymore and they are massively over-parameterized for tasks in hand. Specifically for Cifar-10. I think focusing on more recent networks and larger datasets would increase the impact of the work."}
{"id": "iclr2020_227", "title": "Selective Brain Damage: Measuring the Disparate Impact of Model Pruning | OpenReview", "abstract": "Abstract:###Neural network pruning techniques have demonstrated it is possible to remove the majority of weights in a network with surprisingly little degradation to top-1 test set accuracy. However, this measure of performance conceals significant differences in how different classes and images are impacted by pruning. We find that certain individual data points, which we term pruning identified exemplars (PIEs), and classes are systematically more impacted by the introduction of sparsity. Removing PIE images from the test-set greatly improves top-1 accuracy for both sparse and non-sparse models. These hard-to-generalize-to images tend to be of lower image quality, mislabelled, entail abstract representations, require fine-grained classification or depict atypical class examples.", "review": "Review:###The paper claims that neural network pruning methods have different impact on accuracy in class-wise and sample-wise. To this end, the paper performs statistical tests to identify such classes (and samples) from a population of neural network models of different levels of pruning. In particular, the identified samples, called PIEs, are shown to have significantly lower test accuracy, i.e., the PIEs are much harder to classify, and a user study is performed to support this claim. The paper also demonstrates that pruned models tend to have lower accuracy against adversarial attacks or common corruptions. In overall, the paper addresses an important problem of investigating the effects of pruning. The experiments performed here seems fairly extensive. One of my primary concerns, however, is that I am still not convinced whether the empirical findings presented here is indeed significant, as some reader might feel those results are not that surprising. I personally feel that it is much more likely that pruning gives heterogeneous effects over the classes, since current pruning schemes do not enforce the *uniform* brain damage, i.e., they simply aim to minimize total accuracy drops. Also, the accuracy differences in Table 1 do not appear to be that large in my opinion. The questions in what follows may help to resolve such concerns: - Section 2.1: For ImageNet dataset, data imbalance in training set might be the reason of such disparate impact? As CIFAR-10 dataset is balanced, I wonder if CIFAR-10 results could be also presented in more details. - What if the whole training and pruning is re-performed after excluding the classes which the accuracy is decreased more by pruning? Would it lead to better pruning results, or better overall accuracy? - I feel there should be more explanation about the actual statistical implication of the use of Welch*s t-test: Is it ok that S^c_t may not be independent to S^c_0? What is the key benefits of not just picking outlier classes among S^c_t with respect to c? - What would happens if we exclude the PIEs in training set, and performs training & pruning from scratch? The overall claims may be strenghten if this could improve the pruning efficiency. - Figure 7: I think the pure ImageNet top-1 accuracy should be also presented in each plot as a baseline for fair comparison, as this accuracy will be decreased with respect to the sparsity as well."}
{"id": "iclr2020_228", "title": "Improving Federated Learning Personalization via Model Agnostic Meta Learning | OpenReview", "abstract": "Abstract:###Federated Learning (FL) refers to learning a high quality global model based on decentralized data storage, without ever copying the raw data. A natural scenario arises with data created on mobile phones by the activity of their users. Given the typical data heterogeneity in such situations, it is natural to ask how can the global model be personalized for every such device, individually. In this work, we point out that the setting of Model Agnostic Meta Learning (MAML), where one optimizes for a fast, gradient-based, few-shot adaptation to a heterogeneous distribution of tasks, has a number of similarities with the objective of personalization for FL. We present FL as a natural source of practical applications for MAML algorithms, and make the following observations. 1) The popular FL algorithm, Federated Averaging, can be interpreted as a meta learning algorithm. 2) Careful fine-tuning can yield a global model with higher accuracy, which is at the same time easier to personalize. However, solely optimizing for the global model accuracy yields a weaker personalization result. 3) A model trained using a standard datacenter optimization method is much harder to personalize, compared to one trained using Federated Averaging, supporting the first claim. These results raise new questions for FL, MAML, and broader ML research.", "review": "Review:###This paper studies the application of techniques from meta-learning (a method to train a single model which can then be easily adjusted to perform well on multiple tasks) to federated learning (the task of distributed training of models on distributed datasets). The paper notes that standard meta-learning algorithms are similar to standard federated learning algorithms, and uses this perspective to produce a merged method and evaluate it empirically. Pros. + The motivation of the paper is clear and indeed these methods seems similar, and meta-learning can help with federated learning. Cons. - The resulting method appears somewhat underdeveloped; it is simply to run some amount of federated learning and then some amount of meta-learning, whereas the first parts of the paper led me to believe that a single simultaneous merge of the methods is the way to go. The paper does not report any fine-grained evaluation of various such choices, thus I don*t know why they did that they did, and thus do not find their choices compelling. - The Reptile method is already presented in the original paper with a distributed counterpart, so why not just run that? I am not convinced that some more minor modification of Reptile could not already do well on this paper. - The empirical evaluation is not very extensive, so I am also not convinced there, and in particular I need convincing of this type to believe that regular reptile is beaten by FedAvg+reptile. Minor comments. Page 1, second paragraph, the word *outperform*. I*m not sure what the performance measure is; in federated learning, we care about many things, for instance privacy, keeping the work on the distributed clients low, etc. Page 2, the *three objectives*. I feel meta-learning is doing all three too. Page 3, Algorithm 1. I realize space is a concern, but this was hard to read. Page 4, Algorithm 2. *relatively larger* is vague."}
{"id": "iclr2020_229", "title": "Implicit ?-Jeffreys Autoencoders: Taking the Best of Both Worlds | OpenReview", "abstract": "Abstract:###We propose a new form of an autoencoding model which incorporates the best properties of variational autoencoders (VAE) and generative adversarial networks (GAN). It is known that GAN can produce very realistic samples while VAE does not suffer from mode collapsing problem. Our model optimizes ?-Jeffreys divergence between the model distribution and the true data distribution. We show that it takes the best properties of VAE and GAN objectives. It consists of two parts. One of these parts can be optimized by using the standard adversarial training, and the second one is the very objective of the VAE model. However, the straightforward way of substituting the VAE loss does not work well if we use an explicit likelihood such as Gaussian or Laplace which have limited flexibility in high dimensions and are unnatural for modelling images in the space of pixels. To tackle this problem we propose a novel approach to train the VAE model with an implicit likelihood by an adversarially trained discriminator. In an extensive set of experiments on CIFAR-10 and TinyImagent datasets, we show that our model achieves the state-of-the-art generation and reconstruction quality and demonstrate how we can balance between mode-seeking and mode-covering behaviour of our model by adjusting the weight ? in our objective.", "review": "Review:###The paper proposes to replace the KL-divergence in VAE training with the lambda-jeffreys divergence of which the symmetric KL-divergence is a special case. The paper proposes a pure implicit likelihood approach that uses three discriminator models to estimate the KL-divergences. Experiments are conducted on CIFAR-10 and TinyImageNet and several scores are reported to show that the proposed method performs as good if not better than current approaches. --------- I think the paper tries to achieve too much in too little space and foregoes scientific exactness for the sake of claiming SOTA. Since there is a difference between claiming SOTA on a task and validating a new method, the small amount of space makes it difficult to substantiate both claims at the same time. In the rest of the review i will try to substantiate the claim: 1. The paper claims on page 2: *These models do not have a sound theoretical justification about what distance [...] they optimize*. While the paper tries to substantiate its claims by showing theoretically that it does the right thing using the optimal discriminator, it leaves the question open what happens with any other discriminator. The theory does not justify non-optimal solutions. It is argued on page 6 that non-optimality of the discriminator serves as some form of regularization, but this requires some justification. Moreover, the paper uses LPIPS to measure reconstruction quality - but this measure is a deep neural network. So if those measures are good enough to compare solutions with and the theoretical justification of the proposed method is shaky in practice - why not use LPIPS for training? 2. The paper proposes the discriminator in order to allow for an implicit likelihood. However, the r-function used in the experiments does not fulfill the property of a well defined likelihood, and Theorem 1 does not hold, since technically the KL-divergence is infinity. If we ignore this by adding a small amount of Gaussian noise around the sampled cyclical shifts - like the r* used in the experiments, we can easily write down the explicit likelihood function since: r(y|x)=sum_i w_i N(y|Shift_i(x), sigma) where Shift_i is the i-th shift in the set described in the paper and w_i its probability p(y|q). So the explicit solution of theorem 1 can be written down and another ablation study would be training the method with the explicit formulation for this KL-term(i.e. only training two discriminator models). If the results are not equivalent, this implies that the discriminator does not reach the optimum. The implications of that should be discussed regarding 1. 3. Existing ablation studies are a bit of a straw-man: the paper compares changing r(y|x) by standard Gaussian or Laplace. However, we know that a large variance does not make any sense and almost all papers use tiny variances (e.g. in beta-VAE the beta-values tend to be very small, which is equivalent to small variances here). --------------------------- Smaller things - Are the experimental results all with the same architecture for encoder/generator for all results you compared to? if not, the effect of that should also be tested. - my personal biased view on the generated images is: it looks worse than alpha-GAN. Every reconstructed image has a grey tone and the generated images also offer a strong grey palette. The details don*t look better as well. - typo inroduce->introduce"}
{"id": "iclr2020_230", "title": "Filter redistribution templates for iteration-lessconvolutional model reduction | OpenReview", "abstract": "Abstract:###Automatic neural network discovery methods face an enormous challenge caused for the size of the search space. A common practice is to split this space at different levels and to explore only a part of it. Neural architecture search methods look for how to combine a subset of layers, which are the most promising, to create an architecture while keeping a predefined number of filters in each layer. On the other hand, pruning techniques take a well known architecture and look for the appropriate number of filters per layer. In both cases the exploration is made iteratively, training models several times during the search. Inspired by the advantages of the two previous approaches, we proposed a fast option to find models with improved characteristics. We apply a small set of templates, which are considered promising, for make a redistribution of the number of filters in an already existing neural network. When compared to the initial base models, we found that the resulting architectures, trained from scratch, surpass the original accuracy even after been reduced to fit the same amount of resources.", "review": "Review:###This paper investigates the impact of several predefined filter templates, including uniform template, reverse template, quadratic template, and negative quadratic template, to the performance of different neural networks such as VGG-19, ResNet-50, Inception Network, and MobileNet. This paper uses some templates. However, it is impossible to enumerate all possible templates and hence some good templates may not be included and studied in this paper, which makes the empirical studies in this paper less useful. In experiments, authors need to compare with the results of neural architecture search. Based on such comparison, we can see whether the templates used in this paper are reasonable. In experiments, it seems that different templates may have their own characteristics and their usefulness also depends on the neural network used. So it is not easy to make general conclusion."}
{"id": "iclr2020_231", "title": "A Mutual Information Maximization Perspective of Language Representation Learning | OpenReview", "abstract": "Abstract:###We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).", "review": " The paper proposes to make a clear connection between the InfoNCE learning objective (which is a lower bound of the mutual information) and multiple language models like BERT and XLN. Then based on the observation that classical LM can be seen as instances of InfoNCE, they propose a new (InfoWord) model relying on the same principles, but taking inspiration from other models also based on InfoNCE. Mainly, the proposed model differs both in the nature of the a and b variables used in InfoNCE, and also on the fact that it uses negative sampling instead of softmax. Experiments are made on two tasks and compared to a classical BERT model, and on the BERT-NCE model that is a BERT variant proposed by the authors which is somehow in-between BERT and InfoWord. They show that their approach works quite well. I have a very mitigated opinion on the paper. I) First, I really like the idea of trying to unify different models under the same learning principles, and then show that these models can be seen as specific instances of generic principles. But the way it is presented and explained lacks of clarity: for instance in Section 2, some notations are not well defined (e.g what is f?) . Moreover, the way classical models are casted under the InfoNCE principle is badly written: it assumes that readers have a very good knowledge of the models, and the paper does not show well the mapping between the loss function of each model and the InfoNCE criterion. It gives technical details that could (in my opinion) get ignored, and I would clearly prefer to catch the main differences between the different models that being flooded by technical details. So, my suggestion would be to improve the writing of this section to make the message stronger and relevant for a larger audience. II) The Infoword model can be seen as a simple instance of word masking based models, and as an extension of deep infomax for sequences (it would be certainly nice to describe a little bit what Deep InfoMax is to facilitate the reading). Here again, the article moves from technical details (e.g *hidden state of the first token (assumed to be a special start of sentence symbol *) without providing formal definitions. Having a first loss function after paragraph 4 could help to understand the principle of this model (before restricting the model to n-grams). Moreover, the equation J_DIM seems to be wrong since it contains g_omega twice while I think (but maybe I am wrong) that it has also to be defined by g_psi. J_MLM is also not clear since x_i is never defined (I assume it is x_{i:i}). At last, after unifying multiple models under one common learning objective, the authors propose to mix two different losses which is strange (the effect of the second term is slightly studied in the experimental section) without allowing us to understand why it is important to have this second loss function and why the first one is not sufficient enough. At last, I am pretty sure to not be able to reproduce the model described in the paper (adding a section on that in the supplementary material would help), and many concrete aspects are described too fast (like the way to sample negative pairs). Concerning the experimental section, experiments are convincing and show that the model is able to achieve a performance which is close to classical models. In my opinion, tis section has to be interpreted as a proof that the proposed unified vision is a good way to easily define new and efficient models. To summarize, the unification under the InfoNCE principle is interesting, but the way the paper is written makes it very difficult to follow, and the description of the proposed model is unclear (making the experiments difficult to reproduce) and lacks of a better discussion about the interest of mixing multiple loss."}
{"id": "iclr2020_232", "title": "Self-supervised Training of Proposal-based Segmentation via Background Prediction | OpenReview", "abstract": "Abstract:###While supervised object detection and segmentation methods achieve impressive accuracy, they generalize poorly to images whose appearance significantly differs from the data they have been trained on. To address this in scenarios where annotating data is prohibitively expensive, we introduce a self-supervised approach to detection and segmentation, able to work with monocular images captured with a moving camera. At the heart of our approach lies the observations that object segmentation and background reconstruction are linked tasks, and that, for structured scenes, background regions can be re-synthesized from their surroundings, whereas regions depicting the object cannot. We encode this intuition as a self-supervised loss function that we exploit to train a proposal-based segmentation network. To account for the discrete nature of the proposals, we develop a Monte Carlo-based training strategy that allows the algorithm to explore the large space of object proposals. We apply our method to human detection and segmentation in images that visually depart from those of standard benchmarks, achieving competitive results compared to the few existing self-supervised methods and approaching the accuracy of supervised ones that exploit large annotated datasets.", "review": "Review:###This paper provides a new self-supervised proposal-based approach for object detection and segmentation. The author introduces a Monte Carlo-based optimization to solve the inefficiency problem in the discrete proposal-based forward process defined in (Crawford and Pineau 2019). Also, the paper redefines the decoder part for self-supervised from minimizing reconstruction loss with background segmentation to maximize reconstruction error with learning a foreground segmentation. The method is then verified with a suite of experiments for people-detection on video datasets. The main benefit over many previous unsupervised object detection/segmentation approaches is that they did not make use of optical flow or other readily available cues during training. However, given that the framework directly came from (Crawford & Pineau 2019), and the only change is from variational inference to an importance-sampling (MC) approach. This would be fine if it is verified in experiments, however, the experiments did not show any comparison w.r.t. (Crawford & Pineau 2019) hence we have no way of understanding what is the relative performance w.r.t. that baseline approach. Besides, in all the experiments a single object is in the view. How does the method perform in images where multiple objects are in the view? A little bit of a philosophical question is whether this a problem worth pursuing as well. For self-supervised motion estimation (e.g. optical flow), it is clear why we want to do that. However, the current type of algorithm is dependent on the assumption that the background follows relatively consistent textures, this may not necessarily be true in practice, and hence the application could be quite limited. Many previous unsupervised video object segmentation methods make use of optical flow and boundary detection, which I thought are OK cues to be used, especially when both can be learned in a self-supervised manner. This is not entirely related to the assessment, but I would still like to hear what the authors think. Minor: In the paragraph after *Training strategy*(Section 3.2, Page 5), is it *the foreground objective O of Eq. (2)* or *Eq.(4)*?"}
{"id": "iclr2020_233", "title": "Phase Transitions for the Information Bottleneck in Representation Learning | OpenReview", "abstract": "Abstract:###In the Information Bottleneck (IB) (Tishby et al., 2000), when tuning the relative strength between compression and prediction terms, how do the two terms behave, and what’s their relationship with the dataset and the learned representation? In this paper, we set out to answer this question by studying multiple phase transitions in the IB objective: IB?[p(z|x)] = I(X;Z) ? ?I(Y ;Z), where sudden jumps of dI(Y ;Z)/d? and prediction accuracy are observed with increasing ?. We introduce a definition for IB phase transitions as a qualitative change of the IB loss landscape, and show that the transitions correspond to the onset of learning new classes. Using second-order calculus of variations, we derive a formula that provides the condition for IB phase transitions, and draw its connection with the Fisher information matrix for parameterized models. We provide two perspectives to understand the formula, revealing that each IB phase transition is finding a component of maximum (nonlinear) correlation between X and Y orthogonal to the learned representation, in close analogy with canonical-correlation analysis (CCA) in linear settings. Based on the theory, we present an algorithm for discovering phase transition points. Finally, we verify that our theory and algorithm accurately predict phase transitions in categorical datasets, predict the onset of learning new classes and class difficulty in MNIST, and predict prominent phase transitions in CIFAR10 experiments.", "review": "Review:###This paper studies the phase transition problem in the information bottleneck (IB) objective and derives a formula for IB phase transitions. Based on the theory developed in the paper, an algorithm is developed to find phase transition points. The interesting observation is phase transition can correspond to learning new class and this paper conjectures in IB for classification, the number of phase transitions is at most C-1, C the number of classes. This observation deserves to be further explored and may be a key to a deeper understanding of neural networks. The theory developed connects the IB objective, dataset and the representation, and thorough proofs are given. The experiments matches the theoretical findings."}
{"id": "iclr2020_234", "title": "DeepEnFM: Deep neural networks with Encoder enhanced Factorization Machine | OpenReview", "abstract": "Abstract:###Click Through Rate (CTR) prediction is a critical task in industrial applications, especially for online social and commerce applications. It is challenging to find a proper way to automatically discover the effective cross features in CTR tasks. We propose a novel model for CTR tasks, called Deep neural networks with Encoder enhanced Factorization Machine (DeepEnFM). Instead of learning the cross features directly, DeepEnFM adopts the Transformer encoder as a backbone to align the feature embeddings with the clues of other fields. The embeddings generated from encoder are beneficial for the further feature interactions. Particularly, DeepEnFM utilizes a bilinear approach to generate different similarity functions with respect to different field pairs. Furthermore, the max-pooling method makes DeepEnFM feasible to capture both the supplementary and suppressing information among different attention heads. Our model is validated on the Criteo and Avazu datasets, and achieves state-of-art performance.", "review": "Review:###The paper applies Multi-Head Self-Attention (MHSA) to a CTR prediction model with some small changes. The empirical results on two public datasets show it improves performance over some baselines. First of all, the novelty of the proposed algorithm is limited in that it mainly applies existing mulit-head self-attention. The paper does include some small modifications to MHSA and achieves better performance, such as bi-linear similarity and max-pooling. However, the nature of these changes seems more incremental. The experiment section is very detailed and the paper conducts several ablation studies to understand which components contribute the most, which is nice. However, the paper is missing several important baselines, for example, Deep & Cross [1], which makes the results less convincing. Another issue with the paper is that it does not control the model capacity when comparing performance. It is usually the case that increasing model capacity leads to better performance. Given that MHSA and bi-linear similarity have increased a lot of model parameters, it is more fair to compare performance across models with similar capacity. In fact, in [1], they show the logloss on Criteo dataset can be as low as 0.4423 when using large enough parameters. Minor: in the ablation study, it shows head = 1 has the best performance. In this case, why max-pooling is needed? Reference: [1] Wang, R., Fu, B., Fu, G. and Wang, M., 2017, August. Deep & cross network for ad click predictions. In Proceedings of the ADKDD*17 (p. 12). ACM."}
{"id": "iclr2020_235", "title": "OPTIMAL BINARY QUANTIZATION FOR DEEP NEURAL NETWORKS | OpenReview", "abstract": "Abstract:###Quantizing weights and activations of deep neural networks results in significant improvement in inference efficiency at the cost of lower accuracy. A source of the accuracy gap between full precision and quantized models is the quantization error. In this work, we focus on the binary quantization, in which values are mapped to -1 and 1. We introduce several novel quantization algorithms: optimal 2-bits, optimal ternary, and greedy. Our quantization algorithms can be implemented efficiently on the hardware using bitwise operations. We present proofs to show that our proposed methods are optimal, and also provide empirical error analysis. We conduct experiments on the ImageNet dataset and show a reduced accuracy gap when using the proposed optimal quantization algorithms.", "review": "Review:###this paper looks at different quantization schemes (replace values of vector in R to, basically, plus or minus v, for well chosen v). Different schemes are considered, namely the low rank binary quantization (a matrix is approximated by the componentwise product of a low rank matrix and a +/-1 matrix) and k-bit binary quantization. Finding the best quantization ends up in solving a program, which is convex and quite straightforward fo k=1 and 2, and unfortunately (apparently) non-convex for k>2. So the authors suggest a greedy approach for k>2. The motivation of this work is DNN, arguing that quantized vectors should improve computations cost, hence some experiments are provide on DNN, yet they only illustrate the fact that quantization does not deteriorate too much the learning & test error. The main criticisms is therefore the lack of concrete evidences that those schemes are actually helpful and, on the other hand, the relative simplicity (so the theoretical part of the paper are not sufficient by itself)"}
{"id": "iclr2020_236", "title": "BAIL: Best-Action Imitation Learning for Batch Deep Reinforcement Learning | OpenReview", "abstract": "Abstract:###The field of Deep Reinforcement Learning (DRL) has recently seen a surge in research in batch reinforcement learning, which aims for sample-efficient learning from a given data set without additional interactions with the environment. In the batch DRL setting, commonly employed off-policy DRL algorithms can perform poorly and sometimes even fail to learn altogether. In this paper we propose anew algorithm, Best-Action Imitation Learning (BAIL), which unlike many off-policy DRL algorithms does not involve maximizing Q functions over the action space. Striving for simplicity as well as performance, BAIL first selects from the batch the actions it believes to be high-performing actions for their corresponding states; it then uses those state-action pairs to train a policy network using imitation learning. Although BAIL is simple, we demonstrate that BAIL achieves state of the art performance on the Mujoco benchmark, typically outperforming BatchConstrained deep Q-Learning (BCQ) by a wide margin.", "review": "Review:###Summary of Claims: The paper proposes a batch RL method that they claim is simpler than most existing methods that try to avoid the extrapolation error that is prevalent among batch RL methods. They do this by completely avoiding the minimization/maximization (cost/reward) of the approximate value function that is fit to the batch transitions. Instead, they train an approximation for the state value function*s tight upper bound (which they refer to as the upper-envelope) by using their monte-carlo returns. By fitting such an approximator, they sample the state-action pairs that are close to the envelope (thus have high values/returns), and use behavioral cloning to fit a parameterized policy to those state-action pairs. Decision: Weak Reject. My decision is influenced by two main reasons: (1) Although the simplicity of the method is apparent and a very desirable feature, the authors don*t highlight situations where this can lead to bad policies. For example, consider that there are two pairs (s, a_1, s*) and (s, a_2, s*) in the batch that are close to the upper-envelope, and hence will both be used for training the policy. Using Behavioral cloning, the policy would regress to the mean of a_1 and a_2, which could be a terrible action altogether. The issue here is that only one of these two pairs has higher return and our policy needs to only predict that action (or in the case of tie, either one.) This can be really bad in situations where two very different actions can lead to same returns (e.g. in a reacher-like task the arm can reach a goal in two different rotations.) Even though I pointed out a very specific case, one could think of many other cases where the proposed approach might result in a bad policy. Having said all of this, it might be true that such cases do not appear in practice (which I highly doubt) but its the authors job to raise and clarify that. The current set of experimental setups (mujoco locomotion problems) are not good enough evidence for that and they need experiments where optimal-policies can be multi-modal or have diverse experimental setups (manipulation etc.) (2) Experimental results are a little unsettling. The primary reason is that in all of the plots, BCQ, BAIL, BC aren*t starting from the same test return at 0 parameter updates! In most plots BAIL starts off way higher in return than BCQ, BC with no parameter updates yet, which suggests that the experiments were not setup well. Maybe, they didn*t initialize the policy in the same way for all the approaches, maybe the random seeds were not the same for all approaches, or maybe BAIL had some sort of pretraining for the policy that was not accounted for in the parameter updates. In any way, this needs to be addressed. This is also highlighted by the fact that the learning curves for BAIL are almost always flat across a million parameter updates! If you are starting off with a random initialization, there should be an upwards slope for the learning curve. Also, as raised in the previous point I think using these Mujoco locomotion environments is not convincing enough to claim that BAIL is a viable competitive batch RL approach. Comments and Questions: (1) I like the simplicity of the approach and the fact that it is much more easier to understand than existing works like BCQ (2) Paper is well-written. It was clear, lucid and descriptive. (3) Why is the deterministic dynamics assumption needed? I am curious (4) The paper makes some subjective statements such as *BEAR is also complex*, which is not substantiated well enough. Refrain from making such statements (5) Not comparing to BEAR because their code is not publicly available is a contentious reason. I personally feel that the authors could have reimplemented it and compared but I am not sure what the community feels about that (6) Is there any reason why REM cannot be applied to mujoco environments? If it can be, then why did the authors not compare to REM as well? (7) Another subjective statement (that is clearly wrong) *many robotic tasks are expected to be deterministic environments* - although this is slightly true, the reason we model environments to be stochastic is not because there is inherent randomness in them but because our state descriptions are never complete. The state descriptors are always partial and we account for them by assuming stochasticity in the dynamics. For example, consider a robotic manipulation task where if you know all the environmental factors as part of your state space(such as the friction coefficients) you can assume deterministic dynamics, else you are better off assuming stochastic dynamics because the same actuation might not result in the same motion every time (because of varying friction) (8) Concatenating subsequent episodes in a batch only makes sense (as the authors point out) if the policy doesn*t change much across episodes. But this is not true of current off-policy RL methods like DDPG, SAC. You either need very small learning rate or a trust-region constraint to ensure that the policy doesn*t change much across episodes. (9) Why do different batches with different seeds and the same algorithm lead to widely different results for batch RL? There is clearly something fishy here. Is it because of the off-policy RL methods used to collect the data, is it due to the batch RL method used? More investigation needed"}
{"id": "iclr2020_237", "title": "A Mechanism of Implicit Regularization in Deep Learning | OpenReview", "abstract": "Abstract:###Despite a lot of theoretical efforts, very little is known about mechanisms of implicit regularization by which the low complexity contributes to generalization in deep learning. In particular, causality between the generalization performance, implicit regularization and nonlinearity of activation functions is one of the basic mysteries of deep neural networks (DNNs). In this work, we introduce a novel technique for DNNs called random walk analysis and reveal a mechanism of the implicit regularization caused by nonlinearity of ReLU activation. Surprisingly, our theoretical results suggest that the learned DNNs interpolate almost linearly between data points, which leads to the low complexity solutions in the over-parameterized regime. As a result, we prove that stochastic gradient descent can learn a class of continuously differentiable functions with generalization bounds of the order of ( : the number of samples). Furthermore, our analysis is independent of the kernel methods, including neural tangent kernels.", "review": "Review:###This paper suggests a new technique to analyze the implicit regularization caused by ReLU activations. They bound the generalization error by two terms: 1) one term that represents the distance between the trained network output and a piecewise linear function built based on the set of training points and 2) another term that represents the distance between the piecewise linear approximation and the desired target. The first term is bounded using a random walk type of analysis, which to the best of my knowledge is novel. I find this technique rather interesting and technically sound, although I do have a number of concerns and I*m at the moment more on the reject side, although I will re-consider my score if the authors can provide satisfactory answers. Generalization to more complex activation functions If I understand correctly, the interpolation technique between two points only works for ReLU functions. If one were to try to generalize the analysis to more complex non-linear functions by using a more complex interpolation schemes, wouldn’t you then have a random walk in high-dimensions? If so, wouldn’t that be a problem given the different properties of Brownian motion in high-dimensions? Generalization to smooth activation functions Another question related to the previous one is whether one could hope to generalize the analysis to smooth activation functions. I believe this is also a drawback of combinatorial techniques such as Hanin and Rolnick which have to rely on the discrete nature of the breakpoints. Generalization bound is only derived for 1-d functions Theorem 2 is derived for each dimension independently while the generalization results in Theorem 4 are for 1-dimensional inputs. Where is the difficulty in generalizing these results to higher dimensions? Prior work on generalization of SGD I was really expecting a discussion about how the generalization bound derived in this paper compares to prior work, e.g. Hardt, Moritz, Benjamin Recht, and Yoram Singer. *Train faster, generalize better: Stability of stochastic gradient descent.* arXiv preprint arXiv:1509.01240 (2015). Kuzborskij, Ilja, and Christoph H. Lampert. *Data-dependent stability of stochastic gradient descent.* arXiv preprint arXiv:1703.01678 (2017). Brutzkus, Alon, et al. *Sgd learns over-parameterized networks that provably generalize on linearly separable data.* arXiv preprint arXiv:1710.10174 (2017). And many others… For instance the bound derived in Hardt et al. is also of the order O(n^-2). The bounds in Kuzborskij are also data-dependent and so are yours since your generalization bound depends on the density of the training points. Can you comment on this? What specific insights do we gain your analysis? Noise SGD My understanding is that the authors assume that the noise of SGD is Gaussian. Although this is commonly used when analyzing SGD, there is evidence that the noise is actually not Gaussian, see e.g. Daneshmand, Hadi, et al. *Escaping saddles with stochastic gradients.* arXiv preprint arXiv:1803.05999 (2018). Simsekli, Umut, Levent Sagun, and Mert Gurbuzbalaban. *A tail-index analysis of stochastic gradient noise in deep neural networks.* arXiv preprint arXiv:1901.06053 (2019). I feel this is worth pointing out and one could perhaps also extend this analysis to Heavy-tail noise. I would expect that the results would still hold in expectation but perhaps with a slightly worse probability. Influence step size SGD Using larger step sizes in the SGD updates increase the influence of the noise. I was expecting this to somehow be captured in your analysis but I fail to see where it appears. Can you comment on this? Proof Lemma 3 The derivation of Eq. 10 does not seem completely justified in the proof in the appendix. The authors essentially prove that the length of the gradient gap is bounded by |S| but why is the coefficient omega distributed according to a normal distribution. It seems to me that you need the noise of SGD to be Gaussian for such statement to hold. Can you confirm this? If so, I think this needs to be clearly stated as an assumption since -- as pointed out above -- this is not necessarily true in practice. Minor: proof Theorem 2 It seems rather trivial but for completeness, you should write the proof of Eq. (6) in Theorem 2. “A priori estimates” This is a terminology that is often used in the paper but never defined. What do you mean by “a priori” in this context? Minor comment I would move footnote 3 directly in the main step. I think it is important to point out that the steps of the random walk correspond to the breakpoints."}
{"id": "iclr2020_238", "title": "A SPIKING SEQUENTIAL MODEL: RECURRENT LEAKY INTEGRATE-AND-FIRE | OpenReview", "abstract": "Abstract:###Stemming from neuroscience, Spiking neural networks (SNNs), a brain-inspired neural network that is a versatile solution to fault-tolerant and energy efficient information processing pertains to the ”event-driven” characteristic as the analogy of the behavior of biological neurons. However, they are inferior to artificial neural networks (ANNs) in real complicated tasks and only had it been achieved good results in rather simple applications. When ANNs usually being questioned about it expensive processing costs and lack of essential biological plausibility, the temporal characteristic of RNN-based architecture makes it suitable to incorporate SNN inside as imitating the transition of membrane potential through time, and a brain-inspired Recurrent Leaky Integrate-and-Fire (RLIF) model has been put forward to overcome a series of challenges, such as discrete binary output and dynamical trait. The experiment results show that our recurrent architecture has an ultra anti-interference ability and strictly follows the guideline of SNN that spike output through it is discrete. Furthermore, this architecture achieves a good result on neuromorphic datasets and can be extended to tasks like text summarization and video understanding.", "review": "Review:####### A. Summarize what the paper claims to do/contribute. Be positive and generous. #### The paper translates the Leaky Integrate and Fire model of neural computation via spike trains into a discrete-time RNN core similar to LSTM. The architecture would be readily amenable to the modern deep learning toolkit if not for the non-differentiability of the hard decision to spike or not. The hard decision is made by thresholding. The paper adopts a simple approximation of backpropagating a *gradient* of 1.0 through the operation if the threshold is within a neighbourhood [thresh - a, thresh + a], and otherwise 0.0, so the system can be trained by backpropagation. The architecture is tested on a few *neuromorphic* video classification datasets including MNIST-DVS and CIFAR-DVS. Experiments are also run on a text summarization task. #### B. Clearly state your decision (accept or reject) with one or two key reasons for this choice. #### The reviewer thinks the paper should be rejected in its current state. The proposed architecture is a straightforward change to a standard LSTM core. Thus it should be compared head-to-head to LSTM on standard datasets for these models (e.g. classic synthetic tasks, language modeling, speech recognition, machine translation, etc) with everything else held constant (hidden size, learning rate, sequence length, etc etc). It also doesn*t really carry over any of the benefits of Spiking Neural Nets even though it is inspired by Leaky Integrate and Fire because it operates in discrete time like a normal RNN, just with an extra binary output produced by spiking. It*s unclear that a spiking inductive bias is actually useful, even though event-driven computation could in theory allow much less computation, the proposed method does not have that property. So the paper doesn*t really provide evidence to back up their claim that the proposed model combines the complimentary advantages of Deep Learning and Spiking Neural Nets. #### C. Provide supporting arguments for the reasons for the decision. While the proposed method is in-spirit inspired by the leaky integrate and fire model, it is operated/trained in discrete time which does not allow it to achieve the benefits of continuous time integrate-and-fire models which allow for less computation and time-discretization-invariance. The conversion of the spiking model to the deep learning framework is rather crude, as the differentiable approximation to the non-differentiable threshold operation is biased and not well-motivated either empirically, intuitively, or theoretically (i.e. there are no comparisons to alternative choices). There are new techniques for marrying continuous-time models and deep learning which seem more promising to investigate to this end (e.g. Neural ODE). So in summary, the method doesn*t have the computational benefits of a biologically plausible spiking algorithms and is not well-tested against competing deep learning methods, making it hard to verify the motivation of pushing toward a performant yet biologically plausible algorithm. #### #### D. Provide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment. #### There are many grammatical and word-choice mistakes which make the paper hard to read. Mainly, from a practical perspective, the paper would be much-improved by showing what benefit the spiking inductive bias confers over a standard LSTM on standard tasks in the deep learning community. The method/landscape should be developed and studied in further detail until claims can be made about combining the strengths of spiking and deep-learning models."}
{"id": "iclr2020_239", "title": "CZ-GEM: A FRAMEWORK FOR DISENTANGLED REPRESENTATION LEARNING | OpenReview", "abstract": "Abstract:###Learning disentangled representations of data is one of the central themes in unsupervised learning in general and generative modelling in particular. In this work, we tackle a slightly more intricate scenario where the observations are generated from a conditional distribution of some known control variate and some latent noise variate. To this end, we present a hierarchical model and a training method (CZ-GEM) that leverages some of the recent developments in likelihood-based and likelihood-free generative models. We show that by formulation, CZ-GEM introduces the right inductive biases that ensure the disentanglement of the control from the noise variables, while also keeping the components of the control variate disentangled. This is achieved without compromising on the quality of the generated samples. Our approach is simple, general, and can be applied both in supervised and unsupervised settings.", "review": "Review:###Summary: The paper proposes the use of a hierarchical model for a generative modeling task. They propose a framework of introducing an intermediate latent variable to enforce the independence of the control and noise variable. The paper report extensive experimental results to validate the proposed hierarchical model. The authors also provide the anonymized code to observe the exact implementation in TensorFlow to visualize the latent variable traversals. Comments: The paper proposes the use of a hierarchical model for a generative modeling task by introducing an intermediate latent variable to enforce the independence of the control and noise variable. The paper report extensive experimental results to validate the proposed hierarchical model. This type of framework of crude to fine hierarchical generative model has already been successfully introduced by StackGAN and it*s recent variants. On the unsupervised disentangled feature learning, the framework provides incremental advancement by using beta-VAE in conjunction with GAN to use the best of both the worlds. Even though the proposed approach is similar to StackGAN, the experiments and the results mentioned in the paper are noteworthy. Questions to Authors: There are 2 main claims of novelty made in the paper. 1. Architectural Biases: How is the approach different in comparison to the StackGAN and it*s variable which also use multiple levels of crude to fine image generation? 2. Unsupervised control variable discovery: This part is just the use of existing disentanglement VAEs to extract the control variables. So how does the paper try to make contributions to improve the disentangled features with the proposed method? Apart from combining these to existing ideas, what can be considered as an added novelty to improve the quality of the disentangled features? In summary, I find there is no novelty involved apart from combining the already existing SOTA model in disentangled feature learning (beta-VAE) and image generation (StackGAN)."}
{"id": "iclr2020_240", "title": "On Stochastic Sign Descent Methods | OpenReview", "abstract": "Abstract:###Various gradient compression schemes have been proposed to mitigate the communication cost in distributed training of large scale machine learning models. Sign-based methods, such as signSGD (Bernstein et al., 2018), have recently been gaining popularity because of their simple compression rule and connection to adaptive gradient methods, like ADAM. In this paper, we perform a general analysis of sign-based methods for non-convex optimization. Our analysis is built on intuitive bounds on success probabilities and does not rely on special noise distributions nor on the boundedness of the variance of stochastic gradients. Extending the theory to distributed setting within a parameter server framework, we assure exponentially fast variance reduction with respect to number of nodes, maintaining 1-bit compression in both directions and using small mini-batch sizes. We validate our theoretical findings experimentally.", "review": "Review:###This paper performs a general analysis of sign-based methods for non-convex optimization. They define a new norm-like function depending on the success probabilities. Using this new norm-like function and under an assumption, they prove exponentially variance reduction properties in both directions and small mini-batch sizes. I am not convinced about assumption 1, which plays the key role of the proof. It assumes that success probabilities are always large or equal to 1/2. How can we guarantee this property hold for an algorithm? I suggest the authors provide some real learning examples, under which it will satisfy the condition. I may revise my rating according to this."}
{"id": "iclr2020_241", "title": "Fast Task Inference with Variational Intrinsic Successor Features | OpenReview", "abstract": "Abstract:###It has been established that diverse behaviors spanning the controllable subspace of a Markov decision process can be trained by rewarding a policy for being distinguishable from other policies. However, one limitation of this formulation is the difficulty to generalize beyond the finite set of behaviors being explicitly learned, as may be needed in subsequent tasks. Successor features provide an appealing solution to this generalization problem, but require defining the reward function as linear in some grounded feature space. In this paper, we show that these two techniques can be combined, and that each method solves the other*s primary limitation. To do so we introduce Variational Intrinsic Successor FeatuRes (VISR), a novel algorithm which learns controllable features that can be leveraged to provide enhanced generalization and fast task inference through the successor features framework. We empirically validate VISR on the full Atari suite, in a novel setup wherein the rewards are only exposed briefly after a long unsupervised phase. Achieving human-level performance on 12 games and beating all baselines, we believe VISR represents a step towards agents that rapidly learn from limited feedback.", "review": "Review:###Summary: This paper proposes an algorithm to combine the ideas of unsupervised skill/option discovery (Eysenbach et. al., 2018, Gregor et. al., 2016, referred to as “BMI” in the paper) with successor features “SFs” (Barreto et. al., 2017, 2018). While unsupervised skill/option discovery algorithms employ mutual information maximization of visited states and the latent variables corresponding to options (typically discrete), this paper adds a restriction that this latent variable (now continuous) should be the task vector specified by some learnt successor features. With such a restriction, the algorithm can now be used in an unsupervised pre-training stage to learn conditional policies corresponding to several different task vectors and can be used to directly infer (without training or fine-tuning) a good policy for a supervised phase where external reward is present (i.e. via GPI from Barreto et. al., 2018) by simply regressing to the best task vector. Such unsupervised pre-training is shown to outperform DIAYN (Eysenbach et. al., 2018) in 3 different Atari suites (including the full 57 game suite) and also ablations to the proposed model where GPI and SFs are excluded individually. Decision: I vote for accept as this paper proposes a novel technique to combine mutual information based intrinsic control objectives with successor features, which allow for combining the benefits of both in a complementary way. An unsupervised phase can now discover good conditional policies with successor features which can be used to infer a good policy to solve an external reward task in a supervised phase, with such a policy capable of attaining human level performance in several Atari games and outperforming several baselines such as DQNs in limited data regimes. Other comments: - The technique for enforcing the restriction in Eq. 10, as well as being able to use it with generalized policy improvement is a good novel contribution in the paper. - The detailed comparison with baselines on the full Atari suite is sufficient to back the claims in the paper that the strengths of BMI and SFs do complement each other. - The fact that fast task inference is sufficient to get good performance is impressive i.e. without the need to fine-tune the best inferred policy. Minor typos: - In section 5 para 5, “UFVA” -> “UVFA”, “UFSA” -> “USFA”."}
{"id": "iclr2020_242", "title": "YaoGAN: Learning Worst-case Competitive Algorithms from Self-generated Inputs | OpenReview", "abstract": "Abstract:###We tackle the challenge of using machine learning to find algorithms with strong worst-case guarantees for online combinatorial optimization problems. Whereas the previous approach along this direction (Kong et al., 2018) relies on significant domain expertise to provide hard distributions over input instances at training, we ask whether this can be accomplished from first principles, i.e., without any human-provided data beyond specifying the objective of the optimization problem. To answer this question, we draw insights from classic results in game theory, analysis of algorithms, and online learning to introduce a novel framework. At the high level, similar to a generative adversarial network (GAN), our framework has two components whose respective goals are to learn the optimal algorithm as well as a set of input instances that captures the essential difficulty of the given optimization problem. The two components are trained against each other and evolved simultaneously. We test our ideas on the ski rental problem and the fractional AdWords problem. For these well-studied problems, our preliminary results demonstrate that the framework is capable of finding algorithms as well as difficult input instances that are consistent with known optimal results. We believe our new framework points to a promising direction which can facilitate the research of algorithm design by leveraging ML to improve the state of the art both in theory and in practice.", "review": "Review:###This paper introduces a new approach to solve optimization problems without relying on any human-provided data beyond the specification of the optimization problem itself. The approach is inspired by the two-player zero-sum game paradigm and follow a generative adversarial network (GAN) setting. One network is trained to output the optimal behavior for a given problem, while the other is trained to output difficult instances of the given problem. These two networks are trained simultaneously and compete against the other until some equilibrium is achieved. This approach is tested on two small problems for which the optimal behavior is known and seems to perform near theoretical optimality. I weakly reject this paper because although the approach is indeed interesting, the paper is lacking some structure, as described below: - The paper clearly mentions that no optimization of the training setup or the hyperparameters has been done because the authors are not interested in extending ML techniques. However, hyperparameter searching is not extending any ML technique, it is just an approach to find a good training configuration and show robustness in different hyperparameters settings. It is thus unclear if the approach is robust against different hyperparameter settings. - Very little details (apart from the optimization algorithm) are given regarding the architecture used (types of input, output, neural units, activation functions, number of hidden layers, loss function, etc...), which makes it very hard to reproduce this approach. - Section 1.1 presents results with too many details without introducing the problem. I would suggest the authors to either introduce the two problems earlier or to simply say that near-optimal results are achieved, without giving detailed results, because it is very hard to understand them without any introduction of the task being achieved. - One task is presented in Section 2 *Preliminaries* while the other task is presented in Section 4 *AdWords*. It is hard to follow the flow of ideas present in the paper when similar things are not together. I would suggest restructuring the paper into a more classical structure such as: <intro without detailed results - previous work & problematic - approach taken with more details for reproducibility - description of the two tasks - description of experiments with more details for reproducibility - results - conclusion>. - The paper mentions the MSVV algorithm twice but no reference or explanation is provided. It is very hard to understand sentences referring to this. - This work only considers problems for which the optimal input distribution is known, but is motivated by the fact that it could be applied to problems for which the optimal distribution is unknown and thus being able to discover new algorithms. It is hard to support this motivation when no experiments are done in its favor. - No comparison has been made between their approach and other previous approaches. We only know that the proposed approach finds near-optimal solutions with a difference of 0.01 competitive ratio. It is thus very hard to know if this new approach brings any improvement to previous work. Below are a few things that were not considered to make a decision, but are only details that would make the paper slightly better: - typo at the beginning of section 3.1: missing *be* in *This can either *be* by an ...* - typo at the beginning os section 4: missing *be* in *... the algorithm must irrevocably *be* allocated to ...* - Axis* names to the different plots in the Figures would help understand them better. Also, the description of some figures could benefit more details that could be taken off from the text."}
{"id": "iclr2020_243", "title": "word2ket: Space-efficient Word Embeddings inspired by Quantum Entanglement | OpenReview", "abstract": "Abstract:###Deep learning natural language processing models often use vector word embeddings, such as word2vec or GloVe, to represent words. A discrete sequence of words can be much more easily integrated with downstream neural layers if it is represented as a sequence of continuous vectors. Also, semantic relationships between words, learned from a text corpus, can be encoded in the relative configurations of the embedding vectors. However, storing and accessing embedding vectors for all words in a dictionary requires large amount of space, and may stain systems with limited GPU memory. Here, we used approaches inspired by quantum computing to propose two related methods, word2ket and word2ketXS, for storing word embedding matrix during training and inference in a highly efficient way. Our approach achieves a hundred-fold or more reduction in the space required to store the embeddings with almost no relative drop in accuracy in practical natural language processing tasks.", "review": "Review:###This paper explores two related methods to reduce the number of parameters required (and hence the memory footprint) of neural NLP models that would otherwise use a large word embedding matrix. Their method, inspired by quantum entanglement, involves computing word embeddings on-the-fly (or by directly computing the output of the *word embedding* with the first linear layer of network). They demonstrate their method can save an impressive amount of memory and does not exhibit big performance losses on three nlp tasks that they explore. This paper is clearly written (with only a couple of typos) but does not yet reach publication standard. Whilst the empirical performance of their approach is promising from the perspective of saving reducing memory requirements, more experiments are required and more careful comparisons to baselines and other methods in the literature for saving memory/parameters. In general the related work and experimental sections are weak and brief, with only superficial analysis. There is lack of careful analysis and insight into their results, as well as a careful comparisons to other work in this area. The choice of tasks to evaluate on is broad, which is a strength, but is missing simpler tasks that one would expect to see, such as a text classification dataset, or simple bag-of-vectors style models. In addition, the choice of models are somewhat outdated baselines. It seems that transformers would be an ideal setting for their approach, as transformers have rather high dimensional word embedding matrices, but the authors do not run experiments with the now-ubiquitous Transformer. The quantum inspiration is largely a distraction, and I think the paper would benefit from this element being scaled back or removed in order to free up space for more experiments. The authors acknowledge one key weakness of their approach, that both training and inference time are increased (by 28% or 55% longer for DocQA depending on compression) but much more work could be done to understand the best way to mitigate for longer training and inference times. The authors argue that reducing the memory footprint of models is vital to address hardware limitations for training and inference for large models like BERT or ROBERTA, but this argument is not particularly strong. Generally current limitations for training these kinds of models are the long training times and being able to fit large batches onto our hardware, and the vocabulary matrix is only a constant factor here. And since training time is a bottleneck, the added value of saving memory vs slowing the training speed by 30-50% is debatable. Here are some questions for the authors that come to mind when reviewing: How does your method compare to other published methods on your benchmarks? which choices for r and k lead to the best time/memory/performance tradeoff? how does this compare to other compression methods (on your tasks) Seq2Seq models usually involve multiplying the the output hidden state with a vocab matrix before softmaxing over all the vocabulary produce word probabilities - did you account for this? Does your method work for the output vocab matrix? Did you investigate pre-training word2ket like word2vec or Glove?"}
{"id": "iclr2020_244", "title": "$\textrm{D}^2$GAN: A Few-Shot Learning Approach with Diverse and Discriminative Feature Synthesis | OpenReview", "abstract": "Abstract:###The rich and accessible labeled data fuel the revolutionary success of deep learning. Nonetheless, massive supervision remains a luxury for many real applications, boosting great interest in label-scarce techniques such as few-shot learning (FSL). An intuitively feasible approach to FSL is to conduct data augmentation via synthesizing additional training samples. The key to this approach is how to guarantee both discriminability and diversity of the synthesized samples. In this paper, we propose a novel FSL model, called GAN, which synthesizes Diverse and Discriminative features based on Generative Adversarial Networks (GAN). GAN secures discriminability of the synthesized features by constraining them to have high correlation with real features of the same classes while low correlation with those of different classes. Based on the observation that noise vectors that are closer in the latent code space are more likely to be collapsed into the same mode when mapped to feature space, GAN incorporates a novel anti-collapse regularization term, which encourages feature diversity by penalizing the ratio of the logarithmic similarity of two synthesized features and the logarithmic similarity of the latent codes generating them. Experiments on three common benchmark datasets verify the effectiveness of GAN by comparing with the state-of-the-art.", "review": "Review:###The paper proposes a method for feature vector synthesis for few-shot learning problems focusing on few-shot classification. The proposed method uses WGAN implemented using MLP architecture for both generator and discriminator (as synthesizing in feature space and not in image space). The method is meta-trained to produce good samples given the current FSL task support set. During meta-testing, the support set The claimed novelties of the paper are: * GAN based example synthesis for few-shot classification as a stand-alone method and not a plugin on top of existing FSL approaches as in Zhang et al. 2018. * classification regularization for the generator - the synthesized feature vectors are expected to be classified correctly * mode collapse regularization - cosine similarity between synthesized feature vectors for the same class are penalized w.r.t. the cosine similarity between their generating random vectors The experiments demonstrate the proposed approach utility on the miniImageNet, CUB, and CIFAR100 (CIFAR-FS?) benchmarks. I propose to accept this paper if it undergoes a major revision. Specifically, I propose the authors to address the following points: 1. The comparison to previous approaches is lacking, important recent works are missing from the comparison tables, while having better results on all of the 3 tested datasets. One example would be MetaOptNet method (Lee et al. CVPR 2019), but there are others. Now it might be ok to focus on just the comparison to papers that do example synthesys for FSL, yet it is not so at this point, and is a little disappointing in general. 2. The classification regularization is a known trick in GANs, as conditioning on classes is useful, take ACGAN as an example and there are others. So not clear if novelty could be claimed here. 3. Technical note - in equation (7) it looks like a ratio between two logarithms is used where the arguments are in the range of [-1,1] (cosine similarity?). Then (a) one needs to make sure negative values are clamped; (b) it is a ratio of two negative numbers, so it does not optimize what is claimed in the text, as minimizing (7) would try to maximize similarity between the synthesized vectors, and the intent is the opposite; so in short, is a minus sign missing in front? or the intent was to maximize (7)? The main negative point in my opinion is #1, I suggest the authors to focus on that in their revision. If it is not addressed, I would be inclined to suggest to reject at this point."}
{"id": "iclr2020_245", "title": "iWGAN: an Autoencoder WGAN for Inference | OpenReview", "abstract": "Abstract:###Generative Adversarial Networks (GANs) have been impactful on many problems and applications but suffer from unstable training. Wasserstein GAN (WGAN) leverages the Wasserstein distance to avoid the caveats in the minmax two-player training of GANs but has other defects such as mode collapse and lack of metric to detect the convergence. We introduce a novel inference WGAN (iWGAN) model, which is a principled framework to fuse auto-encoders and WGANs. The iWGAN jointly learns an encoder network and a generative network using an iterative primal dual optimization process. We establish the generalization error bound of iWGANs. We further provide a rigorous probabilistic interpretation of our model under the framework of maximum likelihood estimation. The iWGAN, with a clear stopping criteria, has many advantages over other autoencoder GANs. The empirical experiments show that our model greatly mitigates the symptom of mode collapse, speeds up the convergence, and is able to provide a measurement of quality check for each individual sample. We illustrate the ability of iWGANs by obtaining a competitive and stable performance with state-of-the-art for benchmark datasets.", "review": "Review:###This paper presents an inference WGAN (iWGAN) which fully considers to reduce the difference between distributions of G(X) and Z, G(Z) and X. In this algorithm, the authors show a rigorous probabilistic interpretation under the maximum likelihood principle. This algorithm has a stable and efficient training process. The authors provided a lots of theoretical and experimental analysis to show the effectiveness of the proposed algorithm. Therefore, the innovation of this paper is very novel. The theoretical analysis is sufficient, and the technology is sound."}
{"id": "iclr2020_246", "title": "Monotonic Multihead Attention | OpenReview", "abstract": "Abstract:###Simultaneous machine translation models start generating a target sequence before they have encoded or read the source sequence. Recent approach for this task either apply a fixed policy on transformer, or a learnable monotonic attention on a weaker recurrent neural network based structure. In this paper, we propose a new attention mechanism, Monotonic Multihead Attention (MMA), which introduced the monotonic attention mechanism to multihead attention. We also introduced two novel interpretable approaches for latency control that are specifically designed for multiple attentions. We apply MMA to the simultaneous machine translation task and demonstrate better latency-quality tradeoffs compared to MILk, the previous state-of-the-art approach. Code will be released upon publication.", "review": "Review:###This paper proposes a fully transformer-based monotonic attention framework that extends the idea of MILK. Though the idea of monotonic multi-head attention sounds interesting, I still have some questions below: About the method: 1. Is that possible that the MMA would have worse latency than MILK since all the attention heads need to agree to write while MILK only has one attention head? 2. Is there any attention order between different attention head? 3. I think the MMA only could control the latency during training time, which would produce different models with different latency. Is there any way that enables MMA to control the latency during inference time? Can we change the latency for on given model by tuning the requirements mentioned in (1)? About the experiments: 1. Do you have any explanation of why both MMA-H and MMA-IL have better BLEU when AL is small? The results in fig 2 seem counterintuitive. 2. I suggest the authors do more analysis of the difference between different attention heads to prove the effectiveness of MMA. 3. For the left two figures in fig 4, which one is the baseline, and which one is the proposed model? I also suggest the authors present more real sample analysis and discussions about the experiments."}
{"id": "iclr2020_247", "title": "Stiffness: A New Perspective on Generalization in Neural Networks | OpenReview", "abstract": "Abstract:###We investigate neural network training and generalization using the concept of stiffness. We measure how stiff a network is by looking at how a small gradient step on one example affects the loss on another example. In particular, we study how stiffness depends on 1) class membership, 2) distance between data points in the input space, 3) training iteration, and 4) learning rate. We experiment on MNIST, FASHION MNIST, and CIFAR-10 using fully-connected and convolutional neural networks. Our results demonstrate that stiffness is a useful concept for diagnosing and characterizing generalization. We observe that small learning rates reliably lead to higher stiffness at a given epoch as well as at a given training loss. In addition, we measure how stiffness between two data points depends on their mutual input-space distance, and establish the concept of a dynamical critical length that characterizes the distance over which datapoints react similarly to gradient updates. The dynamical critical length decreases with training and the higher the learning rate, the smaller the critical length.", "review": "Review:###This submission introduces a metric, termed stiffness, to evaluate the generalization capability of neural networks. The metric is novel and straightforward, it measures how stiff a network is by looking at how a small gradient step on one example affects the loss on another example. The authors study several configurations on three small datasets. They demonstrate that stiffness is a useful concept for diagnosing and characterizing generalization. I give an initial rating of weak accept because (1) The paper is well motivated and well written. Studying generalization is important for neural networks. (2) It seems from experiments that stiffness is a useful metric to indicate models* generalization capability. However, I have a few concerns. First, the authors study several configurations like train-train, train-val and val-val. However, these configurations are still in-domain analysis, the data distribution is quite similar. It can not support author*s claims well. Adding an experiment where domain gap is large will make the submission stronger, such as train-test, cross-dataset or challenging tasks like semantic segmentation. Second, the datasets being used are very small. I understand that for theoretically analysis, small datasets are quick to converge and easy to demonstrate. However, this submission focuses on generalization problem during transfer learning. Hence, it needs at least a bigger dataset, like ImageNet, to show it really works."}
{"id": "iclr2020_248", "title": "EDUCE: Explaining model Decision through Unsupervised Concepts Extraction | OpenReview", "abstract": "Abstract:###Providing explanations along with predictions is crucial in some text processing tasks. Therefore, we propose a new self-interpretable model that performs output prediction and simultaneously provides an explanation in terms of the presence of particular concepts in the input. To do so, our model*s prediction relies solely on a low-dimensional binary representation of the input, where each feature denotes the presence or absence of concepts. The presence of a concept is decided from an excerpt i.e. a small sequence of consecutive words in the text. Relevant concepts for the prediction task at hand are automatically defined by our model, avoiding the need for concept-level annotations. To ease interpretability, we enforce that for each concept, the corresponding excerpts share similar semantics and are differentiable from each others. We experimentally demonstrate the relevance of our approach on text classification and multi-sentiment analysis tasks.", "review": "Review:###This paper presents a classification model focused on interpretability. The model, Explaining model Decision through Unsupervised Concepts Extraction (EDUCE), is applied to a text classification task, while the authors argue in the appendix that this is also applicable to a wider problem, such as image classification. The model is composed of three parts: the first part is detecting salient spans of text relevant to the text classification problem, the second part assigning each salient span a concept label, and the third part which does the classification task based on the binary concept feature label. The models’ loss is composed of two parts: (A) minimizing the cross-entropy of text classification loss and (B) minimizing the cross-entropy of concept classification loss. For (A), as the first and second part of the model introduce discrete choices, they use a RL with Monte-Carlo approximation of gradients. The system is evaluated under two measure: 1) classification accuracy and 2) concept accuracy. They define the concept accuracy as follows: after training, they train a classifier that takes output (in the form of <salient span, their concept label>) of the model from the test portion of the data. They split this output into train and test, and report the test accuracy. This aims to show how consistent is the labeling of the salient spans for different methods: if the concept label set correctly merged together semantically similar spans, this “concept accuracy” would be higher. This is a new metric they are proposing. While it is interesting, I would like to see *some* studies on how this correlates with human’s judgements on how interpretable the model is. The paper is introducing a new measure *and* new model, and it’s hard to be persuaded the model is doing well based on this new measure, when there is little ground to know what this measure really measures. Overall, I’m not impressed with the models’ performances. The aspect rationale annotated beer sentiment dataset, presented by Lei et al (2016), has provided one of few opportunities to evaluate interpretability / rationale model quantitatively. The paper evaluates on this measure, which is included in the appendix, and the results are pretty disappointing compared to the existing models such as Lei et al’s initial baseline or Bastings et al. While the paper argues this method isn’t necessarily designed for this task unlike the other methods, I’m not sure this is necessarily the case. Bastings et al could be applied to other tasks that model is evaluated on, such as DBPedia and AGNews classification. The difference comes on how easy it is to interpret the methods, as these other rationale-based text processing methods would make use of captured words, while EDUCE would make use of detected “concept” clusters. Currently, the only real baselines are the ablations of its own model. Table 3 is quite interesting, different “concepts” capture different aspects fairly well. Not having a concept loss actually helps the classification accuracy. Would the concepts learned without concept loss qualitatively very different? This goes back to my original point that their new measure of *concept accuracy* is vague. Other comments and Q: - Figure (3), the visualization is a bit confusing cause it is unclear whether it is each span is a set of spans or a single span. Also, I would recommend making figures colorblind friendly, if possible. Q: what kind of classifier was used for the evaluation metric “concept accuracy” classifier? I don’t think it’s mentioned. Q: why are you sampling a test set for DBPedia experiments? Is it for efficiency reason? Q: how sensitive is model’s performance to the hyper parameters, especially the number of concepts? Q: the current baseline classifier is a simple BiLSTM one, which definitely perform a lot worse than recent pre-trained LMs such as BERT. Would it be easy to use this method on top of richer representation such as pertained LM outputs? Q: how would this connects to saliency map literature in computer vision? I guess these would be mostly “a posteriori” explanations? Discussion would be helpful."}
{"id": "iclr2020_249", "title": "Measuring Calibration in Deep Learning | OpenReview", "abstract": "Abstract:###Overconfidence and underconfidence in machine learning classifiers is measured by calibration: the degree to which the probabilities predicted for each class match the accuracy of the classifier on that prediction. We propose two new measures for calibration, the Static Calibration Error (SCE) and Adaptive Calibration Error (ACE). These measures take into account every prediction made by a model, in contrast to the popular Expected Calibration Error.", "review": "Review:###Summary: This paper proposes two novel scalar-valued evaluation measures, namely Static Calibration Error (SCE) and Adaptive Calibration Error (ACE) for evaluating the quality of calibration (the accuracy of estimating p(y|x) in classification) in deep learning. The authors suggested that the existing Expected Calibration Error (ECE), which is the current most popular scalar-valued metric for calibration, has several drawbacks, especially in the multi-class scenario. Intuitively, ECE only focuses on the predicted class probability (argmax) and ignores class probabilities of other K-1 classes, which implies that ECE may fail to capture the class probability estimates for all classes. They also illustrated the drawback of ECE under label noise. ======================================================== Contribution: 1. Pointing out that ECE has several drawbacks in multiclass scenario, e.g., does not take predictions of all classes but only the one with maximum confidence. 2. Proposing two novel measures: SCE and ACE, where SCE is a natural extension to make sure the metric consider all class probability estimates, and ACE is adaptive in the sense that it is focus on the regions where many predictions are made. 3. Conducting experiments to illustrate that although temperature scaling may work very well when we used ECE as a metric, vector scaling can be advantegous when we consider SCE or ACE. Clarity: Although there are typos, it is not difficult to understand the motivation and what this paper is trying to propose. But it is suggested that the paper was done in a rush manner. 1. Issue in Figure 1 and experiment 7.2: I found that Figure 1 is difficult to understand and I may misunderstand. Moreover, I couldn*t get the main message of it. 1.1 (Top-left), the message I got is that the error based on each metric is bigger as the label noise increases (but each metric is incomparable). 1.2 (Top-right), I couldn*t get what Predictions ECE omits over threshold 0.1 means. It would be better to clearly explain it, e.g., how to compute % ECE omits over a certain threshold. 1.3 (Bottom), I learned that with label noise, both accuracy and model confidence (which I think is max p(y|x) decreases as the noise increases, which is common. Moreover, does Against and vs. mean the same thing in this context, then using the same word can make it more consistent. Moreover, I wonder why we have to focus on the label noise because even in the normal scenario, ECE should have drawbacks already too and it is more interesting for me to see the illustration in the normal scenario. On the other hand, if I don*t misunderstand, ECE did not omit a lot of predictions according to the top-right figure if there is no label noise. In practice, we may use a more sophisticated method to handle label noise. Because under label noise, the class probability estimation is already incorrect theoretically, i.e., it may shift depending on the noise type (Menon+, ICML2015: Learning from Corrupted Binary Labels via Class-Probability Estimation). And I am not sure why we have *(Figure 1)* at the end of the first sentence of Sec. 3.1, is that sentence related to Figure 1? In my opinion, Figure 2 is much better to visualize that ACE may capture things that ECE fails to capture. Finally, Figure 1 is a part of experiment 7.2 and I think it is fine to move this to the experiment section as an additional experiment under the label noise scenario. Finally, how to train your model in Figure 1, is it uncalibrated version? i.e., without temperature scaling or other modifications. 2. Figure 3 is difficult to understand. 2.1 (Left) what is sharpness, how to calculate sharpness and what is the y-axis? And what is the x-axis here, is it a confidence score?. 2.2 (Right) What is the training step? And there is no value specified in the x-axis. I couldn*t understand how to plot this figure. 3. Tables 1 and 2 are never discussed and in *Table 1: ECE, TACE, SCE, and ACE*, there is no TACE and never mention in the main body of the paper. Instead, the main body discussed about Tables 3 and 4, which is not in the main body (or is it? since it is in between the reference). 4. How many trials did you run the experiment and what criteria you use to give boldface to a method? Since this paper also highly relies on the experimental results, it would be great to clarify. ======================================================== Comments: The authors did a great job to point out the problem of ECE. Although SCE is a very simple and natural extension of ECE, its contribution is significant because it relevates the drawback of ECE as the authors suggested. I believe this work can make an impact to the field. For ACE, I have an impression that it is difficult to use. Also, it would be nice to see the performance with respect to Maximum Calibration Error (MCE), which is completely ignored in this paper. Because in MCE, we can see that temperature scaling did not almost always perform significantly better than other methods as it performed with respect to ECE (in Appendix of Guo et al., 2017), which is similar to what we observed in SCE and ACE. Unfortunately, although I like the idea of this paper, I found that the clarity of the paper is insufficient in its current state. It seems that the paper was really done in a rush and thus the writing can be highly improved. As a result, given the current manuscript, I vote a weak reject for this paper. ======================================================== Additional questions: 1. Why the name of SCE is static calibration error? If you mean it is not adaptive as ACE, then ECE is also static in this sense. Therefore, it may be a good idea to come up with a different name,e.g., classwise calibration error. 2. May SCE and ACE suffer from class-imbalance scenario more than ECE? 3. Are there any advantages of ECE over SCE and ACE? ======================================================== Potential typos I found: 1. Abstract: the last sentence: taks -> takes 2. Abstract: Overconf. and underconf. is -> are 2. INTRO: the first sentence of the last paragraph may algorithms -> many algorithms 3. INTRO: 4th paragraph: has lead -> has led 4. INTRO: last sentence: Static Calibratinon -> Static Calibration 5. INTRO: last sentence Adaptive Calibration -> Adaptive Calibration Error 6. 2.1: {(x,y)} should be {(x,y)}_{i=1}^{N}? 7. all predictions made by the mode -> all predictions made by the model 8. in fig.3: calibraion -> calibration 9. Table 1: remove TACE"}
{"id": "iclr2020_250", "title": "Learning Entailment-Based Sentence Embeddings from Natural Language Inference | OpenReview", "abstract": "Abstract:###Large datasets on natural language inference are a potentially valuable resource for inducing semantic representations of natural language sentences. But in many such models the embeddings computed by the sentence encoder goes through an MLP-based interaction layer before predicting its label, and thus some of the information about textual entailment is encoded in the interpretation of sentence embeddings given by this parameterised MLP. In this work we propose a simple interaction layer based on predefined entailment and contradiction scores applied directly to the sentence embeddings. This parameter-free interaction model achieves results on natural language inference competitive with MLP-based models, demonstrating that the trained sentence embeddings directly represent the information needed for textual entailment, and the inductive bias of this model leads to better generalisation to other related datasets.", "review": " The paper proposes a few heuristic scorers to model entailment and contradiction, based on encoded sentence embeddings. These scores include an entailment score, a contradiction score, a neutral score, and two similarity scores. They are defined heuristically, e.g., entailment score = geometric avg of such thing: 1 - sigma(premise not satisfied) * sigma(hypothesis satisfied). This is similar to fuzzy logic (for example, https://en.wikipedia.org/wiki/Fuzzy_logic) and some citations are needed in this regard. Different from fuzzy logic, this paper learns whether an anonymous feature is true or false by NN encoders end-to-end. Thus, the model actually has enough power to extract those features suitable for fuzzy logic-like heuristic matching. The experiments are well designed. I especially appreciate the comparison to random matching heuristics, which already exhibits non-trivial performance. This is very reasonable because the neural network underlying random matching heuristics is still learnable. However, the proposed matching heuristics achieve 7% improvement compared with random ones, showing the effectiveness of the approach. The authors also have ablation test and experiments on out-of-domain datasets. I have two concerns: 1. One limitation of this paper is that the heuristic matching scorers are pretty ad hoc to the inference task. The two similarity scores are not too novel, for example, sim_diff is the L1-distance between two vectors. Entailment, contradiction, and neutral scores are interesting, but hardly generalize to other sentence matching tasks (e.g., various IR applications). 2. I have a feeling that the importance of NLI is over-estimated. While logical reasoning is important in AI, NLI datasets are somehow degenerated, and existing solutions are basically connecting neural edges. As mentioned in the paper, NLI models do not transfer well to out-of-domain NLI samples, not to mention non-NLI tasks. It would be interesting to see if the well-designed heuristic matching scores could ease the underlying model, so that it learns more generic sentence embeddings in general. Minor: In references: Williams, Nagnia, Bowman: duplicate entry"}
{"id": "iclr2020_251", "title": "Understanding Architectures Learnt by Cell-based Neural Architecture Search | OpenReview", "abstract": "Abstract:###Neural architecture search (NAS) searches architectures automatically for given tasks, e.g., image classification and language modeling. Improving the search efficiency and effectiveness have attracted increasing attention in recent years. However, few efforts have been devoted to understanding the generated architectures and particularly the commonality these architectures may share. In this paper, we first summarize the common connection pattern of NAS architectures. We empirically and theoretically show that the common connection pattern contributes to a smooth loss landscape and more accurate gradient information, and therefore fast convergence. As a consequence, NAS algorithms tend to select architectures with such common connection pattern during architecture search. However, we show that the selected architectures with the common connection pattern may not necessarily lead to a better generalization performance compared with other candidate architectures in the same search space, and therefore further improvement is possible by revising existing NAS algorithms.", "review": "Review:###Summary: This paper tries to understand the characteristics of the architectures found by common NAS methods in the cell-search space. Specifically it characterizes the cell-search space used by DARTS, SNAS, AmeobaNet and finds that a most of these search methods find cells which are wide and shallow in depth (they give a specific definition of width and depth for characterizing cells). In fact these cells are usually the widest and shallowest architectures in their search space. The author empirically find that because these kinds of topologies converge faster during training and inevitably every NAS algorithm during search don*t train upto convergence but only up to a bit and make decisions based on partially converged statistics there is a bias in selection towards these topologies. They also provide theoretical intuition to back-up these empirical findings. Secondly they analyze the generalization performance of such wide and shallow cell structures accidentally emphasized by search procedures. They take the common cell structures found by common NAS algorithms (NASNet AmoebaNet, ENAS, DARTS, SNAS) and make them the widest and shallowest possible in the search space (following the SNAS cell connection pattern) while keeping number of parameters as constant as possible. They find that on cifar10 the test error of the adapted architectures usually increase a bit while on cifar100 the adapted architectures decrease a bit. Comments: - Overall the paper is interesting and well-written. Definitely liked the fact that wide and shallow networks are being accidentally biased towards during search. Liked the empirical analysis and theoretical insights backing it up. - The generalization experiments suggest to me that on bigger datasets wider and shallower networks might be better for generalization actually. Can we take the cell architectures found by various algorithms and *scale-up* to ImageNet by doing the usual trick of replicating more of the cells together and training? At least going by Table 1 I find myself not agreeing with the statement *The results above have shown that architectures with the common connection pattern may not generalize better despite of a faster convergence.* On cifar100 wider and shallower is better. Perhaps on ImageNet they will be even better? So NAS algorithms* strategy of training partially may be exactly the right thing to do? Any thoughts? - Any idea about if this pattern extends to RNN space as well or only limited to CNNs? - Overall my main gripe is that while it is interesting findings but I am not sure I understood the main takeaway or significance of these results especially the generalization ones and how it informs search algorithm design."}
{"id": "iclr2020_252", "title": "Improving End-to-End Object Tracking Using Relational Reasoning | OpenReview", "abstract": "Abstract:###Relational reasoning, the ability to model interactions and relations between objects, is valuable for robust multi-object tracking and pivotal for trajectory prediction. In this paper, we propose MOHART, a class-agnostic, end-to-end multi-object tracking and trajectory prediction algorithm, which explicitly accounts for permutation invariance in its relational reasoning. We explore a number of permutation invariant architectures and show that multi-headed self-attention outperforms the provided baselines and better accounts for complex physical interactions in a challenging toy experiment. We show on three real-world tracking datasets that adding relational reasoning capabilities in this way increases the tracking and trajectory prediction performance, particularly in the presence of ego-motion, occlusions, crowded scenes, and faulty sensor inputs. To the best of our knowledge, MOHART is the first fully end-to-end multi-object tracking from vision approach applied to real-world data reported in the literature.", "review": "Review:###This paper deals with the problem of multiple object tracking and trajectory prediction in multiple frames of videos. The main focus is adding a relation-reasoning building block to the original HART framework. With multiple objects, the key is to be able to learn the permutation invariant representation during potential changing and dynamic object trajectories. The paper also uses toy examples to show that the proposed block of relation reasoning is not necessarily beneficial when the object trajectory is less random and more static. Finally, experiments on real data demonstrate that the proposed method that accounts for relation reasoning is helpful by a limited magnitude. The main contribution of this paper is the novel relation reasoning block. However, there are three key concerns of mine: 1. It is not clear when the new architecture would be helpful. I am really happy to see a more careful study of the success and failure cases of the method. I also appreciate the honesty that the current model is not competitive with the ones that have an accurate bounding box as input. But I think a more detailed study can be conducted, especially in the toy example case. Maybe it is possible to design different levels of randomness in the trajectory and further figure out when is the reasoning block helpful. 2. Even the real data experiments do not have very impressive results, from my biased observation. (I am not in the field, so maybe I am wrong here.) 3. The end-to-end approach is also another perspective where the authors try to differentiate their methods from others. I think it is potentially an interesting problem that why end-to-end is something you would prefer (which is definitely a harder problem), and when."}
{"id": "iclr2020_253", "title": "GraphZoom: A Multi-level Spectral Approach for Accurate and Scalable Graph Embedding | OpenReview", "abstract": "Abstract:###Graph embedding techniques have been increasingly deployed in a multitude of different applications that involve learning on non-Euclidean data. However, existing graph embedding models either fail to incorporate node attribute information during training or suffer from node attribute noise, which compromises the accuracy. Moreover, very few of them scale to large graphs due to their high computational complexity and memory usage. In this paper we propose GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. GraphZoom first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. This fused graph is then repeatedly coarsened into a much smaller graph by merging nodes with high spectral similarities. GraphZoom allows any existing embedding methods to be applied to the coarsened graph, before it progressively refine the embeddings obtained at the coarsest level to increasingly finer graphs. We have evaluated our approach on a number of popular graph datasets for both transductive and inductive tasks. Our experiments show that GraphZoom increases the classification accuracy and significantly reduces the run time compared to state-of-the-art unsupervised embedding methods.", "review": "Review:###Summary: The authors propose a way to fuse information on nodes of a graph with the topology of the graph in the large scale setting. The proposed approach is done in four phases where (i) the covariates in the nodes of the graph is first mapped in the graph space for fusion and fused using linear combination of the topological graph and feature graph, (ii) the resulting *adjacency* matrix will almost surely not be sparse even if the original graph space, so they use eigenvalues of the graph laplacian to coarsen the graph -- remove edges; (iii) they then propose to embed the coarsened graph using *any* unsupervised learning technique; (iv) then the embedded representation is refined using iterative procedures. Cheap procedures are introduced to do Phases (i) and (iv). Experimentally the authors see improvements in the performance using their approach compared to the baselines considered. Novelty: 1. The approach suggested in this paper is already there in MILE Fig 1., the authors mention that MILE requires training GCN but I am not sure why this is critical. The authors mention that *MILE cannot support inductive embedding models due to the transductive property of GCN*, can you clarify what this means? I guess one can easily replace GCNs? 2. Covariate adjusted clustering is known to work only when when the features are independent like Stochastic Block Model, see Covariate-assisted spectral clustering by Binkiewicz et al, 2014. Is there a reason why the features that we see on nodes are not correlated? Results: It is hard to see where the performance improvement actually comes from, if at all. It is interesting to see that the proposed approach saves time and is more accurate in the variety of settings considered, but it is not clear why we see the improvement. After rebuttal: I have raised my score to 6 after going through the authors* response for my questions, and other reviewers* concerns. While the approach performs well in many datasets (thanks to the authors for providing more experimental evidence!), I*m still not convinced with the authors* response on their fusion step -- it seems to me that node attributes are *side* information, that can *boost* the signal on the original neighborhood graph. Recall that spectral approaches do have a fundamental barrier -- they fail on *thin* graphs (see https://arxiv.org/abs/1608.04845 ). Hence, node covariance/fusion matrix being dense will be a blessing for spectral approaches since they make spectral methods work. However, is this what we want in *all* the cases? I*m not sure. This means that the choice of _x0008_eta in their fusion step is *very* important, and I don*t see any plots on the sensitivity of their procedure with respect to _x0008_eta. I kindly request the authors to include a plot or results showing the sensitivity of the final results with respect to the choice of _x0008_eta. Thanks!"}
{"id": "iclr2020_254", "title": "GRAPHS, ENTITIES, AND STEP MIXTURE | OpenReview", "abstract": "Abstract:###Graph neural networks have shown promising results on representing and analyzing diverse graph-structured data such as social, citation, and protein interaction networks. Existing approaches commonly suffer from the oversmoothing issue, regardless of whether policies are edge-based or node-based for neighborhood aggregation. Most methods also focus on transductive scenarios for fixed graphs, leading to poor generalization performance for unseen graphs. To address these issues, we propose a new graph neural network model that considers both edge-based neighborhood relationships and node-based entity features, i.e. Graph Entities with Step Mixture via random walk (GESM). GESM employs a mixture of various steps through random walk to alleviate the oversmoothing problem and attention to use node information explicitly. These two mechanisms allow for a weighted neighborhood aggregation which considers the properties of entities and relations. With intensive experiments, we show that the proposed GESM achieves state-of-the-art or comparable performances on four benchmark graph datasets comprising transductive and inductive learning tasks. Furthermore, we empirically demonstrate the significance of considering global information. The source code will be publicly available in the near future.", "review": " The paper presents a graph neural network model that aims at improving the feature aggregation scheme to better handle distant nodes, therefore mitigating the “smoothing” problem of classic averaging. I find the paper clearly motivated and easy to follow, although some sentences could be streamlined and some repetitions could be removed. For instance last paragraph in page 3, this should be clear already and should be restated. One thing that is not clear is how does the model cope with the increase in feature dimensionality due to the concatenation over different steps. Isn’t this leading to overfitting? Did the authors experiment with other schemes such as averaging or gating? If so it would be nice to see the results for each of the configurations as it is not clear to me what should be chosen a-priori. Experiments are nicely executed and the proposed approach is compared against a rich array of other models. Results are state-of-the-art and also the analysis of the model is interesting, i.e. it doesn’t diverge when increasing # steps at test time. How does the attention vector look like? Does it tend to peak at a given k, or is it more uniformly distributed? How does the model compare to having k GAT layers, each constrained to use neighboring nodes at step k as input for the attention computation? Did the authors experiment on this? Overall I like the work but find the novelty quite limited, more effort could have been put into motivating the soundness of the use of multiple random walks. Perhaps some theory could be developed to make the paper stronger."}
{"id": "iclr2020_255", "title": "Learning to solve the credit assignment problem | OpenReview", "abstract": "Abstract:###Backpropagation is driving today*s artificial neural networks (ANNs). However, despite extensive research, it remains unclear if the brain implements this algorithm. Among neuroscientists, reinforcement learning (RL) algorithms are often seen as a realistic alternative: neurons can randomly introduce change, and use unspecific feedback signals to observe their effect on the cost and thus approximate their gradient. However, the convergence rate of such learning scales poorly with the number of involved neurons. Here we propose a hybrid learning approach. Each neuron uses an RL-type strategy to learn how to approximate the gradients that backpropagation would provide. We provide proof that our approach converges to the true gradient for certain classes of networks. In both feedforward and convolutional networks, we empirically show that our approach learns to approximate the gradient, and can match the performance of gradient-based learning. Learning feedback weights provides a biologically plausible mechanism of achieving good performance, without the need for precise, pre-specified learning rules.", "review": "Review:###It*s unclear how multi-layer biological neural networks could implement gradient-based learning, as they don*t have the symmetric connections needed for backpropagation. This paper proposes a perturbation-based synthetic gradient estimator that does not rely on symmetric backward connections. Hidden unit perturbation is used to estimate the loss gradient, and backward connections are trained via gradient descent to predict the approximate gradients from the perturbation-based estimator. The topic is important and the paper is well-written. I don*t follow this area closely, but from what I can tell it*s a novel idea. The results are strong. The method beats various alternatives and closely matches backpropagation in terms of performance on the MNIST tasks (less so on CIFAR). It*s especially curious that the method performs better than backpropagation on the autoencoder task. I have an unresolved question: What do the learnable backward connections add beyond the perturbation estimator for the gradients? If the perturbation-based estimator could be used to train the forward model, does the trained backward model have advantages in terms of efficiency or performance? I would appreciate more discussion of this key choice, or a direct comparison with only the perturbation-based estimator (only) to understand the differences."}
{"id": "iclr2020_256", "title": "Fix-Net: pure fixed-point representation of deep neural networks | OpenReview", "abstract": "Abstract:###Deep neural networks (DNNs) dominate current research in machine learning. Due to massive GPU parallelization DNN training is no longer a bottleneck, and large models with many parameters and high computational effort lead common benchmark tables. In contrast, embedded devices have a very limited capability. As a result, both model size and inference time must be significantly reduced if DNNs are to achieve suitable performance on embedded devices. We propose a soft quantization approach to train DNNs that can be evaluated using pure fixed-point arithmetic. By exploiting the bit-shift mechanism, we derive fixed-point quantization constraints for all important components, including batch normalization and ReLU. Compared to floating-point arithmetic, fixed-point calculations significantly reduce computational effort whereas low-bit representations immediately decrease memory costs. We evaluate our approach with different architectures on common benchmark data sets and compare with recent quantization approaches. We achieve new state of the art performance using 4-bit fixed-point models with an error rate of 4.98% on CIFAR-10.", "review": "Review:###The paper proposes a regularization strategy for training a neural network classifier using full weight precision that can then be quantized to a low-bit fixed-point representation. The main claimed contributions are - A solution to the problem of the soft quantization of batch normalization layers, which revolves around regularizing the (folded) shifting coefficient values towards their (post-training) fixed-point quantization. - An exponential schedule for the regularization hyperparameters. - An overall model that outperforms other quantization approaches. My assessment is that the paper is slightly below the acceptance bar. As an outsider to the low-precision DNN literature, the proposed soft quantization strategy for batch normalization layers looks interesting, but the submission suffers from clarity issues (see clarity-related comments), and the evaluation is insufficient to support some claimed contributions (see evaluation-related comments). Clarity-related comments: - The spacing between paragraphs has been removed to satisfy the soft 8-page limit. This has a negative impact on the submission’s readability. - “Since BN layers operate on different statistics after training [...]” The use of the term “statistics” in this statement is imprecise. Unless there’s an input distribution shift between training and evaluation, shouldn’t the “statistics” BN layers operate on be the same? Maybe the authors are trying to convey that the way in which these statistics are approximated (using a mini-batch during training vs. using a running average during evaluation) is different? - The submission claims to accommodate pure fixed-point models, but the soft quantization of biases is swept aside in the presentation: “Since additions play a subordinate role for complexity, we focus on weight multiplications.” Can the authors clarify whether and how soft quantization is applied to biases? - Soft quantization of multiplicative factors is handled in two distinct ways in the model: weights are quantized using a symmetric uniform function, and batch normalization scaling factors are quantized using a logarithmic function. Why are they handled differently? - “Furthermore, we determine the layer-wise step-size on pre-trained weights [...]. In the next chapter, however, we see that the actual step-sizes are learned within the batch normalization component.” I have trouble reconciling these two statements. They give the impression that the step size is both determined post-training *and* learned during training. In fact, Equation 12 suggests that the step size is learned. Can the authors clarify? - “This also prevents the regularization parameter from being too high.” I don’t understand what “regularization parameter” refers to. Is it the lambda values (Equation 13), or is it the R values (Equation 13)? Evaluation-related comments: - I don’t understand the reasoning behind the bolding of entries in Table 2. In the MNIST section, why is the Add-Net entry bolded but not the TWN and SGM entries, even though they share the same error rate? Is the 0.04% error rate difference between Fix-Net and SGM significant, considering that this corresponds to four additional misclassified examples on the test set? - More generally, the presentation in Table 2 doesn’t group together comparable approaches, which leads to incongruities like Add-Net and Fix-Net entries being both bolded in each section despite Fix-Net systematically outperforming Add-Net. - As far as I can tell there is no empirical evidence to support the asserted benefits of the exponential regularization schedule. - I’m also doubtful of the significance of the claim that the submission achieves state-of-the-art quantization results, given that in most settings bit-sizes do not directly compare to those of related methods. Additional comments: - I don’t believe L2-regularizing weight values towards the closest quantized weight value has a direct probabilistic interpretation, because the way regularization is applied depends on the weight values themselves. If I’m mistaken, what unconditional PDF over weight values would that correspond to? - “Due to their lower number of redundancies, DenseNet and ResNet20 are considered as difficult to quantize.” Is this common knowledge? Can the authors point to related work supporting this assertion?"}
{"id": "iclr2020_257", "title": "Optimizing Data Usage via Differentiable Rewards | OpenReview", "abstract": "Abstract:###To acquire a new skill, humans learn better and faster if a tutor, based on their current knowledge level, informs them of how much attention they should pay to particular content or practice problems. Similarly, a machine learning model could potentially be trained better with a scorer that “adapts” to its current learning state and estimates the importance of each training data instance. Training such an adaptive scorer efficiently is a challenging problem; in order to precisely quantify the effect of a data instance at a given time during the training, it is typically necessary to first complete the entire training process. To efficiently optimize data usage, we propose a reinforcement learning approach called Differentiable Data Selection (DDS). In DDS, we formulate a scorer network as a learnable function of the training data, which can be efficiently updated along with the main model being trained. Specifically, DDS updates the scorer with an intuitive reward signal: it should up-weigh the data that has a similar gradient with a dev set upon which we would finally like to perform well. Without significant computing overhead, DDS delivers strong and consistent improvements over several strong baselines on two very different tasks of machine translation and image classification.", "review": "Review:###This paper presents a reinforcement learning approach towards using data that present best correlation with a validation set’s gradient signal. The broader point of this paper is that there is inevitably some distribution shift going from train to test set - and the validation set can be a small curated set whose distribution is closer to the testing distribution than what the training dataset*s distribution is. The problem setup bears relationship to several areas including domain adaptation/covariate shift problems, curriculum learning based approaches amongst others. One assumption that I see which needs to be understood more is equation (6) - wherein, somehow, there is a Markov assumption used to zero out the contribution of the scoring network on parameters unto previous time step. Trying to understand the implications of this assumption (how the performance varies with/without this assumption) would be instructive for understanding potential shortcomings of this framework. I think the paper is well written, handles an important question. That said, I am not too aware of recent work in this area to make a decisive judgement on this paper’s novelty/contributions."}
{"id": "iclr2020_258", "title": "Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems | OpenReview", "abstract": "Abstract:###Recently, several studies have proven the global convergence and generalization abilities of the gradient descent method for two-layer ReLU networks. Most studies especially focused on the regression problems with the squared loss function, except for a few, and the importance of the positivity of the neural tangent kernel has been pointed out. However, the performance of gradient descent on classification problems using the logistic loss function has not been well studied, and further investigation of this problem structure is possible. In this work, we demonstrate that the separability assumption using a neural tangent model is more reasonable than the positivity condition of the neural tangent kernel and provide a refined convergence analysis of the gradient descent for two-layer networks with smooth activations. A remarkable point of our result is that our convergence and generalization bounds have much better dependence on the network width in comparison to related studies. Consequently, our theory significantly enlarges a class of over-parameterized networks with provable generalization ability, with respect to the network width, while most studies require much higher over-parameterization.", "review": "Review:###This paper studied the generalization performance of gradient descent for training over-parameterized two-layer neural networks on classification problems. The authors proved that under a neural tangent based separability assumption, as long as the neural network width is , the number of training examples is , within iterations GD can achieve expected -classification error. Overall this paper is well written and easy to follow. The theoretical results on the neural network width and iteration complexity are interesting. My major concern is that the comparison with Allen-Zhu et al and Cao & Gu seem somewhat unfair. First, Allen-Zhu et al and Cao & Gu both studied the generalization performance of GD for training multi-layer neural networks, which is fundamentally more difficult than two-layer networks. Second, they use ReLU activation functions, which brings in the nonsmoothness along the optimization trajectory. This would also make the condition on the neural network width become worse. Therefore, when claiming the advantage of the derived guarantees, the authors should clearly clarify such differences. Another concern is that whether the derived theoretical results can be generalized to ReLU network? When proving the generalization result, this paper takes advantage of margin-based generalization error bound. However, the generalization results in Cao & Gu are proved via applying standard empirical Rademacher complexity based generalization error bound. I would wonder which technique can give a tighter bound? Can you provide some examples regarding which type of data can satisfy Assumption (A.4) with constant margin ? The authors would like to briefly discuss another data separation assumption adopted in the following papers (although this assumption is typically made for regression problem). [1] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. arXiv preprint arXiv:1811.03962, 2018b. [2] Allen-Zhu, Z., Li, Y. and Song, Z. (2018c). On the convergence rate of training recurrent neural networks. arXiv preprint arXiv:1810.12065 . [3] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018. [4] Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global convergence guarantees for training shallow neural networks. arXiv preprint arXiv:1902.04674, 2019. [5] Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural networks. arXiv preprint arXiv:1906.04688, 2019."}
{"id": "iclr2020_259", "title": "Moniqua: Modulo Quantized Communication in Decentralized SGD | OpenReview", "abstract": "Abstract:###Decentralized stochastic gradient descent (SGD), where parallel workers are connected to form a graph and communicate adjacently, has shown promising results both theoretically and empirically. In this paper we propose Moniqua, a technique that allows decentralized SGD to use quantized communication. We prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Moniqua improves upon prior works in that it (1) requires no additional memory, (2) applies to non-convex objectives, and (3) supports biased/linear quantizers. We demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms. We also show that Moniqua is robust to very low bit-budgets, allowing less than 4-bits-per-parameter communication without affecting convergence when training VGG16 on CIFAR10.", "review": "Review:###This paper studies an important problem in the decentralized optimization, i.e., communication compression. Unlike gradient compression, the model compression in decentralized optimization is more challenging because the model parameters will not vanish like gradient. To solve this problem, the authors proposed Moniqua, where only lower-order bits of model are communicated since their higher-order bits are getting close. A hyperparameter is used as a “criterion” to separate the higher-order bits and lower-order bits of the model parameter via a modular arithmetic. The authors also apply Moniqua on D^2 and AD-PSGD. There are several major concerns: 1. It is not clear how the performance is measured in this paper. If the authors simply take the average of training loss over all nodes, this is not a good evaluation unless consensus is achieved. Unless clarifying this, it is hard to judge the performance. 2. It is better to provide evaluation on the consensus error which meansures the model consistency. 3. The modular hyperparameter is not easy to choose and seems cannot help achieve consensus. On the one hand, the theory suggests its value to be proportional to the gradient magnitude bound, which could be very large in practice. But if is large, we cannot achieve even approximate consensus because of the quantization error. On the other hand, a small cannot ensure sufficient model average since higher-order bits are ignored by the modular arithmetic but actually they should be communicated. Either way, it doesn’t seem to be good for achieving consensus. Overall, the idea of Moniqua is interesting and the authors provide useful extensions based on it. But the evaluation is not convincing and whether the proposed Moniqua can achieve consensus is unclear."}
{"id": "iclr2020_260", "title": "GraphQA: Protein Model Quality Assessment using Graph Convolutional Network | OpenReview", "abstract": "Abstract:###Proteins are ubiquitous molecules whose function in biological processes is determined by their 3D structure. Experimental identification of a protein*s structure can be time-consuming, prohibitively expensive, and not always possible. Alternatively, protein folding can be modeled using computational methods, which however are not guaranteed to always produce optimal results. GraphQA is a graph-based method to estimate the quality of protein models, that possesses favorable properties such as representation learning, explicit modeling of both sequential and 3D structure, geometric invariance and computational efficiency. In this work, we demonstrate significant improvements of the state-of-the-art for both hand-engineered and representation-learning approaches, as well as carefully evaluating the individual contributions of GraphQA.", "review": " This manuscript describes a new deep learning method for the prediction of the quality of a protein 3D model in the absence of the experimental 3D structure of the protein under study. The major idea is to model a protein 3D model using a graph. That is, each residue in a protein is modeled as a node and one edge is added to connect two residues if they are spatially close to each other. Based upon this graph representation, the manuscript describes a graph convolutional neural network (GCN) to predict both local (i.e., per residue) and global quality. The authors showed that this GCN method works well on the CASP11 and CASP12 data. Unfortunately, there is no experimental result on CASP13 models, which significantly reduce my interest on this paper. Minor concerns: References are missing or misplaced at some places. For example, in the 1st sentence of the 4th graph, *While computational protein folding has recently received attention...*, only protein design papers are cited. Some representative protein structure prediction papers shall be cited here."}
{"id": "iclr2020_261", "title": "Dynamic Time Lag Regression: Predicting What & When | OpenReview", "abstract": "Abstract:###This paper tackles a new regression problem, called Dynamic Time-Lag Regression (DTLR), where a cause signal drives an effect signal with an unknown time delay. The motivating application, pertaining to space weather modelling, aims to predict the near-Earth solar wind speed based on estimates of the Sun*s coronal magnetic field. DTLR differs from mainstream regression and from sequence-to-sequence learning in two respects: firstly, no ground truth (e.g., pairs of associated sub-sequences) is available; secondly, the cause signal contains much information irrelevant to the effect signal (the solar magnetic field governs the solar wind propagation in the heliosphere, of which the Earth*s magnetosphere is but a minuscule region). A Bayesian approach is presented to tackle the specifics of the DTLR problem, with theoretical justifications based on linear stability analysis. A proof of concept on synthetic problems is presented. Finally, the empirical results on the solar wind modelling task improve on the state of the art in solar wind forecasting.", "review": "Review:###The topic is out of my scope. I talked to the PC about it, and the PC asked me to continue reviewing this paper. So I tried to understand this paper, and I found it is difficult for me. I am not sure if the writing is not good or for some other reasons. I read Section 2.1, and I thought the following description would discuss how to solve phi(t). Unfortunately, I did not find where phi(t) is mentioned in Section 2.2. Maybe, I misunderstood Section 2.2. I do not know how to rate this paper. To avoid killing a good paper, I rated this paper as Weak Accept, and leave the decision to other reviewers and the AC."}
{"id": "iclr2020_262", "title": "Context-Aware Object Detection With Convolutional Neural Networks | OpenReview", "abstract": "Abstract:###Although the state-of-the-art object detection methods are successful in detecting and classifying objects by leveraging deep convolutional neural networks (CNNs), these methods overlook the semantic context which implies the probabilities that different classes of objects occur jointly. In this work, we propose a context-aware CNN (or conCNN for short) that for the first time effectively enforces the semantics context constraints in the CNN-based object detector by leveraging the popular conditional random field (CRF) model in CNN. In particular, conCNN features a context-aware module that naturally models the mean-field inference method for CRF using a stack of common CNN operations. It can be seamlessly plugged into any existing region-based object detection paradigm. Our experiments using COCO datasets showcase that conCNN improves the average precision (AP) of object detection by 2 percentage points, while only introducing negligible extra training overheads.", "review": "Review:###This paper proposes a CRF-based context module for CNN-based object detectors. In particular for the two-stage region-based detector, like Faster RCNN, the context module is added right before the output layer of the classification head. Every box proposed by the RPN is a node in the CRF, and its label is the classification label. Message passing is unrolled as neural network layers. Potentials are defined based on object detector outputs, box overlap, and co-occurrence of class labels. Experiments are performed on the MS COCO object detection task. The proposed method is similar to the CRF-based method for FCN segmentation networks except that the nodes are boxes instead of pixels. And simpler ideas had been explored in the pre-DL era. In this sense, the main idea of this work is incrementally novel. The presentation of the paper is fine in general. But the “global message passing” paragraph in Sec 4.2 can be improved. In particular, how p(l) is used in message passing? It is better to provide more specific descriptions so that the paper is self-contained. The proposed model outperformed the baseline with a reasonably significant margin. However, its average accuracy on all 80 COCO categories is no better than “the Faster RCNN + Relation”. Since the proposed model is also not significantly simpler than the Relation Network, the experimental results do not establish a new state-of-the-art. The experimental results are also a bit thin. More ablative studies can be helpful, e.g., global message parsing/local message passing. Given the moderate novelty (which is good but not good enough as a standalone reason to accept this paper) and the not-strong-enough experimental results, I feel more work is needed for the paper to be readily publishable."}
{"id": "iclr2020_263", "title": "Dynamic Scale Inference by Entropy Minimization | OpenReview", "abstract": "Abstract:###Given the variety of the visual world there is not one true scale for recognition: objects may appear at drastically different sizes across the visual field. Rather than enumerate variations across filter channels or pyramid levels, dynamic models locally predict scale and adapt receptive fields accordingly. The degree of variation and diversity of inputs makes this a difficult task. Existing methods either learn a feedforward predictor, which is not itself totally immune to the scale variation it is meant to counter, or select scales by a fixed algorithm, which cannot learn from the given task and data. We extend dynamic scale inference from feedforward prediction to iterative optimization for further adaptivity. We propose a novel entropy minimization objective for inference and optimize over task and structure parameters to tune the model to each input. Optimization during inference improves semantic segmentation accuracy and generalizes better to extreme scale variations that cause feedforward dynamic inference to falter.", "review": "Review:###The authors propose a method to dynamically adapt some structural features of a semantic image segmentation model at inference time based on the entropy of the predictions. Using a model that explicitly controls the size of the filters at each layer, they show that running a small number of SGD steps on the scale and final prediction parameters in the last layer to minimize the entropy of least confident predictions for a specific example leads to better performance overall, and especially better generalization when there is a size discrepancy between training and test set. Strengths: The method is inspired, and leads to significant improvements. The dynamic inference setup is clearly explained, and well motivated for the case of the scale parameters. Extensive ablation experiments and the inclusion of an oracle system help understand the contributions of each component of the setup, and the potential of inference-time optimization of the considered parameters. Weaknesses: Some information is missing from the description of the experimental setting. A quick review of the DLA model would be welcome, to get a better sense of the roles of \theta_{scale} and \theta_{score}. The authors should include published numbers for a relevant baseline and the current or recent state-of-the-art for the considered dataset (Table 1should also report 1x numbers in both settings). Finally, while the authors make a strong case for dynamic adaptation of the scale parameters, the prior reason for adapting \theta_{score} is less obvious and would require further explanation. Questions and miscellaneous remarks: “As reported in Table 1, our method consistently improves on the baseline by ?2 points for all scales” > this statement is a little misleading, since the improvement is ~2 points on average, not ~2 points for each scale. Wouldn’t simply multiplying \theta_{score} by a large number decrease the entropy of the predictions? Do you do anything to prevent that from happening? Similarly, couldn’t the adversary simply rotate \theta_{score} to reduce IoU? Is the adversary optimized for long enough?"}
{"id": "iclr2020_264", "title": "Reanalysis of Variance Reduced Temporal Difference Learning | OpenReview", "abstract": "Abstract:###Temporal difference (TD) learning is a popular algorithm for policy evaluation in reinforcement learning, but the vanilla TD can substantially suffer from the inherent optimization variance. A variance reduced TD (VRTD) algorithm was proposed by Korda and La (2015), which applies the variance reduction technique directly to the online TD learning with Markovian samples. In this work, we first point out the technical errors in the analysis of VRTD in Korda and La (2015), and then provide a mathematically solid analysis of the non-asymptotic convergence of VRTD and its variance reduction performance. We show that VRTD is guaranteed to converge to a neighborhood of the fixed-point solution of TD at a linear convergence rate. Furthermore, the variance error (for both i.i.d. and Markovian sampling) and the bias error (for Markovian sampling) of VRTD are significantly reduced by the batch size of variance reduction in comparison to those of vanilla TD.", "review": "Review:###Summary: In this paper, the authors study the variance reduced TD (VRTD) algorithm, by Korda and Prashanth (2015) (KP15), for policy evaluation in RL. They first highlight technical errors in the analysis of KP15, and then provide new convergence analysis for this algorithm. The new analysis is based on a new technique to bound the bias of the VRTD gradient estimator, and shows the advantage of VRTD over vanilla TD (the analyses by Bhandari et al. 2018 and Srikant and Ying 2019), both in terms of variance and bias, that are reduced by increasing the batch size. The authors show that while the variance and bias of vanilla TD are both of O(alpha) (where alpha is the step-size), they are of O(alpha/M) and O(1/sqrt{M}) (where M is the batch size) in VRTD. This shows that a good convergence is possible for VRTD without reducing the step-size alpha, that causes slow convergence, by increasing the batch size M. In the middle of their analysis, the authors propose a slight modification of VRTD for the case that the samples are obtained iid from the stationary distribution of the evaluating policy, and provide its analysis. Finally, the authors provide simple experiments to support their theoretical findings. Comments: - More discussion on the parameter C_0, a constant between 0 and infty that depends on the MDP, is necessary in the paper. The authors provide no discussion about this constant and only in the appendix, refer the readers to Dedecker and Gouezel (2015). This constant is important because if it is large, then the batch size M should be very big in order for the bias error to go to zero. It would be good to see that which MDP properties affect the value of C_0. - As we increase M, the final performance gets better, but the (sample) cost of each gradient update increases. It would be good to have a comparison between different M values, in terms of performance vs. number of samples. What I mean is to have figures similar to 1(a) and 1(b) in which the performance is on the y-axis and the number of samples on the x-axis. - I was wondering if we can get the same improvement in terms of performance by having a decreasing schedule for alpha. What I mean is instead of increasing M, we keep M constant and then define a decreasing schedule for alpha and prove similar bounds that go to zero as alpha goes to zero. Minor Comments: - The projection Pi_{R_\theta} in Algorithm 2 has not been explained. This is important as R_\theta appears in some of the theoretical results later in the paper. - lambda_A has been used on Page 5 without definition. The authors define this quantity later on Page 6. - What is Psi in the discussions at the bottom of Page 5 (Eqs. 2, 3, and 4). Is it Phi? Overall, I think this discussion would be more meaningful if the authors first introduce the terms used in Eqs. 2 and 3."}
{"id": "iclr2020_265", "title": "Strategies for Pre-training Graph Neural Networks | OpenReview", "abstract": "Abstract:###Many domains in machine learning have datasets with a large number of related but different tasks. Those domains are challenging because task-specific labels are often scarce and test examples can be distributionally different from examples seen during training. An effective solution to these challenges is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective for improving many language and vision domains, pre-training on graph datasets remains an open question. Here, we develop a strategy for pre-training Graph Neural Networks (GNNs). Crucial to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs. We systematically study different pre-training strategies on multiple datasets and find that when ad-hoc strategies are applied, pre-trained GNNs often exhibit negative transfer and perform worse than non-pre-trained GNNs on many downstream tasks. In contrast, our proposed strategy is effective and avoids negative transfer across downstream tasks, leading up to 11.7% absolute improvements in ROC-AUC over non-pre-trained models and achieving state of the art performance.", "review": "Review:###This paper proposes new pre-training strategies for GNN with both a node-level and a graph-level pretraining. For the node-level pretraining, the goal is to map nodes with similar surrounding structures to nearby context (similarly to word2vec). The main problem is that directly predicting the context is intractable because of combinatorial explosion. The main idea is then to use an additional GNN to encode the context and to learn simultaneously the main GNN and the context GNN via negative sampling. Another method used is attribute masking where some masked node and edge attributes need to be predicted by the GNN. For graph-level pretraining, some general graph properties need to be predicted by the graph. Experiments are conducted on datasets in the chemistry domain and the biology domain showing the benefit of the pre-training. The paper addresses an important and timely problem. It is a pity that the code is not provided. In particular, the node-level pretraining described in section 3.1.1. seems rather complicated to implement as a context graph needs to be computed for each node in the graph. In particular I do not think the satement *all the pre-training methods are at most linear with respect to the number of edges* made in appendix F is correct."}
{"id": "iclr2020_266", "title": "Unrestricted Adversarial Attacks For Semantic Segmentation | OpenReview", "abstract": "Abstract:###Despite the rapid development of adversarial attacks for machine learning models, many types of new adversarial examples still remain unknown. Uncovered types of adversarial attacks pose serious concern for the safety of the models, which raise the question about the effectiveness of current adversarial robustness evaluation. Semantic segmentation is one of the most impactful applications of machine learning; however, their robustness under adversarial attack is not well studied. In this paper, we focus on generating unrestricted adversarial examples for semantic segmentation models. We demonstrate a simple yet effective method to generate unrestricted adversarial examples using conditional generative adversarial networks (CGAN) without any hand-crafted metric. The naive implementation of CGAN, however, yields inferior image quality and low attack success rate. Instead, we leverage the SPADE (Spatially-adaptive denormalization) structure with an additional loss item, which is able to generate effective adversarial attacks in a single step. We validate our approach on the well studied Cityscapes andADE20K datasets, and demonstrate that our synthetic adversarial examples are not only realistic, but also improves the attack success rate by up to 41.0% compared with the state of the art adversarial attack methods including PGD attack.", "review": "Review:###This paper mainly studies the robustness of semantic segmentation network by designing novel unrestricted adversarial examples. The authors propose to obtain adversarial examples through generative adversarial networks with SPADE structure. Through experiments, the authors try to prove that the adversarial examples are indistinguishable to natural images for humans, while can cause great attack for the current state-of-the-art models. Furthermore, the authors argue that these adversarial examples can help improve the robustness of existing models through adversarial training. Clarity: I think this paper has complete experiments to support their opinions. However, there should be some additional experiments to make the claims more credible. Thus, I think this paper could be weakly accepted with some supplementary experiments detailed in the following. Novelty: 1. In practice, users will often notice that there are adversarial perturbations in images if we adopt unrestricted adversarial attacks. How can such unrestricted adversarial attacks be employed in reality? Or just to improve the robustness of networks? 2. The SPADE structure is built upon existing ideas and concepts. Are there some more effective generation structures for this task besides SPADE? Experiments Results: 1) The authors execute experiments on two popular semantic segmentation datasets, to illustrate the attack ability of their methods by comparing with traditional norm-bounded attacks. Limits: a. In Table 3 and Table 4, the authors have made a comparison under the white-box setting while not consider the setting of the black-box setting which is a more practical situation. 2) The authors use FID and human evaluation to indicate that adversarial examples are indistinguishable to natural images for humans, and the semantic meanings of adversarial examples are consistent with corresponding ground truth. Limits: a. For the human evaluation, the authors should indicate the total number of persons in evaluation, rather than only quantitative result in Section 5.3 and Table 5. b. Although authors have proved that AdvSPADE has better generation performance, this is already pointed out by the paper “Semantic Image Synthesis with Spatially-Adaptive Normalization”. In fact, the authors should also compare with these generation methods under the attack setting: whether the adversarial examples are generated from other methods, such as Pix2PixHD, have higher attack ability if the SPADE generator is replaced by other structures. 3) The experiments in Section 5.4 are designed to show that AdvSPADE can help improve the robustness of the network. Limits: a. The authors have not indicated the training epoch for AdvSPADE in adversarial training. b. In adversarial training, the adversarial examples for training are generated according to the state of the network. Thus, the training adversarial examples are different even for the same training setting. The authors should indicate whether the adversarial training results are stable or not."}
{"id": "iclr2020_267", "title": "Using Explainabilty to Detect Adversarial Attacks | OpenReview", "abstract": "Abstract:###Deep learning models are often sensitive to adversarial attacks, where carefully-designed input samples can cause the system to produce incorrect decisions. Here we focus on the problem of detecting attacks, rather than robust classification, since detecting that an attack occurs may be even more important than avoiding misclassification. We build on advances in explainability, where activity-map-like explanations are used to justify and validate decisions, by highlighting features that are involved with a classification decision. The key observation is that it is hard to create explanations for incorrect decisions. We propose EXAID, a novel attack-detection approach, which uses model explainability to identify images whose explanations are inconsistent with the predicted class. Specifically, we use SHAP, which uses Shapley values in the space of the input image, to identify which input features contribute to a class decision. Interestingly, this approach does not require to modify the attacked model, and it can be applied without modelling a specific attack. It can therefore be applied successfully to detect unfamiliar attacks, that were unknown at the time the detection model was designed. We evaluate EXAID on two benchmark datasets CIFAR-10 and SVHN, and against three leading attack techniques, FGSM, PGD and C&W. We find that EXAID improves over the SoTA detection methods by a large margin across a wide range of noise levels, improving detection from 70% to over 90% for small perturbations.", "review": "Review:###Summary: The authors propose an explanation-based adversarial example detection algorithm. The main idea is to train a discriminator to detect whether the explanatory saliency map is consistent with the input. Experiments have been conducted on CIFAR10 and SVHN to validate the method. Comments: + The idea is straightforward and easy to follow. - The use of SHAP as the only explanation method is not well explained. There are a plenty of works on visual explanation methods, such as guided-backprop[1], excitation-backprop[2], integrated gradient[3], Grad-CAM[4], real-time saliency[5] and so on. And based on my expertise, SHAP cannot generate the most accurate saliency among these methods. If the proposed framework is general, why not to conduct ablation study on the different choice of explainer? - Doubts on the effectiveness of the proposed method. According to former works[6, 7], explanatory saliency methods are vulnerable and unreliable with respect to input perturbations. But in this paper, the authors assume that the explanation saliency map for normal examples are perfectly correct and used as positive instances for training the discriminator. I think they only focus on target attack, in which the attacking target label is semantically distinct from the original label, and the resulting saliency map distribution is very different from the correct one. However, considering a tabby cat image is perturbed to become tiger cat, since two classes are very close, the resulting saliency maps should be similar and the detector may fail to detect the adversarial example. Therefore, I encourage the authors to provide more results on this challenging scenario (for example, conduct un-target attack on imagenet dataset). - The reported results in Figure 2(e) is abnormal. First, the blue line (authors* method) is very close to AUC=1.0 across different noise levels, which means that the detector can perfectly classify all the adversarial examples in all the situation. Second, the reported values of other methods are not correct. For example, the black line (original Mahalanobis) is below AUC=0.5 across all the noise level. However, in the Table3 ResNet-CIFAR10 row of its original paper[8], the reported AUC under C&W attack is 95.84, which is much larger than those shown in the figure. Therefore, I think the comparison is invalid. Similar problems also appear in Figure 2(f). [1] J. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller. (2015). Striving for simplicity: The all convolutional net. In ICLR (workshop track). [2] J. Zhang, Z. Lin, J. Brandt, X. Shen, and S. Sclaroff. (2016). Top-down neural attention by excitation backprop. In ECCV. [3] Sundararajan, M., Taly, A., & Yan, Q. (2017). Axiomatic attribution for deep networks. In ICML. [4] Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). Grad-cam: Visual explanations from deep networks via gradient-based localization. In ICCV. [5] Dabkowski, P., & Gal, Y. (2017). Real time image saliency for black box classifiers. In NeurIPS. [6] Kindermans, P. J., Hooker, S., Adebayo, J., Alber, M., Schütt, K. T., Dähne, S., ... & Kim, B. (2019). The (un) reliability of saliency methods. In Explainable AI: Interpreting, Explaining and Visualizing Deep Learning (pp. 267-280). Springer, Cham. [7] Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., & Kim, B. (2018). Sanity checks for saliency maps. In NeurIPS [8] Lee, K., Lee, K., Lee, H., & Shin, J. (2018). A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In NeurIPS"}
{"id": "iclr2020_268", "title": "Spatial Information is Overrated for Image Classification | OpenReview", "abstract": "Abstract:###Intuitively, image classification should profit from using spatial information. Recent work, however, suggests that this might be overrated in standard CNNs. In this paper, we are pushing the envelope and aim to further investigate the reliance on and necessity of spatial information. We propose and analyze three methods, namely Shuffle Conv, GAP+FC and 1x1 Conv, that destroy spatial information during both training and testing phases. We extensively evaluate these methods on several object recognition datasets (CIFAR100, Small-ImageNet, ImageNet) with a wide range of CNN architectures (VGG16, ResNet50, ResNet152, MobileNet, SqueezeNet). Interestingly, we consistently observe that spatial information can be completely deleted from a significant number of layers with no or only small performance drops.", "review": "Review:###=== Summary === The authors study the importance of spatial information in (later layers of) CNNs for Image Classification. Specifically, they propose to remove spatial information by either 1) shuffling the input to a convolution, 2) average pooling and replacing convolutions by fully connected layers or 3) replacing spatial convolution by pointwise (1x1) convolutions. They find that removing spatial information in the last layers of CNNs at training time doesn*t impact image classification performance while removing parameters (and potentially FLOPs) for VGG16, ResNet50. === Recommendation === The presented study contains a few experimental flaws. The choice of method for removing spatial information is rather arbitrary and it seems the authors confuse spatial information (being aware of the relative spatial position between pixels) and spatial processing (processing nearby spatial positions). Specifically: - For shuffling, it makes more sense to keep spatial consistency by applying the same shuffling operation to all feature maps. Shuffling independently for each feature map does more than destroy spatial information, it also destroys spatial consistency which probably hinders training more than it needs to. - The authors should have tried 3x3 convolutions where the weights are the same for each relative position (ie C^2 parameters instead of 9C^2 parameters, but the same amount of FLOPS). This allows to still increase the receptive field size in the last layer but without using spatial information. Experimental improvements of accuracy with respect to the number of parameters are shown on very suboptimal architectures such as VGG16 and ResNet50. The improvements on better architectures (from a parameter efficiency perspective) are negligible. In particular, the rule of thumb provided by the authors is already applied in today*s most efficient models such as MnasNet and EfficientNet. The small importance of spatial information for image classification, especially in later layers, is relatively well known and has already been mentioned by other work such as Approximating CNNs with bag-of-local-features models works surprisingly well on imagenet. This also relates to the *Picasso problem* and the motivation for capsule networks. Additionally, work on applying self-attention for vision is also concerned with adding spatial information to the self-attention operation (As such, it could have been interesting to replace the 3x3 convolutions with local self-attention layers without spatial information). The additional questions raised by this work are rather non-conclusive (e.g: relationship with receptive field size, removing spatial information as regularization) In summary, the presented study, while interesting, contains a few flaws, is of limited scientific novelty and experimental results are rather underwhelming. The current draft doesn*t warrant for ICLR acceptance."}
{"id": "iclr2020_269", "title": "Accelerated Information Gradient flow | OpenReview", "abstract": "Abstract:###We present a systematic framework for the Nesterov*s accelerated gradient flows in the spaces of probabilities embedded with information metrics. Here two metrics are considered, including both the Fisher-Rao metric and the Wasserstein- metric. For the Wasserstein- metric case, we prove the convergence properties of the accelerated gradient flows, and introduce their formulations in Gaussian families. Furthermore, we propose a practical discrete-time algorithm in particle implementations with an adaptive restart technique. We formulate a novel bandwidth selection method, which learns the Wasserstein- gradient direction from Brownian-motion samples. Experimental results including Bayesian inference show the strength of the current method compared with the state-of-the-art.", "review": "Review:###The paper attempts to develop a counterpart of the well-known Nesterov accelerated gradient method for gradient flows on the space of probability measures equipped with an information metric. This is an important problem which is useful for optimization on probability spaces. The accelerated gradient flow is developed by leveraging a damping Hamiltonian flow. The paper focuses mainly on the case with the Wasserstein metric and provides a convergence analysis. Practical considerations such as discretizing the accelerated flow and bandwidth selection are developed for the use of the method in practical problems. Although the paper has some important merit, I find the paper extremely hard to follow, partly because of its writing style. There is not enough motivation and explanation for the ideas presented. Some discussions and sentences either do not make much sense to me or read badly. For example, this sentence *For the Wasserstein gradient, many classical methods such as Markov Chain Monte Carlo .... are based on this framework...* doesn*t make sense, as the development of MCMC is never based on Wasserstein gradient. Or the sentence right before it *For the Fisher-Rao gradient, classical results including Adam ... and K-FAC .. demonstrate its effectiveness in ...*: it*s not clear what the authors are trying to say here. Adam is not relevant to the Fisher-Rao natural gradient while K-FAC is just an approximation method and isn*t a good reference for demonstrating the effectiveness of the natural gradient. Also, there are many English typos and grammar errors. I didn*t read the proof carefully due to the time constraint, so I cannot judge on the theoretical part of the paper. The numerical experiment is quite limited as it considers very simple problems (a toy example, a single Gaussian distribution and a logistic regression problem). As such, I think there isn*t enough evidence to judge the usefulness of the proposed method in practice. Having said that, I believe this paper can be an important contribution if the authors invest more time on refining its presentation and if more thorough experimental studies are conducted."}
{"id": "iclr2020_270", "title": "Semi-supervised Pose Estimation with Geometric Latent Representations | OpenReview", "abstract": "Abstract:###Pose estimation is the task of finding the orientation of an object within an image with respect to a fixed frame of reference. Current classification and regression approaches to the task require large quantities of labelled data for their purposes. The amount of labelled data for pose estimation is relatively limited. With this in mind, we propose the use of Conditional Variational Autoencoders (CVAEs) cite{Kingma2014a} with circular latent representations to estimate the corresponding 2D rotations of an object. The method is capable of training with datasets that have an arbitrary amount of labelled images providing relatively similar performance for cases in which 10-20% of the labels for images is missing.", "review": "Review:###This paper presents a semi-supervised approach to learn the rotation of objects in an image. The primary motivation is that for rotation estimation datasets may not always be fully labeled, so learning partially from labeled and partially for unlabeled is important. The approach is to use a CVAE with a supervised loss and an unsupervised loss and to jointly train the network. Limited experiments that show performance are presented. First, the paper solves a very interesting problem with potentially wide applications. The paper is reasonably well-written. Unfortunately, I don*t believe that the contributions of the paper meet the standards of ICLR. I justify my opinion below. The experiments are also very weak. - While the high level goal of *pose estimation* is clear. Even after reading the paper multiple times, I did not understand the setting well. It appears like the paper looks at the problem of 2D orientation estimation of objects in images. However, this setting is restrictive and not very practical in reality. We mostly care about 3D pose estimation. It would have been good to see results on 3D rotations at the very least. - Contribution: It is unclear to me what the primary contribution(s) of the paper is. The entire section on CVAE*s and losses are quite standard in literature. The interesting part is in combining the supervised and unsupervised parts of the method for the task for pose estimation. But in the end this is a simple weighted loss function (equation 5). So I wonder what is the novelty? What are the new capabilities enabled by this approach? - Related Work: Implicit 3D Orientation Learning for 6D Object Detection from RGB Images, ECCV 18 - I would have loved to see a description of the differences in the loss functions (1) and (2). Perhaps this can help elevate the contribution more? - I also missed justification of why the particular design choice is suitable for this problem? Would direct regression using a simple CNN work better? - In equation (4), how are the two losses balanced? - The dataset generation part is just confusing. ModelNet40 is rendered but only 2D rotation is predicted? What does 2D rotation mean for a 3D object? - Could this method be tested on a dataset like dSprites (https://github.com/deepmind/dsprites-dataset) which has 3D rotations? - Regarding experiments: I was disappointed to see no comparisons with other approaches or even a simple baseline. A CNN that directly regresses orientation could help put the tables and plots in perspective. Overall, the problem is important (if lifted to 3D) with important applications. However, the paper does not say anything new about how to solve the problem and the experiments are weak. In its current state, I am unable to recommend acceptance."}
{"id": "iclr2020_271", "title": "Neural Machine Translation with Universal Visual Representation | OpenReview", "abstract": "Abstract:###Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pre-trained ResNet. An attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodel NMT. Experiments on four widely used translation datasets, including the WMT*16 English-to-Romanian, WMT*14 English-to-German, WMT*14 English-to-French, and Multi30K, show that the proposed approach achieves significant improvements over strong baselines.", "review": "Review:###Summary: This paper uses visual representation learned over monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs for multimodal NMT. Their approach enables visual information to be integrated into large-scale text-only NMT. Experiments on four widely used translation datasets show that the proposed approach achieves significant improvements over strong baselines. Strengths: - This paper is well motivated and well written. I especially like how they use external paired sentence-image data from Multi30k to learn weak pairs for sentences in machine translation. - Experimental results are convincing. I like how low-resource translation is included as a priority in their experiments. Weaknesses: - Do you have any explanations as to why the number of images, if too large, actually hurts translation performance? Is it because more images also leads to a higher chance of noisy images? - It would be nice to have an experiment that varies the size of the external paired sentence-image dataset and tested the impact on performance. - Please comment on the extra computation required for obtaining image data for MT sentences and for learning image representations. - Why are there missing BLEU scores and the number of parameters in Table 1? ### Post rebuttal ### Thank you for your detailed answers to my questions."}
{"id": "iclr2020_272", "title": "Learning Disentangled Representations for CounterFactual Regression | OpenReview", "abstract": "Abstract:###We consider the challenge of estimating treatment effects from observational data; and point out that, in general, only some factors based on the observed covariates X contribute to selection of the treatment T, and only some to determining the outcomes Y. We model this by considering three underlying sources of {X, T, Y} and show that explicitly modeling these sources offers great insight to guide designing models that better handle selection bias. This paper is an attempt to conceptualize this line of thought and provide a path to explore it further. In this work, we propose an algorithm to (1) identify disentangled representations of the above-mentioned underlying factors from any given observational dataset D and (2) leverage this knowledge to reduce, as well as account for, the negative impact of selection bias on estimating the treatment effects from D. Our empirical results show that the proposed method (i) achieves state-of-the-art performance in both individual and population based evaluation measures and (ii) is highly robust under various data generating scenarios.", "review": "Review:###The paper proposes an algorithm that identifies disentangled representation to find out an individual treatment effect. A very specific model that tries to find out the underlying dynamics of such a problem is proposed and is learned by minimizing a suggested objective that takes the strengths of previous approaches. The method is demonstrated in a synthetic dataset and IHDP dataset and shown to outperform other previous methods by a large margin. My initial review was negative, but I changed my mind after reading a few papers in this area. It seems that explicit learning of underlying factors that are described in (Hassanpour & Greiner, 2019) is a nice idea and works well. My only concern is that the paper has a lot of overlap with (Hassanpour & Greiner, 2019), even using identical figures. I am not sure whether it is OK."}
{"id": "iclr2020_273", "title": "Attention on Abstract Visual Reasoning | OpenReview", "abstract": "Abstract:###Attention mechanisms have been boosting the performance of deep learning models on a wide range of applications, ranging from speech understanding to program induction. However, despite experiments from psychology which suggest that attention plays an essential role in visual reasoning, the full potential of attention mechanisms has so far not been explored to solve abstract cognitive tasks on image data. In this work, we propose a hybrid network architecture, grounded on self-attention and relational reasoning. We call this new model Attention Relation Network (ARNe). ARNe combines features from the recently introduced Transformer and the Wild Relation Network (WReN). We test ARNe on the Procedurally Generated Matrices (PGMs) datasets for abstract visual reasoning. ARNe excels the WReN model on this task by 11.28 ppt. Relational concepts between objects are efficiently learned demanding only 35% of the training samples to surpass reported accuracy of the base line model. Our proposed hybrid model, represents an alternative on learning abstract relations using self-attention and demonstrates that the Transformer network is also well suited for abstract visual reasoning.", "review": "Review:###This work introduced an attention-based model to solve the RPM cognitive tasks. The model is based on the transformer network, which performs relational reasoning through its self-attention mechanisms. Technical novelty: The method seems to be a straightforward application of the transformer network to the PGM task. The technical novelty of the proposed approach is unclear. I’d love to hear what the authors have to say about the technical contributions of the proposed ARNe model in comparison to prior work. Supervision with meta-targets: It also seems that the meta-targets are crucial for attaining a good performance with the ARNe model. According to Table 4, the model without meta-target training (beta=0) only achieved 12% accuracy in train/val/test sets. However, prior work [Santoro* et al. 2018] has demonstrated that even without training on meta-targets, WReN still achieves a performance of over 60% accuracy (Table 1). This result suggests that the proposed ARNe model does not work well when training with weaker supervision without meta-targets. The results could be a lot stronger if the authors show ARNe outperforms the prior work when beta is set to 0. Ablation studies: This model is only tested in the neutral PGM dataset. The evaluation would be strengthed with the generalization results of this model in different generalization regimes (see Table 1, Santoro* et al. 2018) and comparing its performance with prior works."}
{"id": "iclr2020_274", "title": "Sticking to the Facts: Confident Decoding for Faithful Data-to-Text Generation | OpenReview", "abstract": "Abstract:###Neural conditional text generation systems have achieved significant progress in recent years, showing the ability to produce highly fluent text. However, the inherent lack of controllability in these systems allows them to hallucinate factually incorrect phrases that are unfaithful to the source, making them often unsuitable for many real world systems that require high degrees of precision. In this work, we propose a novel confidence oriented decoder that assigns a confidence score to each target position. This score is learned in training using a variational Bayes objective, and can be leveraged at inference time using a calibration technique to promote more faithful generation. Experiments on a structured data-to-text dataset -- WikiBio -- show that our approach is more faithful to the source than existing state-of-the-art approaches, according to both automatic metrics and human evaluation.", "review": "Review:###The authors propose several approaches to making a data-to-text generation system more precise, that is, less prone to hallucination. In particular, they propose an attention score, which attempts to measure to what degree the model is relying on its attention mechanism in making a prediction. This attention score is used to weight a mixture distribution (a *confidence score*) over the generation model*s next-word distribution and the next-word distribution of an unconditional language model. The learned conditional distribution can then be calibrated to the confidence score. The authors also propose a variational-inference inspired objective, which attempts to allow the model to ignore certain tokens it isn*t confident about. The authors evaluate their approach on the WikiBio dataset, and find that their approaches make their system more precise, at the cost of some coverage. This paper is well motivated, timely, and it presents several interesting ideas. However, I think parts of the proposed approach need to be better justified. In particular: - What justifies defining the attention score A_t in this way? First, is there an argument (empirical or otherwise) for using the magnitude of the attention vector (rather than some other statistic)? Is it obvious that if the attention vector has a high magnitude then it ought to be trusted? Note that this might be a reasonable assumption in the case of a pointer-generator style model, where a single attention vector is used both for attending and for copying, but in a model where attention isn*t constrained in this way the magnitude of the attention vector may be misleading. - The variational objective seems difficult to justify. First, I don*t understand what it means for p(y | z, x) to be assumed to 1. Is this for any z (in which case y is independent of z)? Otherwise, how can it be removed from the objective? (Put another way: Equation (17) is not in general true; it neglects an expected log likelihood term). I*m also not entirely clear on how Equation (12) is modeled: do the z*s really only rely on the other sampled z*s? Doesn*t that require a different model than the one that calculates P^{kappa}? - Somewhat minor: the claim that optimizing the joint objective needn*t hurt perplexity relies on kappa being 0; have you confirmed empirically that when it isn*t zero the perplexity improves over the baseline model? - Finally, I*m not sure I understand why there needs to be a stop-gradient in equation (4). It would be nice to also verify empirically that this is important."}
{"id": "iclr2020_275", "title": "RPGAN: random paths as a latent space for GAN interpretability | OpenReview", "abstract": "Abstract:###In this paper, we introduce Random Path Generative Adversarial Network (RPGAN) --- an alternative scheme of GANs that can serve as a tool for generative model analysis. While the latent space of a typical GAN consists of input vectors, randomly sampled from the standard Gaussian distribution, the latent space of RPGAN consists of random paths in a generator network. As we show, this design allows to associate different layers of the generator with different regions of the latent space, providing their natural interpretability. With experiments on standard benchmarks, we demonstrate that RPGAN reveals several interesting insights about roles that different layers play in the image generation process. Aside from interpretability, the RPGAN model also provides competitive generation quality and allows efficient incremental learning on new data.", "review": " This paper presents a variation on the generator of GANs. The authors modify the generator by adding a concept of *blocks* which are randomly activated based on part of the random input vector. It is similar to adding random dropout in the generator, except that the dropout would apply to larger sets of activations instead of single component. The authors also add a diversity term to force the different blocks to have different blocks. This is a term based on the L2 distance between the weights of different blocks. Although this term is ad-hoc and could probably be refined into something more grounded in theory, it should indeed provide some diversity. This block structure allows for more understanding of what each layer of the generator does, since it is easy to change the discrete variables that switch blocks. The paper presents an empirical evaluation of what each switch does, and show that concepts are well disentangled between layers (for instance, one layer changes the background whereas another one changes the color of the foreground). They also show that blocks can be added after training is done which is a nice property for incremental training. They also show that this framework can train a generator without non-linearities (except for the block switching), which could potentially simplify the analysis of such networks. Generated samples are presented up to 128x128 pixels which, although far from state of the art, proves that the concept works."}
{"id": "iclr2020_276", "title": "Lazy-CFR: fast and near-optimal regret minimization for extensive games with imperfect information | OpenReview", "abstract": "Abstract:###Counterfactual regret minimization (CFR) methods are effective for solving two-player zero-sum extensive games with imperfect information with state-of-the-art results. However, the vanilla CFR has to traverse the whole game tree in each round, which is time-consuming in large-scale games. In this paper, we present Lazy-CFR, a CFR algorithm that adopts a lazy update strategy to avoid traversing the whole game tree in each round. We prove that the regret of Lazy-CFR is almost the same to the regret of the vanilla CFR and only needs to visit a small portion of the game tree. Thus, Lazy-CFR is provably faster than CFR. Empirical results consistently show that Lazy-CFR is significantly faster than the vanilla CFR.", "review": "Review:###The paper proposes an improvement to Counterfactual Regret Minimization, avoiding traversing the whole tree on each iteration. The idea is not to change the strategy in those infosets, where the reach probability of opponents is low. The strategy in such infosets is only updated once in several iterations, when the sum of reach probabilities over these iterations if higher than the threshold. The straightforward implementation of the idea still has the same running time as CFR. Therefore, the paper presents an efficient implementation, exploiting the structure of the game tree. However, this implementation comes at the cost of additional memory requirements. Overall, the paper proves the theoretical result of about O(sqrt(|I|)/D) times faster than CFR to achieve the same approximation error, while the memory requirements increase by a factor of O(|H|/|I|). Here |I| is the number of infosets, D is the depth of the game tree and |H| is the number of histories. The idea of eliminating unnecessary computations for infosets with low probability is a valuable contribution. The presented theoretical analysis takes an important place in the series of works refining the regret upper bound of CFR and its variants. The experiment confirms performance of the idea. That being said, I follow up with some questions/criticism. 1. Implementation in Section 3.2.1 and Appendix E is rather hard to follow. Is there any intuition on how the segment [\tau_t(h), t] is divided, i.e. what does t_1, t_2 and \tau’(h) mean? Also, clarity could be increased if these variables would be defined before they are used. 2. How is a segmentation rule for Lazy-RM in OLO designed in such a way, that equation sum_{i=1}^n max_a c’_i(a)^2 approx sum_{j=1}^T max_a c_j(a)^2 holds? 3. Section 3.2: “following step (1)”. (1) is an equation for RM in OLO, probably some other reference was meant. 4. Recently, Linear Cfr was introduced, which outperforms Cfr+. Thus, citation is needed Brown, Noam and Sandholm, Tuomas “Solving Imperfect-Information Games via Discounted Regret Minimization”. Worth to mention, LazyCfr is straightforwardly compatible with Linear Cfr. 5. The specified space requirements significantly limit the applicability of the presented Lazy-RM implementation. For example, in state-of-art approaches to solve/resolve No-Limit Holdem (Libratus, DeepStack, Pluribus), either the game tree is too large, making the space requirements unrealistic, or the game tree is small enough for getting a good equilibrium approximation fast even with CFR+. UPD: score updated"}
{"id": "iclr2020_277", "title": "A Function Space View of Bounded Norm Infinite Width ReLU Nets: The Multivariate Case | OpenReview", "abstract": "Abstract:###A key element of understanding the efficacy of overparameterized neural networks is characterizing how they represent functions as the number of weights in the network approaches infinity. In this paper, we characterize the norm required to realize any function as a single hidden-layer ReLU network with an unbounded number of units (infinite width), but where the Euclidean norm of the weights is bounded, including precisely characterizing which functions can be realized with finite norm. This was settled for univariate functions in Savarese et al. (2019), where it was shown that the required norm is determined by the L1-norm of the second derivative of the function. We extend the characterization to multi-variate functions (i.e., multiple input units), relating the required norm to the L1-norm of the Radon transform of a higher-order Laplacian of the function. This characterization allows us to show that all functions in a Sobolev space, can be represented with bounded norm, to calculate the required norm for several specific functions, and to obtain a depth separation result. These results have important implications for understanding generalization performance and the distinction between neural networks and more traditional kernel learning.", "review": "Review:###The paper studies the function space regularization behavior of learning with an infinite-width ReLU network with a bound on the l2 norm of weights, in arbitrary dimension, extending the univariate study of Savarese et al. (2019). The authors show that the corresponding regularization function is more or less an L1 norm of the (weak) (d+1)st derivatives of the function, and provide a rigorous formal characterization in terms of the *R-norm*, which is expressed via duality through the Radon transform and powers of the Laplacian. In addition, the paper provides a number of implications of this study, such as approximation results through Sobolev spaces, an analysis of the norm of radial bump functions, and a new type of depth separation result in terms of norm as opposed to width. Overall, this is a strong paper making several interesting and important contributions for our understanding of the inductive bias of ReLU networks. I thus recommend acceptance. A few comments: * is it possible to obtain precise characterizations of interpolating solutions in this setting (other than a mere representer theorem with ReLUs), as done in Savarese et al (2019, Theorem 3.3) for the univariate case? * perhaps the results of Section 5.1 should be contrasted with those of Bach (2017, e.g. Prop. 5), which only require ~ d/2 derivatives instead of ~ d here, albeit with stronger requirements, for essentially the same functional space (though the approximation result is obtained from an associated RKHS, which is smaller). * are the results on radial bump functions intended to provide insight on approximation or depth separation? what was the motivation behind this section? Other minor comments/typos: - after Prop. 1: *intertwining* appears twice - eq. (22): missing f in l.h.s. - eq. (23): is the first minus sign needed? - before Thm. 1: point to which Appendix - Section 4.1, *In particular, this is what would happen ... d+1*: this should be further explained - Section 4.1, final paragraph, *in order R-norm to be*: rephrase - Section 5.4, *required norm with three layers is finite*: which norm? maybe point to a reference? Also, Example 5 could be explained in further detail - Section 5.5: what is an RKHS semi-norm? you*d always have ||f|| = 0 => f = 0 in an RKHS, by the reproducing property"}
{"id": "iclr2020_278", "title": "Task-Mediated Representation Learning | OpenReview", "abstract": "Abstract:###Traditionally, unsupervised representation learning is used to discover underlying regularities from raw sensory data without relying on labeled data. A great number of algorithms in this field resorts to utilizing proxy objectives to facilitate learning. Further, learning how to act upon these regularities is left to a separate algorithm. Neural encoding in biological systems, on the other hand, is optimized to represent behaviorally relevant features of the environment in order to make inferences that guide successful behavior. Evidence suggests that neural encoding in biological systems is shaped by such behavioral objectives. In our work, we propose a model of inference-driven representation learning. Rather than following some auxiliary, a priori objective (e.g. minimization of reconstruction error, maximization of the fidelity of a generative model, etc.) and indiscriminately encoding information present in an observation, our model learns to build representations that support accurate inferences. Given a set of observations, our model encodes underlying regularities that de facto are necessary to solve the inference problem in hand. Rather than labeling the observations and learning representations that portray corresponding labels or learning representation in a self-supervised manner and learning explicit features of the input observations, we propose a model that learns representations that implicitly shaped by the goal of correct inference.", "review": "Review:###Thank you for your submission. The paper studies representation learning with vs. without supervision. It claims unsupervised learning is not enough for learning strong representations for downstream tasks. The paper refers to previous work that supports the claim that tasks are important for representation learning in biological systems. The paper*s results provide evidence that supervised training with a specific task training set yields a richer representation than unsupervised training. The performance is measured on the same set of tasks, but with held-out realizations/configurations. I vote for rejection of this paper. The paper is making a case for the fact that supervised training signals are richer for representation learning that unsupervised ones. This is widely accepted. The paper makes a valid point that optimizing unsupervised learning criteria (such as log-likelihood of reconstructed inputs) is not in itself the solution to learn good representations. The point of unsupervised learning is leveraging data that is often more abundant than data for supervised learning. In my view the way to reconcile these two points is to identify/develop adequate unsupervised losses and, when possible, compare their representation learning on supervised tasks of interest. In contrast, replacing the unsupervised learning with supervised learning does not reconcile the two points, but only bypasses the issues of adequate unsupervised representation learning. The paper*s arguments in favor of task-mediated representation learning are an interesting motivation to support multitask learning. A few technical remarks: * The bandit problem is not really a bandit problem, because there*s no decision-making involved. It*s a binary prediction task with sequences as inputs. * I think it would be useful to introduce the problem being addressed and mention the contributions before going into the details of the dataset and network architectures. A quick few-paragraph overview of the paper (problem being addressed; what the results look like) in the introduction will help the reader navigate the paper better."}
{"id": "iclr2020_279", "title": "DeepAGREL: Biologically plausible deep learning via direct reinforcement | OpenReview", "abstract": "Abstract:###While much recent work has focused on biologically plausible variants of error-backpropagation, learning in the brain seems to mostly adhere to a reinforcement learning paradigm; biologically plausible neural reinforcement learning frameworks, however, were limited to shallow networks learning from compact and abstract sensory representations. Here, we show that it is possible to generalize such approaches to deep networks with an arbitrary number of layers. We demonstrate the learning scheme - DeepAGREL - on classical and hard image-classification benchmarks requiring deep networks, namely MNIST, CIFAR10, and CIFAR100, cast as direct reward tasks, both for deep fully connected, convolutional and locally connected architectures. We show that for these tasks, DeepAGREL achieves an accuracy that is equal to supervised error-backpropagation, and the trial-and-error nature of such learning imposes only a very limited cost in terms of training time. Thus, our results provide new insights into how deep learning may be implemented in the brain.", "review": "Review:###This paper presents DeepAGREL, a framework for biologically plausible deep learning that is modified to use reinforcement learning as a training mechanism. This framework is shown to perform similarly to error-backpropagation on a set of architectures. The idea relies on feedback mechanism that can resemble local connections between real neurons. This paper is an interesting approach to provide a reinforcement learning paradigm for training deep networks, it is well written and the experiments are convincing, although more explanation about why these specific architectures were tested would be more convincing. I also think the assumptions about feedback connections in real neurons should be visited and more neuroscientific evidence from the literature should be included in the paper. Do we expect feedback to happen at each level of a neuron-neuron interaction and between each pair of connected neurons? Is there a possibility that feedback is more general to sets of neurons, or skips entire layers of neurons? I think more neuroscience background would help this paper (and others on the topic). Nonetheless, I think the paper does offer an interesting proposal of a more biologically plausible form of deep learning."}
{"id": "iclr2020_280", "title": "Avoiding Negative Side-Effects and Promoting Safe Exploration with Imaginative Planning | OpenReview", "abstract": "Abstract:###With the recent proliferation of the usage of reinforcement learning (RL) agents for solving real-world tasks, safety emerges as a necessary ingredient for their successful application. In this paper, we focus on ensuring the safety of the agent while making sure that the agent does not cause any unnecessary disruptions to its environment. The current approaches to this problem, such as manually constraining the agent or adding a safety penalty to the reward function, can introduce bad incentives. In complex domains, these approaches are simply intractable, as they require knowing apriori all the possible unsafe scenarios an agent could encounter. We propose a model-based approach to safety that allows the agent to look into the future and be aware of the future consequences of its actions. We learn the transition dynamics of the environment and generate a directed graph called the imaginative module. This graph encapsulates all possible trajectories that can be followed by the agent, allowing the agent to efficiently traverse through the imagined environment without ever taking any action in reality. A baseline state, which can either represent a safe or an unsafe state (based on whichever is easier to define) is taken as a human input, and the imaginative module is used to predict whether the current actions of the agent can cause it to end up in dangerous states in the future. Our imaginative module can be seen as a ``plug-and-play** approach to ensuring safety, as it is compatible with any existing RL algorithm and any task with discrete action space. Our method induces the agent to act safely while learning to solve the task. We experimentally validate our proposal on two gridworld environments and a self-driving car simulator, demonstrating that our approach to safety visits unsafe states significantly less frequently than a baseline.", "review": "Review:###This paper proposes to use learned transition models to do two separate things: (i) avoid unsafe states and (ii) allow an alternative channel for task reward specification. The idea is to create a comprehensive connectivity graph of the states in the environment. Once done, an agent can avoid unsafe states by avoiding states that are unconnected to a specified safe state. A practitioner might also specify safe/unsafe states as an additional source of information about the reward. This paper suffers from poor and loose writing, incomplete specification of its experiments, unrealistic assumptions during evaluation (Sec 5.3 *we create the graph using rollouts from the actual environment* to avoid errors from learning a transition model). The paper does not address basic concerns with its approach: how is the model to be learned at all, if it is to be comprehensive in the way that is necessary for the connectivity graph (which this paper calls an *imaginative module*)? The authors say this is done through multiple agents performing random actions in the environment, in which case, isn*t this extremely unsafe training time by the paper*s own definition of safe exploration? Further, creating a complete connectivity graph is unrealistic even for fully known transition models in most reasonably complex settings, such as, say, Go or Chess. If the transition model is fully known as in the car racing setting, why not directly use that to plan and solve the game? Experiments show fewer *unsafe* states for the paper*s approach compared to a method that has no way to know that those states are unsafe. How is this a reasonable validation, especially when the transition model is fully known? Also, this is an insufficient metric by itself as it says nothing about whether the method actually performed well at the task."}
{"id": "iclr2020_281", "title": "Recurrent Neural Networks are Universal Filters | OpenReview", "abstract": "Abstract:###Recurrent neural networks (RNN) are powerful time series modeling tools in ma- chine learning. It has been successfully applied in a variety of fields such as natural language processing (Mikolov et al. (2010), Graves et al. (2013), Du et al. (2015)), control (Fei & Lu (2017)) and traffic forecasting (Ma et al. (2015)), etc. In those application scenarios, RNN can be viewed as implicitly modelling a stochastic dy- namic system. Another type of popular neural network, deep (feed-forward) neural network has also been successfully applied in different engineering disciplines, whose approximation capability has been well characterized by universal approxi- mation theorem (Hornik et al. (1989), Park & Sandberg (1991), Lu et al. (2017)). However, the underlying approximation capability of RNN has not been fully understood in a quantitative way. In our paper, we consider a stochastic dynamic system with noisy observations and analyze the approximation capability of RNN in synthesizing the optimal state estimator, namely optimal filter. We unify the recurrent neural network into Bayesian filtering framework and show that recurrent neural network is a universal approximator of optimal finite dimensional filters under some mild conditions. That is to say, for any stochastic dynamic systems with noisy sequential observations that satisfy some mild conditions, we show that (informal) ?_x000f_ > 0, ? RNN-based filter, s.t. lim sup x? k|k ? E[x k |Y k ] < _x000f_, k?? where x? k|k is RNN-based filter’s estimate of state x k at step k conditioned on the observation history and E[x k |Y k ] is the conditional mean of x k , known as the optimal estimate of the state in minimum mean square error sense. As an interesting special case, the widely used Kalman filter (KF) can be synthesized by RNN.", "review": "Review:###This paper shows that RNN (of infinite horizon) can be universal approximators for any stochastic dynamics system. It does that by using the Bayesian framework of filtering, which shows that if sufficient statistics given the observations so far can be computed at each stage of filtering, then the expected hidden states at each stage can be estimated as well. The paper then follows this framework by constructing deep neural network mapping from “observations so-far” to “sufficient statistics” and by universal approximation theorem this is possible. Then as long as after each stage, the image of such a mapping is bounded into a certain region, and if the mapping is contractive (|C_psi C_phi| < 1), this can be applied to arbitrarily long sequence (and thus infinite horizon). The paper is well-written and easy to follow. However, I have concerns about its novelty. Overall the paper seems to be a straightforward application of universal approximation theorem of deep neural network. The authors don’t specify when the condition will hold (e.g., |C_psi C_phi| < 1), what kind of empirical RNNs fall into this category and whether the trained models are operating in the contractive region. Also with the same technique the authors also reach the same conclusion for nonlinear dynamics system (Sec. 4.2). From the proof, there is no much difference between linear case (Kalman Filter) and nonlinear case, since apparently DNN can fit everything. Then what are the new discoveries and takeaway messages? It is not clear what can we learn from this analysis and its impact is likely to be limited."}
{"id": "iclr2020_282", "title": "Neural Subgraph Isomorphism Counting | OpenReview", "abstract": "Abstract:###In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms. Although the learning based approach is inexact, we are able to generalize to count large patterns and data graphs in polynomial time compared to the exponential time of the original NP-complete problem. Different from other traditional graph learning problems such as node classification and link prediction, subgraph isomorphism counting requires more global inference to oversee the whole graph. To tackle this problem, we propose a dynamic intermedium attention memory network (DIAMNet) which augments different representation learning architectures and iteratively attends pattern and target data graphs to memorize different subgraph isomorphisms for the global counting. We develop both small graphs (<= 1,024 subgraph isomorphisms in each) and large graphs (<= 4,096 subgraph isomorphisms in each) sets to evaluate different models. Experimental results show that learning based subgraph isomorphism counting can help reduce the time complexity with acceptable accuracy. Our DIAMNet can further improve existing representation learning models for this more global problem.", "review": " This paper proposes a dynamic inter-medium attention memory network and model the sub-graph isomorphism counting problem as a learning problem with both polynomial training and prediction time complexities. Since the testing time is reported in this paper, and the time complexity is one of the main contribution of this paper. The hardware and software used to run the algorithm should be reported in the main article. The author argues that if we use neural networks to learn distributed representations for V_G and V_p or xi_G and xi_P without self-attention, the computational cost will acceptable for large graphs, but the missing of self-attention will hurt the performance. It’s encouraged to do corresponding experiments to compare it with the proposed method and better support the algorithm. One of the main advantages of this paper is that the proposed method can efficiently deal with large graph tasks, so the model behaviors of different models in large dataset similar to Figure 5 is encouraged to be given."}
{"id": "iclr2020_283", "title": "Deep Auto-Deferring Policy for Combinatorial Optimization | OpenReview", "abstract": "Abstract:###Designing efficient algorithms for combinatorial optimization appears ubiquitously in various scientific fields. Recently, deep reinforcement learning (DRL) frameworks have gained considerable attention as a new approach: they can automatically learn the design of a good solver without using any sophisticated knowledge or hand-crafted heuristic specialized for the target problem. However, the number of stages (until reaching the final solution) required by existing DRL solvers is proportional to the size of the input graph, which hurts their scalability to large-scale instances. In this paper, we seek to resolve this issue by proposing a novel design of DRL*s policy, coined auto-deferring policy (ADP), automatically stretching or shrinking its decision process. Specifically, it decides whether to finalize the value of each vertex at the current stage or defer to determine it at later stages. We apply the proposed ADP framework to the maximum independent set (MIS) problem, a prototype of NP-complete problems, under various scenarios. Our experimental results demonstrate significant improvement of ADP over the current state-of-the-art DRL scheme in terms of computational efficiency and approximation quality. The reported performance of our generic DRL scheme is also comparable with that of the state-of-the-art solvers specialized for MIS, e.g., ADP outperforms them for some graphs with millions of vertices.", "review": "Review:###The paper introduces auto-deferring policies (ADPs) for deep reinforcement learning (RL). ADPs automatically stretching or shrinking their decision process, in particular, deciding whether to finalize the value of each vertex at the current stage or defer to determine it at later stages. ADPs are evaluated on maximum independent set problems. The paper is in principle well written and structured. Some statements of the paper appear a little bit too strong. For instance, saying that deep RL approaches *can automatically learn the design of a good solver without using any sophisticated knowledge or hand-crafted heuristic specialized for the target problem* is misleading as thee designer of the RL setup is putting a lot of knowledge into the design. Likewise the statement *without any human guidance* is not true, at least at the current stage. It would be great to acknowledge this by softening this statement. The basic idea is also fine, learning to expand or not nodes in the current fringe of the combinatorial solver. However, being an informed outsider, I would like to understand more why encoding NP-hard problem using an (discrete, finite) MDP, which is rather efficient to solve, is a good idea. Moreover, while the focus on MIS is justified in the paper, showing the (potential) benfit on other tasks such as TSP is useful to convince the reader that ADPs apply across different problem classes. Without, it is not clear whether ADPs work fine on other problem classes (even if one may expect that this is the case). Anyhow, the main idea of implementing two independent agents/networks to implement a reward signal that rewards deviation is interesting. Moreover, the experimental results show that the approach works. It actually manages to be on par with ReduMIS. However, it does not really improve upon this well-known heuristic. Hence some experiments across different problems would really be beneficial. Without, it is just interesting to see that RL can also come up with good heuristics but the existing heuristic already works pretty well. To sum up, a very nice idea that shows promise but experiments on other problems is missing for a complete picture. Also some statements should be soften."}
{"id": "iclr2020_284", "title": "Variational Hetero-Encoder Randomized GANs for Joint Image-Text Modeling | OpenReview", "abstract": "Abstract:###For bidirectional joint image-text modeling, we develop variational hetero-encoder (VHE) randomized generative adversarial network (GAN) that integrates a probabilistic text decoder, probabilistic image encoder, and GAN into a coherent end-to-end multi-modality learning framework. VHE randomized GAN (VHE-GAN) encodes an image to decode its associated text, and feeds the variational posterior as the source of randomness into the GAN image generator. We plug three off-the-shelf modules, including a deep topic model, a ladder-structured image encoder, and StackGAN++, into VHE-GAN, which already achieves competitive performance. This further motivates the development of VHE-raster-scan-GAN that generates photo-realistic images in not only a multi-scale low-to-high-resolution manner, but also a hierarchical-semantic coarse-to-fine fashion. By capturing and relating hierarchical semantic and visual concepts with end-to-end training, VHE-raster-scan-GAN achieves state-of-the-art performance in a wide variety of image-text multi-modality learning and generation tasks. PyTorch code is provided.", "review": "Review:###This paper proposed VHE-GAN for the text-to-image generation task. The proposed method utilizes the off-the-shell modules and feeds the VHE variational posterior into the generator. The experiments are conducted on three datasets. The motivation for the paper is not clear. Most of the components used, such as text-encoder, image-encoder, generator-discriminator follow previous works. Therefore, the authors should claim how the proposed VHE variational posterior can help the task. However, I did not see the clear motivation for this part. Besides the basic version VHE-StackGAN++, it proposed another version VHE-raster-scan-GAN. However, the paper also fails to tell the intuition of the deep topic model and PGBN text decoder. The experimental results are not solid. The comparison only included old baselines. However, several recent state-of-the-art approaches are missing: a. attnGAN (CVPR18), b. TA-GAN (NIPS18), c. Object-GAN (CVPR19). Without these comparisons, it is difficult to evaluate how the method works. In addition, the paper does not provide an ablation study to analyze the effect of each component proposed (e.g., Poisson gamma belief network, a deep topic mode)."}
{"id": "iclr2020_285", "title": "Reweighted Proximal Pruning for Large-Scale Language Representation | OpenReview", "abstract": "Abstract:###Recently, pre-trained language representation flourishes as the mainstay of the natural language understanding community, e.g., BERT. These pre-trained language representations can create state-of-the-art results on a wide range of downstream tasks. Along with continuous significant performance improvement, the size and complexity of these pre-trained neural models continue to increase rapidly. Is it possible to compress these large-scale language representation models? How will the pruned language representation affect the downstream multi-task transfer learning objectives? In this paper, we propose Reweighted Proximal Pruning (RPP), a new pruning method specifically designed for a large-scale language representation model. Through experiments on SQuAD and the GLUE benchmark suite, we show that proximal pruned BERT keeps high accuracy for both the pre-training task and the downstream multiple fine-tuning tasks at high prune ratio. RPP provides a new perspective to help us analyze what large-scale language representation might learn. Additionally, RPP makes it possible to deploy a large state-of-the-art language representation model such as BERT on a series of distinct devices (e.g., online servers, mobile phones, and edge devices).", "review": "Review:###Models such as BERT are pretrained language models which provide significant improvement for different tasks, however they suffer from high huge size and complexity. This paper has proposed using proximal gradient descent to find sparse weights for BERT to reduce the number of parameters and make the model smaller. They concentrate on the drawbacks of the previous sparse-based approaches and claimed that they have convergence issues (they have provided some evidence in the appendix). therefore, they propose to use reweighed sparse method and optimise it using proximal gradient descent which provides a closed form solution for sparse constraint. ALthough proposing a minor novelty (reweighted sparse optimization ), they have provided interesting results for both pretrained structure and fine-tuning for several different tasks. they have also provided some visualisation for the weight matrices after sparsification. Their results are notably stronger than simply adding the L1 regularizer to the optimisation method. The paper is well written and easy to follow with nearly comprehensive related work. However, there are some drawbacks: 1. They have claimed that “ To the best of our knowledge, we are the first to apply reweighted l1 and proximal algorithm in the DNN weight pruning domain, and achieve effective weight pruning on BERT. ”, however proximal optimization has been used for DNN in works like “Combined Group and Exclusive Sparsity for Deep Neural Networks, 2017”. 2. It should be explained clearly about all the matrices included in the sparsification steps, despite only saying “parameters of the model”. 3. More analysis is required on the results, specially the diagrams for fine-tuning over different datasets. 4. It is essential to compare the method with other related works for Bert and transformer compression, including quantisation-based, factorisation-based, pruning, knowledge distillation papers such as: --Prato, Gabriele, Ella Charlaix, and Mehdi Rezagholizadeh. *Fully Quantized Transformer for Improved Translation.* arXiv preprint arXiv:1910.10485 (2019). --Tang, Raphael, et al. *Distilling Task-Specific Knowledge from BERT into Simple Neural Networks.* arXiv preprint arXiv:1903.12136 (2019). --Sanh, Victor, et al. *DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.* arXiv preprint arXiv:1910.01108 (2019). --Ziheng Wang, et al. *Structured Pruning of Large Language Models.*"}
{"id": "iclr2020_286", "title": "Information lies in the eye of the beholder: The effect of representations on observed mutual information | OpenReview", "abstract": "Abstract:###Learning can be framed as trying to encode the mutual information between input and output while discarding other information in the input. Since the distribution between input and output is unknown, also the true mutual information is. To quantify how difficult it is to learn a task, we calculate a observed mutual information score by dividing the estimated mutual information by the entropy of the input. We substantiate this score analytically by showing that the estimated mutual information has an error that increases with the entropy of the data. Intriguingly depending on how the data is represented the observed entropy and mutual information can vary wildly. There needs to be a match between how data is represented and how a model encodes it. Experimentally we analyze image-based input data representations and demonstrate that performance outcomes of extensive network architectures searches are well aligned to the calculated score. Therefore to ensure better learning outcomes, representations may need to be tailored to both task and model to align with the implicit distribution of the model.", "review": " The paper proposes to introduce the notion of Observed Mutual Information (OMI), that captures how easily a given model can extract information from the data, and show correlation between this measure and the performance of the model. However, the reason why the OMI score should be a good predictor of the model performance is not clearly justified in the paper, either formally or informally. In fact, I believe it is easy to construct examples where the OMI score can vary arbitrarily, while the difficulty of the learning task remains virtually unchanged. For example, let p(x) be the distribution of MNIST digits, to which arbitrary amount of noise is added to one corner of the image. This increases H(x) arbitrarily, and hence arbitrarily varies the OMI score. On the other hand, any network can easily learn to solve the task by just ignoring the noise in the (unused) corner. In general, it seems to me that for images the OMI score will depend almost entirely on the amount of noise or variability in the image, rather than from the actual difficulty of the task. The idea that Shannon Mutual Information does not capture the actual complexity of extracting the information is also already well explored in the literature. For example, Kolmogorov (or Algorithmic) Mutual Information explicitly account for the complexity of the program extracting the representation, and will change based on how data is represented. Montanari (https://arxiv.org/abs/1409.3821) explores the computational implications of representing the data through a minimal sufficient statistic. Achille and Soatto (https://arxiv.org/abs/1905.12213) study the relation between complexity of the learning task, complexity of the DNN, and *effective information* contained in the activations. Regarding the bounds on entropy and mutual information described at page 3-5, they seem to be vacuous for most problems of interest, since H(x) will be very large in those cases (even for MNIST H(x) is estimated to be ~80 nats, which would require a very large number of samples to properly estimate mutual information using the given bound)."}
{"id": "iclr2020_287", "title": "EINS: Long Short-Term Memory with Extrapolated Input Network Simplification | OpenReview", "abstract": "Abstract:###This paper contrasts the two canonical recurrent neural networks (RNNs) of long short-term memory (LSTM) and gated recurrent unit (GRU) to propose our novel light-weight RNN of Extrapolated Input for Network Simplification (EINS). We treat LSTMs and GRUs as differential equations, and our analysis highlights several auxiliary components in the standard LSTM design that are secondary in importance. Guided by these insights, we present a design that abandons the LSTM redundancies, thereby introducing EINS. We test EINS against the LSTM over a carefully chosen range of tasks from language modelling and medical data imputation-prediction through a sentence-level variational autoencoder and image generation to learning to learn to optimise another neural network. Despite having both a simpler design and fewer parameters, this simplification either performs comparably, or better, than the LSTM in each task.", "review": "Review:###General Comments The authors proposed a new type of gated recurrent unit for RNNs, and show comparable performance on a number of RNN applications, some achieved with less number of parameters. Detailed Comments Section 3 (Interpretation) This section basically draws similarity between the RNN update equations with discrete temporal difference equations. It’s unclear why showing that the equations look similar is motivating. It would be better if the authors can borrow analyses done on differential equations and apply them here. Section 5 (Experiments) To me, the main contribution of this work lies (potentially) in the newly proposed EINS unit. To demonstrate that it’s practically appealing, below are my suggestions. # of parameters is only practically meaningful for saving storage. More interesting analyses should include memory cost during training/test (i.e., the total number of activations, and the required gradients for learning), and computational time. Ease of tuning (i.e., is this method easier to tune than LSTM/GRU). One benefit of a new method can be that it’s more robust to tuning-/hyper-parameters. If the authors can show that EINS is easier to tune, this can be a good result. Experiments are diverse, not I rather see a large scale experiment because the strength of the proposed method was motivated as a “simpler” method. The provided experiments do not strengthen the claim that EINS is “simpler” for the practitioners. Understandably, this will require a lot of effort, but many of the interesting applications of RNNs as of today are large-scale speech recognition, language modelling, and video recognition. It’s unclear whether EINS has any benefits in that regime. Overall, the authors demonstrated a variant of RNN updates, but neither the theoretical nor empirical evidence was strong enough to 1. Convince practitioners to switch from LSTM/GRU to EINS, 2. Provide insight/understanding to the workings of RNNs. It’s interesting to look for new building blocks for neural nets. However, perhaps more interesting than manually designing it, maybe you can use the reasonings in this paper to design constraints for neural architecture search model. Best,"}
{"id": "iclr2020_288", "title": "CP-GAN: Towards a Better Global Landscape of GANs | OpenReview", "abstract": "Abstract:###GANs have been very popular in data generation and unsupervised learning, but our understanding of GAN training is still very limited. One major reason is that GANs are often formulated as non-convex-concave min-max optimization. As a result, most recent studies focused on the analysis in the local region around the equilibrium. In this work, we perform a global analysis of GANs from two perspectives: the global landscape of the outer-optimization problem and the global behavior of the gradient descent dynamics. We find that the original GAN has exponentially many bad strict local minima which are perceived as mode-collapse, and the training dynamics (with linear discriminators) cannot escape mode collapse. To address these issues, we propose a simple modification to the original GAN, by coupling the generated samples and the true samples. We prove that the new formulation has no bad basins, and its training dynamics (with linear discriminators) has a Lyapunov function that leads to global convergence. Our experiments on standard datasets show that this simple loss outperforms the original GAN and WGAN-GP.", "review": "Review:###The paper attempts to perform global analysis of GAN on the issue of sub-optimal strict local minima and mode collapse, and proposes a new GAN formulation (CoupleGAN) that enjoys nice global properties. The paper is overall well written and conveys an interesting new formulation of GANs. However, the reviewer is concerned with the following questions: The paper is mainly on analyzing the case when the true data has n points instead of on a continuous support. It would be more interesting to see theoretical guarantee on even Gaussian mixture model. Also since GANs are mostly known for generalizing what is seen to generate new data, whether converging only to the n points are good or not still worth debating. In claim 4.2 and 4.3, what if the initialization of y is completely random? Then the claim cannot say anything on mode collapse. So is the formulation in the paper the real characterization of mode collapse?"}
{"id": "iclr2020_289", "title": "Parallel Neural Text-to-Speech | OpenReview", "abstract": "Abstract:###In this work, we first propose ParaNet, a non-autoregressive seq2seq model that converts text to spectrogram. It is fully convolutional and obtains 46.7 times speed-up over Deep Voice 3 at synthesis while maintaining comparable speech quality using a WaveNet vocoder. ParaNet also produces stable alignment between text and speech on the challenging test sentences by iteratively improving the attention in a layer-by-layer manner. Based on ParaNet, we build the first fully parallel neural text-to-speech system using parallel neural vocoders, which can synthesize speech from text through a single feed-forward pass. We investigate several parallel vocoders within the TTS system, including variants of IAF vocoders and bipartite flow vocoder.", "review": "Review:###This paper proposes improvements to the TTS architecture in DeepVoice3. For the most part (as far as I am aware), neural architectures for TTS are encoder-decoder based. The backbone can either be an RNN or a wavenet style model (autoregressive). Synthesizing audio from the feature representation (mel spectrogram) by a neural vocoder is usually an autoregressive model from the wavenet family. In the current work, the main novelties seem to be the following: 1) Replace the autoregressive component in the decoder (seemingly inspired by DeepVoice3) with a non-autoregressive model. This could be very advantageous because synthesis in autoregressive models can be quite slow owing to the sample level generations (and to overcome this defect, faster sampling with inverse autoregressive flows has been used in parallel wavenet, clarinet, etc. and probability density distillation). Here, if I can interpret the paper correctly, the authors use a trained attention model (autoregressive), to distill attention for the non-autoregressive setup used, which would otherwise have difficulties learning alignment. The paper claims speed up of ~50X over DeepVoice 3 which is very significant. 2) In the mel to audio converter (vocoder), the proposal is a VAE Wavenet, with appropriate modifications for the sequence modeling. The authors mention that there are similarities to the approach used in Clarinet (this has a closed form KLD between the distilled distributions, which makes things easy). Experiments: It is mentioned that poor attention alignments are the cause of many issues in these architectures (repeats, mispronunciations, skipped words, etc.), and they go on to show that their architecture fares well as regards these metrics, while maintaining a comparable MOS score with DeepVoice3+Wavenet. Evaluations: I am reasonably convinced by the audio demos provided. My thoughts: The paper is generally a good addition to the TTS literature. These are difficult to implement, brittle setups, and a practitioner could spend a lot of time debugging their broken attention curves. To that end, I feel that any technique that throws light into the modeling process would be useful for the practitioner. It is suggested that we can use a non-autoregressive model with speedups. Likewise, they also use a wavenet VAE, which they claim can be trained without distillation (could the authors please clarify this point?) I do, however, feel that the paper has a few drawbacks. 1) The presentation is not at all clear. This is a very subjective comment, but I feel that this work might be unreadable to someone who hasn*t studied the literature (starting from Tacotron, DeepVoice 1, 2, 3, wavenet, parallel wavenet, transformer, distillation, etc.). Furthermore, the paper does not seem to make things any clearer with succinct architecture diagrams. Just as a comparison, I would like to draw attention to Tacotron (1, 2) in which I think the details can actually be worked out with some effort. 2) Why are we using the WaveVAE - is this just to do away with the probability distillation in wavenet? Are we also using the IAF setup as described in Kingma*s work? 3) I really feel that we need more architecture diagrams for this work to be useful. That being said, the authors do provide a list of hyperparameters for potential use. 4) Distilling attention seems to require a previously trained autoregressive attention model. What if we don*t have one ready at hand? In summary, while I see that the work will definitely be useful to the Speech Synthesis practitioner, the clarity of the paper could be improved and we need a few more diagrams (maybe even code) to make it implementable."}
{"id": "iclr2020_290", "title": "Discriminative Variational Autoencoder for Continual Learning with Generative Replay | OpenReview", "abstract": "Abstract:###Generative replay (GR) is a method to alleviate catastrophic forgetting in continual learning (CL) by generating previous task data and learning them together with the data from new tasks. In this paper, we propose discriminative variational autoencoder (DiVA) to address the GR-based CL problem. DiVA has class-wise discriminative latent embeddings by maximizing the mutual information between classes and latent variables of VAE. Thus, DiVA is directly applicable to classification and class-conditional generation which are efficient and effective properties in the GR-based CL scenario. Furthermore, we use a novel trick based on domain translation to cover natural images which is challenging to GR-based methods. As a result, DiVA achieved the competitive or higher accuracy compared to state-of-the-art algorithms in Permuted MNIST, Split MNIST, and Split CIFAR10 settings.", "review": "Review:###-- This paper seeks to combine several ideas together to propose an approach for image classification based continual learning tasks. In this effort, the paper combines previously published approaches from generative modeling with VAEs, mutual information regularization and domain adaptation. I am a making a recommendation for reject for this paper with the main reason being that I believe the primary derivations for their method appear flawed. --In the main section describing the approach (Section 4), the authors start with a claim that Equation 1 and 2 are equal; I don’t believe 1 and 2 are equal. --In Section 4.1, it appears that they are instead making a claim about Equation 2 being a bound for equation 1; but even this derivation appears to have a problem. The following is the concern: --In the second line of Equation 5, the KL term appears to be measuring a distance between distributions on two different variables; z|c and c|z. If one were to interpret the second one as the unnormalized distribution on z defined via the likelihood for c given z; even this has an issue because then the expression for KL where we plug the unnormalized density in place of the normalized need not be positive which is something they need to derive their bound. --Another issue is that the regularization lambda should apply to both the terms in the bound but in Equation (7) only appears selectively for one of the two terms. It is also not clear how the loss function proposed differs from that of the CDVAE, etc. If the novelty is in applying to continual learning and new datasets, it is not clear that this is sufficient. Additional feedback for authors (not part of the main decision reasoning): - What is dt in Algorithm 1 description? Figure 1: -typo “implmented” -What’s the 3d plot supposed to represent? Doesn*t the classification loss have a dependency on the input condition? --What does a *heavy classifier* imply concretely? --“Redundant weights” seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper). --The notation for the proposed parameters theta, theta’, phi, phi’ are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder. In later sections they use theta and theta’ for encoder/decoder resp. -- “When the encoder and decoder networks are sufficiently complex, it is enough to implement each the prior and classification network as one fully-connected layer” ? what do the authors mean “when … networks are sufficiently complex” or do they actually mean when the “when the problem is simple enough”?"}
{"id": "iclr2020_291", "title": "SVQN: Sequential Variational Soft Q-Learning Networks | OpenReview", "abstract": "Abstract:###Partially Observable Markov Decision Processes (POMDPs) are popular and flexible models for real-world decision-making applications that demand the information from past observations to make optimal decisions. Standard reinforcement learning algorithms for solving Markov Decision Processes (MDP) tasks are not applicable, as they cannot infer the unobserved states. In this paper, we propose a novel algorithm for POMDPs, named sequential variational soft Q-learning networks (SVQNs), which formalizes the inference of hidden states and maximum entropy reinforcement learning (MERL) under a unified graphical model and optimizes the two modules jointly. We further design a deep recurrent neural network to reduce the computational complexity of the algorithm. Experimental results show that SVQNs can utilize past information to help decision making for efficient inference, and outperforms other baselines on several challenging tasks. Our ablation study shows that SVQNs have the generalization ability over time and are robust to the disturbance of the observation.", "review": "Review:###The paper proposes SVQN, an algorithm for POMDPs based on the soft Q-learning framework which uses recurrent neural networks to capture historical information for the latent state inference. In order to obtain this formulation, the author first derive the variational bound for POMDPs and then present a practical algorithm. The key idea of the paper is to replace DQN with Soft Q-learning that already demonstrated better performance on a variety of tasks. This seems to be an obvious extension of DRQNs (Hausknecht & Stone, 2015) even though it did not appear in the literature. The authors evaluate the final algorithm on a set of ALE and DoomViz tasks. The algorithm outperforms the previous methods, in particular, DRQNs. The set of tasks and prior methods is adequate. Overall, the contribution of the paper is not significant enough to be accepted to ICLR."}
{"id": "iclr2020_292", "title": "Out-of-Distribution Detection Using Layerwise Uncertainty in Deep Neural Networks | OpenReview", "abstract": "Abstract:###In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.", "review": "Review:###This paper tackles out-of-distribution samples detection via training VAE-like networks. The key idea is to inject learnable Gaussian noise to each layer across the network in the hope that the variance of the noise correlates well with the uncertainty of the input features. The network is trained to minimize the empirical loss subject to noise perturbation. The paper is well written, and the background is introduced clearly. As I understand it, the goal of *out-of-distribution sample detection* is to train a deep network that simultaneously generalizes well and also be discriminative to outliers. However, it’s not clear to me why the proposed method server this purpose; empirical results are not convincing either. My major concerns are as follows: First of all, from my intuition, it would be much easier to train deterministic networks than their counterparts with randomness. Empirically, researchers also often observe near-zero training loss for large deterministic networks such as Dense-BC trained on simple CIFAR/SVHN datasets. Especially, in this case, the training goal is simply to map higher-dimensional inputs to lower-dimensional classification categories. That being said, one would expect the variances go to zero at convergence to achieve lower empirical loss in the case of no additional diversity (or uncertainty) promotion terms. It is not clear to me how to avoid degenerate solutions at convergence while maintaining good testing performance with the proposed training strategy. From the empirical results, it also appears that all models reported might not be fully optimized? The baseline results are significantly worse than those reported in previous work. Specifically, in table 1, the testing accuracy of Dense-BC trained on CIFAR-100 is only 71.6. In table 2, the reported testing accuracy on CIFAR-10 using Dense-BC is 92.4. However, the results of DenseNet-BC (k=12, L=100, table 2) reported in the original paper are: CIFAR10 94.0 (also leave 5K examples as validation set) CIFAR100 75.9 Meanwhile, the reported accuracy of WRN-40-4 trained on CIFAR-10 and CIFAR-100 are 89.6 and 66.0, respectively. However, the corresponding baseline numbers in the original WRN paper are much higher, CIFAR-10 95.03 CIFAR-100 77.11 Could the authors comment on that? References: Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger. Densely Connected Convolutional Networks https://arxiv.org/abs/1608.06993 Sergey Zagoruyko, Nikos Komodakis. Wide Residual Networks. https://arxiv.org/pdf/1605.07146.pdf"}
{"id": "iclr2020_293", "title": "ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring | OpenReview", "abstract": "Abstract:###We improve the recently-proposed ``MixMatch semi-supervised learning algorithm by introducing two new techniques: distribution alignment and augmentation anchoring. - Distribution alignment encourages the marginal distribution of predictions on unlabeled data to be close to the marginal distribution of ground-truth labels. - Augmentation anchoring} feeds multiple strongly augmented versions of an input into the model and encourages each output to be close to the prediction for a weakly-augmented version of the same input. To produce strong augmentations, we propose a variant of AutoAugment which learns the augmentation policy while the model is being trained. Our new algorithm, dubbed ReMixMatch, is significantly more data-efficient than prior work, requiring between 5 times and 16 times less data to reach the same accuracy. For example, on CIFAR-10 with 250 labeled examples we reach 93.73% accuracy (compared to MixMatch*s accuracy of 93.58% with 4000 examples) and a median accuracy of 84.92% with just four labels per class.", "review": "Review:###This paper presents ReMixMatch an improved version of MixMatch. The main contributions are the distribution alignment and the augmentation anchoring. Distribution alignment rescales the predictions based on the difference between the model marginals and the ground truth running average estimation. Augmentation anchoring instead of computing the guessed probabilities on unlabelled data as the average probabilities on transformed samples (as in MixMatch), it considers as guessed labels the average probabilities obtained from weak transformations (flip+crop) even when using stronger transformations (Autoaugment like). The paper is well written, has interesting experiments and very impressive results. However, there are some negative points that the authors should clarify: - The final method is a mixup of many different techniques, thus, not a strong contribution, but many smaller contributions. - As shown in the ablation study, the main contribution on the obtained results seems to be the use of stronger transformations than in MixMatch. This is not so interesting, even though results are impressive. If this is the case, authors should state it more clearly in the paper that a large proportion of the gap in performance between MixMatch and ReMixMatch is the introduction of stronger transformations (Autoaugment style). Overall the paper is well presented and contributes to further improve the performance on semi-supervised learning. I there fore recommend it for acceptance. However, I would like to see in the paper a more general overview on the fact that strong transformations can further improve semi-supervised methods and ReMixMatch is a way to leverage those transformations. Additional comments: - Instead of using the rescaling trick for distribution alignment, what about enforcing the marginal distribution on the annotated data and the marginal distribution of the model to be similar with KL divergence? Would it be better or worse than the proposed approach?"}
{"id": "iclr2020_294", "title": "Training Neural Networks for and by Interpolation | OpenReview", "abstract": "Abstract:###In modern supervised learning, many deep neural networks are able to interpolate the data: the empirical loss can be driven to near zero on all samples simultaneously. In this work, we explicitly exploit this interpolation property for the design of a new optimization algorithm for deep learning. Specifically, we use it to compute an adaptive learning-rate in closed form at each iteration. This results in the Adaptive Learning-rates for Interpolation with Gradients (ALI-G) algorithm. ALI-G retains the main advantage of SGD which is a low computational cost per iteration. But unlike SGD, the learning-rate of ALI-G uses a single constant hyper-parameter and does not require a decay schedule, which makes it considerably easier to tune. We provide convergence guarantees of ALI-G in the stochastic convex setting. Notably, all our convergence results tackle the realistic case where the interpolation property is satisfied up to some tolerance. We provide experiments on a variety of architectures and tasks: (i) learning a differentiable neural computer; (ii) training a wide residual network on the SVHN data set; (iii) training a Bi-LSTM on the SNLI data set; and (iv) training wide residual networks and densely connected networks on the CIFAR data sets. ALI-G produces state-of-the-art results among adaptive methods, and even yields comparable performance with SGD, which requires manually tuned learning-rate schedules. Furthermore, ALI-G is simple to implement in any standard deep learning framework and can be used as a drop-in replacement in existing code.", "review": "Review:###This paper addresses designing and analyzing an optimization algorithm. Like SGD, it maintains a low computational cost per optimization iteration, but unlike SGD, it does not require manually tuning a decay schedule. This work uses the interpolation property (that the empirical loss can be driven to near zero on all samples simultaneously in a neural network) to compute an adaptive learning rate in closed form at each optimization iteration, and results show that this method produces state-of-the-art results among adaptive methods. I can say the paper was very well written and easy to follow along/understand. Prior work seems comprehensive, and the intuitive comparisons to the prior methods were also useful for the reader. My current decision is a weak accept, for a well-written paper, thorough results including meaningful baselines and numerous hyperparameter searches, and a seemingly high-impact tool. Some concerns are listed as follows: 1- Convergence was only discussed in the stochastic convex setting, which seems limiting because we rarely deal with convex problems in problems requiring neural networks. 2- Regularization of the weights during the optimization is dealt with by projecting onto the feasible set of weights, but it seems like there are other types of losses that don’t necessarily to go 0. For example, terms in the objective such as entropy seem worrisome. 3- One detail that I did not fully follow along with is section 3.1. How does Theorem 1 (Regarding convexity) related to the “each training sample can use its own learning rate without harming progress on the other ones” and/or “allow the updates to rely on the stochastic estimate rather than the exact”? 4- Unfortunately, I am not an expert in this particular area, so I’m not confident about the novelty. For example, the difference between L4 and this is stated to be the utilization of the interpolation policy (which just sets f*=0) and the maximal learning rate, and the stated benefit of convergence guarantees in stochastic convex settings seems poor since most problems will not be convex anyway. More generally, it seems like all details of the algorithm came from elsewhere, although the presented synthesis of ideas does have clear benefits. After reading the author response: -I*m fine with points 4 and 3. -My feelings about 1 are still the same. -My comment on 2 wasn*t about cross-entopy loss, but rather other types of objectives that people are often interested in optimizing (such as max-ent RL, where we aim for maximizing rewards as well as maximizing entropy of the policy), in which case, it*s not clear to me how we could apply this optimizer. -My decision stays as a weak accept"}
{"id": "iclr2020_295", "title": "AutoQ: Automated Kernel-Wise Neural Network Quantization | OpenReview", "abstract": "Abstract:###Network quantization is one of the most hardware friendly techniques to enable the deployment of convolutional neural networks (CNNs) on low-power mobile devices. Recent network quantization techniques quantize each weight kernel in a convolutional layer independently for higher inference accuracy, since the weight kernels in a layer exhibit different variances and hence have different amounts of redundancy. The quantization bitwidth or bit number (QBN) directly decides the inference accuracy, latency, energy and hardware overhead. To effectively reduce the redundancy and accelerate CNN inferences, various weight kernels should be quantized with different QBNs. However, prior works use only one QBN to quantize each convolutional layer or the entire CNN, because the design space of searching a QBN for each weight kernel is too large. The hand-crafted heuristic of the kernel-wise QBN search is so sophisticated that domain experts can obtain only sub-optimal results. It is difficult for even deep reinforcement learning (DRL) DDPG-based agents to find a kernel-wise QBN configuration that can achieve reasonable inference accuracy. In this paper, we propose a hierarchical-DRL-based kernel-wise network quantization technique, AutoQ, to automatically search a QBN for each weight kernel, and choose another QBN for each activation layer. Compared to the models quantized by the state-of-the-art DRL-based schemes, on average, the same models quantized by AutoQ reduce the inference latency by 54.06%, and decrease the inference energy consumption by 50.69%, while achieving the same inference accuracy.", "review": "Review:###This paper proposes a new method for quantizing neural network weights and activations that uses deep reinforcement learning to select the appropriate bitwidth for individual kernels in each layer. The algorithm uses a reward function that weights accuracy of the quantized model with latency, energy, and FPGA area, and leverages a high level and low-level controller to create quantized models that can take into account these factors. Compared to prior approaches taht only perform layer-wise instead of kernel-wise quantization, the quantized models can achieve better performance, or latency. While the problem of more effectively quantizing neural network weights and activations is interesting, I found this paper hard to follow and evaluate. The proposed hierarchical deep RL method mentions a number of tricks and design decisions that are not ablated, and the notation and text around the method were difficult to follow. There were also no baselines comparing to other approaches for performing kernel-wise quantization (e.g. random search, quantize based off variance, etc.). Without improved baselines and text I would recommend rejecting this paper. Major comments: * Based on the analysis of kernel-wise results, it seems that a very simple strategy that chooses QBNs based on weight variance could be sufficient to achieve good performance. However, there’s no comparisons to these kinds of heuristics or even to random search over QBN per-kernel. There are also no ablations that study whether the hierarchical-RL based approach beats a baseline that just directly predicts the kernel-wise QBNs independently. Without these baselines, it’s hard to know how well AutoQ works. * The text that details the hierarchical RL approach with multiple controllers that is core to the new AutoQ approach is extremely hard to follow, with many undefined symbols. Minor comments: * FIg 1: where in the neightwork do these weight kernels come from? What network/dataset? * You repeatedly state that ML experts obtain only sub-optimal results at selecting QBNs, can you cite something for this? What if you give experts access to additiional information like visualizations of weight distributions? * What’s the tradeoff between using different QBNs for each kernel and specifying this number? Is there additional memory overhead? * Table 1: hard to read the exponent in kernel-wise math, add parentheses * Please use citep vs. citet where appropriate * Why not also search for kernel-specific activation quantization? * The notation in Fig 3 and surrounding text is really hard to follow, e.g. w_w, h_w, and indices into states (roundup could be ceil, names are confusing iRd, eRd?). * Eqn 2 comes out of nowhere… why optimize for log(accuracy) vs accuracy? How do you choose the user-defined constants? Decay factor? * Hardware overhead estimator: are these models accurate independent of QBN? * Why do you have to perform Gaussian augmentation? I didn’t follow the re-labeling of transitions after Eqn. 6, is this needed? * Define mu when you first use it (Eqn 4?) * Is delta_a in implementation details the same as sigma around Eqn 4? * Table 3: Is there variance in these results based off fine-tuning? Could you include error bars or stderr on these estimates? * Figure 6: are the search spaces different for these different approaches? I.e. does AutoQ perform better due to the upper/lower bounds you set on QBN? ============= Thank you to the authors for addressing many of my concerns. The updated paper still does not present any baselines for methods that can search or learn kernel-wise quantization parameters, e.g. run random search and select the best-performing models according to your reward function. Additionally, there were no revisions made to the text to improve the presentation, and thus I continue to recommend rejection of this work."}
{"id": "iclr2020_296", "title": "LSTOD: Latent Spatial-Temporal Origin-Destination prediction model and its applications in ride-sharing platforms | OpenReview", "abstract": "Abstract:###Origin-Destination (OD) flow data is an important instrument in transportation studies. Precise prediction of customer demands from each original location to a destination given a series of previous snapshots helps ride-sharing platforms to better understand their market mechanism. However, most existing prediction methods ignore the network structure of OD flow data and fail to utilize the topological dependencies among related OD pairs. In this paper, we propose a latent spatial-temporal origin-destination (LSTOD) model, with a novel convolutional neural network (CNN) filter to learn the spatial features of OD pairs from a graph perspective and an attention structure to capture their long-term periodicity. Experiments on a real customer request dataset with available OD information from a ride-sharing platform demonstrate the advantage of LSTOD in achieving at least 6.5% improvement in prediction accuracy over the second best model.", "review": "Review:###Review Summary -------------- While I think there are some interesting innovations here, I don*t think this is ready for ICLR. I have concerns that the evaluation doesn*t focus enough on application-relevant scenarios (why exclude cells that are not in the top 50? why not use 7-day windows to capture day-of-week effects?), and that the evaluations don*t quantify uncertainty (and thus the claimed improvements due to attention may not be significant). The presentation quality of the method details in Sec. 3 needs a significant rewrite to improve clarity. I do think the non-standard convolution operator is a nice idea. Paper Summary ------------- This paper addresses the forecasting of origin-destination demand data, with a focus on ride-sharing transportation applications. We assume an urban area has been divided up into N cells (known in advance). The problem is to forecast demand for *directed* rides from cell i to cell j, using historical demand data. The paper*s first contribution is the design of a convolutional neural net architecture that uses non-rectangular receptive fields more suitable to origin-destination data. Instead of assuming that cells in a given 2D euclidean neighborhood are correlated, it assumes that all journeys that overlap with the one of the cells in the current origin-destination pair are similar. They call this the *SACN* architecture. The second contribution is developing an *ST-Conv* block that combines the above SACN convolution with gated temporal convolutions, so that both space and time can be summarized via convolution operators. A periodically-shifting attention mechanism is further used to measure long-term data*s similarities to short-term data and weight accordingly. Overall, these two ideas (SACN convolutions and ST-Conv blocks with attention) are combined into what they call their Latent Spatio-Temporal Origin-Destination or *LSTOD* architecture, which is claimed to be the first to use both short-term and long-term features in prediction. Evaluation examines demand for ride-sharing in two major cities, A and B (presumably masked because they are proprietary). RMSE error comparing to several classic (e.g. ARIMA) and deep feature learning (e.g. LSTMs and SRCN) baselines, with primary results in Table 1. A few side experiments examine superiority over standard convolutions (Fig 3) and experiments without the attention and long-term bits of the model (Table 2). Novelty & Significance ----------------------- The current paper is quite niche when it comes to significance; I think it may be a bit too specialized for most ICLR readers. Solving this kind of forecasting problem seems important to ride-sharing applications, but is highly specialized for origin-destination demand data. Would be nice to see the paper attempt to connect to other problems beyond ride-sharing (maybe animal migration? maybe package logistics?). That said, I think there*s sufficient novelty. The architectural design contributions here do appear new to me (though I don*t follow this kind of data closely). Method Concerns ------------------ ## M1: Complexity analysis missing, scaling could be a problem When comparing the current square receptive field architecture for CNNs with the proposed SACN, I felt there was an opportunity to clarify how both scale with N and other key problem size parameters in terms of number of parameters or execution time. I*m concerned that it will be non-scalable given the O(N) cost in terms of number of parameters and the O(N^2) cost of runtime (you need to execute Eq. 2 for each of the N^2 entries at each layer). I*d like to see some careful breakdown of this compared to other approaches. Experimental Concerns --------------------- ## E1: Why not use a 7-day past history? Shouldn*t ride share demand have a day-of-week trend? Why wouldn*t we use last Sunday*s demand to predict this Sunday*s demand? I find it quite odd that for the Historical Average baseline, only the last 5 days (rather than last 7) are used, and for the presented method, only the last 3 days are used. I*d be happy to be proven wrong, but I*d guess including a 7-day window would lead to noticeably better results. ## E2: Lack of error bars / uncertainty quantification A natural question is, can we reliably tell that the difference between (for example) RMSE 2.49 and 2.54 is significant and not noise? I*d like to see some attempt at quantifying the uncertainty for measurements in Table 1 (perhaps taking each full day in test set as its own *mini* test set that produces one RMSE score, then reporting average as well as 2.5th and 97.5th percentiles or something across each day). Without this, I think the claim that attention is useful here is unproven, since the change in performance is so small (less than 0.1 RMSE). ## E3: Why focus only on the N=50 most common cells? I would think to really assess demand forecasting, you want to know when to task drivers to visit less-common cells. The focus on the cells with only the top 80% of journeys means that 1 of every 5 rides would not be covered by this prediction system. I wonder if part of the reason is that using N much larger than 50 is problematic due to the scaling mentioned earlier. Presentation Concerns --------------------- ## P1: Need to simplify notation I found the descriptions of the neural net architecture throughout Sec. 3 quite hard to parse. I think there*s an overreliance on math notation and there could be more simple description of high level intent and motivation. For example, Eq. 2 could be simplified to avoid the channel *m* and layer *l* notation and thus let the reader focus on what matters, which is how any part of input A that involves the origin node i or the destination node j is included in the weighted sum. Words can be used around this to clarify this same operation can happen across channels and layers. Similarly, Eq. 3 and Eq. 4 could perhaps be replaced by a good diagram. Currently, Eq. 3 both P and Q are undefined and quite confusing. ## P2: Name of the convolution operation I*m not sure *SACN* is the best name. The receptive field of a standard CNN looks much more *spatially adjacent* to me (a compact rectangle surrounding the target cell). Instead, you might call it *topologically adjacent* or even something like *vertex adjacent*. You want to emphasize that you are getting strength by finding all journeys whose start or end overlaps with one of the endpoints of the current journey."}
{"id": "iclr2020_297", "title": "Decoupling Hierarchical Recurrent Neural Networks With Locally Computable Losses | OpenReview", "abstract": "Abstract:###Learning long-term dependencies is a key long-standing challenge of recurrent neural networks (RNNs). Hierarchical recurrent neural networks (HRNNs) have been considered a promising approach as long-term dependencies are resolved through shortcuts up and down the hierarchy. Yet, the memory requirements of Truncated Backpropagation Through Time (TBPTT) still prevent training them on very long sequences. In this paper, we empirically show that in (deep) HRNNs, propagating gradients back from higher to lower levels can be replaced by locally computable losses, without harming the learning capability of the network, over a wide range of tasks. This decoupling by local losses reduces the memory requirements of training by a factor exponential in the depth of the hierarchy in comparison to standard TBPTT.", "review": "Review:###Summary: The paper introduces a hierarchical RNN architecture that could be trained more (memory) efficiently. The difference in the architecture seems to be an auxiliary loss that decodes k step inputs and some perturbation of TBPTT. Comments on the paper 1. The paper seems to be have been written in a rush. The language could be improved, the format is not always consistent and in general the paper could be much better written. There are quite some typos as well in the paper, for example , Trinh et al. is not a proper citation. 2. The authors mentioned that TBPTT is not memory efficient, this is not very clear to me, as it only needs to keep the number of truncation steps that it backprops through and hence much more memory efficient compared to full BPTT. 3. It is not clear to me what is the benefit of gr-HMRNN. It is not clear why cutting of the gradients from the higher level to the lower level would help. 4. It is surprising to me that HMRNN could only solve the copy task upto a length of 108. 5. I would also suggest another copy task from Hochreiter, Sepp and Schmidhuber, Jürgen. Long short-term memory. Neural computation, 9(8): 1735–1780, 1997. In general, the paper seems to have been written in a rush. I would recommend the papers to be revised."}
{"id": "iclr2020_298", "title": "HyperEmbed: Tradeoffs Between Resources and Performance in NLP Tasks with Hyperdimensional Computing enabled embedding of n-gram statistics | OpenReview", "abstract": "Abstract:###Recent advances in Deep Learning have led to a significant performance increase on several NLP tasks, however, the models become more and more computationally demanding. Therefore, this paper tackles the domain of computationally efficient algorithms for NLP tasks. In particular, it investigates distributed representations of n-gram statistics of texts. The representations are formed using hyperdimensional computing enabled embedding. These representations then serve as features, which are used as input to standard classifiers. We investigate the applicability of the embedding on one large and three small standard datasets for classification tasks using nine classifiers. The embedding achieved on par F1 scores while decreasing the time and memory requirements by several times compared to the conventional n-gram statistics, e.g., for one of the classifiers on a small dataset, the memory reduction was 6.18 times; while train and test speed-ups were 4.62 and 3.84 times, respectively. For many classifiers on the large dataset, the memory reduction was about 100 times and train and test speed-ups were over 100 times. More importantly, the usage of distributed representations formed via hyperdimensional computing allows dissecting the strict dependency between the dimensionality of the representation and the parameters of n-gram statistics, thus, opening a room for tradeoffs.", "review": "Review:###This paper shows a trade-off relationship between computational cost (memory usage, train/test time) and performance on several NLP machine learning algorithms that use n-gram statistics. The authors claim that a simple n-gram representation vector with a conventional classifier (MLP, SVC, Naive Bayes...) is computationally efficient. The large set of experiments on various conventional NLP models and n-gram statistics provide detail information about the trade-off relation between performance and computational cost. My concern is that the computational efficiency of the conventional NLP model is well known to NLP researchers. It would be nice if the authors provide a more persuasive explanation for the importance of this research question."}
{"id": "iclr2020_299", "title": "Sign-OPT: A Query-Efficient Hard-label Adversarial Attack | OpenReview", "abstract": "Abstract:###We study the most practical problem setup for evaluating adversarial robustness of a machine learning system with limited access: the hard-label black-box attack setting for generating adversarial examples, where limited model queries are allowed and only the decision is provided to a queried data input. Several algorithms have been proposed for this problem but they typically require huge amount (>20,000) of queries for attacking one example. Among them, one of the state-of-the-art approaches (Cheng et al., 2019) showed that hard-label attack can be modeled as an optimization problem where the objective function can be evaluated by binary search with additional model queries, thereby a zeroth order optimization algorithm can be applied. In this paper, we adopt the same optimization formulation but propose to directly estimate the sign of gradient at any direction instead of the gradient itself, which enjoys the benefit of single query. Using this single query oracle for retrieving sign of directional derivative, we develop a novel query-efficient Sign-OPT approach for hard-label black-box attack. We provide a convergence analysis of the new algorithm and conduct experiments on several models on MNIST, CIFAR-10 and ImageNet. We find that Sign-OPT attack consistently requires 5X to 10X fewer queries when compared to the current state-of-the-art approaches, and usually converges to an adversarial example with smaller perturbation.", "review": "Review:###In this paper, the authors propose a new algorithm for evaluating adversarial robustness of black-box models. The aim of the proposed algorithm, SIGN-OPT is generating adversarial examples as close as possible to the decision boundary using as less queries of the black-box model as possible. The authors follow the approach of (Cheng et al 2019) by modeling the problem as an optimization problem, where the objective function is to find the direction with the shortest distance to the decision boundary. They propose a smart modification of the previous approach by evaluating the sign of the gradient rather than the gradient itself. The advantage is that the sign of the gradient can be evaluated using a single query while the estimation of the gradient needs many queries. The authors have analyzed the proposed algorithm. They showed that using SIGN-OPT, the expectation of the gradient tends to zero in , meaning that a (local) minimum is reached. The algorithm is favorably compared with the state-of-the-art on three image test sets (MNIST, CIFAR-10n ImageNet). This paper is technically sound, well-written and propose an interesting modification of a previous algorithm. I vote for acceptance. However, I have some concerns: - I think that the L-smoothness assumption should be discussed. Is it realistic for Deep Learning models? Does it hold for the three attacked CNN networks? - The analytical results of the previous algorithm RGF and the proposed algorithm SIGN-OPT are not compared and discussed. It is a pity."}
{"id": "iclr2020_300", "title": "Evaluating Lossy Compression Rates of Deep Generative Models | OpenReview", "abstract": "Abstract:###Deep generative models have achieved remarkable progress in recent years. Despite this progress, quantitative evaluation and comparison of generative models remains as one of the important challenges. One of the most popular metrics for evaluating generative models is the log-likelihood. While the direct computation of log-likelihood can be intractable, it has been recently shown that the log-likelihood of some of the most interesting generative models such as variational autoencoders (VAE) or generative adversarial networks (GAN) can be efficiently estimated using annealed importance sampling (AIS). In this work, we argue that the log-likelihood metric by itself cannot represent all the different performance characteristics of generative models, and propose to use rate distortion curves to evaluate and compare deep generative models. We show that we can approximate the entire rate distortion curve using one single run of AIS for roughly the same computational cost as a single log-likelihood estimate. We evaluate lossy compression rates of different deep generative models such as VAEs, GANs (and its variants) and adversarial autoencoders (AAE) on MNIST and CIFAR10, and arrive at a number of insights not obtainable from log-likelihoods alone.", "review": "Review:###This paper considers the rate-distortion tradeoffs of deep generative models such as variational autoencoders (VAEs) and generative adversarial networks (GANs). The authors propose an annealed importance sampling (AIS) method to compute the rate-distortion curve efficiently. In experiments, the authors compare the rate-distortion curves for VAEs and GANs and discuss the properties of rate-distortion curves. The method for computing the rate-distortion curves of deep generative models is interesting and the rate-prior distortion curve is promising as a performance measure. However, the main technical contribution of this work is the estimated AIS rate-prior distortion curve and it is based on a straight-forward application of AIS. In fact, Sections 2 and 3 discuss already known result in literature although summarizing them in a paper is nice for readers. Although the findings in the experiments are interesting and insightful, they are still preliminary and further investigations are desirable. In Section 5, the authors mention the consistency of their framework with Shannon’s rate distortion theorem. This seems to be a little overstatement because the authors discuss little about the optimization of the prior p(z)."}
{"id": "iclr2020_301", "title": "Robust anomaly detection and backdoor attack detection via differential privacy | OpenReview", "abstract": "Abstract:###Outlier detection and novelty detection are two important topics for anomaly detection. Suppose the majority of a dataset are drawn from a certain distribution, outlier detection and novelty detection both aim to detect data samples that do not fit the distribution. Outliers refer to data samples within this dataset, while novelties refer to new samples. In the meantime, backdoor poisoning attacks for machine learning models are achieved through injecting poisoning samples into the training dataset, which could be regarded as “outliers” that are intentionally added by attackers. Differential privacy has been proposed to avoid leaking any individual’s information, when aggregated analysis is performed on a given dataset. It is typically achieved by adding random noise, either directly to the input dataset, or to intermediate results of the aggregation mechanism. In this paper, we demonstrate that applying differential privacy could improve the utility of outlier detection and novelty detection, with an extension to detect poisoning samples in backdoor attacks. We first present a theoretical analysis on how differential privacy helps with the detection, and then conduct extensive experiments to validate the effectiveness of differential privacy in improving outlier detection, novelty detection, and backdoor attack detection.", "review": "Review:###This paper leverages differential privacy’s stability properties to investigate its use for improved anomaly and backdoor attack detection. Under an assumption (called “uniformly asymptotic empirical risk minimization”), the authors show that difference between the expected loss of a differentially private learning algorithm on an outlier (where the expectation is taken over the randomness of the learning algorithm) and the expected loss of the same algorithm on data from the underlying distribution (expectation taken over data & randomness of the algorithm) is lower bounded by a (possibly/hopefully) non-negative quantity with high probability. The authors then conduct a set of experiments to show that differential privacy improves the performance of outliers, novel examples, and backdoor attack detection. Overall, the paper is very well written and easy to read. The paper also tackles an important and timely problem that is relevant to the ICLR community. While there has been some recent work on connecting differential privacy to robustness & attacks, this paper investigates the use of differential private model training as a means to improve novelty detection at inference time. A few points that need attention from the authors: 1. The theory developed is insightful in general but has very little (to no) practical value. For starters, it assumes that differentially private model training is uniformly asymptotic to empirical risk minimization. This is not necessarily true for highly non-convex models trained with SGD. Further, it cannot be verify via experimentations (despite the authors’ attempt to sanity check it using Figure 1). More importantly, the theory developed in Section 3 is not used in any meaningful way in the experiments section — the anomaly detection schemes are agnostic to it. 2. The authors make no attempt to co-optimize the performance of the model with its ability to be used for better anomaly detection. For instance, the authors choose an l2-clipping-norm C of 1 and do not consider trading off C with the noise variance. When the training set contains anomalies, this work can be viewed as “what is the impact of differential privacy” on a training sets with a majority group (training examples from a given distribution) and a minority group (training examples from a different distribution). Under this view, this paper essentially says that “differential privacy leads to disparate impact on model accuracy/loss”. This has been recently investigated in the following NeurIPS19 paper: https://arxiv.org/abs/1905.12101. Thus the contributions of the paper are not substantial."}
{"id": "iclr2020_302", "title": "Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network | OpenReview", "abstract": "Abstract:###One of the biggest issues in deep learning theory is the generalization ability of networks with huge model size. The classical learning theory suggests that overparameterized models cause overfitting. However, practically used large deep models avoid overfitting, which is not well explained by the classical approaches. To resolve this issue, several attempts have been made. Among them, the compression based bound is one of the promising approaches. However, the compression based bound can be applied only to a compressed network, and it is not applicable to the non-compressed original network. In this paper, we give a unified frame-work that can convert compression based bounds to those for non-compressed original networks. The bound gives even better rate than the one for the compressed network by improving the bias term. By establishing the unified frame-work, we can obtain a data dependent generalization error bound which gives a tighter evaluation than the data independent ones.", "review": "Review:###This paper obtains a compression-based generalization bound (Theorem 1) for the original network, while prior work gives bounds for the compressed network. The general bound given by Theorem 1 is further applied to networks with low-rank weight matrices (Theorem 2 and Corollary 1) or low-rank covariance matrices (Theorem 3 and 4). In some cases, the bound given by Theorem 1 for the original network could be better than the bound for the compressed network. In terms of proof techniques, Lemma 2 is a general result to control the local Rademacher complexity using upper bounds on the covering numbers, which is interesting and could be useful in other problems. On the other hand, there are two technical concerns. (1) In eq. (5), the covering number of {phi(f)-phi(g)} is bounded by the covering number of {f-g}, which is not necessarily true. For example, in the 1-dimensional case, it is possible that f-g is always 1, while phi(f)-phi(g) is not a constant. This example might appear since f and g are not freely chosen from F and G; they further need to satisfy the condition that |f-g|_{L_2} is bounded by r. If the claim in eq. (5) is indeed true, a proof is needed. (2) Despite the issue in (1), many bounds in the paper may actually be okay, since in the proofs the covering numbers of F (the original networks) are used (e.g., in eq. (6) and Lemma 2). Therefore it looks like the local Rademacher complexity of F can be controlled directly using Lemma 2. The question then is how compression helps in the analysis? I hope the above points can be clarified, and I would like to participate in the discussion."}
{"id": "iclr2020_303", "title": "Transformer-XH: Multi-hop question answering with eXtra Hop attention | OpenReview", "abstract": "Abstract:###Transformers have obtained significant success modeling natural language as a sequence of text tokens. However, in many real world scenarios, textual data inherently exhibits structures beyond a linear sequence such as tree and graph; an important one being multi-hop question answering, where evidence required to answer questions are scattered across multiple related documents. This paper presents Transformer-XH, which uses eXtra Hop attention to enable the intrinsic modeling of structured texts in a fully data-driven way. Its new attention mechanism naturally “hops” across the connected text sequences in addition to attending over tokens within each sequence. Thus, Transformer-XH better answers multi-hop questions by propagating information between multiple documents, constructing global contextualized representations, and jointly reasoning over multiple pieces of evidence. This leads to a simpler multi-hop QA system which outperforms previous state-of-the-art on the HotpotQA FullWiki setting by large margins.", "review": " The paper is proposing an extension of the Transformer matching and diffusion mechanism to multi-document settings. To do so, the authors introduce a special representation for gathering document level information which is then used for propagation among documents latent representations. The extension seems quite simple and natural. The method is evaluated on multi-hop machine reading over the hotpotqa dataset in the Fullwiki settings. However, it could have made sense to evaluate the method in the distractor settings too. In this context, the evidence graph where the model is trained is built using the canonical retrieval technique. Then, the method is using a pre-trained NER model to extract entities on the question and the candidate documents on Wikipedia for matching. Finally, a BERT ranker model is used to re-rank the retrieved candidate documents. The proposed method seems to heavily dependant on this hand-crafted extraction process. Unfortunately, one concern is that the reasoning model, while been quite original, is not tested in large scale retrieval cases to assess its robustness. Indeed, the number of retrieved documents to create the evidence graph seems to not have been mentioned. The method improves the current state of the art."}
{"id": "iclr2020_304", "title": "Efficient Multivariate Bandit Algorithm with Path Planning | OpenReview", "abstract": "Abstract:###In this paper, we solve the arms exponential exploding issues in multivariate Multi-Armed Bandit (Multivariate-MAB) problem when the arm dimension hierarchy is considered. We propose a framework called path planning (TS-PP) which utilizes decision graph/trees to model arm reward success rate with m-way dimension interaction, and adopts Thompson sampling (TS) for heuristic search of arm selection. Naturally, it is quite straightforward to combat the curse of dimensionality using a serial processes that operates sequentially by focusing on one dimension per each process. For our best acknowledge, we are the first to solve Multivariate-MAB problem using graph path planning strategy and deploying alike Monte-Carlo tree search ideas. Our proposed method utilizing tree models has advantages comparing with traditional models such as general linear regression. Simulation studies validate our claim by achieving faster convergence speed, better efficient optimal arm allocation and lower cumulative regret.", "review": "Review:###This paper approaches the problem of exploding arms in multivariate multi-armed bandits. To solve this problem, the authors suggest an approach that uses Thompson sampling for arm selection and decision trees and graphs (inspired from Monte-Carlo tree search) to model reward success rates. They propose to versions of this path planning procedure and 2 version inspired from Hill-climbing methods. They validate their claims on a simulation showing better performance and faster convergence, and providing an extensive analysis of the results. The idea seems novel. The paper is well structured, but the writing can be improved, and some parts are hard to read and follow (See minor comments below). Here are few questions: - The authors claim that the approach can be extended to categorical/numeric rewards. Can they give more details on how? - The experiments are done only with D=3 and N=10. How easy would it be to scale to higher dimensions? - In the curves of Figure 3, N^D-MAB and DS seem not converged yet contrarily to the other methods. Have the authors tried to let them run for longer to see to which values they converge? Minor comments (non-exhaustive examples): - Punctuation issues: *Multi-Armed Bandit (MAB) problem, is widely ... * Generally, RT is on the order O( T) under linear payoff settings Dani et al. (2008)Chu et al. (2011)Agrawal & Goyal (2013). Although the optimal regret of non-contextual Multivariate MAB is on the order O(logT ) ... - Imprecise statements: ... for each combination of C (assuming not too many), ... - Sentences to rewrite: * The posterior sampling distribution of reward is its likelihood integrates with some fixed prior distribution of weights ?. * ..., and would call joint distribution of (A, C) and (B, C) are independent. * ..., which means the algorithm converges selection to single arm (convergence) as well as best arm. - Typos: Jointly distribution/relationship -> joint ,It worth to note -> it is worth to note, extensively exam -> extensively examine, a serial processes -> process, would been -> be ..."}
{"id": "iclr2020_305", "title": "Better Knowledge Retention through Metric Learning | OpenReview", "abstract": "Abstract:###In a continual learning setting, new categories may be introduced over time, and an ideal learning system should perform well on both the original categories and the new categories. While deep neural nets have achieved resounding success in the classical setting, they are known to forget about knowledge acquired in prior episodes of learning if the examples encountered in the current episode of learning are drastically different from those encountered in prior episodes. This makes deep neural nets ill-suited to continual learning. In this paper, we propose a new model that can both leverage the expressive power of deep neural nets and is resilient to forgetting when new categories are introduced. We demonstrate an improvement in terms of accuracy on original classes compared to a vanilla deep neural net.", "review": "Review:###This paper presents a possible way to mitigate catastrophic forgetting by using a k-nearest neighbor (kNN) classifier as the last layer of a neural network as opposed to a SoftMax classifier. I think this an interesting and possibly novel use of a kNN layer (I haven*t seen similar uses although I*m not that familiar with the specific research area). At the same time it*s not presenting a ground breaking new algorithm or anything like that. Overall the paper is fairly well written and not too hard to follow. I would say overall results in Table 1 are positive although the authors* approach has the lowest performance after just training on set A if that initial accuracy is important, and also doesn*t have quite as high of an accuracy on test B compared to most of the other baselines. Additionally, if you add the accuracy on both set A and set B after training on set B the sum is slightly higher for Rtf. If you look at the minimum accuracy between set A and set B after training on set B, however, the authors* method has the highest value which might be what someone is looking to maximize. One weakness of this is paper is that I think there are other baselines that should be compared against in Table 1 such as something as basic as SGD with dropout (some of the baselines that are compared against in Table 1 were compared against SGD with dropout in their citations). There are a number of additional approaches outlined in https://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf. Also maybe even something with self attention such as Serra at al. https://arxiv.org/pdf/1801.01423.pdf. Another potential issue I have with this paper is that it only reports results for the authors* method and the vanilla baseline for more complex CIFAR-10 and ImageNet data sets in Table 2. Assuming there aren*t restrictive assumptions for some of the methods that prevent them from being run on the other data sets (at least SI was previously evaluated on CIFAR-10), I would like to see how other baselines perform on these more complex datasets too. The lack of some more baselines such as SGD with dropout, and not reporting the performance of the same baselines from Table 1 in Table 2, cause me to be very borderline on this paper. I do appreciate the sensitivity analysis and ablation study provided. As alluded to in future work I*m curious how the authors* approach might be applied to reinforcement learning, and if there could be a way to deal with continuous action spaces in RL."}
{"id": "iclr2020_306", "title": "LambdaNet: Probabilistic Type Inference using Graph Neural Networks | OpenReview", "abstract": "Abstract:###As gradual typing becomes increasingly popular in languages like Python and Typescript, there is a growing need to infer type annotations. While type annotations help with tasks like code completion and static error catching, these annotations cannot be fully inferred by compilers and are tedious to annotate by hand. This paper proposes a probabilistic type inference scheme for Typescript based on a graph neural network. Our approach first uses lightweight source code analysis to generate a program abstraction called a type dependency graph, which links type variables with logical constraints as well as name and usage information. Given this program abstraction, we then use a graph neural network to propagate information between related type variables and eventually make type predictions. Our neural architecture can predict both standard types, like number or string, as well as user-defined types that have not been encountered during training. Our experimental results show that our approach outperforms prior work in this space by 14% (absolute) on library types, while having the ability to make type predictions that are out of scope for existing techniques.", "review": " = Summary A method to predict likely type of program variables in TypeScript is presented. It consists of a translation of a program*s type constraints and defined objects into a (hyper)graph, and a specialised neural message passing architecture to learn from the generated graphs. Experiments show that the method substantially outperforms sound typing in the TypeScript compiler, as well as a recent method based on deep neural networks. = Strong/Weak Points + The graph representation of the problem is novel, and draws both on core ideas from Hindley-Milner typing (in the subtyping/assignment graph bits) as well as neural ideas (in name similiarity) + The neural message passing architecture is adapted to the problem, handling features not present in the standard GNN literature (hyperedges, ...) + Experiments compare with relevant baselines and consider interesting ablations, studying the effect of the GNN extensions in detail. - The hyperparameter selection regime (and the experiments used to find them) is not described = Recommendation This is an application-driven paper with nice practical results. The fact that standard neural architectures are extended and adapted to the task, and the way domain knowledge is used to design the graph representation makes this interesting even to people outside the task-specific audience, and hence I strongly recommend acceptance. = Minor Comments - page 2: *network*s type to be class* -> *to be a class* - Evaluation Datasets: Did you take duplication in the crawled datasets into account? (Lopes et al. 2017 (DéjàVu: a map of code duplicates on GitHub) suggests that this is particularly problematic for JavaScript/TypeScript)"}
{"id": "iclr2020_307", "title": "Generative Adversarial Nets for Multiple Text Corpora | OpenReview", "abstract": "Abstract:###Generative adversarial nets (GANs) have been successfully applied to the artificial generation of image data. In terms of text data, much has been done on the artificial generation of natural language from a single corpus. We consider multiple text corpora as the input data, for which there can be two applications of GANs: (1) the creation of consistent cross-corpus word embeddings given different word embeddings per corpus; (2) the generation of robust bag-of-words document embeddings for each corpora. We demonstrate our GAN models on real-world text data sets from different corpora, and show that embeddings from both models lead to improvements in supervised learning problems.", "review": "Review:###Summary The paper proposes extensions of Generative Adversarial Networks to modeling multiple text corpora. Concretely, the paper looks at two problems: 1) given independently pretrained word embeddings from K corpora, finding a common word embdding, 2) extracting document representations from a discriminator of a GAN trained to generate tf-idf vectors. Preliminary experiments show that the proposed approaches outperform baseline classifiers trained with word2vec. Strengths + The paper clearly mentions all the experimental details + The paper has a nice set of qualitative examples that probe what the proposed model is learning Weaknesses * It is not clear what problem the paper is trying to solve. Is the goal to get better document representations, in which case why do we think that using a GAN is a good idea. Learning a generative model to use representations for a downstream task seems like a pretty roundabout way of doing things (if we dont care about generating anything in the first place). Further, if better document representations are desired then the paper should compare to works like Bert (Devlin et.al. [1]). * Sec. 3.1: Calling the proposed weGAN model a GAN seems a bit inconsistent/ wrong. The proposed model is not a generative adversarial network, there is no sampling of a noise or a notion of a generative distribution. The real data, similar to the image case is a bunch of samples from the data distribution (where the stochasticity comes not from the word embeddings but from the tf-idf of thte document), but the generator also uses the tf-idf representation, so essentially both the generator as well as the real world data use the document as an input. Hence it feels like a somewhat odd model formulation, which would be nice to clarify / explain. * Further, it is not clear why one would want to train “cross-corpus” embeddings in the way describeed in Sec. 3. 1. Why not just train word2vec on the union of all the corpora (instead of per-corpus) and use it as the word representation? What are the conditions in which one would not want to do the common word representation? * Sec. 3.2: Why not formulate modeling multiple corpora in the spirit of CoGAN (Liu and Tuzel, 2016), by getting M discriminators (potentially with parameter sharing) to solve binary classification tasks as opposed to the 2M way classification problem? Atleast a comparison to an approach like this seems warranted. Further, it is not clear why one would want to generate tf-idf vectors. It seems like all the experiments are around representation learning and classification as opposed to generating or evaluating tf-idf vectors for documents. * As a side note, for the experiments concerning deGAN vs the baseline, it would be nice to check what happens when the last classification layer is not initialized with LDA parameters. Since that is an additional source of information. References [1]: Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv [cs.CL]. arXiv. http://arxiv.org/abs/1810.04805."}
{"id": "iclr2020_308", "title": "Confidence-Calibrated Adversarial Training: Towards Robust Models Generalizing Beyond the Attack Used During Training | OpenReview", "abstract": "Abstract:###Adversarial training is the standard to train models robust against adversarial examples. However, especially for complex datasets, adversarial training incurs a significant loss in accuracy and is known to generalize poorly to stronger attacks, e.g., larger perturbations or other threat models. In this paper, we introduce confidence-calibrated adversarial training (CCAT) where the key idea is to enforce that the confidence on adversarial examples decays with their distance to the attacked examples. We show that CCAT preserves better the accuracy of normal training while robustness against adversarial examples is achieved via confidence thresholding. Most importantly, in strong contrast to adversarial training, the robustness of CCAT generalizes to larger perturbations and other threat models, not encountered during training. We also discuss our extensive work to design strong adaptive attacks against CCAT and standard adversarial training which is of independent interest. We present experimental results on MNIST, SVHN and Cifar10.", "review": "Review:###====== AFTER READING THE AUTHOR RESPONSE ====== Many thanks for the extensive response and the respective revision from the author(s). Mainly, I found the main results are adjusted in the revision to rather demonstrate its good detection performance at 99% TPR, and I feel the message of the manuscript becomes more strengthen. However, at the same time I feel the proposed method would be slightly less motivated without the improved results of the *pure* robust accuracy, as the method itself is anyway a variant of adversarial training. There are several works that specially focus on detecting adversarial examples [1, 2], and comparing the results with them on diverse threat models would more strengthen the paper. In overall, I keep my score unchanged to the current version of manuscript. [1] A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks, NeurIPS 2018 [2] The Odds are Odd: A Statistical Test for Detecting Adversarial Examples, ICML 2019 =============================================== The paper proposes a new adversarial training scheme to improve generalization of robustness over unseen threat models, e.g. larger perturbations or different noise distributions. The key idea is to impose uniform confidence on the adversarial examples depending on the distance from the original example, based on a pre-determined distance metric, e.g. L-infinity distance. Experimental results shows its effectiveness on detecting adversarial examples and unseen robustness for MNIST, SVHN, and CIFAR-10 datasets, under L-infinity and L-2 adversaries. In overall, I agree that improving generalization over unforeseen adversaries is a very important problem in adversarial training, and the paper addresses this problem with a novel approach. The manuscript is generally well-presented with clear motivation. In particular, I appreciated the simplicity of the proposed idea, and the thoroughness of experiments as a defense paper, e.g. presenting per-example worst-case results across diverse attacks. However, I am currently on a slightly negative side, due to some unclear points in the experimental results. I would like to increase the score if the issue could be addressed, regarding the importance of the problem and their approach, which seems valuable to be shared in the community. Mainly, it is still hard for me to interpret the presented experimental results at the positive side, unless the authors could further address on the important points of the results. In general, the results are not that clear as claimed, especially when tau=0, to show that CCAT improves robustness: At the original threat model (L-inf with the smallest epsilon), CCAT shows much inferior results across all the datasets. It does improves the L-2 results, except for CIFAR-10 which should be a bare baseline to show the scalability of the method. Although the paper also point out that CCAT sometimes achieves much lower clean test error, but sometimes this also signals the less robustness. I hope the paper could justify such points in the Table 2 for better presentation. - Perhaps ResNet-20 is too small for CIFAR-10 tasks, as the training loss would hardly minimized into 0? I think WRN-10 is a more fair standard for CIFAR-10 in AT, and wonder if the result could show more effectiveness (or get more aligned tendency across datasets) of the proposed method if the capacity of network is increased. - Apart from the thoroughness of the attack methods assumed, considering only L-inf and L-2 adversaries may be not enough to claim the *generalization ability* of a defense. Could the method also improves robustness against other attacks, e.g. general corruption (such as MNIST-C, CIFAR-10-C), or unrestricted adversarial examples? - Eq 4: Does it mean that the logits other than the 2nd predictions are not considered when generating adversarial examples? Personally, I don*t much get the motivation of using this objective, even while it is described in below Eq. 4."}
{"id": "iclr2020_309", "title": "Certified Robustness for Top-k Predictions against Adversarial Perturbations via Randomized Smoothing | OpenReview", "abstract": "Abstract:###It is well-known that classifiers are vulnerable to adversarial perturbations. To defend against adversarial perturbations, various certified robustness results have been derived. However, existing certified robustnesses are limited to top-1 predictions. In many real-world applications, top- predictions are more relevant. In this work, we aim to derive certified robustness for top- predictions. In particular, our certified robustness is based on randomized smoothing, which turns any classifier to a new classifier via adding noise to an input example. We adopt randomized smoothing because it is scalable to large-scale neural networks and applicable to any classifier. We derive a tight robustness in norm for top- predictions when using randomized smoothing with Gaussian noise. We find that generalizing the certified robustness from top-1 to top- predictions faces significant technical challenges. We also empirically evaluate our method on CIFAR10 and ImageNet. For example, our method can obtain an ImageNet classifier with a certified top-5 accuracy of 62.8\\% when the -norms of the adversarial perturbations are less than 0.5 (=127/255). Our code is publicly available at: url{https://github.com/jjy1994/Certify_Topk}.", "review": " This paper builds upon the random smoothing technique for top-1 prediction proposed by Cohen et al. for certifying top-k predictions with probabilistic guarantees, which enjoys good scalability to large neural networks and in principle can be applied to any classifier. - Contributions: 1. The authors aim to provide (probabilistic) certification on top-k predictions, which to my knowledge is the first work to consider this setup. Many applications such as recommendation systems indeed use top-k predictions as a performance measure. The problem setup is new and important in the research of robustness certification. 2. In terms of technical contributions, the authors identify the difficulty of extending top-1 prediction to top-k prediction, due to the requirement of simultaneous confidence interval estimation of the bounds on the actual class predictions. To cope with this difficulty, the authors proposed simultaneous confidence interval estimation based on Clopper-Pearson method and Bonferroni correction. However, I am not sure the difficulty is caused by the necessity of estimating multiple probability bounds, or simply the limitation of the proposed algorithm. I hope the authors can address my concerns in the Questions below. 3. Experimental results on Cifar-10 and ImageNet showed improved lower bound on certified L2-norm radius when increasing k. The authors also performed an ablation study of different parameters in the proposed algorithm. - Questions: 1. Intuitively, when extending top-1 certification to top-k certification, one would expect using ordered statistics of the prediction outputs from the randomly perturbed inputs. As long as the original label*s prediction probability is in the top-k label set, the smoothed classifier is directly certified. Instead of ordered statistics, the authors tackle this problem by considering estimating upper and lower bounds of each class prediction probability. Therefore, the problem becomes more difficult as k increases, since this indirect approach needs to simultaneous estimate those probability bounds. I wonder the current approach will be suboptimal when compared to the ordered statistics approach. I would like to know the authors thoughts on this regard. That is, is the claimed difficulty an outcome when using the proposed indirect bound estimation for certification, or it*s provably more difficult? 2. The discussion on Fig.3 says *We observe that _x001b_ sigma controls a trade-off between normal accuracy under no attacks and robustness. Specifically, when _x001b_ is larger, the accuracy under no attacks (i.e., the accuracy when radius is 0) is larger, but the certified top-k accuracy drops more quickly as the radius increases.* However, it seems that larger sigma actually gives lower accuracy under no attacks in Figure 3. Please clarify. Overall, this paper brings some new insights and results in robustness certification, but some claims and statements need to be further justified. I am happy to increase my rating if my concerns are addressed."}
{"id": "iclr2020_310", "title": "Behavior Regularized Offline Reinforcement Learning | OpenReview", "abstract": "Abstract:###In reinforcement learning (RL) research, it is common to assume access to direct online interactions with the environment. However in many real-world applications, access to the environment is limited to a fixed offline dataset of logged experience. In such settings, standard RL algorithms have been shown to diverge or otherwise yield poor performance. Accordingly, much recent work has suggested a number of remedies to these issues. In this work, we introduce a general framework, behavior regularized actor critic (BRAC), to empirically evaluate recently proposed methods as well as a number of simple baselines across a variety of offline continuous control tasks. Surprisingly, we find that many of the technical complexities introduced in recent methods are unnecessary to achieve strong performance. Additional ablations provide insights into which design choices matter most in the offline RL setting.", "review": "Review:###This paper presents a framework for evaluating offline reinforcement learning (RL) algorithms. Results from a thorough series of experiments are presented which suggest that certain details of recently proposed RL methods are not necessary for achieving strong performance. These results suggests that some of the complexity in RL design can be ignored. I commend the authors for performing a valuable test and comparison of existing offline RL methodology. This paper could be improved by providing more clear insight and intuition about the deeper meaning of these results regarding the *unnecessary* technical complexities. Could the authors suggest why certain complexities are unnecessary? Clearly the authors of those previous works thought they were needed. To really help researchers design better algorithms, we need to be guided by some insight about not only what doesn*t work but why it doesn*t work. Also, the paper could be improved by being more clear about the nature of the evaluations. The authors provide extensive results; but it wasn*t clear whether these were *apples-to-apples* comparisons with the previous results in the papers that proposed the *unnecessary* technical complexities. For example, I didn*t see the authors say that they reproduced the results of previous works, only that they tested previous methods in certain tests. Does the BRAC framework reproduce the results for previous papers? If so, this should be made more clear and stated prominently in the paper so that the reader knows that BRAC is, in this reproduction of previous results sense, reliable. If not, how if the reader to know that the *unnecessary* technical complexities, are truly unneccessary? Finally, the paper presents lots of results, but I did not see any mention of the statistical significance of these results. Minor issue: In the Conclusion Section, the authors say, *Unfortunately, off-policy ... is an challenging open problem.* Unfortunately?! A challenging open problem is a good thing! And I think the authors did a good job addressing a difficult problem."}
{"id": "iclr2020_311", "title": "Objective Mismatch in Model-based Reinforcement Learning | OpenReview", "abstract": "Abstract:###Model-based reinforcement learning (MBRL) has been shown to be a powerful framework for data-efficiently learning control of continuous tasks. Recent work in MBRL has mostly focused on using more advanced function approximators and planning schemes, leaving the general framework virtually unchanged since its conception. In this paper, we identify a fundamental issue of the standard MBRL framework -- what we call the objective mismatch issue. Objective mismatch arises when one objective is optimized in the hope that a second, often uncorrelated, metric will also be optimized. In the context of MBRL, we characterize the objective mismatch between training the forward dynamics model w.r.t. the likelihood of the one-step ahead prediction, and the overall goal of improving performance on a downstream control task. For example, this issue can emerge with the realization that dynamics models effective for a specific task do not necessarily need to be globally accurate, and vice versa globally accurate models might not be sufficiently accurate locally to obtain good control performance on a specific task. In our experiments, we study this objective mismatch issue and demonstrate that the likelihood of the one-step ahead prediction is not always correlated with downstream control performance. This observation highlights a critical flaw in the current MBRL framework which will require further research to be fully understood and addressed. We propose an initial method to mitigate the mismatch issue by re-weighting dynamics model training. Building on it, we conclude with a discussion about other potential directions of future research for addressing this issue.", "review": "Review:###The paper *OBJECTIVE MISMATCH IN MODEL-BASED REINFORCEMENT LEARNING* explores the relationships between model optimization and control improvement in model-based reinforcement learning. While it is an interesting problem, the paper fails at demonstrating really useful effects, and the writting needs to be greatly improved to help reader to focus on salient points. From my point of view, the main problem of this paper is that it is too messy and it is very difficult to understand what authors want to show, as i) there is a very important lack of experimental details (e.g., main aspects of models and controllers should be clearly stated) and ii) analysis is to wordy, authors should emphasize the message in each part. From the experiments in 4.1, the only thing that I got is from the last sentence *noisy trend of higher reward with better model loss*. are these results from LL computed on a validation set ? If not, this is not reallly meaningfull since high LL may only indicate overfitting. If yes, how was the validation data collected ? If the collection is not inline with training it is difficult to understand what we observe since we only need LL to be good on the path from the current to the opitmal policy, not everywhere. Even if the validation data is inline with training, there remains the difficulty of over-fitting in the policy area (for the on-policy experiments at least). Is there something else ? From 4.2 we observe that it is unsurprisingly better to learn the model from the policy trajectories. From 4.3, we observe that an adversarial is able to reduce rewards without losing in LL. Ok, the adversarial is able to lock the controler in a sub-optimal area while still being good to model the dynamics elsewhere, but what does it show ? Finally, proposal to cope with the identified mismatch are not clearly explained and not very convincing. Is re-weighting helping in collecting higher rewards ? From my point of view, this work is in a too preliminary state to be published at ICLR"}
{"id": "iclr2020_312", "title": "GLAD: Learning Sparse Graph Recovery | OpenReview", "abstract": "Abstract:###Recovering sparse conditional independence graphs from data is a fundamental problem in machine learning with wide applications. A popular formulation of the problem is an regularized maximum likelihood estimation. Many convex optimization algorithms have been designed to solve this formulation to recover the graph structure. Recently, there is a surge of interest to learn algorithms directly based on data, and in this case, learn to map empirical covariance to the sparse precision matrix. However, it is a challenging task in this case, since the symmetric positive definiteness (SPD) and sparsity of the matrix are not easy to enforce in learned algorithms, and a direct mapping from data to precision matrix may contain many parameters. We propose a deep learning architecture, GLAD, which uses an Alternating Minimization (AM) algorithm as our model inductive bias, and learns the model parameters via supervised learning. We show that GLAD learns a very compact and effective model for recovering sparse graphs from data.", "review": "Review:###The paper proposes a neural network architecture to address the problem of estimating a sparse precision matrix from data (and therefore inferring conditional independence if the random variables are gaussian). The authors base their algorithm in the semidefinite relaxation by Banerjee et al. They add a regularization terms and penalization parameters, which they learn using neural networks. They consider an alternating minimization implementation similar to ADMM and the neural networks are only used to find the regularization parameters. In order to learn the parameters, the training optimizes the regularization parameters that maximize the recovery objective function (meaning how far is the estimated precision matrices from the true given precision matrices) and doesn’t consider the sparsity. Something that is not a priori obvious is the setting of using a family of precision matrices from a family of graphs and trying to learn an underlying precision matrix (by averaging them?). Further explanation of beginning of section 3 would be useful. Something else that is not clear to this reviewer is the motivation for the loss (9). If the objective is to find the parameters that maximize the recovery objective without taking the sparsity into consideration then why not choose them that way in (1), why there should be learning involved? And what is the learning exactly pursuing? Is it trying to learn a way to combine the information from the different samples consistently? [I acknowledge this is probably a naive question, but maybe addressing this in section 3.3 will help understanding]. I think the overall idea is interesting. Regularization parameters are usually problematic because it is not obvious how to choose them. Having an automatic, data-driven way to choose them is a useful algorithm design tool. The objective pursued in the choice of the loss function is a key concept of the paper and I believe it is not clearly explained. Explaining this point in a convincing way will improve the paper and my assessment from weak reject to strong accept. I suggest cutting the introduction to half and use that space to justify and explain sections 3 and 3.3 in depth. --- Edit: I thank the authors and reviewer 1 for their explanations. I changed my rating to accept. I think it would be useful for the readers to include some of these remarks in the paper."}
{"id": "iclr2020_313", "title": "Antifragile and Robust Heteroscedastic Bayesian Optimisation | OpenReview", "abstract": "Abstract:###Bayesian Optimisation is an important decision-making tool for high-stakes applications in drug discovery and materials design. An oft-overlooked modelling consideration however is the representation of input-dependent or heteroscedastic aleatoric uncertainty. The cost of misrepresenting this uncertainty as being homoscedastic could be high in drug discovery applications where neglecting heteroscedasticity in high throughput virtual screening could lead to a failed drug discovery program. In this paper, we propose a heteroscedastic Bayesian Optimisation scheme which both represents and optimises aleatoric noise in the suggestions. We consider cases such as drug discovery where we would like to minimise or be robust to aleatoric uncertainty but also applications such as materials discovery where it may be beneficial to maximise or be antifragile to aleatoric uncertainty. Our scheme features a heteroscedastic Gaussian Process (GP) as the surrogate model in conjunction with two acquisition heuristics. First, we extend the augmented expected improvement (AEI) heuristic to the heteroscedastic setting and second, we introduce a new acquisition function, aleatoric-penalised expected improvement (ANPEI) based on a simple scalarisation of the performance and noise objective. Both methods are capable of penalising or promoting aleatoric noise in the suggestions and yield improved performance relative to a naive implementation of homoscedastic Bayesian Optimisation on toy problems as well as a real-world optimisation problem.", "review": "Review:###The main contribution of this paper are: 1. The use of a heteroscedastic GP when performing Bayesian Optimization, this is in contrast to the more common practice of assuming homoscedastic noise, even when this does not quite fit the data. They use the existing algorithm called most likely heteroscedastic GP, and quote previous work that performed BO using different heteroscedastic GP implementations. 2. They introduce two new acquisition functions that incorporate the predicted observation noise, either making candidates more likely or less likely to be chosen when predicted noise is higher, depending on the requirements. This are fairly minor extensions/heuristics if taken on their own, as they do not provide a very strong motivation indicating why these acquisition functions are useful or better than existing ones, other than that they take heteroscedasticity into account. 3. They run a set of experiments on the above settings. Unfortunately the experiments are very limited, and their method does not improve on the baselines in a statistically significant way. Two of the experiments are on simple synthetic settings, the third approximates a real world setting, although the approximation is quite rough and they don*t convincingly argue for it being realistic, neither do they give convincing motivation of their objective which uses g +- standard deviation. 4. They provide source code of their implementation. The paper is easy to understand, and covers an interesting topic, so while I don*t think it meets the bar of ICLR (due to lack of convincing and non-trivial contributions) I think it could perhaps be made into a workshop submission with some of the following changes: * A wider set of experimental settings, and more replication such that any differences become statistically significant. It would also be worthwhile comparing to random search. * A better justification of the objective used in the experiments, using g +- the standard deviation appears fairly arbitrary, and there is no strong enough reason to believe this is a good approximation of what the cost is in the case of real world problems. * Better theoretical justification of the acquisition function; one option is to introduce more principled acquisition function like, say, expected upper/lower bound. Other notes/comments: abstract: as well as a real-world -> as well as *on* a real-world... section 1 *As a case study* -> not very clear what this means incumbent best is not well defined, is it the empirical value of f used or the mean predicted f on the evaluated candidates? section 6.2 *outperforms* -> this is not clear if one looks at the confidence intervals in the results"}
{"id": "iclr2020_314", "title": "How much Position Information Do Convolutional Neural Networks Encode? | OpenReview", "abstract": "Abstract:###In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.", "review": "Review:###This paper studied the problem of the encoded position information in convolution neural networks. The hypothesis is that CNN can implicitly learn to encode the position information. The author tests the hypothesis with lots of experiments to show how and where the position information is encoded. Clarity: This paper is interesting for me. It tries to understand the encoded position information that is easily ignored by researchers. I like adequate experiments with learned position information and position illustrations. Experiments: 1. The paper mainly discussed the zero-padding and found it is the source of position information. How about other padding modes like constant-padding, reflection-padding, and replication-padding? 2. The partial convolution-based padding method [1] (padded regions are masked out) shows that its recognition accuracy is higher than the traditional zero-padding approach. Can you help investigate where the position information comes from for this case? [1] Partial Convolution based Padding, https://arxiv.org/pdf/1811.11718.pdf. Some of my concerns are well addressed by the author thus I upgrade my score."}
{"id": "iclr2020_315", "title": "Min-max Entropy for Weakly Supervised Pointwise Localization | OpenReview", "abstract": "Abstract:###Pointwise localization allows more precise localization and accurate interpretability, compared to bounding box, in applications where objects are highly unstructured such as in medical domain. In this work, we focus on weakly supervised localization (WSL) where a model is trained to classify an image and localize regions of interest at pixel-level using only global image annotation. Typical convolutional attentions maps are prune to high false positive regions. To alleviate this issue, we propose a new deep learning method for WSL, composed of a localizer and a classifier, where the localizer is constrained to determine relevant and irrelevant regions using conditional entropy (CE) with the aim to reduce false positive regions. Experimental results on a public medical dataset and two natural datasets, using Dice index, show that, compared to state of the art WSL methods, our proposal can provide significant improvements in terms of image-level classification and pixel-level localization (low false positive) with robustness to overfitting. A public reproducible PyTorch implementation is provided.", "review": "Review:###This work explores the problem of WSL -- in particular, learning to segment with only global labels. The contributions are mainly two-folded. (1) An novel design of regularization terms that promotes the identification of relevant and irrelevant regions for global classification. (2) An recursive erasing algorithm to predict those two types of regions. Pros: 1. The writing is clear 2. The literature review seems thorough 3. Segmentation results provide ample evidence Cons: 1. The global classification results are not convincing enough to claim *significant improvements in terms of image-level classification*, as stated in the paper, according to the experiments. 2. There*s a missing link between *false positive reduction* claims and the proposed algorithm. The design of the entropy-based loss terms does not seem to directly address this, but covers both false positive and false negative scenarios. Equ (10) used in Algorithm 1 (in the appendix) seems to explicitly reduce false negatives with as it iteratively adds relevant regions that are missed with a max operation. 3. Using multiple iteration for the mask prediction model seems to experimentally perform better. Without any theoretical analysis, it is however not clear why this is the case at least intuitively. Each step depends on the previous one and could be facing the problem of error accumulation over time (e.g., at the first step, the relevant regions could already been erased). It is hard to imagine how the mask model can produce a better output with simple global classification objectives. 4. Section 2 is thorough but not entirely clear on the difference between the proposed methods and existing ones. For instance, (1) the following claims are not clear: *... but places more emphasis on mining consistent regions, and being performed on the fly during backpropagation...*. What is *consistency*? What is *on the fly*? (2) *... provide automatic mechanism to stop erasing over samples independently from each other...* What is the automatic mechanism? What does *samples independently* mean? (3) *...To avoid such issues, we detach the upscaling operation...* If the upscaling op is not parameterized by the model, what is its formulation? 5. DICE and F1 score seem to be computed with a threshold of 0.5. Is that an optimal choice? To make the evaluation independent of the threshold, one could use the continuous version of DICE instead. 6. With small datasets, one would expect to see confidence interval/standard error on the results table, with multiple runs of the same experiments, for instance."}
{"id": "iclr2020_316", "title": "VILD: Variational Imitation Learning with Diverse-quality Demonstrations | OpenReview", "abstract": "Abstract:###The goal of imitation learning (IL) is to learn a good policy from high-quality demonstrations. However, the quality of demonstrations in reality can be diverse, since it is easier and cheaper to collect demonstrations from a mix of experts and amateurs. IL in such situations can be challenging, especially when the level of demonstrators* expertise is unknown. We propose a new IL paradigm called Variational Imitation Learning with Diverse-quality demonstrations (VILD), where we explicitly model the level of demonstrators* expertise with a probabilistic graphical model and estimate it along with a reward function. We show that a naive estimation approach is not suitable to large state and action spaces, and fix this issue by using a variational approach that can be easily implemented using existing reinforcement learning methods. Experiments on continuous-control benchmarks demonstrate that VILD outperforms state-of-the-art methods. Our work enables scalable and data-efficient IL under more realistic settings than before.", "review": "Review:###This paper proposes an imitation learning algorithm for the setting where the demonstration data consists of trajectories from sources of varying expertise. The authors proceed by defining a parameterized model of the (demonstration) trajectory distribution (Equation 2), which uses the MaxEnt-RL model for the optimal policy, and a distribution (p_w) to model the level of expertise. Imitation learning is then reduced to maximum-likelihood training under the provided demonstrations. Using appropriate variational distributions and model specification, the MLE objective is transformed to the VILD objective (Equation 5), which can be optimized with gradient descent. Expertise-level (p_w) is modeled as a Gaussian blur over the optimal action, wherein the variance is correlated with expertise (lower is better). Furthermore, a truncated IS approach is proposed for learning a better reward function. It samples more frequently from the experts that have a higher estimated expertise. Overall, I really enjoyed reading the paper. The writing and the presentation of material (both background and novel solutions) is clean and concise. The Appendix, with all the derivations and the summarizations of the related approaches, is very informative. I would like the authors to comment on the following: 1. It is claimed in Section 1 that prior approaches for imperfect imitation learning rely on auxiliary information from the expert, in the form of confidence scores or ranking, while VILD doesn’t use any. In my opinion, the fact that VILD uses “labeled” expert demonstrations (i.e. each demonstration is tagged with a number {1..K}) classifies as auxiliary information. Contrary to approaches such as InfoGAIL, which infer the latent structure of the expert demonstrations in a completely unsupervised fashion, VILD fixes the demonstrations-model and instead attempts to learn the parameters corresponding to this model – this holds exactly for the Gaussian policy, and approximately for the TSD policy setting. 2. The difference in performance of VILD w/ and w/o IS is surprising. I understand the motivation in Section 3.4 that IS should help to improve the convergence rate, but for benchmarks like HalfCheetah, Walker, the performance seems to have saturated to a significantly lower value. I would like to know if the authors have some thoughts on this wide discrepancy w/ and w/o IS. 3. Baselines – I’m not sure if InfoGAIL with a uniform prior on the context is a fair comparison to VILD-IS. Since VILD-IS changes the demonstrator sampling from uniform to expertise-dependent, one could do something similar for InfoGAIL – e.g. after training, report the best performing context, or sample context based on a performance-dependent distribution. 4. Sample-efficiency in terms of expert data – Is the number of trajectories that are collected from each of 10 demonstrators reported somewhere?"}
{"id": "iclr2020_317", "title": "Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks | OpenReview", "abstract": "Abstract:###High-performance Deep Neural Networks (DNNs) are increasingly deployed in many real-world applications e.g., cloud prediction APIs. Recent advances in model functionality stealing attacks via black-box access (i.e., inputs in, predictions out) threaten the business model of such applications, which require a lot of time, money, and effort to develop. Existing defenses take a passive role against stealing attacks, such as by truncating predicted information. We find such passive defenses ineffective against DNN stealing attacks. In this paper, we propose the first defense which actively perturbs predictions targeted at poisoning the training objective of the attacker. We find our defense effective across a wide range of challenging datasets and DNN model stealing attacks, and additionally outperforms existing defenses. Our defense is the first that can withstand highly accurate model stealing attacks for tens of thousands of queries, amplifying the attacker*s error rate up to a factor of 85 with minimal impact on the utility for benign users.", "review": " This paper proposed an effective defense against model stealing attacks. Merits: 1) In general, this paper is well written and easy to follow. 2) The approach is a significant supplement to existing defense against model stealing attacks. 3) Extensive experiments. However, I still have concerns about the current version. I will possibly adjust my score based on the authors* response. 1) In the model stealing setting, attacker and defender are seemingly knowledge limited. This should be clarified better in Sec. 3. It is important to highlight that the defender has no access to F_A, thus problem (4) is a black-box optimization problem for defense. Also, it is better to have a table to summarize the notations. Additional questions on problem formulation: a) Problem (4) only relies on the transfer set, where , right? b) For evaluation metrics, utility and non-replicability, do they have the same D^{test}? How to determine them, in particularly for F_A? c) One utility constraint is missing in problem (4). I noticed that it was mentioned in MAD-argmax, however, I suggest to add it to the formulation (4). 2) The details of heuristic solver are unclear. Although the authors pointed out the pseudocode in the appendix, it lacks detailed analysis. 3) In Estimating G, how to select the surrogate model? Moreover, in the experiment, the authors mentioned that defense performances are unaffected by choice of architectures, and hence use the victim architecture for the stolen model. If possible, could the author provide results on different architecture choices for the stolen model as well as the surrogate model? ############## Post-feedback ################ I am satisfied with the authors* response. Thus, I would like to keep my positive comments on this paper. Although the paper is between 6 and 8, I finally decide to increase my score to 8 due to its novelty in formulation and extensive experiments."}
{"id": "iclr2020_318", "title": "Common sense and Semantic-Guided Navigation via Language in Embodied Environments | OpenReview", "abstract": "Abstract:###One key element which differentiates humans from artificial agents in performing various tasks is that humans have access to common sense and semantic understanding, learnt from past experiences. In this work, we evaluate whether common sense and semantic understanding benefit an artificial agent when completing a room navigation task, wherein we ask the agent to navigate to a target room (e.g. ``go to the kitchen*), in a realistic 3D environment. We leverage semantic information and patterns observed during training to build the common sense which guides the agent to reach the target. We encourage semantic understanding within the agent by introducing grounding as an auxiliary task. We train and evaluate the agent in three settings: (i)~imitation learning using expert trajectories (ii)~reinforcement learning using Proximal Policy Optimization and (iii)~self-supervised imitation learning for fine-tuning the agent on unseen environments using auxiliary tasks. From our experiments, we observed that common sense helps the agent in long-term planning, while semantic understanding helps in short-term and local planning (such as guiding the agent when to stop). When combined, the agent generalizes better. Further, incorporating common sense and semantic understanding leads to 40\\% improvement in task success and 112\\% improvement in success per length (\textit{SPL}) over the baseline during imitation learning. Moreover, initial evidence suggests that the cross-modal embeddings learnt during training capture structural and positional patterns of the environment, implying that the agent inherently learns a map of the environment. It also suggests that navigation in multi-modal tasks leads to better semantic understanding.", "review": "Review:###This submission aims to understand whether common sense and semantic understanding can benefit in a room navigation task. The authors design several interesting auxiliary tasks to understand the significance of common sense and semantic understanding for room navigation. They conduct experiments to understand the importance of each auxiliary task empirically. However, the clarity and quality of writing is poor which makes it difficult to follow the manuscript at many places and evaluate its significance. Some auxiliary tasks are not defined well, several ideas are not motivated well and some paragraphs seem unnecessary. I find the experiments inconclusive due to lack of statistical significance testing, insufficient data, and inadequate hyperparameter tuning. More details in the following comments. Comments on Section 3: - The agent architecture description is not easy to follow. Terms are used without defining them. For example, acronyms CS and SU are stated in Sec3 Baseline model without defining what they stand for or what they mean, in Sec 3 Input, what are “ generating grounding questions”?. There are too many acronyms to keep track of and they are often unintuitive until you read the whole section. There are many cross-references between paragraphs which makes it very difficult to follow the description. For example, In Section 3.1 “generic sequence of rooms (between source and target room) generated by the common sense planning module (CS_RS)”. Here, what is a generic sequence of rooms? How are they generated? What is the common sense planning module? Why is it abbreviated CS_RS? - Why is Das et al. (2017a) cited for LSTM baseline? I can think of many papers before this doing reinforcement and Imitation learning using LSTMs or recurrent networks. - I do not understand the motivation behind the proposed method of self-supervised learning. The prediction of SU_RD (room detection) is used as labels for fine-tuning SU_CR (current room detection) and SU_PN (post navigation grounding) in unseen environments. Why would SU_RD generalize well on unseen environments but not SU_CR and SU_PN? - What is the need for SU_CR (current room detection) when SU_RD (room detection) already consists of current room? Why not just use SU_RD as an auxiliary task for SGN instead of SU_CR? - The motivation behind SU_PN is unclear. Why does the agent need to remember which rooms during the trajectory when it stops? How is this supposed to help in navigation? - The definition of some auxiliary tasks is not clear. It is not clear how the labels were generated for some auxiliary tasks. What are the left, front and right rooms in SU_RD? Do they need to be visible in the current image or does the agent need to predict likely rooms which are behind walls? What if there are multiple rooms in one direction? The CS_RS task is also not defined clearly. It seems from Figure 2 that the input to the CS_RS model is the sequence of rooms in the ground truth trajectory but it is not stated anywhere. What is it predicting? What are “generic room sequence patterns”? How are the labels of CS_RS, SU_RD generated? - If CS_RS is taking the sequence of rooms in the ground truth trajectory as input, the model makes a strong assumption of having access to sequence of rooms to reach the target room at test time. It also makes the comparison to the baseline unfair. If it is not taking sequence of rooms as input, what is the input to the CS_RS model? Comments on Section 4: - What is the meaning of ‘games’ in this context? I am assuming trajectories. - What hyperparameters were tuned using the validation set? - Looking at the results in Table 1, I am not sure whether the difference in the performance of different models is significant or not. I believe statistical significance tests need to be conducted especially because the difference between the performance of different models is small and the test set consists of only 324 trajectories, which are further split into easy, medium and hard sets. - The performance of several models is unintuitive in many places. For example, the performance of CS_RS lower than baseline in medium trajectories, CS_RS improves the performance when added to CS_Nxt in easy and medium but not in hard, SU_PN is better than SU_CR in medium but not in easy, however, SU_PN+CS_Nxt is better than SU_CR+CS_Nxt in easy but not in medium. SU_PN+SU_RD is worse than SU_CR+SU_RD across all settings but SU_PN is overall better than SU_CR. The authors have provided some explanation for some of the above but not for all. I am suspecting that many of the above observations are due to the variance in the results, because of small test sets and not due to the difference in auxiliary tasks. And this again raises questions about the statistical significance of all the results. - Several statements in the results indicate inadequate hyperparameter tuning such as “We hypothesize that hyperparameter tuning is important in combining different modules” and “the performance generally degrades as the agent tends to focus more on the auxiliary tasks than the action prediction task”. I believe it is essential to tune the hyperparameters for auxiliary tasks if the goal is to understand the importance of auxiliary tasks empirically. - What is the purpose of Section 4.2? What are we supposed to learn from this experiment? - It is not clear how are the embeddings visualized in Sec 4.4 and Figures 5 and 6. How are the embeddings projected in two dimensions? How are they aligned? Suggestions for improvement: - The writing can be improved significantly. I suggest first describing all the auxiliary tasks and then describing the agent architecture for training the auxiliary tasks. The description of auxiliary tasks can be improved, especially SU_RD and CS_RS. - I suggest increasing the size of the test set and performing statistical significance tests for making sure that the difference in performance numbers is not due to noise/variance. - The manuscript needs to be proof-read. There are some grammatical errors."}
{"id": "iclr2020_319", "title": "Training Deep Networks with Stochastic Gradient Normalized by Layerwise Adaptive Second Moments | OpenReview", "abstract": "Abstract:###We propose NovoGrad, an adaptive stochastic gradient descent method with layer-wise gradient normalization and decoupled weight decay. In our experiments on neural networks for image classification, speech recognition, machine translation, and language modeling, it performs on par or better than well tuned SGD with momentum and Adam/AdamW. Additionally, NovoGrad (1) is robust to the choice of learning rate and weight initialization, (2) works well in a large batch setting, and (3) has two times smaller memory footprint than Adam.", "review": "Review:###The authors present a variation of Adam that combines layerwise normalization and weight decay decoupling. They test quite extensively their algorithm against other optimization methods on varied tasks. From a theoretical point of view, the contribution seems very incremental. The authors acknowledge that weight decay decoupling is already present in the literature, so it appears that the only contribution is the layer-wise normalization. No justification is proposed as to why this kind of normalization would either accelerate the convergence or leads to better generalization. Proof of convergence even in a deterministic convex setting are missing, so the reader has to extrapolate correctness from previous work on adaptive gradient descent. On the other hand, the proposed algorithm is tested on a great variety of tasks, using state of the art models. The reported performance for the benchmarks (checked for Resnet and Transformer-XL) are on par with what can be found in the literature. The proposed method outperform consistently the other optimizers in terms of generalization performance. I have some concerns regarding section 4: “SGD converges nicely toward (1, 1) but its trajectory is still slightly off of the optimal solution”. It is unclear to me what the reader should understand. Does it converge to the optimal solution? If yes, why should we expect the trajectory to follow the hyperbola? Using the same learning rate for all methods is a bit odd. Why not search for the best learning rate for each optimizer and report its performance? It seems that the oscillation of some of the optimizers could be fixed by using a smaller learning rate. Also, it can be seen in section 5 that Adam consistently needs a smaller learning rate than Novograd. If the difference between NovoGrad and AdamW lies in the layer-wise second moment, it is unclear to me why their performance should differ on this task, as each layer has only one weight. It would be great if the authors could clarify this point. As a conclusion, I am a bit conflicted regarding this paper. The motivation for this modified version of AdamW are unclear, but the empirical results are convincing and rigorous. The authors made a great effort in testing in a variety of different settings. I’m leaning toward accepting this paper, to give the community a chance of testing and, maybe, adopting it."}
{"id": "iclr2020_320", "title": "NADS: Neural Architecture Distribution Search for Uncertainty Awareness | OpenReview", "abstract": "Abstract:###Machine learning systems often encounter Out-of-Distribution (OoD) errors when dealing with testing data coming from a different distribution from the one used for training. With their growing use in critical applications, it becomes important to develop systems that are able to accurately quantify its predictive uncertainty and screen out these anomalous inputs. However, unlike standard learning tasks, there is currently no well established guiding principle for designing architectures that can accurately quantify uncertainty. Moreover, commonly used OoD detection approaches are prone to errors and even sometimes assign higher likelihoods to OoD samples. To address these problems, we first seek to identify guiding principles for designing uncertainty-aware architectures, by proposing Neural Architecture Distribution Search (NADS). Unlike standard neural architecture search methods which seek for a single best performing architecture, NADS searches for a distribution of architectures that perform well on a given task, allowing us to identify building blocks common among all uncertainty aware architectures. With this formulation, we are able to optimize a stochastic outlier detection objective and construct an ensemble of models to perform OoD detection. We perform multiple OoD detection experiments and observe that our NADS performs favorably compared to state-of-the-art OoD detection methods.", "review": " The authors propose a neural architecture search (NAS) method to construct a Bayesian ensemble of deep learning models. This ensemble is then employed to detect out-of-distribution examples. The authors propose to use a differentiable architecture search method which model the architectural parameters using a concrete distribution. This idea was originally proposed by Xie et al. (2019) but this work was not discussed. Similarly, the work by Chang et al. (2019) is not discussed. In my opinion the novelty with respect to NAS is the WAIC objective function and its application to out-of-distribution detection. The idea of using ensemble to detect out-of-distribution examples is not new. The authors already refer to the works by Choi & Jang (2018) and Lakshminarayanan et al. (2017). I*d like to add MC-Dropout (Gal et al., 2016) to this list which was used e.g. to detect adversarial examples. The experimental section is well-written and the proposed method is able to outperform the chosen baselines. Obvious baselines are missing. There is no experiment that proof that this way of searching architectures finds better suited ensembles. How about maximizing the cross-entropy and train the discovered architecture multiple times from scratch and use these models in an ensemble to detect out-of-distribution examples? How about any ensemble-based method mentioned in the previous paragraph? Concluding, the idea is nice but based on the current state of the paper it seems incremental. Experiments to back the usefulness of the described method are missing. Sirui Xie, Hehui Zheng, Chunxiao Liu, Liang Lin: SNAS: stochastic neural architecture search. ICLR 2019 Jianlong Chang, Xinbang Zhang, Yiwen Guo, Gaofeng Meng, Shiming Xiang, Chunhong Pan: Differentiable Architecture Search with Ensemble Gumbel-Softmax. arXiv (2019) Yarin Gal, Zoubin Ghahramani: Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. ICML 2016: 1050-1059"}
{"id": "iclr2020_321", "title": "Gaussian Process Meta-Representations Of Neural Networks | OpenReview", "abstract": "Abstract:###Bayesian inference offers a theoretically grounded and general way to train neural networks and can potentially give calibrated uncertainty. It is, however, challenging to specify a meaningful and tractable prior over the network parameters. More crucially, many existing inference methods assume mean-field approximate posteriors, ignoring interactions between parameters in high-dimensional weight space. To this end, this paper introduces two innovations: (i) a Gaussian process-based hierarchical model for the network parameters based on recently introduced unit embeddings that can flexibly encode weight structures, and (ii) input-dependent contextual variables for the weight prior that can provide convenient ways to regularize the function space being modeled by the NN through the use of kernels. Furthermore, we develop an efficient structured variational inference scheme that alleviates the need to perform inference in the weight space whilst retaining and learning non-trivial correlations between network parameters. We show these models provide desirable test-time uncertainty estimates, demonstrate cases of modeling inductive biases for neural networks with kernels and demonstrate competitive predictive performance of the proposed model and algorithm over alternative approaches on a range of classification and active learning tasks.", "review": "Review:###**Summary**: This paper proposes a hierarchical Bayesian approach to hyper-networks by placing a Gaussian process prior over the latent representation for each weight. A stochastic variational inference scheme is then proposed to infer the posterior over both the Gaussian process and the weights themselves. Experiments are performed on toy regression, classification, (edit: post rebuttal) and transfer learning tasks, as well as an uncertainty quantification experiment on MNIST. post rebuttal (noticed recently): Many apologies for updating the review the day before the deadline; however, I recently remembered that Kronecker inference is often used in variational methods - particularly within the vein of literature of deep kernel learning. Indeed, structure exploiting SVI was proposed in Stochastic Variational Deep Kernel Learning, https://arxiv.org/pdf/1611.00336.pdf, and this method is currently the default in Gpytorch: https://github.com/cornellius-gp/gpytorch/tree/master/examples/08_Deep_Kernel_Learning . Furthermore, Kronecker inference for non-Gaussian likelihoods for Laplace approximations was proposed back in 2015: http://proceedings.mlr.press/v37/flaxman15.pdf. I am not updating my score because it would be unfair; however, the record should be set somewhat straight here. post rebuttal: Thank you for the many clarifications and detailed responses. I*m now satisfied with their many changes and and tend to accept this paper despite the experimental results being somewhat limited. I would really encourage the authors to fix the color schemes (please less black and more brighter colors) on their decision boundaries plots however. **tldr**: While I appreciate the concept of this paper, I tend to reject this paper because I find the experimental results to be on too small scale of datasets. Specifically, I would like to see either a larger scale problem being solved with this kind of approach or a tough to model applied problem that is solved with this approach. **Originality**: As far as I can tell, this seems to be a novel approach to hyper-networks. Neural processes (Garnelo et al; 2018) propose a somewhat similar approach to training – with a latent process over some stored weight space. However, even that is quite distinct from the method proposed in this paper, and I tend to prefer this approach. **Quality**: I really appreciate the merging of neural network and Gaussian process methods; however, tragically, I do wonder if the proposed approach combines the worst of both worlds – the necessity of architecture search for neural networks with the choice of kernel function (as illustrated in Figure 5). If the method is truly kernel dependent, is it also architecture dependent? That is, is it robust to different settings of nonlinearities and depths? Active learning experiment: While I appreciate the comparison here, it seems like here standard HMC should be trainable over well-designed priors on these architectures. So why not include a comparison instead of just MFVI? **Significance**: Unfortunately, I think that the experiments section is just a bit too limited to warrant acceptance right now. This is despite the fact that I really do appreciate the thoroughness and thoughtfulness of the experiments as they are. Specifically, in Section 6.2 why is the metaGP prior only applied to the last layer of the network? If as I suspect, it is due to the complexity and difficulty of inference, that makes the method doubly tough to use in practice. With that being said, to only have experiments on the last layer implies that one should compare to Bayesian logistic regression and linear regression on the last layer of neural networks (e.g Perrone et al, 2018 and Riquelme et al, 2018). Experiments with other methods that combine Gaussian processes with representations on the final layer (e.g. Wilson et al, 2015) are also probably worth running. Figure 4 is a very well-done experiment, if a bit tough to read. I’d suggest that the out of distribution examples get their own figure, with the in distribution examples going into the appendix. I’d also suggest computing the expected calibration error (Naeini et al, 2015) for in and out of distribution examples on the test sets for both MNIST and K-MNIST in order to have quantitative results on the entire test set. To recommend acceptance, I’d really have to see experiments on either a CIFAR sized dataset for classification or a larger scale regression experiment. A larger dataset on either transfer learning (after all you do have a meta-representation over functions that the NN can learn), a larger active learning experiment, or semi-supervised learning. **Clarity**: Overall, the paper is well-written and mostly easy to follow. The meat of the paper is found in Section 4, which I found a bit difficult to follow. (edit: post rebuttal. This concern is somewhat resolved due to the field not being well developed in this area, although it is a useful place to possibly extend the method in the future.) My primary concern here is that the prior ends up becoming Kronecker structured (after Eq. 7), so it isn’t clear to my why dense matrices and dense variational bounds have to be derived in this setting. Can one not follow the lead of the Gaussian process literature (e.g. Saatci 2012, Wilson & Nickisch, 2015) to exploit the Kronecker structure here to make computation of the log likelihoods fast? (edit: post rebuttal. This concern is somewhat resolved.) As a result, it’s not immediately clear to me why a diagonal approximation (Eq. 10) is even necessary? Furthermore, this may be a setting where iterative methods (e.g conjugate gradients and Lanczos decompositions as in Pleiss et al, 2018) for the predictive means and variances may shine and be fast. I do agree that the approximation in Figure 2 does seem to be relatively accurate, although I would ask the authors to compute a relative error for that plot if possible. Additionally, what is the strange high off diagonal correlations in the marginal covariances? (edit: post rebuttal. Thank you for the clarifications here.) Finally, I was a bit confused by the effect of adding the input dependent kernel in Section 3; this seems to make the weights much more complicated to model – now each data point has its own set of weights and therefore, we might have to store considerably more weight matrices over time. Could the authors perform a set of experiments showing the necessity of this kernel matrix in the rebuttal? **Minor Comments**: - Above Eq. 9, “splitted” should be split. - Figure 3: could the data points be plotted in a brighter fashion? On a dark background, they are quick tough to see. Additionally, what is the difference between the two levels of classification plots? References: Naeini, et al. Obtaining Well Calibrated Probabilities by Bayesian Binning, AAAI, 2015. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4410090/ Perrone, V, et al. Scalable Hyperparameter Transfer Learning, NeurIPS, 2018. http://papers.nips.cc/paper/7917-scalable-hyperparameter-transfer-learning Pleiss, G, et al. Constant Time Predictive Distributions for Gaussian Processes, ICML, 2018. https://arxiv.org/abs/1803.06058 Riquelme, C, Tucker, G, Snoek, J. Deep Bayesian Bandits Showdown, ICLR, 2018. https://arxiv.org/abs/1802.09127 Saatci, Y. Scalable Inference for Structured Gaussian process models, PhD Thesis, U. of Cambridge, 2011. http://mlg.eng.cam.ac.uk/pub/pdf/Saa11.pdf Wilson, AG and Nickisch, H. Kernel Interpolation for Scalable Structured Gaussian Processes, ICML, 2015. http://proceedings.mlr.press/v37/wilson15.pdf Wilson, AG, et al. Deep Kernel Learning, AISTATS, 2015. https://arxiv.org/abs/1511.02222"}
{"id": "iclr2020_322", "title": "Optimising Neural Network Architectures for Provable Adversarial Robustness | OpenReview", "abstract": "Abstract:###Existing Lipschitz-based provable defences to adversarial examples only cover the L2 threat model. We introduce the first bound that makes use of Lipschitz continuity to provide a more general guarantee for threat models based on any p-norm. Additionally, a new strategy is proposed for designing network architectures that exhibit superior provable adversarial robustness over conventional convolutional neural networks. Experiments are conducted to validate our theoretical contributions, show that the assumptions made during the design of our novel architecture hold in practice, and quantify the empirical robustness of several Lipschitz-based adversarial defence methods.", "review": "Review:###Summary: This paper presents interesting theoretical contributions towards measuring the robustness of Lipschitz constrained neural networks. The empirical results are fairly limited and lack explanation in places and appropriate comparisons to existing work. The paper is missing references to several key pieces of related work tackling similar problems. I identified (fixable) issues with the theoretical results in the paper and felt that overall the paper was rushed and difficult to read in places as a result. Further, this paper exceeds the recommended 8 page limit with ill-formatted references. Overall 1) Claim that threat models beyond l2 has not been explored before is false. See Anil et al. for networks implementing an L-infinity Lipschitz constraint [1] and Huster et al. [2] for a discussion of infinity norm constrained architectures (the latter is cited). 2) In this work you claim through empirical evidence that adversarial training has only a small effect on the upper bound of the Lipschitz constant. This is contested by e.g. [3] which focused on measuring the Lipschitz constant of neural networks tightly and highlighted the impact of adversarial training. 3) Ignoring issues discussed below, this paper presents an interesting theoretical result on providing stochastic robustness guarantees for Lipschitz constrained neural networks. However, the paper does not discuss existing baselines which provide similar stochastic guarantees e.g. [4]. 4) There are several (fixable) issues present in the theoretical results. First, notation is inconsistent throughout with the loss function taking either one or two arguments in different places (e.g. Eqn. 2 vs. Proposition 1). In fact, in Proposition 1 the loss function on the left hand side of the inequality takes two arguments not one (one of which is vector valued). From the proof, it is clear that the authors mean to make the loss a function of the margin (which should be discussed clearly). Further, Proposition 2 presents the wrong terms in the sum (so that E[L] != adversarial risk). L should instead consider the sum over the maximum change in the margin for the dataset and use Proposition 1 after Eqn 11 as described. Further, this proof is a straight forward application of McDiarmid*s inequality and does not need to define a Doob martingale (in fact, it is not clear that one is defined here as no filtration is presented). I also believe that the presented bound is computed incorrectly, as the maximum change in the sum value through modifying a single element would be B/n . I believe the stated result can be made rigorous and correct, however in its current form there are mistakes. 5) I do not understand the proposed training strategy. It seems as though standard cross entropy training is used but with a modified Lipschitz constraint which allows the output of the vector-valued function to have different Lipschitz constants at each index. If this is the case, could you please elaborate on what you mean by the argument that subnetworks associated with each class can achieve higher accuracy? 6) In section 4.1 you introduce the methods use to constrain the Lipschitz constant of the network. The first of which is a regularization scheme which provides no provable guarantees on the Lipschitz constant itself. Further, you describe spectral normalization as a regularization technique which is inaccurate --- it is typically implemented as a projection. Spectral normalization is also unable to tightly enforce the required Lipschitz constant (see e.g. [1]). Finally, I would like to ask how the authors compute the Lipschitz constants of the networks used throughout the experiments. Is the product of the linear layer p-norms computed after training? If so, this would give a loose upper bound (see [3]). 7) I am also concerned by the range of perturbation size used to measure the theoretical robustness of the networks -- a maximum perturbation size on CIFAR-10 of 4/255 is used which is very small compared to e.g. [4] which searches up to a radius 1.5. I would consider raising my score if the issues present in the theoretical results are addressed by the authors. Minor: - In introduction: *so-called adversarial examples, appear to humans as normal images* may be considered to strict a definition. For example, images which look like random noise can force high classification confidence in classifiers and are often described as adversarial examples. Similarly, the thread model defined in equation (2) is presented as the general adversarial objective but is only a special case. One should be also be concerned with threat models violating a p-norm constaint. - Page 5, typo *This means the the* References: [1] Sorting out Lipschitz function approximation, Cem Anil, James Lucas, and Roger Grosse [2] Limitations of the Lipschitz constant as a defense against adversarial examples, Todd Huster, Cho-Yu Jason Chiang, and Ritu Chadha [3] Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural Networks, Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George J. Pappas [4] Certified Adversarial Robustness via Randomized Smoothing, Jeremy M Cohen, Elan Rosenfeld, and J. Zico Kolter"}
{"id": "iclr2020_323", "title": "Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization | OpenReview", "abstract": "Abstract:###Lead bias is a common phenomenon in news summarization, where early parts of an article often contain the most salient information. While many algorithms exploit this fact in summary generation, it has a detrimental effect on teaching the model to discriminate and extract important information. We propose that the lead bias can be leveraged in a simple and effective way in our favor to pretrain abstractive news summarization models on large-scale unlabelled corpus: predicting the leading sentences using the rest of an article. Via careful data cleaning and filtering, our transformer-based pretrained model without any finetuning achieves remarkable results over various news summarization tasks. With further finetuning, our model outperforms many competitive baseline models. For example, the pretrained model without finetuning outperforms pointer-generator network on CNN/DailyMail dataset. The finetuned model obtains 3.2% higher ROUGE-1, 1.6% higher ROUGE-2 and 2.1% higher ROUGE-L scores than the best baseline model on XSum dataset.", "review": "Review:###For news article it has been know since long that the LEAD baseline is a tough-to-beat competitor. This paper proposes to use this knowledge as self-supervision for training summarization models. For this the author download and clean 3 years of news articles and use this to (pre-)train a Tranformer model. This alone already provides a competitive baseline, which is greatly improved by fine-tuning it on 3 different data-sets. While the data-set can probably not be released, it would be very helpful to have the model available for reproductivity and benchmarking. The paper is clear and well-written. Section 4 I believe is very redundant for an ICLR audience and could be moved to the appendix, making space for a more detailed analysis. One criticism is that the paper is light: the author show that a simple idea works (this is a compliment), but I would have expected to have used the remaining space for ablation studies or a discussion on where this leads. One important point which I would like to see before recommending acceptance is a comparison to know if what is helping is just more data, or the summarization objective. Using lots of more data beats all those numbers (see BERTSUM paper, Liu & Lapata 2019). The comparison I am missing is training BERT on your crawled data-set, and use that for BERTSUM (the code is available). If that helps as much as the summarization pre-training then it would be disappointing but a nice result in favor of language modeling. If not, then it is a strong support for your idea. Two other points which should at least be discussed, as it gives the impression of cherry-picking results instead: 1/ Table 1 is recall; Table 2&3 F1. Why? 2/ The parameters of fine-tuning of the appendix vary wildly depending on the data-set (in particular, the difference in the width of the beam search is striking). Was this optimized on test-data? What is the sensitivity of the summaries to this? I do not understand the last two sentences of Sect 4 (*A candidate word leading...). Could you explain?"}
{"id": "iclr2020_324", "title": "A Generalized Training Approach for Multiagent Learning | OpenReview", "abstract": "Abstract:###This paper investigates a population-based training regime based on game-theoretic principles called Policy-Spaced Response Oracles (PSRO). PSRO is general in the sense that it (1) encompasses well-known algorithms such as fictitious play and double oracle as special cases, and (2) in principle applies to general-sum, many-player games. Despite this, prior studies of PSRO have been focused on two-player zero-sum games, a regime wherein Nash equilibria are tractably computable. In moving from two-player zero-sum games to more general settings, computation of Nash equilibria quickly becomes infeasible. Here, we extend the theoretical underpinnings of PSRO by considering an alternative solution concept, ?-Rank, which is unique (thus faces no equilibrium selection issues, unlike Nash) and tractable to compute in general-sum, many-player settings. We establish convergence guarantees in several games classes, and identify links between Nash equilibria and ?-Rank. We demonstrate the competitive performance of ?-Rank-based PSRO against an exact Nash solver-based PSRO in 2-player Kuhn and Leduc Poker. We then go beyond the reach of prior PSRO applications by considering 3- to 5-player poker games, yielding instances where ?-Rank achieves faster convergence than approximate Nash solvers, thus establishing it as a favorable general games solver. We also carry out an initial empirical validation in MuJoCo soccer, illustrating the feasibility of the proposed approach in another complex domain.", "review": "Review:###The paper studies ?-Rank, a scalable alternative to Nash equilibrium, across a number of areas. Specifically the paper establishes connections between Nash and ?-Rank in specific instances, presents a novel construction of best response that guarantees convergence to the ?-Rank in several games, and demonstrates empirical results in poker and soccer games. The paper is well-written and well-argued. Even without a deep understanding of the subject I was able to follow along across the examples and empirical results. In particular, it was good to see the authors clearly lay out where their novel approach would work and where it would not and to be able to identify why in both cases. My only real concern stems from the empirical results compared to some of the claims made early in the paper. Given the strength of the claims comparing the authors approach and prior approaches, it seems that the empirical results are somewhat weak. The authors make sure to put these results into context, but given the clarity of the results in the toy domains I would have expected clearer takeaways from the empirical results as well. Edit: The authors greatly improved the paper, addressing all major reviewer concerns."}
{"id": "iclr2020_325", "title": "Deep Innovation Protection | OpenReview", "abstract": "Abstract:###Evolutionary-based optimization approaches have recently shown promising results in domains such as Atari and robot locomotion but less so in solving 3D tasks directly from pixels. This paper presents a method called Deep Innovation Protection (DIP) that allows training complex world models end-to-end for such 3D environments. The main idea behind the approach is to employ multiobjective optimization to temporally reduce the selection pressure on specific components in a world model, allowing other components to adapt. We investigate the emergent representations of these evolved networks, which learn a model of the world without the need for a specific forward-prediction loss.", "review": "Review:###This paper is well organized. The applied methods are introduced in detail. But it lacks some more detailed analysis. My concerns are as follows. 1. The idea of using the concept of age is not new. There are many studies that have been conducted using the concept of aging in the literature of evolutionary algorithms. 2. Multi-objective optimizations involve finding a set of optimal solutions for different and often competing objectives. However, the authors did not provide the analysis that the age and the accumulated task reward are conflicting with each other. 3. It would be better the average survival time of the individuals is presented in this paper."}
{"id": "iclr2020_326", "title": "Intrinsic Motivation for Encouraging Synergistic Behavior | OpenReview", "abstract": "Abstract:###We study the role of intrinsic motivation as an exploration bias for reinforcement learning in sparse-reward synergistic tasks, which are tasks where multiple agents must work together to achieve a goal they could not individually. Our key idea is that a good guiding principle for intrinsic motivation in synergistic tasks is to take actions which affect the world in ways that would not be achieved if the agents were acting on their own. Thus, we propose to incentivize agents to take (joint) actions whose effects cannot be predicted via a composition of the predicted effect for each individual agent. We study two instantiations of this idea, one based on the true states encountered, and another based on a dynamics model trained concurrently with the policy. While the former is simpler, the latter has the benefit of being analytically differentiable with respect to the action taken. We validate our approach in robotic bimanual manipulation tasks with sparse rewards; we find that our approach yields more efficient learning than both 1) training with only the sparse reward and 2) using the typical surprise-based formulation of intrinsic motivation, which does not bias toward synergistic behavior. Videos are available on the project webpage: https://sites.google.com/view/iclr2020-synergistic.", "review": "Review:###The paper focuses on using intrinsic motivation to improve the exploration process of reinforcement learning agents in tasks with sparse-reward and that require multi-agent to achieve. The authors proposed to encourage the agents toward the actions which changed the world in the ways that *would not be achieved if the agents were acting alone*. The experiments are done with dual-arm manipulation. The idea of guiding the agents toward the actions that they cannot do without concurrent cooperation is interesting. In this paper, it is presented by two types of intrinsic rewards: compositional prediction error and prediction disparity. The core component is the composition of these single-agent prediction model (f^{composed}). Although the formulation proposed is only based on intuition, the authors did enough ablation study to highlight the advantage of this loss function. Areas to improve: + The objects used in the experiment are symmetric, it is good to open your study to the task in which the objects are asymmetric or even deformable. + It is good to extend to the problem of multiple-agents (>2), while the order of agents who acts is important to compute *the expected outcome with individual agents acting sequentially* (for now, you only assume that the A will act first then the B acts later)."}
{"id": "iclr2020_327", "title": "Blending Diverse Physical Priors with Neural Networks | OpenReview", "abstract": "Abstract:###Rethinking physics in the era of deep learning is an increasingly important topic. This topic is special because, in addition to data, one can leverage a vast library of physical prior models (e.g. kinematics, fluid flow, etc) to perform more robust inference. The nascent sub-field of physics-based learning (PBL) studies this problem of blending neural networks with physical priors. While previous PBL algorithms have been applied successfully to specific tasks, it is hard to generalize existing PBL methods to a wide range of physics-based problems. Such generalization would require an architecture that can adapt to variations in the correctness of the physics, or in the quality of training data. No such architecture exists. In this paper, we aim to generalize PBL, by making a first attempt to bring neural architecture search (NAS) to the realm of PBL. We introduce a new method known as physics-based neural architecture search (PhysicsNAS) that is a top-performer across a diverse range of quality in the physical model and the dataset.", "review": "Review:###The authors apply neural architecture search techniques to the problem of physics based learning. It is interesting because it cleverly tackles the challenge of manually designing priors and network architectures. The results are also impressive as the proposed method surpasses all the considered baselines. Despite of the above upsides, I have the following questions/concerns. 1. There is limited technical novelty as the entire method is mainly based on previous work on neural architecture search. Nevertheless, it might be helpful to have some ablation study to show the improvement of the task-specific adaptations presented in the paper, with which I believe this could be a good paper on the application side. 2. I*m curious about the performance of the baseline methods given the same amount of computation. For example, is it possible to perform intensive hyperparameter tuning for the baselines to also obtain improvement. It seems that the authors did not discuss the computational costs and whether different methods are compared given the same cost."}
{"id": "iclr2020_328", "title": "Learning to Generate 3D Training Data through Hybrid Gradient | OpenReview", "abstract": "Abstract:###Synthetic images rendered by graphics engines are a promising source for training deep networks. However, it is challenging to ensure that they can help train a network to perform well on real images, because a graphics-based generation pipeline requires numerous design decisions such as the selection of 3D shapes and the placement of the camera. In this work, we propose a new method that optimizes the generation of 3D training data based on what we call *hybrid gradient*. We parametrize the design decisions as a real vector, and combine the approximate gradient and the analytical gradient to obtain the hybrid gradient of the network performance with respect to this vector. We evaluate our approach on the task of estimating surface normal and depth from a single image. Experiments on standard benchmarks show that our approach can outperform the prior state of the art on optimizing the generation of 3D training data, particularly in terms of computational efficiency.", "review": " Summary: The paper presents an approach to generate 3D training data to improve the performance. At the core of this work is the use of hybrid gradients, i.e., a combination of analytical gradients (task-specific network) and approximate gradients (graphics renderer). The benefits (computational and generalization) of the proposed approach is shown using three benchmarks for depth and surface-normal estimation: (1). MIT-Berkeley dataset; (2). NYU-v2 depth dataset; and (3). Basel Face. Pros: + well-done evaluation and comparisons with prior art! The authors demonstrate the various design decisions and improvements over existing works on different datasets. + intuitive combination of analytical gradients and approximate renderers to combine the synthesizer and task-specific network. Concerns: - A goal for the use of synthetic data is to enable an extensively large amount of data to train parametric models, which may otherwise not be available because of the constraints on capturing and creating real data. It is, therefore, important as how we can automatically create a humungous diverse synthetic data. However, the proposed approach comes no-where close to do it. There is an upper bound to what we can achieve using the proposed method, and it by no-means come close to what we can make just by using a few samples from real data. Here are more specific instances for my reasoning: (1). Section-5.2 and Figure-6: Only a little modification (camera position and object rotation) is possible in terms of placement of objects in a scene. This constraint does not enable us to create something far different from what already existed in synthetic data. (2). Table-2: The use of hybrid gradients enable smarter ways to do augmentation and leads to better performance than prior art. However, the performance is far lower than what one could achieve with few samples from real data (using only 750 training examples images from NYU-v2 dataset) even when trained from scratch. Here are the numbers of this setup: Mean: 21.2; Median: 13.4; <11.25: 44.2; <22.5: 66.6; and <30: 74. It is by no means clear as to how the proposed approach can ever come close to this performance, and make the current plan unuseful for any practical purposes. It will be useful to understand what future directions can be pursued to close this gap. Additionally, the authors can add experiments that use the network trained on synthetic data as an initialization and fine-tune it for real data distribution. Does it lead to better performance? - Figure-4: please mention the delta difference at the time of convergence of hybrid gradient with other approaches. It gives a sense of how far do we get in terms of computational efficiency. Minor Concerns: -inconsistent citation for PCFG in Sec-1 and Sec-4. - typos and grammatical errors here and there."}
{"id": "iclr2020_329", "title": "iSOM-GSN: An Integrative Approach for Transforming Multi-omic Data into Gene Similarity Networks via Self-organizing Maps | OpenReview", "abstract": "Abstract:###One of the main challenges in applying graph convolutional neural networks on gene-interaction data is the lack of understanding of the vector space to which they belong and also the inherent difficulties involved in representing those interactions on a significantly lower dimension, viz Euclidean spaces. The challenge becomes more prevalent when dealing with various types of heterogeneous data. We introduce a systematic, generalized method, called iSOM-GSN, used to transform ``multi-omic** data with higher dimensions onto a two-dimensional grid. Afterwards, we apply a convolutional neural network to predict disease states of various types. Based on the idea of Kohonen*s self-organizing map, we generate a two-dimensional grid for each sample for a given set of genes that represent a gene similarity network. We have tested the model to predict breast and prostate cancer using gene expression, DNA methylation and copy number alteration, yielding prediction accuracies in the 94-98% range for tumor stages of breast cancer and calculated Gleason scores of prostate cancer with just 11 input genes for both cases. The scheme not only outputs nearly perfect classification accuracy, but also provides an enhanced scheme for representation learning, visualization, dimensionality reduction, and interpretation of the results.", "review": "Review:###In this paper, the authors proposed an approach which combines self-organizing maps and CNN to perform data integration, representation learning and dimensionality reduction in a problem with a lot of high dimensional cancer genomic data. Their method produces gene similarity networks. They have some experiments and validation procedure to show their method works. However, I feel this paper is too preliminary with little technical contribution. There are grammar errors and they used several uninformative figures and tables (e.g. Table 2) to fill in the space."}
{"id": "iclr2020_330", "title": "Towards trustworthy predictions from deep neural networks with fast adversarial calibration | OpenReview", "abstract": "Abstract:###To facilitate a wide-spread acceptance of AI systems guiding decision making in real-world applications, trustworthiness of deployed models is key. That is, it is crucial for predictive models to be uncertainty-aware and yield well-calibrated (and thus trustworthy) predictions for both in-domain samples as well as under domain shift. Recent efforts to account for predictive uncertainty include post-processing steps for trained neural networks, Bayesian neural networks as well as alternative non-Bayesian approaches such as ensemble approaches and evidential deep learning. Here, we propose an efficient yet general modelling approach for obtaining well-calibrated, trustworthy probabilities for samples obtained after a domain shift. We introduce a new training strategy combining an entropy-encouraging loss term with an adversarial calibration loss term and demonstrate that this results in well-calibrated and technically trustworthy predictions for a wide range of perturbations. We comprehensively evaluate previously proposed approaches on different data modalities, a large range of data sets, network architectures and perturbation strategies and observe that our modelling approach substantially outperforms existing state-of-the-art approaches, yielding well-calibrated predictions for both in-domain and out-of domain samples.", "review": "Review:###This paper proposed FALCON, a simple method to produce well-calibrated uncertainty estimation. The idea is to introduce two additional terms, one that directly encourage lower confidence for all negative classes of all data points, and another one that optimizes the ECE for adversarial samples. Experiments show that FALCON outperforms several state-of-the-art methods for calibrating neural network predictions. Although the first term, L_S, affects only negative predictions, it is still somewhat strange to uniformly operate on all data points. It would help to perform an ablation study to see how L_S affect the results. Some description in Section 2 could use more mathematical rigor (e.g., when describing L_{adv}). The authors use EDL as a baseline, I was wondering why not use the more commonly used, and possibly more effective, temperature scaling (TS) method from Guo et al. 2017. Note that the EDL paper does not seem to explicitly compare EDL and TS. Figure 4 (middle and right) is confusing. The line style is not consistent in the figures. There are a few places where the text is rather vague and confusing. For example, what do you mean by ‘non-misleading evidence’ when describing L_{adv}? It would also be better to provide more insight more L_S to help the readers out. For example, it would help to state that L_S operate only on negative predictions. Most baselines are rather simple non-probabilistic (non-Bayesian) methods. Besides, MNF, It would also be interesting to see how FALCON compare other probabilistic NN method such as natural parameter networks, where they also explicitly evaluated uncertainty estimation."}
{"id": "iclr2020_331", "title": "Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks | OpenReview", "abstract": "Abstract:###Recent research shows that the following two models are equivalent: (a) infinitely wide neural networks (NNs) trained under l2 loss by gradient descent with infinitesimally small learning rate (b) kernel regression with respect to so-called Neural Tangent Kernels (NTKs) (Jacot et al., 2018). An efficient algorithm to compute the NTK, as well as its convolutional counterparts, appears in Arora et al. (2019a), which allowed studying performance of infinitely wide nets on datasets like CIFAR-10. However, super-quadratic running time of kernel methods makes them best suited for small-data tasks. We report results suggesting neural tangent kernels perform strongly on low-data tasks. 1. On a standard testbed of classification/regression tasks from the UCI database, NTK SVM beats the previous gold standard, Random Forests (RF), and also the corresponding finite nets. 2. On CIFAR-10 with 10 – 640 training samples, Convolutional NTK consistently beats ResNet-34 by 1% - 3%. 3. On VOC07 testbed for few-shot image classification tasks on ImageNet with transfer learning (Goyal et al., 2019), replacing the linear SVM currently used with a Convolutional NTK SVM consistently improves performance. 4. Comparing the performance of NTK with the finite-width net it was derived from, NTK behavior starts at lower net widths than suggested by theoretical analysis(Arora et al., 2019a). NTK’s efficacy may trace to lower variance of output.", "review": " This paper conducts very interesting and meaningful study of kernels induced by infinitely wide neural networks on small data tasks. They show that on a variety of tasks performance of these kernels are superior to both finite neural networks and Random Forest methods. While neural tangent kernel (NTK) [1] is motivated for studying training dynamics of neural networks, it is also important to ask to find utility of these new powerful kernels that captures functional priors of neural networks. This paper conducted important study on small dataset regime and on a wide range of tasks (90 UCI datasets, small subset of CIFAR-10, few shot image classification task on VOC07. Authors introduce a family of generalized NTK kernels interpolating between NNGP kernels [2] to original NTK[1] by fixing first L’ layers and allowing to train remaining layers. Treating L’ as a hyperparameter, the authors try both NNGP/NTK and kernels in between as well. Another contribution I observe is applying kernel SVM where one utilizes NTK and shows that it can work well. This paper shows that kernels induced by infinitely wide networks could become useful for real world applications where data size is not so large. There are few small concerns regarding experiments which are discussed in detailed comments. Overall I think the message of the paper is clear and well supported therefore I recommend accepting the paper. Detailed comments 1) From reading the paper it was not easy to grasp where point 4 of the abstract was based on. 2) In the first footnote, small nit is that, in practice one should not invert matrix but just do a linear solve for better numerical stability and efficiency (still O(N^3) but with better constant) 3) In section 3, there seems to be no bias. Are NTK and NNs considered in this work contain no bias? Or is bias ignored for ease of presentation? 4) Nit p4 first paragraph in section 4 : multiplayer -> multilayer 5) Regards to NTK initialization performing better than standard He initialization: It was observed in [3] that for multilayer perceptron both parameterization is on-par but for CNN or WideResNet case standard parameterization performed significantly better. 6) Note that similar to analysis in section 5, for CIFAR-10 with fully connected model [1] shows that for all dataset size(100-45k) NNGP performs better than trained neural networks. 7) One may worry that ResNet-34 is not properly tuned as most hyperparameters were fixed for large dataset. 8) Regards to hyperparameters for NTK, is there a consistent trend one could find regards to L’? What percentage of tasks that NTK performed well actually have a high L’? 9) To help the readers, I would suggest adding a little more description on statistics used for comparison as well as what VOC07 task entails. [1] Jacot et al., Neural Tangent Kernel: Convergence and Generalization in Neural Networks, NeurIPS 2018 [2] Lee et al., Deep Neural Networks as Gaussian Processes, ICLR 2018 [3] Park et al., The Effect of Network Width on Stochastic Gradient Descent and Generalization: an Empirical Study, ICML 2019 EDIT AFTER AUTHOR RESPONSE: I have read the response from authors. I appreciate all the efforts to improve the paper."}
{"id": "iclr2020_332", "title": "LabelFool: A Trick in the Label Space | OpenReview", "abstract": "Abstract:###It is widely known that well-designed perturbations can cause state-of-the-art machine learning classifiers to mis-label an image, with sufficiently small perturbations that are imperceptible to the human eyes. However, by detecting the inconsistency between the image and wrong label, the human observer would be alerted of the attack. In this paper, we aim to design attacks that not only make classifiers generate wrong labels, but also make the wrong labels imperceptible to human observers. To achieve this, we propose an algorithm called LabelFool which identifies a target label similar to the ground truth label and finds a perturbation of the image for this target label. We first find the target label for an input image by a probability model, then move the input in the feature space towards the target label. Subjective studies on ImageNet show that in the label space, our attack is much less recognizable by human observers, while objective experimental results on ImageNet show that we maintain similar performance in the image space as well as attack rates to state-of-the-art attack algorithms.", "review": "Review:###This paper proposes a method to create adversarial perturbations whose target labels are similar to their ground truth. The target labels are selected using an existing perceptual similarity measure for images. Perturbations are generated using a DeepFool-like algorithm. Human evaluation supports that the pair of the generated images and target labels are more natural to humans than prior attack algorithms. This paper should be rejected due to the lack of motivation to create adversarial examples less detectable by humans automatically. Attackers can manually select target labels and apply targeted attacks. In the target label selection, attackers can choose less detectable labels if necessary. It is encouraged to provide some applications where attackers want to create less detectable adversarial examples in label space without manually assigning target labels. ========== Update: After reading the authors* responses, the motivation of the paper became clearer. I will not get surprised if this paper is accepted. However, all reviewers still share concerns about the importance of the problem tackled. I think the paper needs to suggest more applications and emphasize the value of the goal in the main paper before being published."}
{"id": "iclr2020_333", "title": "THE EFFECT OF ADVERSARIAL TRAINING: A THEORETICAL CHARACTERIZATION | OpenReview", "abstract": "Abstract:###It has widely shown that adversarial training (Madry et al., 2018) is effective in defending adversarial attack empirically. However, the theoretical understanding of the difference between the solution of adversarial training and that of standard training is limited. In this paper, we characterize the solution of adversarial training for linear classification problem for a full range of adversarial radius *. Specifically, we show that if the data themselves are ”-strongly linearly-separable”, adversarial training with radius smaller than * converges to the hard margin solution of SVM with a faster rate than standard training. If the data themselves are not ”-strongly linearly-separable”, we show that adversarial training with radius * is stable to outliers while standard training is not. Moreover, we prove that the classifier returned by adversarial training with a large radius * has low confidence in each data point. Experiments corroborate our theoretical finding well.", "review": "Review:###TL;DR: The paper gives interesting, theoretical results to adversarial training. The paper only uses linear classifiers, which are hardly the same problem as deep networks where adversarial attacks are problematic. Some conclusions from theorems can be vague or informal, and therefore are not very convincing. I vote for rejecting this paper since it is hard to claim it informs deep learning research (the motivating reason for doing adversarial training). However, I am not familiar with theoretical analysis of adversarial attack/defense, so I am open to counter-arguments. ================= 1. What is the specific question/problem tackled by the paper? The paper gives a theoretical analysis to the theoretically less-studied procedure of adversarial training, and shows properties of adversarial training in comparison to regular training, for both linearly separable data or inseparable data. The paper sheds light on some empirical behavior of adversarially trained networks, namely that they are more robust to outliers and lower in performance. 2. Is the approach well motivated, including being well-placed in the literature? I am not an expert of adversarial samples, so I am ill-equipped to judge the novelty of the paper. The research direction itself is well motivated, and in the realm of deep learning, it is posed as a first paper to theoretically analyze adversarial training. However, the authors only analyzed linear classifiers. This makes the results of the paper ill-suited for deep networks, whose non-linearity is arguably the reason why adversarial samples are such a problem. The motivation of the paper is thus greatly diminished. For linear classifiers, I do not know if there are existing work on their robustness when perturbation of samples are being trained on, but to be well-placed in the literature, the authors must either claim there is none, or cite those papers. 3. Does the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous. Claims and novelties in this paper include: (1) Adversarial training converges faster than regular training if samples are ?-strongly linearly separable, (2) If samples are not *?-strongly linearly separable*, adversarial training is robust to outliers, while regular training is not, (3) Confidence is low for all (training) samples if ? is large. Only (1) seems to be sufficiently proved. I am not certain that this is a very useful result, and I am open to counter-arguments. (2) and (3) have steps that are vague and informal: (2) That regular training is susceptible to outliers is proof by example and people already know that. Also the claim relies on the assumption that pi^1 and pj^2 are on the same scale, while those samples that violate the decision boundary can have arbitrarily large pi^1 or pj^2. Since outliers are often the violators, and inliers often are not, a small number of carefully placed outliers can make ||p2|| quite large while each pi^1 can be very small. It is also worth noting the logic seems to boil down to *N1>>N2, so inliers should overwhelm outliers, making the training robust*. The claim is not guaranteed. (3) I am not very sure if I understand it correctly, but the logic seems to be that the logits are bounded, so they cannot be too large, and so the confidence is low. However, the bound also involves the magnitude of w and x from the other class, so the final step of the proof is either unclear or the bound can indeed be quite large. Note that |wx| does not need to be very large for the confidence to be high (e.g. if logit is 5, the confidence is 1/(1+e^-5)=99.3%). The claim also relies on the assumption that epsilon is larger than the distance between the farthest points in the dataset, which is extreme since you can find an adversarial sample that can be considered to be simultaneously *close enough* to the two most dissimilar samples in the dataset. In the end, the results are not very convincing or useful for informing deep learning research. ============= To improve paper: - Clarify motivation and how this would inform adversarial training highly non-linear classifiers; - Add related work for robustness to perturbation of linear models, or state that they don*t exist; - Clarify weaknesses in the claims. Editorial changes: Definition 2: *logit* <--- people call this probability estimates; logits are wTx Sec. 4.1.2 needs to clarify what k in x_i^k means -- it*s continued from proposition 1 which at first glance is irrelevant ================= Post rebuttal Apologies for not interacting earlier due to deadlines. The rebuttal does not address my major concern (motivation), nor does it discuss its relationship with related work. The math questions are not answered very clearly; I do not see how section 4.1.2 proves p_i^1 has the same scale as p_j^2, except maybe that they are all smaller than some constant for certain samples. And other discussions in the rebuttal simply confirms my concern. In summary, I think this paper is not yet ready for publication."}
{"id": "iclr2020_334", "title": "Accelerate DNN Inference By Inter-Operator Parallelization | OpenReview", "abstract": "Abstract:###High utilization is key to achieve high efficiency for deep neural networks. Existing deep learning frameworks has focused on improving the performance of individual operators but ignored the parallelization between operators. This leads to low device utilization especially for complex deep neural networks (DNNs) with many small operations such as Inception and NASNet. To make complex DNNs more efficient, we need to execute parallely. However, naive greedy schedule leads to much resource contention and do not yield best performance. In this work, we propose Deep Optimal Scheduling (DOS), a general dynamic programming algorithm to find optimal scheduling to improve utilization via parallel execution. Specifically, DOS optimizes the execution for given hardware and inference settings. Our experiments demonstrate that DOS consistently outperform existing deep learning library by 1.2 to 1.4 × on widely used complex DNNs.", "review": "Review:###This paper proposes a trivial scheduling approach to split computation graph of a DNN into several groups. The operations in each group, which is called a stage, will be executed in parallel to improve utilization of hardware. They claim that the proposed Deep Optimal Scheduling (DOS) can discover a globally optimal schedule, but no theoretical proof is provided. This claim is apparently wrong, it can not even find optimal solution for the following example: input ---> op1-------+->ouput | | +---->op2---->op3---+ If execution time of op1 is larger than summation of op2 and op3, the optimal solution should be start two parallel computation threads, one for op1, the other one for op2-->op3. If DOS is used, it will first process op1 and op2 in parallel, than process op3, or process op2 first, then process op1, op3 in parallel. For the experimental part, they only compare DOS with Metaflow in terms of search time, runtime latency comparison should be provided. The writing of this paper is bad, for example, they use *S* to represent stage, then it is used to represent *state* or *set*. It makes me confused. They should polish their paper before submission."}
{"id": "iclr2020_335", "title": "Generalized Zero-shot ICD Coding | OpenReview", "abstract": "Abstract:###The International Classification of Diseases (ICD) is a list of classification codes for the diagnoses. Automatic ICD coding is in high demand as the manual coding can be labor-intensive and error-prone. It is a multi-label text classification task with extremely long-tailed label distribution, making it difficult to perform fine-grained classification on both frequent and zero-shot codes at the same time. In this paper, we propose a latent feature generation framework for generalized zero-shot ICD coding, where we aim to improve the prediction on codes that have no labeled data without compromising the performance on seen codes. Our framework generates pseudo features conditioned on the ICD code descriptions and exploits the ICD code hierarchical structure. To guarantee the semantic consistency between the generated features and real features, we reconstruct the keywords in the input documents that are related to the conditioned ICD codes. To the best of our knowledge, this works represents the first one that proposes an adversarial generative model for the generalized zero-shot learning on multi-label text classification. Extensive experiments demonstrate the effectiveness of our approach. On the public MIMIC-III dataset, our methods improve the F1 score from nearly 0 to 20.91% for the zero-shot codes, and increase the AUC score by 3% (absolute improvement) from previous state of the art. We also show that the framework improves the performance on few-shot codes.", "review": "Review:###The paper deals with the problem of text classification when the number of class is large (17000) and most of the classes do not have examples in the training set. This problem is known as Zero-shot learning. The paper proposes to use adversarial methods and the hierarchical organisation of the classes to improve current models. The paper lacks a clear description of the complete system : Figure 1 seems to be the one but there is no mention of the classification part. The description of each bloc is clear enough independently but many question remains on the global picture. The author should for example emphasis the fact that in their problemn the classes come with a short text description. This is somehow unususal for a text classification problem where usually the classes are not defined by a description. In the figure, the difference of the processing of the text from the clinical document and from the class description is not clear. 3.1 is related to feature extraction for the clinical document, but it is also said that this processing is also apply to class description. 3.2 label encoder : if the notations were the same as on figure A, it would help understanding. Results : the claim *our methods improve the F1 score from nearly 0 to 20.91% for the zero-shot codes* is not true : state-of-the-art models such as Xian2018 and Felix2018 are already over 20% F1. When looking at Figure 3 and the analysis, WGAN-Z seem to provide better representation that WGAN with it has almost no impact on the classification results (F1 20.48 versus F1 20.30) In conclusion, it seems that the proposed methods make the model more complex without bringing a significant improvements."}
{"id": "iclr2020_336", "title": "Learning to Guide Random Search | OpenReview", "abstract": "Abstract:###We are interested in the optimization of a high-dimensional function when only function evaluations are possible. Although this derivative-free setting arises in many applications, existing methods suffer from high sample complexity since their sample complexity depend on problem dimensionality, in contrast to the dimensionality-independent rates of first-order methods. The recent success of deep learning methods suggests that many data modalities lie on low-dimensional manifolds that can be represented by deep nonlinear models. Based on this observation, we consider derivative-free optimization of functions defined on low-dimensional manifolds. We develop an online learning approach that learns this manifold while performing the optimization. In other words, we jointly learn the manifold and optimize the function. Our analysis suggests that the proposed method significantly reduces sample complexity. We empirically evaluate the presented method on continuous optimization benchmarks and high-dimensional continuous control problems. Our method achieves significantly lower sample complexity than Augmented Random Search and other derivative-free optimization algorithms.", "review": "Review:###The computation times for random search methods depend largely on the total dimension of the problem. The larger the problem, the longer it takes to perform a single iteration. I believe the main reason why many people use deep reinforcement learning to solve their problems is due to its dimension-independence. I am not aware of a paper that tries to minimize the sample complexity. Thus, I think the idea in this paper is novel and may have influence on the literature (maybe an encouragement for a shift from deep reinforcement learning to derivative-free optimization methods). In terms of presented results I think that there is not much that they could do wrong. They show in Figure 1 that the reward they achieved with their method is only outperformed by Augmented Random Search (ARS) on the Ant task. On all other tasks, their method at least performs on par with ARS which is a good result. In Table 1 they show the number of episodes that are needed to achieve the reward threshold. Their method required less episodes than all other methods, but I think this is not the only criteria they should have looked at. So, it might be the case that their iterations take longer to compute than the iterations of the ARS and thereby making it slower. The authors have showed that their method has a lower sample complexity, which is their goal of the research (“Our major objective is to improve the sample efficiency of random search.”). However, I am not sure whether this means that it also has a lower computational complexity. They address this issue briefly by stating that “Our method increases the amount of computation since we need to learn a model while performing the optimization. However, in DFO, the major computational bottleneck is typically the function evaluation. When efficiently implemented on a GPU, total time spent on learning the manifold is negligible in comparison to function evaluations.” This would mean that their iterations are performed in less computation time than the ARS, but I would have personally liked to see a number attached to this. If we thus assume that this is the case, then their results are sound. However, I do not see this reduced complexity reflected in the results. If I look at the ratios between the number of episodes it takes to solve the tasks, they seem to be similar to the ones from the ARS. The number of episodes reduces by roughly 50% for all tasks but this keeps the ratio between the different tasks identical. I would have assumed that the ratios would increase in the favor of the larger problems like the Humanoid task. In other words, I still see the influence of the larger dimension in the results. Maybe I am too critical, but to me if they would have just found a faster method without the reduced sample complexity, they would have achieved similar results. Of course, this problem would not be present if the computation time increases with the number of iterations. In that case, the computation time would not reduce by a “fixed” ratio and would therefore decrease relatively much on the tasks with a higher dimension. But that would require an exact comparison between the computation times for all tasks for both their method and the ARS which I do not see in their results. If all these things are common knowledge, then their results are sound and they have found a large improvement to the already well performing ARS."}
{"id": "iclr2020_337", "title": "SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering | OpenReview", "abstract": "Abstract:###Conversational question answering (CQA) is a novel QA task that requires the understanding of dialogue context. Different from traditional single-turn machine reading comprehension (MRC), CQA is a comprehensive task comprised of passage reading, coreference resolution, and contextual understanding. In this paper, we propose an innovative contextualized attention-based deep neural network, SDNet, to fuse context into traditional MRC models. Our model leverages both inter-attention and self-attention to comprehend the conversation and passage. Furthermore, we demonstrate a novel method to integrate the BERT contextual model as a sub-module in our network. Empirical results show the effectiveness of SDNet. On the CoQA leaderboard, it outperforms the previous best model*s F1 score by 1.6%. Our ensemble model further improves the F1 score by 2.7%.", "review": "Review:###1. Summary: In this work, the author proposed a quite complicated model with multiple self-attention and inter-attention mechanisms for multiple-round conversational question answering. They evaluated this model on the CoQA dataset and compare it with several deep learning-based baselines. 2. Overall assessment: This paper studied a very interesting problem and its motivation to use self-attention and inter-attention to thoroughly model and contextual information carried by passage and previous rounds of conversation make sense. However, this paper still has some space to get improved and it*s not good enough to be published at ICLR yet. 3. Comments: 3.1 Figure 2 does not help much in this paper. It*s hard to relate the explanation with components in this figure. I feel it would be helpful to label some symbols at some key positions and decompose this figure into multiple figures, with one for one component of the model. The authors can then include some more details into the figure and make it easier to read. 3.2 There lacks some explanation of designs in the model. The reasons to stack soe many layers of attention mechanisms are not well explained. This also makes a bit hard to fully understand the model and the motivation behind it. 3.3 One big concern to me is it seems to me that this model is overly designed and this design does not really capture the most important information for answering questions. Some error analyses and comparisons over variations of models may be able to prove the effectiveness of each component. But these do not appear in the paper. 3.4 The baseline models are quite old and many recent proposed models are not used for comparison. Meanwhile, the performance presented in this paper is also outdated. It would be more convincing to see the model outperformed most recently proposed models."}
{"id": "iclr2020_338", "title": "Stabilizing Transformers for Reinforcement Learning | OpenReview", "abstract": "Abstract:###Owing to their ability to both effectively integrate information over long time horizons and scale to massive amounts of data, self-attention architectures have recently shown breakthrough success in natural language processing (NLP), achieving state-of-the-art results in domains such as language modeling and machine translation. Harnessing the transformer*s ability to process long time horizons of information could provide a similar performance boost in partially-observable reinforcement learning (RL) domains, but the large-scale transformers used in NLP have yet to be successfully applied to the RL setting. In this work we demonstrate that the standard transformer architecture is difficult to optimize, which was previously observed in the supervised learning setting but becomes especially pronounced with RL objectives. We propose architectural modifications that substantially improve the stability and learning speed of the original Transformer and XL variant. The proposed architecture, the Gated Transformer-XL (GTrXL), surpasses LSTMs on challenging memory environments and achieves state-of-the-art results on the multi-task DMLab-30 benchmark suite, exceeding the performance of an external memory architecture. We show that the GTrXL, trained using the same losses, has stability and performance that consistently matches or exceeds a competitive LSTM baseline, including on more reactive tasks where memory is less critical. GTrXL offers an easy-to-train, simple-to-implement but substantially more expressive architectural alternative to the standard multi-layer LSTM ubiquitously used for RL agents in partially-observable environments.", "review": " This paper is motivated by the unstable performance of Transformer in reinforcement learning, and tried several variants of Transformer to see whether some of them can stabilize the Transformer. The experimental results look good, however, I have problems in understanding the motivation, the intuition of the proposed methods, the experimental design, and the general implication to the research community that is using the Transformer in their day-to-day research. First, the paper was based on the hypothesis of the authors that the Transformer is not stable, however, there is no comprehensive study on the unstability, and deep understanding on the root cause of it. It would be much more convincing to give a form definition of unstability and to add experimental study and theoretical analysis to the motivation part, instead of just based on a hypothesis. Second, the proposal of the new structures (e.g., reordering the layer normalization, adding the gating layer) are quite ad hoc. There is not very solid motivation and theoretical analysis on why they could solve the unstable problem of the Transformer. For example, by changing the order of layer normalization, there are direct identity mapping from the first layer to the last layer, making the information flow smoother. However, why this will make the Transformer more stable? The hypothesis and intuitive analysis are not very convincing. For another example, why replacing the residual connection with the gating layer can make the Transformer more stable? It seems to me that these are mostly heuristics, but not verified or strongly motivated solutions. Third, if the proposal is sound, it should not be effective only for reinforcement learning. It should be able to improve the performance or stability of the Transformer in general (e.g., in NLP tasks). However, there is no experiments and discussions regarding this. Fourth, the experiments on the reinforcement learning is a little narrow, and many famous RL benchmarks and environments were not tested. This makes it unclear whether the proposed approach is generally effective. **I read the author responses, however, they do not really change my assessment on the paper."}
{"id": "iclr2020_339", "title": "LAVAE: Disentangling Location and Appearance | OpenReview", "abstract": "Abstract:###We propose a probabilistic generative model for unsupervised learning of structured, interpretable, object-based representations of visual scenes. We use amortized variational inference to train the generative model end-to-end. The learned representations of object location and appearance are fully disentangled, and objects are represented independently of each other in the latent space. Unlike previous approaches that disentangle location and appearance, ours generalizes seamlessly to scenes with many more objects than encountered in the training regime. We evaluate the proposed model on multi-MNIST and multi-dSprites data sets.", "review": "Review:###After reading reviews and comments I have decided to confirm the initial rating. =================== The work presents an approach to encode latent representations of objects such that there are separate and disentangled representations for location and appearance of objects in a scene. The work presents impressive qualitative results which shows the practical use of the proposal on multi-mnist and multi-dSprites. While the use of inference networks proposing positions for the network as a means of improving the disentanglement is clever and seems novel, though not unlike inference sub-networks which are well-known in conditional generation, the evaluation is not up to a standard I can endorse, resulting in a recommendation to reject. Despite the interesting qualitative results, I will have to quote the work in saying, “All methods cited here are likelihood based so they can and should be compared in terms of test log likelihood. We leave this for future work.”. Indeed the cited works should have been evaluated against, especially Greff et al. 2019, Nash et al, 2017, and Eslami et al, 2016, which are all very similar. As written it’s impossible to tell whether this work actually improves over the state of the art, we only have the constructed baseline (which as a community we all know clearly would not have worked). A figure showing the relevant submodules of the network architecture and what they do in relation to the overall method would be helpful to understand the pipeline and how the inference network relates to the whole."}
{"id": "iclr2020_340", "title": "NAMSG: An Efficient Method for Training Neural Networks | OpenReview", "abstract": "Abstract:###We introduce NAMSG, an adaptive first-order algorithm for training neural networks. The method is efficient in computation and memory, and is straightforward to implement. It computes the gradients at configurable remote observation points, in order to expedite the convergence by adjusting the step size for directions with different curvatures in the stochastic setting. It also scales the updating vector elementwise by a nonincreasing preconditioner to take the advantages of AMSGRAD. We analyze the convergence properties for both convex and nonconvex problems by modeling the training process as a dynamic system, and provide a strategy to select the observation factor without grid search. A data-dependent regret bound is proposed to guarantee the convergence in the convex setting. The method can further achieve a O(log(T)) regret bound for strongly convex functions. Experiments demonstrate that NAMSG works well in practical problems and compares favorably to popular adaptive methods, such as ADAM, NADAM, and AMSGRAD.", "review": "Review:###The authors propose a new training method for learning parameters of NNs, describing a method that combines gradients at different observation points. They also provide theoretical analysis of the convergence of the algorithm. The paper is not very readable, and the authors do not help the reader understand and appreciate their work. This is clear from the beginning, where not even the algorithm acronym is explained. This continues throughout the paper, where there is very little sensible explanations. Detailed comments are given below: - Please introduce the methods and notation better. There are a number of works that are just thrown out there without proper explanation (e.g., acronyms HB, ASGD, and others are not even explained). - Notation could be improved as well, as matrices, scalars, and vectors are all denoted the same now. - Algorithm 1 is quite different from the actual text. E.g., where did line 5 appear from, it was never mentioned in the text. This adds to a lot of confusion. Similarly with several other aspects of the method (such as line 8). - *if the vector operations are run by pipelines*, not sure what this refers to. - Typos: mu instead of mu, stand -> standard, ... - The intuition behind the theorems and what they actually tell us should be discussed. Currently they are just given. - *roughly 1 times faster*, so not faster at all? - The experiments are also not very convincing. In fact, the proposed method doesn*t really outperform the competing method in nearly any experiments. Discussion on what are the added benefits would be welcome. Please note that I am not an expert on the topic (as I indicated below in my self-assessment). However, the paper is nevertheless poorly written, and the empirical evaluation is weak. I am quite interested in seeing other reviews by more qualified researchers."}
{"id": "iclr2020_341", "title": "Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks | OpenReview", "abstract": "Abstract:###While tasks could come with varying the number of instances and classes in realistic settings, the existing meta-learning approaches for few-shot classification assume that number of instances per task and class is fixed. Due to such restriction, they learn to equally utilize the meta-knowledge across all the tasks, even when the number of instances per task and class largely varies. Moreover, they do not consider distributional difference in unseen tasks, on which the meta-knowledge may have less usefulness depending on the task relatedness. To overcome these limitations, we propose a novel meta-learning model that adaptively balances the effect of the meta-learning and task-specific learning within each task. Through the learning of the balancing variables, we can decide whether to obtain a solution by relying on the meta-knowledge or task-specific learning. We formulate this objective into a Bayesian inference framework and tackle it using variational inference. We validate our Bayesian Task-Adaptive Meta-Learning (Bayesian TAML) on two realistic task- and class-imbalanced datasets, on which it significantly outperforms existing meta-learning approaches. Further ablation study confirms the effectiveness of each balancing component and the Bayesian learning framework.", "review": "Review:###Summary ======== This paper introduces a mechanism for gradient-based meta-learning models for few-shot classification to be able to adapt to diverse tasks that are imbalanced and heterogeneous. In particular, each encountered task may have varying numbers of shots (task imbalance) and even within each task, different classes may have different numbers of shots (class imbalance). Further, test tasks might come from a different distribution than the training tasks. They propose to handle this scenario by introducing three new types of variables which control different facets of the degree and type of task adaptation, allowing to decide how much to reuse meta-learned knowledge versus new knowledge acquired from the training set of the given task. Specifically, their newly introduced variables are: 1) the factor for learning rate decay (a scalar) for the inner-loop task adaptation optimization which allows to not deviate too much from the global initialization when insufficient data is available, 2) a class-specific learning rate (one scalar per class) that allows to tune more for under-represented classes of the training set, 3) a set of weights on the global initialization (one scalar per dimension) that can down-weigh each component if it’s not useful for the task at hand (e.g. if test tasks have significantly different statistics than training tasks did). The values of these variables are predicted based on the training set of the task: the support set is encoded via a hierarchical variant of a set encoding (where pooling is done using higher order statistics too instead of simply averaging). The resulting encoded support set is the input to the network that produces the values for the three sets of variables discussed above. Each new variable is treated in a Bayesian fashion: a prior is defined over it (Normal(0,1)), which is updated by conditioning on the training set to form a posterior for each given task. Specifically, each of the above variables is represented by a Gaussian whose mean and variance are the learnable parameters that are produced by the network described above. Experimentally, this method outperforms others on a setting of imbalanced tasks (the shot is sampled uniformly at random from a designated range). The gain over other methods is large in particular when evaluated on out-of-distribution tasks (coming from a different dataset) and when the imbalance is large. Comments (in decreasing order of importance) ======================================== A) The Bayesian framework helps because it offers an elegant way to use a prior. In the deterministic version, was any effort made to resemble the effect of that prior? For example, one can define a regularizer that penalizes behaviors that ignore the meta-knowledge too much (e.g. too large values for gamma, or for the class-specific learning rates etc). Albeit more ‘hacky’, if these regularization coefficients are tuned properly, they might result in a similar effect to that of having a prior. A fair comparison to the deterministic variant should include this. B) I think that gamma and z can be merged into a single set of parameters? In particular, imagine a per-dimension-of-\theta learning rate. This would then be large for a dimension when there is a larger need for adapting that dimension of \theta. In the case of large training sets, this can be large for all dimensions, recovering the behavior of a large gamma. For the case of diverse datasets, this would behave as the current z (updates a lot the dimensions of \theta that are irrelevant for the given task due to the dataset shift). C) Meta-Dataset (https://arxiv.org/abs/1903.03096) is a recent benchmark for few-shot classification that introduces both of what is referred to here as task imbalance and class imbalance and also is comprised of heterogeneous datasets and evaluates performance on some held-out datasets too. The current state-of-the-art on it (as far as I know) is CNAPs [1] which employs a flexible adaptation mechanism on a per-task basis but is fully amortized (performs no gradient-based adaptation to each task) and makes no explicit effort to tackle imbalanced tasks as is done here. I’m curious how this method would compete in that setup. It definitely seems to be a strong candidate for that benchmark! Less important ============= D) which dataset is used in Tables 4 and 5? I assume it’s Omniglot (due to the numbers being in the 90s) but it would be good to say this explicitly. E) In section 5.2, expressions such as x5 and x15 are used to characterize the degree of imbalance of a task. How exactly are these computed? Does x5 mean that the largest shot is 5 times larger than the smallest shot? It would be good to explicitly state this. F) In the Related Work section, in the Meta-learning paragraph there is a sentence that’s not accurate: “Metric-based approaches learn a shared metric space [...] such that the instances are closer to their correct prototypes than to others”. This sentence does not describe all metric based approaches. It describes Prototypical Networks (Snell et al) but not, for example, Matching Networks (Vinyals et al) nor many others that like Matching Networks perform example-based comparisons and don’t aggregate a class’ examples into a prototype. In a nutshell =========== I think this work is a useful contribution for moving towards a more realistic setting in few-shot classification. It captures some desiderata of models that can operate in more realistic settings and outperforms previous models in those scenarios. My comments above are mostly suggesting improvements and clarifications but I am inclined to recommend acceptance of this paper. References ========= [1] Fast and Flexible Multi-Task Classification Using Conditional Neural Adaptive Processes. Requeima et al. NeurIPS 2019."}
{"id": "iclr2020_342", "title": "Regularly varying representation for sentence embedding | OpenReview", "abstract": "Abstract:###The dominant approaches to sentence representation in natural language rely on learning embeddings on massive corpuses. The obtained embeddings have desirable properties such as compositionality and distance preservation (sentences with similar meanings have similar representations). In this paper, we develop a novel method for learning an embedding enjoying a dilation invariance property. We propose two algorithms: Orthrus, a classification algorithm, constrains the distribution of the embedded variable to be regularly varying, i.e. multivariate heavy-tail. and uses Extreme Value Theory (EVT) to tackle the classification task on two separate regions: the tail and the bulk. Hydra, a text generation algorithm for dataset augmentation, leverages the invariance property of the embedding learnt by Orthrus to generate coherent sentences with controllable attribute, e.g. positive or negative sentiment. Numerical experiments on synthetic and real text data demonstrate the relevance of the proposed framework.", "review": "Review:###The paper presented two methods for augmenting sentiment classification from the perspective of applying the Extreme Value Theory (EVT), including: 1) A classification algorithm which has an adversarial classifier to enforce the intermediate representations of a neural network to be similar to one EVT distribution, logistic distribution; 2) An encoder-decoder model that is able to generate grammatically coherent sentences with the same sentiment as the given input sentence. Questions: -- 1) the Fisher–Tippett–Gnedenko theorem states that it is possible that the maximum value of a set of iid samples converges to one of three plausible distributions, and the chosen logistic distribution falls into the Weibull distribution category. I have a couple concerns about this choice: 1.1) In order to show that the EVT indeed helps empirically in the way that an adversarial classifier enforces the inf-norm of vectors follow the Generalised Extreme Value (GEV) distribution, at least three plausible distributions from each form of the GEV distribution needs to be checked. The logistic distribution is interesting, but the marginal improvement gained by enforcing the lengths of the produced vectors to follow the logistic distribution could be a result of hyper-param tuning, which shouldn*t be a piece of supporting evidence. 1.2) From the perspective of applying the EVT, recent successful work from the best of my knowledge is on Anomaly Detection [1], where the EVT enables the system to learn from samples in only one class and also adjust the threshold for detecting the abnormal behaviour of samples. It is also theoretically grounded as the error variable of a logistic regression follows a Gumbel distribution which is one form of the GEV distribution, therefore, applying EVT for binary classification case makes sense. 1.3) From the perspective of learning representations with structured priors, there exists an interesting work on decomposing vector representations into lengths and directions and enforcing lengths to follow a uniform distribution and directions a Von Mises–Fisher (vMF) distribution as in [2]. It would be interesting to see if the proposed method is indeed better than the way that structured priors are enforced in [2]. 1.4) Linguistically, given the distributional hypothesis, the length of learnt vectors tends to be highly correlated with the frequency information of available concepts and the direction of them matters more. The argument is also presented by the paper. However, in sentiment analysis, the length could contain the information about how strong the sentiment of the input sentence is, so I am not convinced that the proposed method would be applicable in fine-grained sentiment analysis, such as Stanford Sentiment Treebank [3]. -- 2) A soft approximation over the inf-norm of a set of iid samples is log-sum-exp function, and it is the cdf of softmax function, which is also theoretically grounded in EVT for classifications. It could be a nicer story than the current one as the choice of the logistic distribution seems to be too intend. -- 3) The construction of the two datasets seems to be very arbitrary given that there exists a large number of sentiment analysis datasets and many with lots of samples, I am not sure that the results on the chosen constructed two datasets are sufficient enough to support the claim. 3.1) The size of the datasets is too small. Given that, the marginal improvement against the NN baseline could be a result of a specific initialisation, which doesn*t generalise to other random initialisations. 3.2) The dimension of vector representations is also too small. Normally, commonly used word embeddings are of 300 dimensions, and contextualised ones are of higher than 1200 dimensions. The chosen 50 dimension could prevent the NN baseline model to perform well and IMO, it is helpful for picking a suitable logistic prior than it is in a very high dimensional space. 3.3) There are many straightforward distributions that could be applied as a prior on the lengths of vector representations, e.g. the Rayleigh distribution in 2D and the Chi-squared distribution in higher-dimension. Then again, the distribution gets flatter and becomes similar to a uniform distribution when the dim goes higher, which is a common issue. It goes back to my concern or doubt on the usability of a prior on the norm of high dimensional vectors. -- 4) I am still interested in seeing EVT being applied in various domains, but I*d be in favor of more justifiable approaches. [1] Siffer, Alban, et al. *Anomaly detection in streams with extreme value theory.* Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2017. [2] Guu, Kelvin, et al. *Generating sentences by editing prototypes.* Transactions of the Association for Computational Linguistics 6 (2018): 437-450. [3] Socher, Richard, et al. *Recursive deep models for semantic compositionality over a sentiment treebank.* Proceedings of the 2013 conference on empirical methods in natural language processing. 2013."}
{"id": "iclr2020_343", "title": "MixUp as Directional Adversarial Training | OpenReview", "abstract": "Abstract:###MixUp is a data augmentation scheme in which pairs of training samples and their corresponding labels are mixed using linear coefficients. Without label mixing, MixUp becomes a more conventional scheme: input samples are moved but their original labels are retained. Because samples are preferentially moved in the direction of other classes iffalse -- which are typically clustered in input space -- fi we refer to this method as directional adversarial training, or DAT. We show that under two mild conditions, MixUp asymptotically convergences to a subset of DAT. We define untied MixUp (UMixUp), a superset of MixUp wherein training labels are mixed with different linear coefficients to those of their corresponding samples. We show that under the same mild conditions, untied MixUp converges to the entire class of DAT schemes. Motivated by the understanding that UMixUp is both a generalization of MixUp and a form of adversarial training, we experiment with different datasets and loss functions to show that UMixUp provides improved performance over MixUp. In short, we present a novel interpretation of MixUp as belonging to a class highly analogous to adversarial training, and on this basis we introduce a simple generalization which outperforms MixUp.", "review": " This paper proposes a novel data augmentation method, untied MixUp (UMixUp), which is a general case of both MixUp and Directional Adversarial Traning (DAT). DAT is referred to in this paper as a scheme that only input feature vectors are mixed, while MixUp also incorporates their corresponding labels. The authors provide a theoretical discussion that both DAT and UMixUp converges to be equivalent to each other when the number of training samples becomes infinity. Experimental results on Cifar 10, Cifar 100, MNIST, and Fashion MNIST show quantitative comparisons among the baseline, MixUp, and UMixUp. According to the author guideline, > There will be a strict upper limit of 10 pages for the main text. Reviewers will be instructed to apply a higher standard to papers in excess of 8 pages. The authors use nine pages. Therefore, the review should be more careful about its quality. Currently, I have three major concerns that keep me from judging this paper acceptable in ICLR 2020. First, the authors failed to cite two closely related papers below: - Tokozume et al., LEARNING FROM BETWEEN-CLASS EXAMPLES FOR DEEP SOUND RECOGNITION. ICLR, 2018. - Tokozume et al., Between-class Learning for Image Classification. CVPR, 2018. The first one is published in the previous ICLR and mixing two samples belonging to different classes. The second one is an application to image classification using ImageNet dataset, which is larger than the dataset used in this paper. What*s more important is that both papers propose that the mixing ratio of two samples is not linearly but depending on the strength of their signals. Since UMixUp is also focusing on the mixing ratio between two training samples, Between-Class Learning should have been compared to the proposed method. Secondly, the theoretical discussion is not so fascinating. Actually, both MixUp and UMixUp are shown to converge to DAT when the number of training samples tends to infinity. Data augmentation is, however, performed to remedy the lack of training samples in general. The discussion that the number of training samples is assumed to be large is the opposite situation. Thirdly, the experimental results show that the performance gain by UMixUp is relatively small in comparison to that of the original MixUp. There are no ablation studies using different values for alpha and beta, which are parameters for the policy of UMixUp. The authors reported that these values are defined using a heuristic search. Thus, we cannot see if the performance is sensitive to the parameter selection. I lean to reject this paper because of these concerns. I*m looking forward to seeing the revised version in another conference."}
{"id": "iclr2020_344", "title": "AN EFFICIENT HOMOTOPY TRAINING ALGORITHM FOR NEURAL NETWORKS | OpenReview", "abstract": "Abstract:###We present a Homotopy Training Algorithm (HTA) to solve optimization problems arising from neural networks. The HTA starts with several decoupled systems with low dimensional structure and tracks the solution to the high dimensional coupled system. The decoupled systems are easy to solve due to the low dimensionality but can be connected to the original system via a continuous homotopy path guided by the HTA. We have proved the convergence of HTA for the non-convex case and existence of the homotopy solution path for the convex case. The HTA has provided a better accuracy on several examples including VGG models on CIFAR-10. Moreover, the HTA would be combined with the dropout technique to provide an alternative way to train the neural networks.", "review": "Review:###The work proposes to learn neural networks using homotopy-based continuation method. The method divides the parameter space into two groups (extendable to multiple groups) and introduces a homotopy function which includes the original optimization problem as an extreme case. By varying the homotopy parameter, one can construct a continuous path from a supposedly easier to solve optimization problem to the problem of interest. The authors prove convergence in the non-convex case, the existence of solution path in the convex case and demonstrate the effectiveness of the proposed method on synthetic and real datasets. While the idea itself is rather intriguing and seems promising, the current presentation and experimentation does not meet the acceptance threshold. The writing of the draft needs a lot of improvement, in particular the notations the authors used are not consistent throughout the paper, which is very confusing. The synthetic example the authors used in section 4.1 are naturally decoupled among the different dimensions of the parameters, which is no surprise the proposed method would achieve 100% convergence as shown in table 1. It seems the division of the parameter space would matter. One would imagine there exists certain division leading to much easier to solve subproblems. Do the authors have any insight or experiments comparing different division strategies? Here*s a very closely-related work that should be cited and discussed: Wang, Xin. *An efficient training algorithm for multilayer neural networks by homotopy continuation method.* Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN*94). Vol. 1. IEEE, 1994. Typos: 1) Equation following remark in page 2, should H() be replaced by G() or \nabla H()? 2) After equation (7), should G():= \nabla H instead of F?"}
{"id": "iclr2020_345", "title": "RaPP: Novelty Detection with Reconstruction along Projection Pathway | OpenReview", "abstract": "Abstract:###We propose RaPP, a new methodology for novelty detection by utilizing hidden space activation values obtained from a deep autoencoder. Precisely, RaPP compares input and its autoencoder reconstruction not only in the input space but also in the hidden spaces. We show that if we feed a reconstructed input to the same autoencoder again, its activated values in a hidden space are equivalent to the corresponding reconstruction in that hidden space given the original input. In order to aggregate the hidden space activation values, we propose two metrics, which enhance the novelty detection performance. Through extensive experiments using diverse datasets, we validate that RaPP improves novelty detection performances of autoencoder-based approaches. Besides, we show that RaPP outperforms recent novelty detection methods evaluated on popular benchmarks.", "review": "Review:###This paper proposes a novelty detection method by utilizing latent variables in auto-encoder. Based on this, this paper proposes two metrics to quantifying the novelty of the input. Their main contribution is the NAP metric based on SVD. Their method is empirically demonstrated on several benchmark datasets, and they compare their proposed metrics with other competing methods using AUROC and experiments results are encouraging. The metrics proposed in this paper are intuitive and interesting. The experiments shown in Table2 is very convincing, and it could be better to extend Table3 to include other datasets (STL,OTTO, etc. )"}
{"id": "iclr2020_346", "title": "Revisiting Self-Training for Neural Sequence Generation | OpenReview", "abstract": "Abstract:###Self-training is one of the earliest and simplest semi-supervised methods. The key idea is to augment the original labeled dataset with unlabeled data paired with the model’s prediction. Self-training has mostly been well-studied to classification problems. However, in complex sequence generation tasks such as machine translation, it is still not clear how self-training woks due to the compositionality of the target space. In this work, we first show that it is not only possible but recommended to apply self-training in sequence generation. Through careful examination of the performance gains, we find that the noise added on the hidden states (e.g. dropout) is critical to the success of self-training, as this acts like a regularizer which forces the model to yield similar predictions for similar inputs from unlabeled data. To further encourage this mechanism, we propose to inject noise to the input space, resulting in a “noisy” version of self-training. Empirical study on standard benchmarks across machine translation and text summarization tasks under different resource settings shows that noisy self-training is able to effectively utilize unlabeled data and improve the baseline performance by large margin.", "review": "Review:###This paper investigates why self-training helps in machine translation and text summarization tasks, identifying that auxiliary noise can amplify the benefits of this process. The paper is well-structured and clearly written, and conducts a fairly thorough analysis of the issue at hand. Comments: - The authors argue self-training enhances smoothness, but I would like to see this explained mathematically/conceptually in greater detail. It is not immediately clear to me why this would be, particularly in the case of discrete text data. - Why not evaluate smoothness on the actual MT task instead of just the toy task? The authors could measure the L2 norm between encodings of source sentences for neighboring sentences (eg. based on edit distance or word-movers) vs very different sentences. And then compare the base model vs the one obtained from self-supervised training. - If the primary beneficial effect of self-supervised training is smoothness as the authors claim, then they should try enforcing smoothness in alternative ways to see if performance improves. Some options here could be (using dropout in all of them): 1) add your same noise process to the original labeled dataset to create augmented examples, 2) rather than the self-supervised objective, use an auxiliary training objective which says the predictions on each unlabeled datapoint should be similar to the predictions on noised versions of this datapoint, 3) some form of virtual adversarial training [VAT]. In fact, the authors should discuss [VAT] a bit more, as this paper also presents a smoothness-regularizer that is highly useful for semi-supervised learning. [VAT] Miyato et al. Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning. https://arxiv.org/abs/1704.03976 - As the authors write, self-training can be viewed as a form of entropy-regularization. Likewise, the input perturbation+dropout process also seems like it should affect the conditional entropies. Can the authors expound upon this connection a bit further? Some mathematical analysis would be nice to have here as well. - *Another way is to treat them separately – first we train the model on pseudo parallel data S, and then fine-tune it on real data L.* The authors should clarify the overall training process with a more precise explanation. I assume is actually: 1) train initial model M on (limited) real data L 2) use M to generate pseudo-targets for unlabeled data in S 3) train M on this pseudo-dataset S 4)fine-tune M on real data L Is this correct? And it should be clarified whether M in step (3) is fine-tuned from the M in step (1) or re-initialized from scratch before training begins (Based on later text, it seems like the latter, but this should be clarified early on). - Baseline in Figure 1 should be described a bit more clearly. - Since the BLEU score dropped from 3 to 1.9 when the authors continued training from the baseline model (Sec 3.2), isn*t the optimum hypothesis not ruled out? I don*t think the authors should make this claim, and rather state that the initialization does seem to play some role, but does not fully explain the benefits of self-training. - *We use a small LSTM model for 10K, Base Transformer for 100K/640K, and Big Transformer for 3.9M* Is this because these are the best performing models on these respective datasets? The authors should explain these decisions. - *We include quantitative comparison regarding joint training, separate training, and pseudo-parallel data filtering in Appendix B* Should clarify here (in main text) that separate training matches the performance of joint training. - typos: *joint traing*"}
{"id": "iclr2020_347", "title": "Angular Visual Hardness | OpenReview", "abstract": "Abstract:###The mechanisms behind human visual systems and convolutional neural networks (CNNs) are vastly different. Hence, it is expected that they have different notions of ambiguity or hardness. In this paper, we make a surprising discovery: there exists a (nearly) universal score function for CNNs whose correlation with human visual hardness is statistically significant. We term this function as angular visual hardness (AVH) and in a CNN, it is given by the normalized angular distance between a feature embedding and the classifier weights of the corresponding target category. We conduct an in-depth scientific study. We observe that CNN models with the highest accuracy also have the best AVH scores. This agrees with an earlier finding that state-of-art models tend to improve on classification of harder training examples. We find that AVH displays interesting dynamics during training: it quickly reaches a plateau even though the training loss keeps improving. This suggests the need for designing better loss functions that can target harder examples more effectively. Finally, we empirically show significant improvement in performance by using AVH as a measure of hardness in self-training tasks.", "review": "Review:###The paper proposes, when given a CNN, an image and its label, a measure called angular visual hardness (AVH). The paper shows that AVH correlates with human selection frequency (HSF) [RRSS19]. Pros: 1. The authors experimented with a representative set of trained models in my opinion. (More on this in Con2) 2. In Section 6, the authors acknowledge a substantive counter-example/argument. (More on this in Con2) Cons (I put the ones that weighed the most on my decision first): 1. The presentation is confusing, and at times self-contradictory. For example, in Section 1, the paper asserts that “the two systems differ in what they view as hard examples that appear ambiguous or uncertain“ but then proceeds to claim that AVH being a CNN-derived quantity (more on this in Con2) correlates well with HSF. In fact, [RRSS19] (heavily cited here) seems to suggest exactly the opposite that harder examples for humans are also hard for CNNs. This is not very surprising as high accuracies of these CNNs imply their agreement with human judgment: we are learning a dataset labeled by humans. (More on this in Que2) 2. AVH is a function of a particular CNN (architecture and parameter values) and the target class label _in addition_ to the input image. These dependencies make AVH a measure of the ambiguity of an image very problematic. Granted that the paper presents evidence that AVH correlates with HSF for a number of _trained_ models but they will be of different values. 3. The work is not well self-contained. HSF, the core quantity studied is not introduced with sufficient details. (See Que1 and Sug1) Some possible mistakes/typos: 1. Feature vector x and class weight w in general do not lie on S^n. Indeed your definition of A(u, v) only relies on u, v being nonzero. 2. There is a missing { above Definition 1. 3. In Definition 1, “for any x” -> for any (x, y). 4. In References, [10] is a duplicate of [11]. 5. The captions in Figures 5, 6, and 7 in Appendix A might be wrong. They say ||x|| whereas the y-axis in the plots is labeled AVH. Questions (I listed the important ones first): 1. What is human visual hardness (HVH)? How is HSF related to HVH? Why is being selected from a pool of images (in the procedure described in [RRSS19]) a good measure of HVH? 2. Since the class logit is exactly <x, w_c>, with arccos being a decreasing function, I expect AVH to behave very much like the opposite of model confidence (Definition 2). And this seems to be confirmed in Table 1 (performing a confidence calibration on validation set might increase this further). I wonder how AVH is different from model confidence _qualitatively_ and consequentially what insights do we gain (or should we expect to gain) by studying AVH instead of model confidence? 3. Degradation levels (DL) are mentioned early on but the experiments and figures were not shown in the main text (deferred till Appendix). What is the rationale? 4. The middle row in Figure 3 has a small range of ~1e-4. Is that expected? Can you provide some simple arguments? The closeness of the initial and final values of AVH in the AlexNet plot also concerns me. 5. How is the visualization in Figure 1 generated? It is not immediately clear to me how the high dimensional space is projected into 2D. My concern is that though suggestive in the figure, the category weights w_c in general do not spread out evenly. Do they? I would suggest reporting the angular separation of the category weights (maybe by showing them in a CxC matrix). 6. In Figure 2, what happens to the dark stripes? Are there no data points with the specified range of HSF values? Minor issues (factored little to none in my decision): 1. There are 60+ citations but their relevance to the current seems questionable in many cases. Many of them are accompanied by little or no technical comparison when they are mentioned. In particular, in Section 2 on the related work from psychology/neuroscience, little specifics are discussed to contextualize the current work. 2. Many arguments come across as (highly) speculative and imprecise. As a result, I find the reasoning and logical story diluted and hard to follow. 3. The comparison with feature norm seems poorly motivated. The other quantities, namely AVH and model confidence, both depend on the class label. 4. The term hardness has a rich history and connotation in the algorithmic analysis literature. I would suggest using a different term, as the hardness of a problem usually reflects some intrinsic aspects of its structure and not dependent on some algorithm. Suggestions: 1. If DL is not important to the core results, it will help simplify and focus the presentation by leaving them out entirely. 2. Try to be more concise and more precise in the presentation. It might also benefit from more formalism wherever possible, and more procedural details, when human studies or notions is involved. The latter seems to be a lesson from [RRSS19] (in regard to reproducibility). In summary, I do not recommend accepting the current article. (To authors and other reviewers) Please do not hesitate to directly point out my misunderstandings. I am open to acknowledging mistakes and revising my assessment accordingly."}
{"id": "iclr2020_348", "title": "Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication | OpenReview", "abstract": "Abstract:###We study the problem of regret minimization for distributed bandits learning, in which agents work collaboratively to minimize their total regret under the coordination of a central server. Our goal is to design communication protocols with near-optimal regret and little communication cost, which is measured by the total amount of transmitted data. For distributed multi-armed bandits, we propose a protocol with near-optimal regret and only communication cost, where is the number of arms. The communication cost is independent of the time horizon , has only logarithmic dependence on the number of arms, and matches the lower bound except for a logarithmic factor. For distributed -dimensional linear bandits, we propose a protocol that achieves near-optimal regret and has communication cost of order , which has only logarithmic dependence on .", "review": " The paper considers the problem of distributed multi-arm bandit, where M players are playing in the same stochastic environment. The goal of the paper is to have small over-all regret for all the players without a significant amount of communication between the players. The main contribution of this paper is obtaining regret ~root(M KT) with ~M bits of communication in MAB, and regret ~d*root(MT) with ~Md bits of communication in linear bandit setting. The main intuition of the algorithms in this paper is to do *best arm identification* with epoching: At every epoch t, the central server sends the set of possible best arms to each player and each player pulls it for 2^t /M times, followed by a communication round. Thus, the cumulative regret is comparable to having one player doing this epoch strategy for MT iterations, where the regret follows. The problem considered in this paper is interesting and the result is new, the technique looks simple on paper but it requires a masterful combination of known tricks in (linear) MAB to obtain the best bound. It seems that in the MAB setting, the lower bound could be further strengthened with a log(K) factor, since removing this factor would ultimately require *dynamic epoching* which is not possible with limited communication. This would mostly complete the picture in the distributed MAB regime. Missing citation: The authors are missing citations relevant to distributed MAB with collisions, see for example *Non-Stochastic Multi-Player Multi-Armed Bandits: Optimal Rate With Collision Information, Sublinear Without* After Rebuttal: I have read the authors* responses and acknowledge the sensibility of the statement."}
{"id": "iclr2020_349", "title": "On the Variance of the Adaptive Learning Rate and Beyond | OpenReview", "abstract": "Abstract:###The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam.", "review": "Review:###In this work, authors show that the bad performance of Adam is from the large variance of adaptive learning rate at the beginning of the training. Pros: 1. Authors demonstrate that the variance of the first few stages is large, which may interpret the degradation in the performance of Adam. 2. The empirical study supports the claim about the large variance. Cons: 1. Theoretically, authors didn’t illustrate why the large variance can result in the bad performance in terms of, e.g., convergence rate, generalization error, etc. 2. The performance of the proposed algorithm is still worse than SGD and it makes the analysis less attractive."}
{"id": "iclr2020_350", "title": "Distribution Matching Prototypical Network for Unsupervised Domain Adaptation | OpenReview", "abstract": "Abstract:###State-of-the-art Unsupervised Domain Adaptation (UDA) methods learn transferable features by minimizing the feature distribution discrepancy between the source and target domains. Different from these methods which do not model the feature distributions explicitly, in this paper, we explore explicit feature distribution modeling for UDA. In particular, we propose Distribution Matching Prototypical Network (DMPN) to model the deep features from each domain as Gaussian mixture distributions. With explicit feature distribution modeling, we can easily measure the discrepancy between the two domains. In DMPN, we propose two new domain discrepancy losses with probabilistic interpretations. The first one minimizes the distances between the corresponding Gaussian component means of the source and target data. The second one minimizes the pseudo negative log likelihood of generating the target features from source feature distribution. To learn both discriminative and domain invariant features, DMPN is trained by minimizing the classification loss on the labeled source data and the domain discrepancy losses together. Extensive experiments are conducted over two UDA tasks. Our approach yields a large margin in the Digits Image transfer task over state-of-the-art approaches. More remarkably, DMPN obtains a mean accuracy of 81.4% on VisDA 2017 dataset. The hyper-parameter sensitivity analysis shows that our approach is robust w.r.t hyper-parameter changes.", "review": " This paper introduces Distribution Matching Prototypical Network (DMPN) for Unsupervised Domain Adaptation (UDA). The proposed method explicitly models the feature distribution as a Gaussian mixture model in both source and target domains. Then the method aligns the target distribution with the source distribution by minimizing losses, which are called Gaussian Component Mean Matching (GCMM) and Pseudo Distribution Matching (PDM). This paper should be rejected because (1) the novelty of the main idea is marginal, and (2) the performance gain over the baseline methods is also marginal. Pan et al. already proposed the idea of transferring the knowledge from the source to the target using the prototype of each class. It is required to explain why explicit modeling performs better than implicit modeling of prototypes by theory or practice. In table 2, the proposed method seems better than TPN, but in the appendix, by comparing then in each category, the proposed method wins six categories, whereas TPN also wins six categories. Therefore, it is hard to say the proposed DMPN is more effective than another method. Each prototype is modeled using a mean and a covariance matrix. Why the authors don*t use the estimated covariance matrix to measure the distance in eq.5? Because the proposed method uses pseudo-labeling for the target domain, it seems that the weights to determine unreliable examples are crucial. The paper should show the sensitivity of ways to determine the weights. What happens if values of 0.1 and 0.9 are changed in (pi-0.1)/0.9 on page 6?"}
{"id": "iclr2020_351", "title": "Selection via Proxy: Efficient Data Selection for Deep Learning | OpenReview", "abstract": "Abstract:###Data selection methods, such as active learning and core-set selection, are useful tools for machine learning on large datasets, but they can be prohibitively expensive to apply in deep learning. Unlike in other areas of machine learning, the feature representations that these techniques depend on are learned in deep learning rather than given, requiring substantial training times. In this work, we show that we can greatly improve the computational efficiency of data selection in deep learning by using a small proxy model to perform data selection (e.g., selecting data points to label for active learning). By removing hidden layers from the target model or training for fewer epochs, we create proxies that are an order of magnitude faster to train. Although these small proxy models have higher error rates, we find that they empirically provide useful signal for data selection. We evaluate this “selection via proxy” (SVP) approach on several data selection tasks across five datasets: CIFAR10, CIFAR100, ImageNet, Amazon Review Polarity, and Amazon Review Full. For active learning, applying SVP can give an order of magnitude improvement in data selection runtime (i.e., the time it takes to repeatedly train and select points) without significantly increasing the final error. For core-set selection, proxies that are over 10x faster to train than their larger, more accurate target models can remove up to 50% of the data without harming the final accuracy of the target, making end-to-end training time savings possible.", "review": "Review:###This paper presents a method to speed up the data selection in active learning and core-set learning. The authors present a simple idea: instead of using the full model to select data points, they use a smaller model with fewer layers, potentially trained for fewer iterations. The authors show that this simple approach is able to speed up the data selection portion of both processes significantly with minimal loss in performance, and also results in significant speedup of the entire pipeline (data selection + training). This paper is timely and important -- there has been a lot of emphasis lately on the environmental costs of training deep learning models (e.g., Strubell et al., ACL 2019; Schwartz et al., 2019 arxiv:1907.10597). This paper shows that simple, almost trivial techniques can lead to significant runtime benefits for active learning and core-set learning. The authors present an extensive set of experiments that validate their hypothesis, and the paper is overall clearly written. Comments: 1. The correlation values in Figure 3 are quite diverse. It seemed *forgetting events* is much more correlated that the other two approaches. 2. Why do the authors think that SVP and their baselines failed to outperform random sampling on Amazon Review Full (towards the end of 3.3)? 3. Table 1 is very hard to interpret. I would advise the authors to look for a more succinct way to present their main findings. While this might seem contradictory, Figure 2, which is more reader-friendly, is also hard to interpret without the runtime values (nobody said visualization is easy!). 4. One thing that*s missing from this paper is training the smaller networks end-to-end. What would be the effect of using the proxy network as the main network as well? this is likely to lead to very significant runtime savings, and I wonder at what costs. Minor: 1. Towards the end of 3.2: I disagree that significant speedups are *uninteresting* if they lead to small reductions in performance."}
{"id": "iclr2020_352", "title": "Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation | OpenReview", "abstract": "Abstract:###Convolutional networks are not aware of an object*s geometric variations, which leads to inefficient utilization of model and data capacity. To overcome this issue, recent works on deformation modeling seek to spatially reconfigure the data towards a common arrangement such that semantic recognition suffers less from deformation. This is typically done by augmenting static operators with learned free-form sampling grids in the image space, dynamically tuned to the data and task for adapting the receptive field. Yet adapting the receptive field does not quite reach the actual goal -- what really matters to the network is the *effective* receptive field (ERF), which reflects how much each pixel contributes. It is thus natural to design other approaches to adapt the ERF directly during runtime. In this work, we instantiate one possible solution as Deformable Kernels (DKs), a family of novel and generic convolutional operators for handling object deformations by directly adapting the ERF while leaving the receptive field untouched. At the heart of our method is the ability to resample the original kernel space towards recovering the deformation of objects. This approach is justified with theoretical insights that the ERF is strictly determined by data sampling locations and kernel values. We implement DKs as generic drop-in replacements of rigid kernels and conduct a series of empirical studies whose results conform with our theories. Over several tasks and standard base models, our approach compares favorably against prior works that adapt during runtime. In addition, further experiments suggest a working mechanism orthogonal and complementary to previous works.", "review": "Review:###Updates: Thanks for the updates and I appreciate the authors* effort in running new experiments, whose results are very interesting and inspiring to me. My score remains unchanged. The main reason is that I am not an expert in this subject. -------------------------------------------- Traditional convolution neural networks are not aware of object’s geometric variations (i.e. rotation), while human being are very good at abstracting out such variation. In this paper, the authors propose an approach known as DKs (deformation kernels) to overcome such issues. The high level picture is make the convolutional filter data dependable. For convnets, the filter is independent of the data. To make it data dependent (potentially able to detect the objects geometric information), DKs first initialize a larger filter (say 9 * 9) that is universal to all inputs and then `subsampling` a smaller filter (say 3 *3), the `subsampling` strategy is input dependent and also learnable (similar to attention mechanism.) I do not have much background in this field, but I found the ideas of DKs very interesting and novel (assuming this is the first work to make the filter data dependent and learnable.) I lean to a weakly accept. Minor Comments: 1. Below equation (1). Z -> Z^2; above (1) : j in R^2 -> Z^2 2. above (5) as a composition --> as a sum ?? 3. can you elaborate on *In practice, we use the bilinear sampling operator to interpolate within the discrete kernel grid.* In particular, how the `smalle`r kernel is learned/subsampled from the larger kernel? (i.e. how Delta k is learned?) More interesting experiments? 1. Compare performance of the two methods below: a. standard architectures using data deformation (translation, rotation, dilation) b. DKs without applying data deformation. 2. Figure 4. Could you produce plots similar to the setting of figure 1., i.e. rotate / dilate the images. Showing that DKs could effectively capture such deformation."}
{"id": "iclr2020_353", "title": "Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation | OpenReview", "abstract": "Abstract:###We focus on temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationship in the generated data is much less explored. This is crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving the learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. We also propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirms the rankings computed with these metrics.", "review": "Review:###The paper presents video generation method with spacio-temporally consistent features. This is done through: a) temporal adversarial learning, b) Ping Pong loss, and c) metrics that quantify the quality. The methods are evaluated on two datasets and user studies. The idea is interesting and the paper is well written. The results are convincing. The originality of the concatenation of several frames is somewhat limited, since it is a standard procedure in other domains such as robotics. Nevertheless the results are positive. Seems like the metrics definitions were not included in the main body of the paper - the authors should either include them to remove from the contributions."}
{"id": "iclr2020_354", "title": "Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability | OpenReview", "abstract": "Abstract:###Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivalent to finding an optimal control of the discrete dynamical system, which allows one to utilize methods of successive approximations, an optimal control algorithm based on Pontryagin*s maximum principle, to train neural nets. This decoupled training method allows us to add constraints to the optimization, which makes the deep model more robust. The constrained optimization problem can be formulated as a semi-definite programming problem and hence can be solved efficiently. Experiments show that our method effectively improves deep model*s adversarial robustness.", "review": "Review:###The paper contributes to the robust training of neural networks as follows: 1) The paper uses the theoretical view of a neural network as a discretized ODE to develop a robust control theory aimed at training the network while enforcing robustness; 2) Such an objective is achieved by introducing Lyaponov stability and practically implemented through the method of successive approximations; 3) Empirical evaluation demonstrate that the newly introduced method performs as well as the SOTA in terms of defensive training. The paper is well written and proposes a well motivated and theoretically original strategy to robustly train neural networks against adversarial examples. The strength of the paper is definitively in its theoretical section, it would be really great to see an empirical improvement improvement on the SOTA. However, I do not believe the paper should be penalized for only matching other algorithm as it relies on a tractable and principled theoretical analysis."}
{"id": "iclr2020_355", "title": "Sensible adversarial learning | OpenReview", "abstract": "Abstract:###The trade-off between robustness and standard accuracy has been consistently reported in the machine learning literature. Although the problem has been widely studied to understand and explain this trade-off, no studies have shown the possibility of a no trade-off solution. In this paper, motivated by the fact that the high dimensional distribution is poorly represented by limited data samples, we introduce sensible adversarial learning and demonstrate the synergistic effect between pursuits of natural accuracy and robustness. Specifically, we define a sensible adversary which is useful for learning a defense model and keeping a high natural accuracy simultaneously. We theoretically establish that the Bayes rule is the most robust multi-class classifier with the 0-1 loss under sensible adversarial learning. We propose a novel and efficient algorithm that trains a robust model with sensible adversarial examples, without a significant drop in natural accuracy. Our model on CIFAR10 yields state-of-the-art results against various attacks with perturbations restricted to l? with ? = 8/255, e.g., the robust accuracy 65.17% against PGD attacks as well as the natural accuracy 91.51%.", "review": "Review:###Summary: The paper studies the phenomenon of trade-off between robust and standard accuracies that is usually observed in adversarial training. Many existing studies try to understand this trade-off and show that it is unavoidable. In contrast, this work shows that under a sensible definition of adversarial risk, there is no trade-off between standard accuracy and sensible adversarial accuracy. It is shown that Bayes optimal classifier has optimal standard and sensible adversarial accuracies. The authors then go on to propose a new adversarial training algorithm which tries to minimize the sensible adversarial risk. Experimental results show that models learned through the proposed technique have high adversarial and standard accuracies. Comments: 1) Sensible Adversarial Risk: The first contribution of the work is to define a new notion of adversarial risk which the authors call **sensible adversarial risk** and study its properties. There is a recent work[1] which also proposes a new definition of adversarial risk and which is similar in spirit to what the current work tries to achieve. In [1], the authors define a perturbation as adversarial only if it doesn*t change the label of the Bayes optimal classifier, which is similar to what the current paper does. Owing to this similarity, the properties of sensible adversarial risk obtained in Theorems 1,2 in the current work look similar to [1]. So the authors should discuss/compare their results with [1]. 2) Sensible Adversarial Training: I believe the major contribution of the paper is to propose an algorithm for minimizing sensible adversarial risk. However, I have some concerns with the proposed algorithm. The authors say that since the Bayes optimal classifier is unknown, they use a reference model f_r (which can be naturally trained). Consider the following scenario. Suppose the true data is given by (x, f^B(x)), for some unknown f^B; that is, the Bayes optimal classifier has perfect standard accuracy. Suppose f_r has perfect standard accuracy, but very bad adversarial accuracy on train and test sets. Suppose f_r is substituted for f^B in the sensible adversarial risk (with 0-1 loss). Then it is easy to see that f_r is a minimizer of the resulting objective. So the proposed algorithm will just output f_r in this scenario. This is clearly not desirable. Given this, I believe a more thorough understanding of the proposed algorithm is needed. When will the algorithm converge to non-robust classifiers? How should one initialize the algorithm to avoid such undesirable behavior? While the notion of sensible adversarial risk is sensible, it is not clear why it should result in such high adversarial accuracies as reported in Tables 1,5. The adversarial perturbation of epsilon=8/255 on cifar10 is considered so small that sensible adversarial risk (with reference model f^B) at any point will almost always be equal to the existing notion of adversarial risk at that point. So, minimizing sensible adversarial risk (assuming you are given f^B) is exactly equivalent to minimizing adversarial risk. But it is known that minimizing adversarial risk results in models with low standard accuracies. So I feel sensible adversarial risk is not the reason behind such high accuracies. Could the authors explain what is the reason for such good adversarial accuracies reported in the paper? 3) Experiments: While the experimental results on cifar10 look impressive, I have some concerns about the way the PGD attacks are run. I downloaded the model provided by the authors and ran PGD attack on it. I ran L_infty attacks with epsilon=8/255, step size=2/255. The results I obtained seem to differ from the results presented in the paper: PGD Steps | Adversarial accuracy of SENSE 20 62.09 50 60.34 100 59.99 I believe PGD attack with multiple random restarts will reduce the adversarial accuracy even further. Given this, I*d appreciate if the authors perform more careful attacks (with appropriate hyper-parameters) on their model. It*d also be great if the authors report the performance of PGD trained model using the same attacks used to report the performance on their model. 4) Other comments: I*m not sure if the toy example (cheese hole distribution) in Section 2 is helpful. What the authors seem to conclude from it is that adversarial training can improve standard accuracy. But I do not agree with these conclusions. What if Figure 1b is the true distribution? Will the same conclusions hold? In general, these toy examples need not be illustrative of the behavior on real datasets. So instead of having these toy examples, I*d suggest the authors have a thorough discussion on theoretical and experimental results. [1] Suggala, A. S., Prasad, A., Nagarajan, V., & Ravikumar, P. (2018). Revisiting Adversarial Risk. arXiv preprint arXiv:1806.02924."}
{"id": "iclr2020_356", "title": "On The Difficulty of Warm-Starting Neural Network Training | OpenReview", "abstract": "Abstract:###In many real-world deployments of machine learning systems, data arrive piecemeal. These learning scenarios may be passive, where data arrive incrementally due to structural properties of the problem (e.g., daily financial data) or active, where samples are selected according to a measure of their quality (e.g., experimental design). In both of these cases, we are building a sequence of models that incorporate an increasing amount of data. We would like each of these models in the sequence to be performant and take advantage of all the data that are available to that point. Conventional intuition suggests that when solving a sequence of related optimization problems of this form, it should be possible to initialize using the solution of the previous iterate---to *warm start** the optimization rather than initialize from scratch---and see reductions in wall-clock time. However, in practice this warm-starting seems to yield poorer generalization performance than models that have fresh random initializations, even though the final training losses are similar. While it appears that some hyperparameter settings allow a practitioner to close this generalization gap, they seem to only do so in regimes that damage the wall-clock gains of the warm start. Nevertheless, it is highly desirable to be able to warm-start neural network training, as it would dramatically reduce the resource usage associated with the construction of performant deep learning systems. In this work, we take a closer look at this empirical phenomenon and try to understand when and how it occurs. Although the present investigation did not lead to a solution, we hope that a thorough articulation of the problem will spur new research that may lead to improved methods that consume fewer resources during training.", "review": "Review:###This paper examines the problem of warm-starting the training of neural networks. In particular, a generalization gap arises when the network is trained on the full training set from the start versus being warm-started, where the network is initially (partially) trained on a subset of the training set, then switched to the full training set. This problem is practical, as it is often preferable to train online while data is collected to make up-to-date predictions for tasks (such as in online advertising or recommendation systems), but it has been found that retraining is necessary in order to obtain optimal performance. The paper also mentions active learning, domain shift, and transfer learning as two other relevant and important problems. The paper attempts to investigate this phenomena from a few different avenues: (1) simple experiments segmenting the training set into two different subsets, then training to completion or partial training on the first subset before switching to training on the full set; (2) looking at the gradients of warm-started models; (3) adding regularization; (4) warm-starting all layers, then training only last layer; (5) perturbing the warm-started parameters. Strengths: I believe very strongly in the practical impact of the problems presented in this paper. These indeed are challenging problems that are relevant to industry that have not been given sufficient attention in the academic literature. I appreciate the initial experimentation on this subject, and the clear demonstration of the problem through simple experiments. The paper is also well-written. Weaknesses: Some questions I had include: - Why is the Pearson correlation between parameters of the neural network a good way of measuring the correlation to their initialization? - Why is it surprising that the magnitude of the gradients of the *new* data is higher than at a random initialization? - Why does this phenomena occur even though the data is sampled from the same distribution? - Does this work have any relationship with work on generalization such as: [1] Recht, Benjamin, et al. *Do CIFAR-10 classifiers generalize to CIFAR-10?.* arXiv preprint arXiv:1806.00451 (2018). [2] Recht, Benjamin, et al. *Do ImageNet Classifiers Generalize to ImageNet?.* arXiv preprint arXiv:1902.10811 (2019). etc. Although I like the topic of this paper, the investigation seems too preliminary at this point. There is no clear hypothesis towards answering the problems proposed in the paper. There is also no analysis, which places the burden on the numerical experiments to demonstrate something interesting, and the experiments seem sparse and small-scale. For these reasons, I am inclined to reject this paper at this time, but I strongly encourage further exploration into the topic. Some potential questions or directions could include: 1. What if only a single epoch of training is used on 50% of the data? Does the gap appear in that setting? I ask because one would expect that a single epoch of training on 50% of the data, then training on new data would be equivalent to training on the full dataset from the start. 2. How does this gap change with respect to the (relative) amount of new data introduced into the problem? For example, if one were to only add a single datapoint to the training set, would one still observe this behavior? Could one potentially add data more incrementally (rather than half of the training set) and potentially mitigate this drastic change in the problem? 3. There are optimization algorithms specifically designed for stochastic optimization (with a fixed distribution) versus for online optimization (online gradient, Adagrad). Is the online optimization framework perhaps more *realistic* than the stochastic optimization framework in these streaming/warm-starting settings?"}
{"id": "iclr2020_357", "title": "Equilibrium Propagation with Continual Weight Updates | OpenReview", "abstract": "Abstract:###Equilibrium Propagation (EP) is a learning algorithm that bridges Machine Learning and Neuroscience, by computing gradients closely matching those of Backpropagation Through Time (BPTT), but with a learning rule local in space. Given an input x and associated target y, EP proceeds in two phases: in the first phase neurons evolve freely towards a first steady state; in the second phase output neurons are nudged towards y until they reach a second steady state. However, in existing implementations of EP, the learning rule is not local in time: the weight update is performed after the dynamics of the second phase have converged and requires information of the first phase that is no longer available physically. This is a major impediment to the biological plausibility of EP and its efficient hardware implementation. In this work, we propose a version of EP named Continual Equilibrium Propagation (C-EP) where neuron and synapse dynamics occur simultaneously throughout the second phase, so that the weight update becomes local in time. We prove theoretically that, provided the learning rates are sufficiently small, at each time step of the second phase the dynamics of neurons and synapses follow the gradients of the loss given by BPTT (Theorem 1). We demonstrate training with C-EP on MNIST and generalize C-EP to neural networks where neurons are connected by asymmetric connections. We show through experiments that the more the network updates follows the gradients of BPTT, the best it performs in terms of training. These results bring EP a step closer to biology while maintaining its intimate link with backpropagation.", "review": "Review:###Summary: this paper introduces a new variant of equilibrium propagation algorithm that continually updates the weights making it unnecessary to save steady states. The also mathematically prove the GDD property and show the effectiveness of their algorithm (Continual-EP) on MNIST. They also show C-EP is conceptually closer to biological neurons than EP. This paper tackles an important problem in bridging the gap between artificial neural networks and biological neurons. It is well-motivated and stands well in the literature as it improves its precedent algorithm (EP). The contributions are clear and well-supported by mathematical proofs. The experiments are accurately designed and results are convincing. I recommend accepting this paper as a plausible contribution to both fields."}
{"id": "iclr2020_358", "title": "Towards Disentangling Non-Robust and Robust Components in Performance Metric | OpenReview", "abstract": "Abstract:###The vulnerability to slight input perturbations is a worrying yet intriguing property of deep neural networks (DNNs). Though some efforts have been devoted to investigating the reason behind such adversarial behavior, the relation between standard accuracy and adversarial behavior of DNNs is still little understood. In this work, we reveal such relation by first introducing a metric characterizing the standard performance of DNNs. Then we theoretically show this metric can be disentangled into an information-theoretic non-robust component that is related to adversarial behavior, and a robust component. Then, we show by experiments that DNNs under standard training rely heavily on optimizing the non-robust component in achieving decent performance. We also demonstrate current state-of-the-art adversarial training algorithms indeed try to robustify DNNs by preventing them from using the non-robust component to distinguish samples from different categories. Based on our findings, we take a step forward and point out the possible direction of simultaneously achieving decent standard generalization and adversarial robustness. It is hoped that our theory can further inspire the community to make more interesting discoveries about the relation between standard accuracy and adversarial robustness of DNNs.", "review": "Review:###The work explores the problem of robustness and adversarial attacks in NN. In a multiclass prediction setting the idea is to use a taylor expansion of a loss coined CCKL which is the KL divergence between predictions for pairs of samples from different classes. The papers seems to find a convoluted route to arrive to something like this: when the Fisher information matrix has a strong eigenvalue the model is not robust. In other words it says that if the landscape close to convergence has valleys, or fast changes, the model is not robust. This appears quite obvious and related to previous similar studies. This statement is then empirically evaluated on CIFAR-10. The mathematical derivations should be made more rigorous. For example the paragraph on Cramer-Rao bound is very handwavy. Typos - is found these -> is found that these"}
{"id": "iclr2020_359", "title": "ManiGAN: Text-Guided Image Manipulation | OpenReview", "abstract": "Abstract:###We propose a novel generative adversarial network for visual attributes manipulation (ManiGAN), which is able to semantically modify the visual attributes of given images using natural language descriptions. The key to our method is to design a novel co-attention module to combine text and image information rather than simply concatenating two features along the channel direction. Also, a detail correction module is proposed to rectify mismatched attributes of the synthetic image, and to reconstruct text-unrelated contents. Finally, we propose a new metric for evaluating manipulation results, in terms of both the generation of text-related attributes and the reconstruction of text-unrelated contents. Extensive experiments on benchmark datasets demonstrate the advantages of our proposed method, regarding the effectiveness of image manipulation and the capability of generating high-quality results.", "review": " The paper presents a method to manipulate images using natural language input. The method uses a deep network to parse the input image, input text, and a generative adversarial network to produce the output image. The architecture is sensible, and close to prior work. The loss function is standard and almost identical to (Li et al., 2019). Strength: + Good results + Good experiments and ablation Areas of improvement: - Highlight contributions - Compare to Li et al The paper is well written, easy to understand. The results look very good, and the model performs well in the quantitative comparison. The experiments are extensive and contain a detailed ablation of the presented method. The paper could be stronger if the authors could highlight the contributions more. A large part of the technical section is either directly copied or slightly modified from prior work. The interesting new contributions (e.g. DCM) are only fully described in supplemental material. I*d recommend the authors to focus more on the contributions and talk about them in the main paper. Since large parts of the paper are derived from (Li et al., 2019), it would be nice to directly compare to it. I understand that Li etal do not use a reference image, but a comparison would help show the presented work improves upon the baselines."}
{"id": "iclr2020_360", "title": "Logic and the 2-Simplicial Transformer | OpenReview", "abstract": "Abstract:###We introduce the 2-simplicial Transformer, an extension of the Transformer which includes a form of higher-dimensional attention generalising the dot-product attention, and uses this attention to update entity representations with tensor products of value vectors. We show that this architecture is a useful inductive bias for logical reasoning in the context of deep reinforcement learning.", "review": "Review:###CONTRIBUTIONS: C1. Simplicialization of attention. Interpreting standard attention weights of a head as the model’s estimate of the probability of an edge = 1-simplex linking the variables encoded by 2 blocks of the Transformer, representing that the blocks stand in a binary relation encoded in the head, a generalization to 2-simplexes is made: now attention also estimates the probability of a 2-simplex indicating that three blocks stand in an arity-3 relation. C2. 2-simplicial attention. The standard query-key matching function, the scalar (dot) product, is related to the area of the 2-simplex determined by the 2 vectors and the origin, and this is generalized to the (unsigned) scalar triple product <a,b,c>, analogously related to the volume of the 3-simplex determined by 3 vectors and the origin. This now serves as the matching function between a query and 2 keys. Each head in each block generates a value vector u and two key vectors k1, k2 and the weight of attention from block(i) (with query p(i)) to the ordered pair (block(j), block(k)), a(i,j,k), is a softmax over <p(i), k1(j), k2(k)>. Attention returns to block(i) a sum in which a(I,j,k) weights B(u(j)*u(k)), with B a learned linear map and * the tensor product. C3. Experimental results applying Transformers T1 (with standard 1-simplicial attention) and T2 (with new 2-simplicial attention) to modeling an agent in bridge BoxWorld, trained with deep RL. This game crucially involves 3-way entity interactions, as keys of 2 colors open a box yielding a key of a 3rd color. T2 learns significantly faster than T1 (in the sense that the 1-standard-deviation-neighborhood of the learning curve of T2 becomes better than that of T1, plotted against environmental steps: Fig.4, and also essentially so when plotted against time adjusted steps: Fig 5). RATING: Accept REASONS FOR RATING (SUMMARY). Generalizing attention from 2nd- to 3rd-order relations is an important upgrade, and the mathematical context in which this is derived is insightful and may lead to further progress in the development of Transformers capable of constructing still richer structures. The experiments yield clear evidence of the value of 3rd-order attention in the context of a game designed to highlight 3rd-order relations. Strengths The exposition is clear and situated in a rather sophisticated formal setting. The connection to Clifford algebras may yield further fruit, besides the scalar triple product that is crucial to the definition of 2-simplicial attention. Although I am not an expert in RL, the experiments reported seem sound and the results clear. I believe that the strength of the paper justifies its length of 8.5 pages: the exposition of the key ideas, for this reader, hits a sweet spot between overly concise and overly verbose, and the ideas call for the quantity of space devoted to them. The decisions of what material to place in the Appendix seem well made. Although I have not studied the entire (13-page) Appendix, what I have read is clear and enlightening, another major contribution of the paper. Weaknesses Future work testing the value of 3rd-order attention in tasks that are less clearly perfectly designed for it will substantially strengthen the case for it. But in my view it is right to start testing a new architecture by showing it can indeed do what it is designed to do; further tests showing that what it is designed to do is of general utility are a second step. In this case, the first step, including creating of the model itself (consuming 5 non-verbose pages), is substantial enough to warrant publication. In my good-quality printout of the paper, I can’t see curves for best runs in Fig. 4."}
{"id": "iclr2020_361", "title": "Variational inference of latent hierarchical dynamical systems in neuroscience: an application to calcium imaging data | OpenReview", "abstract": "Abstract:###A key problem in neuroscience, and life sciences more generally, is that data is generated by a hierarchy of dynamical systems. One example of this is in \textit{in-vivo} calcium imaging data, where data is generated by a lower-order dynamical system governing calcium flux in neurons, which itself is driven by a higher-order dynamical system of neural computation. Ideally, life scientists would be able to infer the dynamics of both the lower-order systems and the higher-order systems, but this is difficult in high-dimensional regimes. A recent approach using sequential variational auto-encoders demonstrated it was possible to learn the latent dynamics of a single dynamical system for computations during reaching behaviour in the brain, using spiking data modelled as a Poisson process. Here we extend this approach using a ladder method to infer a hierarchy of dynamical systems, allowing us to capture calcium dynamics as well as neural computation. In this approach, spiking events drive lower-order calcium dynamics, and are themselves controlled by a higher-order latent dynamical system. We generate synthetic data by generating firing rates, sampling spike trains, and converting spike trains to fluorescence transients, from two dynamical systems that have been used as key benchmarks in recent literature: a Lorenz attractor, and a chaotic recurrent neural network. We show that our model is better able to reconstruct Lorenz dynamics from fluorescence data than competing methods. However, though our model can reconstruct underlying spike rates and calcium transients from the chaotic neural network well, it does not perform as well at reconstructing firing rates as basic techniques for inferring spikes from calcium data. These results demonstrate that VLAEs are a promising approach for modelling hierarchical dynamical systems data in the life sciences, but that inferring the dynamics of lower-order systems can potentially be better achieved with simpler methods.", "review": "Review:###The paper addresses the problem of inferring dynamical systems from observations. It aims at calcium imaging data, but in fact only tests its proposed methods on two synthesized data sets. I am not really familiar with neuroscience, and found the the paper relatively clearly written, and interesting. The model seems to work, for the synthetic data. I appreciate the confluence of neuroscience and machine learning. I rate the paper Weak Reject, for two main reasons. First, the paper is, although readable on a high level, not accessible in detail to readers who are unfamiliar with VLAEs, like myself. I feel to be a good fit for a conference with a broad range of topics, some effort should be made to keep papers, at least to some degree, readable by the broad audience. Otherwise, a narrow conference, e.g. on calcium imaging, might be a better venue. Second, the paper title promises *an application to calcium imaging data*, but that application is not actually presented. I understand the two synthetic data sets are meant to assess the viability of the model for the task, but the title calls for an actual application to real data. Some more feedback: There are a few things not immediately clear (which may or may not have to do with VLAEs). E.g. why does the model contain both an inference network and a generative network? The conclusion had a few grammar errors, please grammar-check it."}
{"id": "iclr2020_362", "title": "Bayesian Residual Policy Optimization: Scalable Bayesian Reinforcement Learning with Clairvoyant Experts | OpenReview", "abstract": "Abstract:###Informed and robust decision making in the face of uncertainty is critical for robots that perform physical tasks alongside people. We formulate this as a Bayesian Reinforcement Learning problem over latent Markov Decision Processes (MDPs). While Bayes-optimality is theoretically the gold standard, existing algorithms do not scale well to continuous state and action spaces. We propose a scalable solution that builds on the following insight: in the absence of uncertainty, each latent MDP is easier to solve. We split the challenge into two simpler components. First, we obtain an ensemble of clairvoyant experts and fuse their advice to compute a baseline policy. Second, we train a Bayesian residual policy to improve upon the ensemble*s recommendation and learn to reduce uncertainty. Our algorithm, Bayesian Residual Policy Optimization (BRPO), imports the scalability of policy gradient methods as well as the initialization from prior models. BRPO significantly improves the ensemble of experts and drastically outperforms existing adaptive RL methods.", "review": "Review:###This paper considers Bayesian Reinforcement Learning problem over latent Markov Decision Processes (MDPs). The authors consider making decisions with experts, where each expert performs well under some latent MDPs. An ensemble of experts is constructed, and then a Bayesian residual policy is learned to balance exploration-exploitation tradeoff. Experiments on Maze and Door show the advantages of residual policy learning over some baselines. 1. The Bayesian Reinforcement Learning problem this work considered is important. However, using experts immediately make the problem much easier. The original Bayesian Reinforcement Learning problem is then reduced to making decision with experts. Under this setting, there are many existing work with respect to exploration-exploitation tradeoff (OFU, Thompson Sampling) with theoretical guarantees. I did not see why using this residual policy learning (although as mentioned residual/boosting is useful under other settings) is reasonable here. There is not theoretical support showing that residual learning enjoys guaranteed performance. The motivation of introducing this heuristic is not clear. 2. The comparisons with UPMLE and BPO seems not convincing. Both BPO and UPMLE do not use experts, and ensemble of experts outperforms them as shown in the experiments. And the ensemble baseline here is kind of weak (why sensing with probability 0.5 at each timestep?) Always 0.5 does not make sense (exploration should decrease as uncertainty reduced). Other exploration methods should be compared, to empirically show the advantages/necessities of residual policy learning. Overall, I consider the proposed BRPO a simple extension of BPO, with a heuristic of learning ensemble policy to make decisions. BRPO is lack of theoretical support, and it is not clear why residual policy learning here is necessary and what exactly the advantage is over other exploration methods. Comparisons with simple baseline like exploration with constant probability is not enough to justify the proposed method. =====Update===== Thanks for the rebuttal. The comparison with PSRL improves the paper. However, I still think this paper needs more improvement as follows. Theorem 1 looks hasty to me. Batch policy optimization Alg is going to solve n_{sample} MDPs, which are generated from P_0. But Eq. (6) or Theorem 1 does not contain information about P_0, implying that P_0 has no impact, which is questionable (an uniform P_0 that can generate different MDPs and a deterministic P_0 can only generate one MDP should be very different). I suggest the authors do more detailed analysis. On the other hand, I expected whether this special *residual action* heuristic has any guarantees in RL? Can decomposing action into a_r + a_e provide us a better exploration method (than others like PSRL, OFU...)? Since this is the main idea of this paper as an extension of BPO, I think this point is important. The experiments shows that it can work in some cases, but I do not see an explanation (the *residual learning* paragraph is high level and I do not get an insight from that.)."}
{"id": "iclr2020_363", "title": "Informed Temporal Modeling via Logical Specification of Factorial LSTMs | OpenReview", "abstract": "Abstract:###Consider a world in which events occur that involve various entities. Learning how to predict future events from patterns of past events becomes more difficult as we consider more types of events. Many of the patterns detected in the dataset by an ordinary LSTM will be spurious since the number of potential pairwise correlations, for example, grows quadratically with the number of events. We propose a type of factorial LSTM architecture where different blocks of LSTM cells are responsible for capturing different aspects of the world state. We use Datalog rules to specify how to derive the LSTM structure from a database of facts about the entities in the world. This is analogous to how a probabilistic relational model (Getoor & Taskar, 2007) specifies a recipe for deriving a graphical model structure from a database. In both cases, the goal is to obtain useful inductive biases by encoding informed independence assumptions into the model. We specifically consider the neural Hawkes process, which uses an LSTM to modulate the rate of instantaneous events in continuous time. In both synthetic and real-world domains, we show that we obtain better generalization by using appropriate factorial designs specified by simple Datalog programs.", "review": "Review:###Review for Temporal Modeling via Logical Specification of Factorial LSTMs This paper addresses a key problem in machine learning: how to control the inductive bias of a model in an interpretable way. The paper contributes a Datalog-based language that allows a human to hand-code structural assumptions (typically based on domain knowledge) that are automatically translated into sparsity patterns in the parameter matrices of an ML model (in this case, a neural Hawkes process, although the idea would [probably] generalize to other cases). The language plus structured-neural-Hawkes process is demonstrated on a few very small problems, with mixed results. This paper is borderline. However, I tend to favor rejection because while the ideas are very interesting (and potentially impactful), validation of the claims is weak. Contributions: On the positive side: A Datalog interface to specifying structural zeros in parameter matrices is a good idea. The language is natural, and the high-level mapping from structure and objects to low-level parameters seems reasonable and potentially useful. The method makes it easier to specify an inductive bias. This is a step in the right direction; but at its heart, this paper does not do anything that couldn*t have been done by hand - it only makes it easier. The method is potentially more interpretable than other attempts at controlling inductive bias (for example, simple weight regularizaion), but see below for why this might be a red herring. The paper is very nicely written. It*s clear that a lot of attention to detail went into writing it. Well done. Weaknesses: There are a few major points to criticize about this paper. First, there is no clear learning or prediction benefit. The results are mixed: while it appears that the SHP learns faster than the unstructured HP, they appear to be asymptoting at the same point. This is perhaps to be expected, as the structural zeros introduced by the corresponding Datalog program effectively reduce the parameter count, but the shape of the learning curves is unchanged. (Also: please include error bars in Fig. 2(a1) and 2(a2)) The proper comparison would probably be to a low-rank parameter matrix, where the parameter count is similarly reduced, but in an unstructured way. That would allow us to disentangle *parameter count reduction* from *inductive bias*, which is currently not done in the paper. The results in Figure 3c are mixed - it appears that SHP is only better in 1/4 of the cases; in all other cases, the error bars seem to indicate that there is no predictive power. Finally, I am concerned that the method may give a false sense of explainability to the model - why it is true that a highly structured, symbolic language is being used to craft an inductive bias, there is no *symbol grounding*. That is, there is no guarantee that the neural part of the learning algorithm will use the parameters in the way the human intended it to, because the parameters are ultimately disconnected from the symbols."}
{"id": "iclr2020_364", "title": "Mem2Mem: Learning to Summarize Long Texts with Memory-to-Memory Transfer | OpenReview", "abstract": "Abstract:###We introduce the Mem2Mem mechanism, a conditional memory-to-memory mechanism that can be appended to general sequence-to-sequence frameworks, and demonstrate its effectiveness in improving long text neural abstractive summarization. Mem2Mem seamlessly transfers *memories* via readable/writable external memory modules that augment both the encoder and decoder. By enabling a memory transfer, Mem2Mem uses representations of highly salient input sentences and performs an implicit sentence extraction step. By allowing the decoder to read and write over encoded input memories, the models learn to store information about the input sequence while keeping track of what has been generated by the decoder. We evaluate Mem2Mem on abstractive text summarization and surpass the current state-of-the-art with less model capacity than competing models and with a full end-to-end training setup. To our knowledge, Mem2Mem is the first mechanism that can effectively use and update memory cells filled with different contextual information.", "review": "Review:###This paper proposes a neural model, Mem2Mem, to summarize long texts. The main contribution is to extend the typical RNN Encoder/Decoder architecture with a memory module used both for selection (from the encoder) and generation (for the decoder): essentially, the encoder transforms a document into L vectors (one per sentence) and uses an r-head attention mechanism to extract r sentences to store in the memory module. The decoder performs text generation by reading a subset of the memory and updates the memory bank during summary generation. My main concerns about this paper are the followings: - Novelty is a little limited: most architectural pieces were already present: either in Lin et al. for the regularization loss to promote diversity and sparsity for the memory bank choices, or in Benmalek et al. for the memory updates. - The experimental section is limited to a single dataset. On the positive side, the results are promising (especially in terms of ROUGE-L), even if limited to a single dataset, and previous summarization MAED (memory-augmented encoder/decoder) models have mostly focused on short documents or extractive summarization. I also appreciated the author presenting all the necessary details for training (in the Appendix)."}
{"id": "iclr2020_365", "title": "ADAPTIVE GENERATION OF PROGRAMMING PUZZLES | OpenReview", "abstract": "Abstract:###AI today is far from being able to write complex programs. What type of problems would be best for computers to learn to program, and how should such problems be generated? To answer the first question, we suggest programming puzzles as a domain for teaching computers programming. A programming puzzle consists of a short program for a Boolean function f(x) and the goal is, given the source code, to find an input that makes f return True. Puzzles are objective in that one can easily test the correctness of a given solution x by seeing whether it satisfies f, unlike the most common representations for program synthesis: given input-output pairs or an English problem description, the correctness of a given solution is not determined and is debatable. To address the second question of automatic puzzle generation, we suggest a GAN-like generation algorithm called “Troublemaker” which can generate puzzles targeted at any given puzzle-solver. The main innovation is that it adapts to one or more given puzzle-solvers: rather than generating a single dataset of puzzles, Tro", "review": "Review:###This paper proposes a trainable *puzzle* program synthesizer that outputs a program f with a specific syntax. These *puzzles* are structured as boolean programs, and a program solver solves the puzzle by finding an input x such that f(x) = True. The authors motivate this task by making a case that puzzles of this sort are a good domain for teaching computers how to program. The paper is fairly clear overall. There is some repetition in the early parts, so this could be restructured a bit, but these are minor points. A more significant restructuring however, is that this work would benefit from the related work being present the beginning of the work. Since this work is so similar in many ways to previous work I think the overall clarity of the paper would be improved, and the contributions clearer, if the work was better situated with respect to related work. The experiments demonstrate that the trainable puzzle generator is able to produce harder (i.e. takes longer time to solve) puzzles than a random or probabilistic generator of the same grammar. While this does show that the program generator is learning something useful, these results are insufficient to show the utility of this approach in any real context. It seems the most interesting solver to assess is a trainable solver. Yet only 1 of the 4 solvers they assess is trainable. I know the authors make a point that they are not putting forward any new solver algorithms. That ok, however, taking existing trainable solvers and assessing how they perform with this guided puzzle generation vs. some other puzzle generation approach is a critical empirical study. Furthermore, it would be helpful to have more discussion of the baseline methods of generating puzzles. When the trainable puzzle solver was originally proposed, how was it trained? Where did the data come from? How does that compare to this approach. I am not very familiar with this literature, and I imagine this paper would be of interest to folks outside the program synthesis space, so it would be very helpful to better explain this (also we note about related work). There are several additional empirical analysis that could be added to improve this work. For example, for the trainable solver, a plot of (training time) vs (time to solve puzzle) would be interesting. Beyond looking at training time, does a solver trained with the guided puzzle generator end up being a *better* solver in some way? Are the resulting puzzles harder but still being solved? Is there a way of quantifying the *hardness* of a puzzle? Perhaps a proxy like size? Then it would be cool to plot (training time) vs (approx puzzle hardness) to demonstrate that . the puzzle generator is really developing a reasonable curriculum. Finally, there is a bunch of related work that I think is missing. Again, I*m not super familiar with this work, but I think there is a lot of curriculum learning stuff within RL that seems super relevant. Of particular relevance is the Alice/Bob framework from *Intrinsic motivation and automatic curricula via asymmetric self-play* seems very similar to the work at hand. Something that is interesting in the Alice/Bob framework that could be transferred over here is the notion of the generator wanting to make a puzzle hard, but not too hard, i.e. make it just outside the solvers current capabilities. Overall my assessment is that this paper doesn*t quite meet the standard for ICLR. My two major critiques are (1) the related work is seriously lacking making it difficult to situate this work in a broader context. The authors also seem to miss the entire curricular learning literature. (2) The empirical evaluations are lacking. In particular, more thorough analysis of how the generator is behaving, the type of curriculum it learns, and the resulting impact this has on a trainable solver all are missing. Furthermore, more focus on trainable solvers would improve this work. I*m not an expert in this area so it is possible I misjudged the significance of this work. I*m certainly open to revising my assessment if the authors are able to address (2) in a meaningful way."}
{"id": "iclr2020_366", "title": "Learning to Control PDEs with Differentiable Physics | OpenReview", "abstract": "Abstract:###Predicting outcomes and planning interactions with the physical world are long-standing goals for machine learning. A variety of such tasks involves continuous physical systems, which can be described by partial differential equations (PDEs) with many degrees of freedom. Existing methods that aim to control the dynamics of such systems are typically limited to relatively short time frames or a small number of interaction parameters. We show that by using a differentiable PDE solver in conjunction with a novel predictor-corrector scheme, we can train neural networks to understand and control complex nonlinear physical systems over long time frames. We demonstrate that our method successfully develops an understanding of complex physical systems and learns to control them for tasks involving multiple PDEs, including the incompressible Navier-Stokes equations.", "review": "Review:###[Summary] This paper proposes to combine deep learning and a differentiable PDE solver for understanding and controlling complex nonlinear physical systems over a long time horizon. The method introduces a predictor-corrector scheme, which employs a hierarchical structure that temporally divides the problem into more manageable subproblems, and uses models specialized in different time scales to solve the subproblems recursively. For dividing the problem into subproblems, they use an observation predictor network to predict the optimal center point between two states. To scale the scheme to sequences of arbitrary length, the number of models scales with O(log N). For each subproblem, the authors propose to use a corrector network to estimate the control force to follow the planned trajectory as close as possible. They have compared their method with several baselines and demonstrated that the proposed approach is both more effective and efficient in several challenging PDEs, including the incompressible Navier-Stokes equations. [Major Comments] Predicting the middle point between two states for modeling the dynamics via deep neural networks is not new, but I did not know any other works that use this idea for controlling PDEs. I like the idea of splitting the control problem into a prediction and a correction phase, which leverages the power of deep neural networks and also incorporates our understanding of physics. The introduction of the hierarchical structure alleviates the problem of accumulating error in single-step forwarding models and significantly improves the efficiency of the proposed method. The videos for fluid control in the supplement materials also convincingly demonstrate the effectiveness of the technique. I still have a few questions regarding the applicability and the presentation of the paper. Please see the following detailed comments. [Detailed Comments] In Section 3, the authors claim that their model *is conditioned only on these observables* and *does not have access to the full state.* However, the model requires a differentiable PDE solver to provide the gradient of how interactions affect the outcome. These seem to contradict each other. Doesn*t the solver require full-state information to predict the behavior of the system? Related to the previous question, how can we make use of the differentiable PDE solver if we are uncertain or unknown of the underlying physics, i.e., partially observable scenarios. The algorithm described in Section 5 seems to be the core contribution of this work. Instead of describing the algorithm in words, I think it would make it more clear if the authors can add an algorithm block in the main paper. It would also be better if the authors can include a few sentences describing the algorithm in the abstract to inform the readers of what to expect. Figure 4 is a bit confusing, and it would be better if the authors can include the label for the x-axis. Besides, in the caption, the authors said that they show *the target state in blue.* However, there are a lot of blue lines in the figure, and it is hard to know, at first glance, which one of them is the target. In Table 1, the bottom two methods are using the same execution scheme and training loss, but the results are different. Is there a typo? Also, it would be better to bold the number that has the best performance."}
{"id": "iclr2020_367", "title": "Selective sampling for accelerating training of deep neural networks | OpenReview", "abstract": "Abstract:###We present a selective sampling method designed to accelerate the training of deep neural networks. To this end, we introduce a novel measurement, the {it minimal margin score} (MMS), which measures the minimal amount of displacement an input should take until its predicted classification is switched. For multi-class linear classification, the MMS measure is a natural generalization of the margin-based selection criterion, which was thoroughly studied in the binary classification setting. In addition, the MMS measure provides an interesting insight into the progress of the training process and can be useful for designing and monitoring new training regimes. Empirically we demonstrate a substantial acceleration when training commonly used deep neural network architectures for popular image classification tasks. The efficiency of our method is compared against the standard training procedures, and against commonly used selective sampling alternatives: Hard negative mining selection, and Entropy-based selection. Finally, we demonstrate an additional speedup when we adopt a more aggressive learning-drop regime while using the MMS selective sampling method.", "review": "Review:###This paper proposes a minimal margin score (MMS) criterion to speed up the training of the deep networks. I would vote for a clear rejection of this paper. This submission is a clearly unfinished one. The two biggest problems are as follows 1. Lack of a comprehensive discussion on rules for sampling section, please see *Automated Curriculum Learning for Neural Networks*. Why previous methods are worse than the proposed one is not clear. 2. All experiments are only compared with baseline approaches. In some experiments, the improvements are really marginal (e.g., Figure 2). In these cases, the STD of these curves is not shown, it is not clear whether the improvements are significant or not."}
{"id": "iclr2020_368", "title": "Mixing Up Real Samples and Adversarial Samples for Semi-Supervised Learning | OpenReview", "abstract": "Abstract:###Consistency regularization methods have shown great success in semi-supervised learning tasks. Most existing methods focus on either the local neighborhood or in-between neighborhood of training samples to enforce the consistency constraint. In this paper, we propose a novel generalized framework called Adversarial Mixup (AdvMixup), which unifies the local and in-between neighborhood approaches by defining a virtual data distribution along the paths between the training samples and adversarial samples. Experimental results on both synthetic data and benchmark datasets exhibit the benefits of AdvMixup on semi-supervised learning.", "review": "Review:###The paper proposes AdvMixup which unifies the local and in-between neighborhood approaches. The key idea is to interpolate between a real example and an adversarial example and use the interpolation between soft labels from the EMA model as the virtual target. They give an intuitive explanation of crafting more aggressive adversarial examples as verified in Figure 3. The authors evaluate AdvMixup on both synthetic data and widely-used benchmarks. AdvMixup outperforms previous baselines. An ablation study and discussion are also provided. The paper is clear and easy to follow. The proposed method is very simple and straightforward given the literature. However, it is hard to say this is novel enough for ICLR since it looks like a combination of existing techniques. The novelty is my main concern. And I wonder whether it is better to generate stronger adversarial examples. It would be more insightful if the authors consider more attack methods than the one-step approach and see whether there is a trade-off. How about the performance of AdvMixup when the classes are larger, e.g. CIFAR-100 and ImageNet?"}
{"id": "iclr2020_369", "title": "A Deep Recurrent Neural Network via Unfolding Reweighted l1-l1 Minimization | OpenReview", "abstract": "Abstract:###Deep unfolding methods design deep neural networks as learned variations of optimization methods. These networks have been shown to achieve faster convergence and higher accuracy than the original optimization methods. In this line of research, this paper develops a novel deep recurrent neural network (coined reweighted-RNN) by unfolding a reweighted l1-l1 minimization algorithm and applies it to the task of sequential signal reconstruction. To the best of our knowledge, this is the first deep unfolding method that explores reweighted minimization. Due to the underlying reweighted minimization model, our RNN has a different soft-thresholding function (alias, different activation function) for each hidden unit in each layer. Furthermore, it has higher network expressivity than existing deep unfolding RNN models due to the over-parameterizing weights. Moreover, we establish theoretical generalization error bounds for the proposed reweighted-RNN model by means of Rademacher complexity. The bounds reveal that the parameterization of the proposed reweighted-RNN ensures good generalization. We apply the proposed reweighted-RNN to the problem of video-frame reconstruction from low-dimensional measurements, that is, sequential frame reconstruction. The experimental results on the moving MNIST dataset demonstrate that the proposed deep reweighted-RNN significantly outperforms existing RNN models.", "review": "Review:###This paper proposes a novel method to solve the sequential signal reconstruction problem. The method is based on the deep unfolding methods and incorporates the reweighting mechanism. Additionally, they derive the generalization error bound and show how their over-parameterized reweighting RNNs ensure good generalization. Lastly, the experiments on the task of video sequence reconstruction suggest the superior performance of the proposed method. I recommend the paper to be accepted for mainly two reasons. First, they derive a tighter generalization bound for deep RNNs; Second, the experiment results align with the theory and show the continuous improvements when increasing the depth of RNNs. Questions: 1. How is the computation complexity of the proposed method when compared with other methods? Will the reweighting l1-l1 norm significantly increase the computation time? 2. The experiments show that increasing the depth and/or width of the networks yields better performance, however, is there a boundary for such performance gain? For example, if the depth continues increasing, will the proposed method suffer the similar problem as other methods (performance does not improve or even degrade)? 3. As the MOVING MNIST dataset is from a relatively simple and special domain, is it possible to reproduce the similar performance gain on other more realistic datasets? 4. Are there any known limitations of the proposed method?"}
{"id": "iclr2020_370", "title": "Understanding Top-k Sparsification in Distributed Deep Learning | OpenReview", "abstract": "Abstract:###Distributed stochastic gradient descent (SGD) algorithms are widely deployed in training large-scale deep learning models, while the communication overhead among workers becomes the new system bottleneck. Recently proposed gradient sparsification techniques, especially Top- sparsification with error compensation (TopK-SGD), can significantly reduce the communication traffic without obvious impact on the model accuracy. Some theoretical studies have been carried out to analyze the convergence property of TopK-SGD. However, existing studies do not dive into the details of Top- operator in gradient sparsification and use relaxed bounds (e.g., exact bound of Random- ) for analysis; hence the derived results cannot well describe the real convergence performance of TopK-SGD. To this end, we first study the gradient distributions of TopK-SGD during training process through extensive experiments. We then theoretically derive a tighter bound for the Top- operator. Finally, we exploit the property of gradient distribution to propose an approximate top- selection algorithm, which is computing-efficient for GPUs, to improve the scaling efficiency of TopK-SGD by significantly reducing the computing overhead.", "review": "Review:###This paper makes two contributions to gradient sparsification to reduce the communication bottleneck in distributed SGD. 1) Based on an assumption on the distribution of gradient coordinate values that are backed by an empirical study, the paper derives a tighter bound on the approximation quality of top-k gradient sparsification. This result induces better convergence bounds. 2) The authors note that the top-k cannot benefit from the highly parallel architectures popular in ML, and propose an approximate top-k sparsification operator. This operator tries up to three thresholds and checks how many entries are larger than this value. The initial guess is based on approximating the distribution of gradient coordinates with a normal distribution again. My score is weak reject. I believe that both observations are valid, and that their solutions might be practically meaningful. However, I would like to see the comparison to another baselines [1]. I would also urge the authors to make it more clear that their theoretical results are based on a strong assumption on the distribution of gradients. The top-k approximation algorithm is practical but I find 3-step threshold search inelegant. Comments: 1) [1] is another baseline -- compare your method with it too. 2) 4.3 Convergence performance *operator can select close elements with Top_k* --- It seems obvious that it can select similar elements. The question is whether the number of elements chosen is accurate. I would like to see this evaluated. It is unclear if this scheme is biased. As far as I can see, it might be over- or under-sparsifying. 3) 3.1 Gradient Distribution *One can easily prove* --- please do so (in the appendix) 4) Theorem 1 - Looks like this can*t be true in general. I think it assumes d -> infinity. 5) 3.1 Gradient Distribution *then pi is a decreasing function* --- should this be pi^2. Also in figure 3, the result of Eqn 7 is only correct if the curve if pi^2. 6) Figure 2: I am not convinced that these distributions are *gaussian*. In fact, they seem peakier. It seems to me that this should improve the results (i.e. make the descending pi^2 curve more convex). If this is true, I would encourage the authors to discuss this. BTW, the distribution is in terms of the whole model, or just one randomly picked layer? 7) Conclusion *theoretically tighter bound* --- because the assumption on the distribution empirical, I find it slightly misleading to call this a *theoretical bound*. I would urge the authors to make this very clear. (This does not mean I find the bound meaningless) 8) Introduction: *O(d), which generally limits the system scalability* --- The O(d) does not explain scalability in terms of number of workers as is suggested. Note that even though bandwidth scales with O(d) in all reduce, the latency does scale with n. This should not be ignored. 9) Related work/Gradient Sparsification --- Please add a reference for empirical success of top-k. I am not aware of much use outside of academia. 10) Quite a few language errors (some paragraphs/sentences don*t make sense at all and there are many cases with missing *a*s etc.) 11) Some of the experimental details are missing - what are the learning rates/batch sizes used in experiments. - How topK is performed? Layer-wise or for the full gradient. - Table 1 *experimental settings*: how these values were chosen. - Table 2 --- please define how scaling efficiency is computed. - Table 2 --- Do these algorithms achieve the same validation accuracy during the training? - Figure 1: which k was used in these plots? 12) Figure 6: in VGG-16, the gap of 1 percentage point is quite large. This seems expected as the compression ratio is very high (1000x). 13) Figure 6: Imagenet training scheme is not standard. SOTA validation accuracy for the Imagenet benchmark with Resnet50 is around 76%. Would it also have similar quality loss on later stages as in training VGG or ResNet20 on cifar? 14) Eqn. 8 - I couldn*t follow the first inequality. 15) Introdcution: The first few times *distribution of gradients* is mentioned, it was unclear to me if this was over *coordinates* (as it seems to be), or over *data points*, or *training time*. Please clarify. [1] Jiarui Fang, Cho-Jui Hsieh. Accelerating Distributed Deep Learning Training with Gradient Compression."}
{"id": "iclr2020_371", "title": "Disentangling Style and Content in Anime Illustrations | OpenReview", "abstract": "Abstract:###Existing methods for AI-generated artworks still struggle with generating high-quality stylized content, where high-level semantics are preserved, or separating fine-grained styles from various artists. We propose a novel Generative Adversarial Disentanglement Network which can disentangle two complementary factors of variations when only one of them is labelled in general, and fully decompose complex anime illustrations into style and content in particular. Training such model is challenging, since given a style, various content data may exist but not the other way round. Our approach is divided into two stages, one that encodes an input image into a style independent content, and one based on a dual-conditional generator. We demonstrate the ability to generate high-fidelity anime portraits with a fixed content and a large variety of styles from over a thousand artists, and vice versa, using a single end-to-end network and with applications in style transfer. We show this unique capability as well as superior output to the current state-of-the-art.", "review": "Review:###The paper proposes an image generation method with the focus on generating anime faces from various artists. The proposed method which is a combination of conditional GANs and conditional VAEs manages to generate high fidelity anime images with various styles. The paper is well-written and easy to follow and understand. The goals are clearly stated and the background (which is more of history), as well as related work, is comprehensive. The decision behind every design decision has been mentioned in detail which makes the paper stronger. The main writing flaw of the paper is in the figures where more annotation and caption is required to make them easy to understand. For example: - The significance of colors in Figure 1 (architecture of the model) is not annotated at all and the letters are not clear either (although they are described in the text itself a figure should be comprehensive by itself). - Figure 2 and Figure 4 are really hard to understand with very limited annotation and caption. I had to read the text multiple time to Figure out what is what in these figures which is not a good sign for clarity. In terms of experiments, I think where the paper suffers the most is in comparison with other conditional methods. In Section 5 it has been clearly mentioned that *this result can be expected from a class-conditional GAN and the focus in on Disentanglement* however very little evidence has been provided for superior disentanglement. More experiments are required to demonstrate the capabilities of the model compared to other conditional methods (which is currently only limited to StarGAN) as well as its capability of disentanglement. I agree with the authors that quantitative evaluation of generated anime faces is not easy (although it is possible with a carefully designed human study), however, the disentanglement (which is the focus of the paper) is easy to evaluate quantitatively. This demands for more experiments on disentanglement datasets with known generative factors. Although the current ablation study in the Appendix provides more details for architectural decisions, a more qualitative and quantitative comprehensive ablation study (by actually ablating the final model) can help to demonstrate these decisions. In conclusion, the paper has great results. We all know a big part of writing this kind of paper is to make the model *work* and authors truly demonstrate that they worked hard. However, the impact of the paper (in the current form) is not clear. With the focus on disentanglement, little evidence has been provided to justify the capability of the proposed method. I believe by addressing my comments on the experiments the paper can be easily pushed above the acceptance bar. Also releasing the code dataset should increase the impact of the paper."}
{"id": "iclr2020_372", "title": "A Theoretical Analysis of Deep Q-Learning | OpenReview", "abstract": "Abstract:###Despite the great empirical success of deep reinforcement learning, its theoretical foundation is less well understood. In this work, we make the first attempt to theoretically understand the deep Q-network (DQN) algorithm (Mnih et al., 2015) from both algorithmic and statistical perspectives. In specific, we focus on a slight simplification of DQN that fully captures its key features. Under mild assumptions, we establish the algorithmic and statistical rates of convergence for the action-value functions of the iterative policy sequence obtained by DQN. In particular, the statistical error characterizes the bias and variance that arise from approximating the action-value function using deep neural network, while the algorithmic error converges to zero at a geometric rate. As a byproduct, our analysis provides justifications for the techniques of experience replay and target network, which are crucial to the empirical success of DQN. Furthermore, as a simple extension of DQN, we propose the Minimax-DQN algorithm for zero-sum Markov game with two players, which is deferred to the appendix due to space limitations.", "review": "Review:###The authors provide a theoretical analysis of deep Q-learning based on the neural fitted Q-iteration (FQI) algorithm [1]. Their analysis justifies the techniques of experience replay and target network, both of which are critical to the empirical success of DQN. Moreover, the authors establish the algorithmic and statistical errors of the neural FQI algorithm. Then, the authors propose the Minimax-DQN algorithm for the zero-sum Markov game with two players. They further establish the algorithmic and statistical convergence rates of the sequence of action-value functions obtained by the Minimax-DQN algorithm. [1] Martin Riedmiller. Neural fitted Q iteration–first experiences with a data efficient neural reinforcement learning method. In European Conference on Machine Learning, pp. 317–328. Springer, 2005. The strengths of this paper are as follows. 1. This paper is theoretically sound. The authors establish the convergence rates with detailed proofs step by step. 2. It is the first theoretical analysis that provides the errors of the neural FQI algorithm with a ReLU network. This analysis provides a rigorous approach to understand deep q-learning algorithms. 3. The authors propose an extension of DQN for the zero-sum Markov game with two players. They further analyze the convergence rates of the sequence of action-value functions obtained by the proposed algorithm. Minor comments: 1. Page 2: In Notation, * * may be * *? 2. Page 3: In the 2th line of Section 2.2, * * may be * *."}
{"id": "iclr2020_373", "title": "Residual EBMs: Does Real vs. Fake Text Discrimination Generalize? | OpenReview", "abstract": "Abstract:###Energy-based models (EBMs), a.k.a. un-normalized models, have had recent successes in continuous spaces. However, they have not been successfully applied to model text sequences. While decreasing the energy at training samples is straightforward, mining (negative) samples where the energy should be increased is difficult. In part, this is because standard gradient-based methods are not readily applicable when the input is high-dimensional and discrete. Here, we side-step this issue by generating negatives using pre-trained auto-regressive language models. The EBM then works in the {em residual} of the language model; and is trained to discriminate real text from text generated by the auto-regressive models. We investigate the generalization ability of residual EBMs, a pre-requisite for using them in other applications. We extensively analyze generalization for the task of classifying whether an input is machine or human generated, a natural task given the training loss and how we mine negatives. Overall, we observe that EBMs can generalize remarkably well to changes in the architecture of the generators producing negatives. However, EBMs exhibit more sensitivity to the training set used by such generators.", "review": "Review:###The paper discusses training an energy-based discriminator to discriminates the generated examples from a language model from human-written sentences. To do this, it assigns lower energy value to the real text and higher energy values to the generated text, by optimizing binary cross-entropy. The motivation of the paper claims that the authors solve (or at least move toward solving) the problem of sampling from energy-based models for high-dimensional sequence problems; however, I do not see any contribution towards that. The importance of having a well-trained energy-based model is its modeling capability, which also enables us to sample from it (that is done using gradient descent and Langevin dynamic for modeling images). But here, the energy-based model has no effect on the quality of samples from the auto-regressive model, so it does not give us a better model for text. It would be similar to EBGAN (Zhao et al., 2017), but only training the energy discriminator and keeping the generator as is (here a pretrained LM). This work would become an interesting and influential work if you can update the generator parameters from the feedback of the energy-based model, so the generator works as a sampling process for the energy-based model (which was what I expected while I was reading the first part of the paper)."}
{"id": "iclr2020_374", "title": "Unbiased Contrastive Divergence Algorithm for Training Energy-Based Latent Variable Models | OpenReview", "abstract": "Abstract:###The contrastive divergence algorithm is a popular approach to training energy-based latent variable models, which has been widely used in many machine learning models such as the restricted Boltzmann machines and deep belief nets. Despite its empirical success, the contrastive divergence algorithm is also known to have biases that severely affect its convergence. In this article we propose an unbiased version of the contrastive divergence algorithm that completely removes its bias in stochastic gradient methods, based on recent advances on unbiased Markov chain Monte Carlo methods. Rigorous theoretical analysis is developed to justify the proposed algorithm, and numerical experiments show that it significantly improves the existing method. Our findings suggest that the unbiased contrastive divergence algorithm is a promising approach to training general energy-based latent variable models.", "review": "Review:###The paper proposes an algorithmic improvement that significantly simplifies training of energy-based models, such as the Restricted Boltzmann Machine. The key issue in training such models is computing the gradient of the log partition function, which can be framed as computing the expected value of f(x) = dE(x; theta) / d theta over the model distribution p(x). The canonical algorithm for this problem is Contrastive Divergence which approximates x ~ p(x) with k steps of Gibbs sampling, resulting in biased gradients. In this paper, the authors apply the recently introduced unbiased MCMC framework of Jacob et al. to completely remove the bias. The key idea is to (1) rewrite the expectation as a limit of a telescopic sum: E f(x_0) + sum_t E f(x_t) - E f(x_{t-1}); (2) run two coupled MCMC chains, one for the “positive” part of the telescopic sum and one for the “negative” part until they converge. After convergence, all remaining terms of the sum are zero and we can stop iterating. However, the number of time steps until convergence is now random. Other contributions of the paper are: 1. Proof that Bernoulli RBMs and other models satisfying certain conditions have finite expected number of steps and finite variance of the unbiased gradient estimator. 2. A shared random variables method for the coupled Gibbs chains that should result in faster convergence of the chains. 3. Verification of the proposed method on two synthetic datasets and a subset of MNIST, demonstrating more stable training compared to contrastive divergence and persistent contrastive divergence. I am very excited about this paper and strongly support its acceptance, since the proposed method should revitalize research in energy-based models. While I find the experiments to be somewhat lacking, this is sufficiently offset by the theoretical contributions of the paper. Pros 1. The paper reads well and introduces all the necessary preliminaries to understand the method. This is important, since I expect many readers to be unfamiliar with the technique. 2. The proposed method solves an important problem which, as far as I understand, has been the roadblock in large-scale training of RBMs and related models. It is also elegant and fairly straightforward to implement. 3. The proof of finite computation time and variance is very nice to have. This is because in some cases removing the bias leads to infinite variance, e.g. a parallel submission on SUMO (https://openreview.net/forum?id=SylkYeHtwr). Cons 1. I don’t think Corollary 1 (convergence of gradient descent to the global optimum) is true for RBMs, as stated on Page 6. This is because the log-likelihood of RBM, or indeed any latent-variable model with permutation-invariant latents, is non-convex. I would suggest removing this corollary and simplifying Algorithm 2 to be regular SGD, as used in the experiments. 2. There is no experimental comparison of Algorithm 1 (the general version) and Algorithm 3 (the specialized RBM version). It seems intuitive that the specialized version should have lower computation time, but this must be confirmed. 3. The experimental section may be significantly improved. * It is unclear what value of k (number of initial Gibbs steps) from Algorithm 2 is used. * The experiments on just the “0” digits of MNIST seem a bit simplistic for the year 2019. It is also not clear what binarization protocol is used. * It would be very helpful to provide estimates of the gradient (not log-likelihood) variance of each method to better understand the trade-off between the bias and the variance. * I would also like to see the wall-clock time comparison of the methods. Minor comments * Page 1. Of this kind -> of this class. The data distribution p_v (v; theta) -> The model distribution * Page 2. Property -> properties. CD-\tau -- I don’t think you can correctly refer to your method in this way, since it has at least double the computation time of CD for the same number of iterations. * Page 3. Provides -> provide. Likelihood gradient -> log-likelihood gradient * Algorithm 1 is an infinite loop with no break clause. It would be good to add a break statement after line 5. This would also simplify the discussion of the method. * Page 7. I wouldn’t call the fact that CD doesn’t converge on the BAS dataset remarkable, given that it’s been reported by Fischer & Igel 2014. * Page 9. The last paragraph stating that the proposed method is not a replacement for CD is confusing. Can you add a short experiment to demonstrate that this combination makes sense?"}
{"id": "iclr2020_375", "title": "COPHY: Counterfactual Learning of Physical Dynamics | OpenReview", "abstract": "Abstract:###Understanding causes and effects in mechanical systems is an essential component of reasoning in the physical world. This work poses a new problem of counterfactual learning of object mechanics from visual input. We develop the COPHY benchmark to assess the capacity of the state-of-the-art models for causal physical reasoning in a synthetic 3D environment and propose a model for learning the physical dynamics in a counterfactual setting. Having observed a mechanical experiment that involves, for example, a falling tower of blocks, a set of bouncing balls or colliding objects, we learn to predict how its outcome is affected by an arbitrary intervention on its initial conditions, such as displacing one of the objects in the scene. The alternative future is predicted given the altered past and a latent representation of the confounders learned by the model in an end-to-end fashion with no supervision. We compare against feedforward video prediction baselines and show how observing alternative experiences allows the network to capture latent physical properties of the environment, which results in significantly more accurate predictions at the level of super human performance.", "review": " This paper studies counterfactual event prediction in physical simulation. The authors proposed a model that leverages object-centric scene representations and graph networks for modeling object interactions. The model also uses a recurrent network to encode and extract the confounder information for counterfactual prediction. In the experiments, the authors compared the proposed method, baselines, and human performance on pose estimation and counterfactual prediction. I reviewed an earlier version of the paper at another venue. Compared with that, the current manuscript has improved a lot. It*s studying an important problem. The model builds upon SOTA techniques such as GCN. Experiments are conducted on multiple physical events with multiple confounders. There are also rich ablation studies. The writing is clear and easy to follow. My recommendation is weak accept. I think the paper can be improved by adding experiments on real data. The model involves *de-rendering* which seems not easily generalizable to complex real scenes. Also, while the block tower scenario has been well studied, the discussion on ball and collision scenarios is quite limited. I encourage the authors to include more results on those datasets. The authors should also conduct human studies there, too."}
{"id": "iclr2020_376", "title": "How to 0wn the NAS in Your Spare Time | OpenReview", "abstract": "Abstract:###New data processing pipelines and unique network architectures increasingly drive the success of deep learning. In consequence, the industry considers top-performing architectures as intellectual property and devotes considerable computational resources to discovering such architectures through neural architecture search (NAS). This provides an incentive for adversaries to steal these unique architectures; when used in the cloud, to provide Machine Learning as a Service (MLaaS), the adversaries also have an opportunity to reconstruct the architectures by exploiting a range of hardware side-channels. However, it is challenging to reconstruct unique architectures and pipelines without knowing the computational graph (e.g., the layers, branches or skip connections), the architectural parameters (e.g., the number of filters in a convolutional layer) or the specific pre-processing steps (e.g. embeddings). In this paper, we design an algorithm that reconstructs the key components of a unique deep learning system by exploiting a small amount of information leakage from a cache side-channel attack, Flush+Reload. We use Flush+Reload to infer the trace of computations and the timing for each computation. Our algorithm then generates candidate computational graphs from the trace and eliminates incompatible candidates through a parameter estimation process. We implement our algorithm on PyTorch and Tensorflow. We demonstrate experimentally that we can reconstruct MalConv, a novel data pre-processing pipeline for malware detection, and ProxylessNAS-CPU, a novel network architecture for the ImageNet classification optimized to run on CPUs, without knowing the architecture family. In both cases, we achieve 0% error. These results suggest hardware side channels are a practical attack vector against MLaaS, and more efforts should be devoted to understanding their impact on the security of deep learning systems.", "review": "Review:###This paper proposes a way to attack and reconstruct a victim*s neural architecture that is co-located on the same host. They do it through cache side-channel leakage and use Flush+Reload to extract the trace of victim*s function call, which tells specific network operations. To recover the computational graph, they use the approximate time each operation takes to prune out any incompatible candidate computation graph. They show that they can reconstruct exactly the MalConv and ProxylessNAS. The paper looks very interesting but also alarming -- more research should be done to countermeasure this attack. I have the following questions: 1. To reconstruct the network, you need to generate potentially exponentially number of candidates and do some pruning based on the estimated parameters. This also looks very expensive. I am wondering compared to just doing NAS yourself, how much gain in terms of resources and time this attack can give? 2. What is the limitation of the proposed approach, i.e., does it work on any network structures, e.g., sequence networks, graph convolutional networks, etc. 3. In the experiments shown, you can reconstruct MalConv and ProxylessNAS with zero error, does the proposed approach alway find the exact match? Under what circumstances can you find the exact match?"}
{"id": "iclr2020_377", "title": "Solving Packing Problems by Conditional Query Learning | OpenReview", "abstract": "Abstract:###Neural Combinatorial Optimization (NCO) has shown the potential to solve traditional NP-hard problems recently. Previous studies have shown that NCO outperforms heuristic algorithms in many combinatorial optimization problems such as the routing problems. However, it is less efficient for more complicated problems such as packing, one type of optimization problem that faces mutual conditioned action space. In this paper, we propose a Conditional Query Learning (CQL) method to handle the packing problem for both 2D and 3D settings. By embedding previous actions as a conditional query to the attention model, we design a fully end-to-end model and train it for 2D and 3D packing via reinforcement learning respectively. Through extensive experiments, the results show that our method could achieve lower bin gap ratio and variance for both 2D and 3D packing. Our model improves 7.2% space utilization ratio compared with genetic algorithm for 3D packing (30 boxes case), and reduces more than 10% bin gap ratio in almost every case compared with extant learning approaches. In addition, our model shows great scalability to packing box number. Furthermore, we provide a general test environment of 2D and 3D packing for learning algorithms. All source code of the model and the test environment is released.", "review": " Summary: The paper proposes heuristics to solve the bin packing problems based on reinforcement learning with deep neural networks. With a new heuristics of conditional queries, the proposed method works favorably with the previous RL-based approach and other baselines. Comments: The idea of applying reinforcement learning to combinatorial optimization itself is not new. The authors, on the other hand, propose new heuristics, called conditional queries, which divides a unit of actions (rotation, box, and etc.), which turns out to be effective compared to the previous reinforcement-learning based method."}
{"id": "iclr2020_378", "title": "Universal Learning Approach for Adversarial Defense | OpenReview", "abstract": "Abstract:###Adversarial attacks were shown to be very effective in degrading the performance of neural networks. By slightly modifying the input, an almost identical input is misclassified by the network. To address this problem, we adopt the universal learning framework. In particular, we follow the recently suggested Predictive Normalized Maximum Likelihood (pNML) scheme for universal learning, whose goal is to optimally compete with a reference learner that knows the true label of the test sample but is restricted to use a learner from a given hypothesis class. In our case, the reference learner is using his knowledge on the true test label to perform minor refinements to the adversarial input. This reference learner achieves perfect results on any adversarial input. The proposed strategy is designed to be as close as possible to the reference learner in the worst-case scenario. Specifically, the defense essentially refines the test data according to the different hypotheses, where each hypothesis assumes a different label for the sample. Then by comparing the resulting hypotheses probabilities, we predict the label and detect whether the sample is adversarial or natural. Combining our method with adversarial training we create a robust scheme which can handle adversarial input along with detection of the attack. The resulting scheme is demonstrated empirically.", "review": "Review:###Summary: This paper focuses on the area of adversarial defense -- both to improve robustness and to detect adversarially perturbed images. They approach the problem from the universal prediction / universal learning framework. Modivation: I had a hard time understanding the motivation of this work -- specifically the connection to the universal learning framework which to be fair I am unfamiliar with. In the absence of the universal learning framework formalism the method proposed here is quite simple and in my opinion clever -- create a new prediction based on performing an adversarial attack to each target class. What value does universal learning bring to this? Second, I do not follow the intuition for the chosen hypothesis class -- why work off of refined images in the first place? Is there some reason to believe this will improve robustness? Finally, the view that adversarial defense and attack are important topics to explore is under some debate. I am not considering this as part of my review but I would encourage the authors to look at [1]. Writing: The writing was clear and typo free. Experiments: Overall the experiments seemed inconclusive. Section 5 shows robustness against the unmodified / unrefined model (the attacks are done on the base model not the refined model). Given that these attacks are performed against the unmodified model then evaluated on the modified model the results seem a bit unfair / harder to interpret. The authors note this, and in Section 6 explore the *Adaptive Adversary* setting. The results presented are performed on Mnist and Cifar10. Overall the results were not convincing to me. Table 1 shows mixed performance -- a drop in natural accuracy in all cases, decreases in FGSM. The main increase in performance is in the PGD. This was noted, but understanding in more depth why this method helps here will hopefully lead to improved performance in FGSM as well. Figure 2a shows very weak correlations. Figure 2b seems promising but also not necessarily a surprise given that the adversarial examples are generated against the base model and not the refined model. For section 6, one risk is that the BPDA attack doesn*t successfully work. Having some more proof that the attacks presented here are strong would greatly improve the work. Larger scale experiments would of course be nice and strengthen the paper but more importantly it would be great to see some form of toy example or demonstration of the principle improving robustness as well over just results. Something to probe the mechanism of action for example. Finally, having some comparisons to other defense strategies would improve this paper. Rating: Given the gap between the universal learning framework and the method proposed, as well as the inconclusive experiments at this point I would not recommend the paper for acceptance. [1] https://arxiv.org/abs/1807.06732"}
{"id": "iclr2020_379", "title": "Adversarial Attacks on Copyright Detection Systems | OpenReview", "abstract": "Abstract:###It is well-known that many machine learning models are susceptible to adversarial attacks, in which an attacker evades a classifier by making small perturbations to inputs. This paper discusses how industrial copyright detection tools, which serve a central role on the web, are susceptible to adversarial attacks. We discuss a range of copyright detection systems, and why they are particularly vulnerable to attacks. These vulnerabilities are especially apparent for neural network based systems. As proof of concept, we describe a well-known music identification method and implement this system in the form of a neural net. We then attack this system using simple gradient methods. Adversarial music created this way successfully fools industrial systems, including the AudioTag copyright detector and YouTube*s Content ID system. Our goal is to raise awareness of the threats posed by adversarial examples in this space and to highlight the importance of hardening copyright detection systems to attacks.", "review": "Review:###The authors of this work bring to light the security vulnerabilities of copyright detection systems to (DL style) adversarial attacks. The work summarizes the basics of copyright detection systems for audio, and notes that recent methodologies for feature extraction incorporate neural networks, as opposed to hand-crafted features. Following which the authors embark on designing a simple copyright system based on a neural network. With the newly proposed system, it is shown that one can construct adversarial examples to obstruct copyright detection systems using differentiable programming. The constructed adversarial examples are shown to be successful in evading popular copyright detection systems for audio (YouTube Content ID, AudioTag) as black box attacks. Overall the paper brings about an interesting and pressing issue in a timely manner that seems to be of broad interest to the security and ML community. The paper is very well written and I enjoyed reading it. Further, the construction of the adversarial objective and attacks seems novel. However, I am not as familiar with the literature with adversarial attacks for audio. Overall I am giving this work a weak accept, which I am willing to change if the authors provide additional insight into their experiments. In particular, it is difficult for me to assess the success of the imperceptibility of the perturbation in the audio domain from l2 and l_infinity norms, so it is hard for me to judge whether such adversarial examples are competitive and useful. For this reason I would like the authors share excerpts from their attack experiments for multiple examples for the different results presented."}
{"id": "iclr2020_380", "title": "Reinforced active learning for image segmentation | OpenReview", "abstract": "Abstract:###Learning-based approaches for semantic segmentation have two inherent challenges. First, acquiring pixel-wise labels is expensive and time-consuming. Second, realistic segmentation datasets are highly unbalanced: some categories are much more abundant than others, biasing the performance to the most represented ones. In this paper, we are interested in focusing human labelling effort on a small subset of a larger pool of data, minimizing this effort while maximizing performance of a segmentation model on a hold-out set. We present a new active learning strategy for semantic segmentation based on deep reinforcement learning (RL). An agent learns a policy to select a subset of small informative image regions -- opposed to entire images -- to be labeled, from a pool of unlabeled data. The region selection decision is made based on predictions and uncertainties of the segmentation model being trained. Our method proposes a new modification of the deep Q-network (DQN) formulation for active learning, adapting it to the large-scale nature of semantic segmentation problems. We test the proof of concept in CamVid and provide results in the large-scale dataset Cityscapes. On Cityscapes, our deep RL region-based DQN approach requires roughly 30% less additional labeled data than our most competitive baseline to reach the same performance. Moreover, we find that our method asks for more labels of under-represented categories compared to the baselines, improving their performance and helping to mitigate class imbalance.", "review": "Review:#### Summary # The paper works on active learning for semantic segmentation, aiming to annotate as few *blocks/patches* as possible while training a strong model. The authors proposed to learn a query policy via Q learning, and design states and actions specifically for segmentation. The experimental results show that the learned policy can attend to informative patches and rare classes to learn a model faster and efficiently. # Strength # S1. The paper is well-motivated; the references are quite sufficient. S2. The paper clearly states the challenges when applying RL algorithms like Q-learning to image segmentation, which should serve as good guidance for other future work. #Weakness/comments# W1. The writing of the technical part can be strengthened. The authors deferred the state and action design entirely to the supplementary, while they are the main contributions to the paper. W2. The proposed algorithms seem to be highly time-consuming. The actions require pairwise comparison, and at every step, the models need to evaluate all the validation images to get the reward. W3. If I understand correctly, the authors use part of the training data D_T, D_S (of an existing dataset) together with the validation data D_R to learn the policy, and then use the learned policy to select patches from the remaining training data D_V to train the segmentation model. I have two questions. 1) The labeled data involved in policy training is indeed quite large (validation plus part of the training, D_S + D_T + D_R). Does it mean that to learn a good active learning policy we indeed need a large number of labeled data? 2) Since (D_S, D_T, and D_R) are used to learn the policy, they should be treated as available training data for segmentation that all the compared algorithms can use without spending the budget. In other words, all the compared algorithms (U, H, B) should use those data to fine-tune a pre-train segmentation network before they start to acquire data from D_V. It would be great if the authors can clarify this. W4. In applying the policy for selecting patches from D_V, do the authors update the model once on the selected patches, or do the authors train with them for multiple iterations together with other previous selected patches? Since deep neural nets are known to forget what has been learned (i.e., catastrophic forgetting), it*s better if the author could clarify this. W5. The authors include an upper bound in Fig. 4; however, I didn*t find the explanation. Why the proposed methods can outperform the upper bound with all the training data, even with only 24% of data? #Rebuttal# Please discuss W1-W5. - Annotating an entire image is definitely easier to annotators than annotating patches. Could the authors discuss how to design an active learning algorithm by selecting informative images to annotate, and maybe compare to such a method? - Can the authors discuss Figure 3 more? As H is based on maximum entropy, why is it outperformed by the proposed method? - There is no explicit mechanism to prevent that the k actions select similar patches. Can the authors provide more discussion? # Post rebuttal The authors responded to most of my concerns. I*d like the authors to incorporate all their responses into the manuscript or the appendix so that future readers can better understand the concepts and details. I would like to raise the score to borderline (4 or 5). I modified my scores to weak accept (6) since there is no option in between. One concern I still have is W3. 2). Given only 360 images are available, it might be inappropriate to use D_T + D_S (roughly 160 images?) to pre-train baseline models and then use D_R with *200* images for validation (early stopping). It will be more appropriate to use, for example, 70% of 360 images for training. This is supported by that many modern datasets use a much larger training set than the validation set. Therefore, I would highly suggest the authors redoing the baseline methods; otherwise, future work that re-splits the data (this is totally valid!) from the 360 images might easily achieve higher accuracy. The authors must also reorganize the paper, taking W1 into account."}
{"id": "iclr2020_381", "title": "An Inductive Bias for Distances: Neural Nets that Respect the Triangle Inequality | OpenReview", "abstract": "Abstract:###Distances are pervasive in machine learning. They serve as similarity measures, loss functions, and learning targets; it is said that a good distance measure solves a task. When defining distances, the triangle inequality has proven to be a useful constraint, both theoretically---to prove convergence and optimality guarantees---and empirically---as an inductive bias. Deep metric learning architectures that respect the triangle inequality rely, almost exclusively, on Euclidean distance in the latent space. Though effective, this fails to model two broad classes of subadditive distances, common in graphs and reinforcement learning: asymmetric metrics, and metrics that cannot be embedded into Euclidean space. To address these problems, we introduce novel architectures that are guaranteed to satisfy the triangle inequality. We prove our architectures universally approximate norm-induced metrics on , and present a similar result for modified Input Convex Neural Networks. We show that our architectures outperform existing metric approaches when modeling graph distances and have a better inductive bias than non-metric approaches when training data is limited in the multi-goal reinforcement learning setting.", "review": "Review:###This manuscript proposes a general framework to learn non-Euclidean distances from data using neural networks. The authors provide a combination of theoretical and experimental results in support of the use of several neural architectures to learn such distances. In particular, the develop “deep norms” and “wide norms”, based either on a deep or shallow neural network. Metrics are elaborated based on norms by combining them with a learnt embedding function mapping the input space de R^n. Theoretical results are mostly application textbook results and intuitive, the overall work forms a coherent line of research bridging theory and applications that sets well justified reference approaches for this topic."}
{"id": "iclr2020_382", "title": "FINBERT: FINANCIAL SENTIMENT ANALYSIS WITH PRE-TRAINED LANGUAGE MODELS | OpenReview", "abstract": "Abstract:###While many sentiment classification solutions report high accuracy scores in product or movie review datasets, the performance of the methods in niche domains such as finance still largely falls behind. The reason of this gap is the domain-specific language, which decreases the applicability of existing models, and lack of quality labeled data to learn the new context of positive and negative in the specific domain. Transfer learning has been shown to be successful in adapting to new domains without large training data sets. In this paper, we explore the effectiveness of NLP transfer learning in financial sentiment classification. We introduce FinBERT, a language model based on BERT, which improved the state-of-the-art performance by 14 percentage points for a financial sentiment classification task in FinancialPhrasebank dataset.", "review": "Review:###This paper presents an analysis of the BERT language model on financial text. FinBERT is evaluated on two datasets from the financial domain: a sentiment prediction dataset (classification with 3 different classes) and a sentiment score prediction (the score is a float number between -1 and 1). I find the phrasing *FinBERT is a language model based on BERT* misleading; I think FinBERT is BERT trained on financial text. There is no modification that is done to the original BERT model. The paper presents several experiments using BERT as the language model and fine-tuning for the financial tasks. FinBERT is compared to a few baselines such as LSTMs with ElMO embeddings and ULMfit. I find interesting that the model performs better on the subset of the dataset for which there is perfect agreement between the annotators. I also find the results on training on financial data interesting. The results seem to indicate that further training on financial text does not seem to result in additional improvement when compared to original BERT. While I find the analysis and the experiments presented in the paper interesting, the novelty of the paper is rather low. There is no new idea introduced in this paper, it contains a series of experiments with BERT on financial text and tasks."}
{"id": "iclr2020_383", "title": "Robust Learning with Jacobian Regularization | OpenReview", "abstract": "Abstract:###Design of reliable systems must guarantee stability against input perturbations. In machine learning, such guarantee entails preventing overfitting and ensuring robustness of models against corruption of input data. In order to maximize stability, we analyze and develop a computationally efficient implementation of Jacobian regularization that increases classification margins of neural networks. The stabilizing effect of the Jacobian regularizer leads to significant improvements in robustness, as measured against both random and adversarial input perturbations, without severely degrading generalization properties on clean data.", "review": "Review:###Summary: Stability is one of the important aspects of machine learning. This paper views Jacobian regularization as a scheme to improve the stability, and studies the behavior of Jacobian regularization under random input perturbations, adversarial input perturbations, train/test distribution shift, and simply as a regularization tool for the classical setting without any distribution shifts nor perturbations. There are already several related works that propose to use Jacobian regularization, but previous works didn’t have an efficient algorithm and also did not have theoretical convergence guarantee. This paper offers a solution that efficiently approximate the Frobenius norm of the Jacobian and also show the optimal convergence rate for the proposed method. Various experiments show that the behavior of Jacobian regularization and show that it is robust. Reasons for the decision: Positives: The contribution of the paper seems to be two-fold: First a theoretically guaranteed and efficient method for Jacobian regularizer, and second, intensive experiments to show the robustness of the Jacobian regularizer. Each of these points seem to have important contributions for the field. Negatives: An issue might be that the latter contribution seems to be orthogonal to the former since there are no experiments comparing with previous methods mentioned in the paper, and it gives the impression that there are two separate stories in one paper. For instance, there are no experiments comparing computational time between Sokolic et al. (2017). On the other hand, there are only regularization methods (that are not necessarily designed to be robust) used as baselines in the experiments to show robustness, instead of algorithms that are designed to be robust, e.g., domain adaptation methods for Table 2. It would make the paper stronger to combine these two lines of contributions into a single story. For example, it might be better to emphasize more that experiments such as Figure S7 was previously not possible due to inefficient implementation. Minor comment: It would make the paper stronger to include some of the main related works in the Introduction section. After author response: Thank you for reading my review and answering the questions. Although I still feel the same for my score (6), in my opinion, the same issues exist for this paper, and the paper can be made stronger on those points."}
{"id": "iclr2020_384", "title": "Are Transformers universal approximators of sequence-to-sequence functions? | OpenReview", "abstract": "Abstract:###Despite the widespread adoption of Transformer models for NLP tasks, the expressive power of these models is not well-understood. In this paper, we establish that Transformer models are universal approximators of continuous permutation equivariant sequence-to-sequence functions with compact support, which is quite surprising given the amount of shared parameters in these models. Furthermore, using positional encodings, we circumvent the restriction of permutation equivariance, and show that Transformer models can universally approximate arbitrary continuous sequence-to-sequence functions on a compact domain. Interestingly, our proof techniques clearly highlight the different roles of the self-attention and the feed-forward layers in Transformers. In particular, we prove that fixed width self-attention layers can compute contextual mappings of the input sequences, playing a key role in the universal approximation property of Transformers. Based on this insight from our analysis, we consider other architectures that can compute contextual mappings and empirically evaluate them.", "review": " This paper discusses the universal approximation capability of the Transformer, under certain assumptions, analyze the role of different components of the Transformer (e.g., self-attention layer for contextual mapping), and propose the use of some other layers that can also provide contextual mapping. Overall speaking, the problem studied by this paper is very important. The transformer has been used extensively in many applications today, however, deep theoretical understanding of it is not sufficient. Universal approximation capability is a very important theoretical property of deep learning, and advances on the universal approximation of the Transformer is important for the deep learning community. Therefore, I think people will be willing to see the results in this paper. While saying so, this paper has some limitations, which could be further improved. 1) The paper studies a variant of the Transformer, where the layer norm is removed. However, according to practical experiences, the layer norm plays a critical role in the Transformer. As a result, there is gap between this paper and practical situations, and the value of the paper becomes not very clear. 2) The paper lacks experimental verifications. It would be better to design some toy experiments with different types of target functions to see whether the Transformer can well approximate them, and see the contribution of different components of the Transformer 3) The discussions on contextual mapping are not solid enough. First, it seems that contextual mapping is kind of sufficient condition for the proof, however, it is unclear whether it is a necessary condition. Consequently, things are not clear regarding” a. If all the structures (self-attention, bilinear projection, depth-wise separable convolutions) are all sufficient conditions, why self-attention is better than the other two, and why the mix of them can generate even better results? b. If they are not necessary conditions, we cannot say one should choose them since other structures may be equally good even if they do not satisfy contextual mapping conditions. 4) The experimental study in the paper is not very comprehensive. Given that the Transform has been used in many NLP scenarios, experiments on more datasets and more tasks are expected. **I read the author rebuttal. Some of my concerns still remain, and I would like to keep the current rating (which is positive)."}
{"id": "iclr2020_385", "title": "Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity | OpenReview", "abstract": "Abstract:###Artificial neural networks (ANNs) lack in biological plausibility, chiefly because backpropagation requires a variant of plasticity (precise changes of the synaptic weights informed by neural events that occur downstream in the neural circuit) that is profoundly incompatible with the current understanding of the animal brain. Here we propose that backpropagation can happen in evolutionary time, instead of lifetime, in what we call neural net evolution (NNE). In NNE the weights of the links of the neural net are sparse linear functions of the animal*s genes, where each gene has two alleles, 0 and 1. In each generation, a population is generated at random based on current allele frequencies, and it is tested in the learning task. The relative performance of the two alleles of each gene over the whole population is determined, and the allele frequencies are updated via the standard population genetics equations for the weak selection regime. We prove that, under assumptions, NNE succeeds in learning simple labeling functions with high probability, and with polynomially many generations and individuals per generation. We test the NNE concept, with only one hidden layer, on MNIST with encouraging results. Finally, we explore a further version of biologically plausible ANNs inspired by the recent discovery in animals of dopaminergic plasticity: the increase of the strength of a synapse that fired if dopamine was released soon after the firing.", "review": "Review:###This paper argues that Artificial Neural Network (ANN) lack in biological plausibility because of the back-propagation process. Therefore, the authors provide an alternative approach, named neural net evolution (NNE) that follows evolutionary theory. This approach uses a large number of genotypes (in the form of vector with binary logits) that will evolve overtime during training. It does not require to calculate the gradient explicitly. The authors have conducted some experiments on MNIST using ANN with only one hidden layer. The experimental results show that the NNE can learn the classification task reasonably well considering that no explicit back propagation is used. I think overall the motivation to combine ANN with evolutionary theory is very interesting. The reviewer is not very familiar with evolutionary theory. So I judge this paper in the perspective of machine learning, from which I think the current approach is a week variant of back-propagation that still relies on gradient (see detailed comments below). Based on this, I give my rating. The approach is formulated as NNE_x(y) = (x^T)*(W^T)*y. In traditional linear regression, W is the weight to be learnt. In this paper*s formulation, W is named as a weight generation matrix, which is choosing to be random and i.i.d. with certain probabilities. The parameters to be optimized is x, which is named as a genotype that is viewed as a vector x in {0, 1}^n. So first of all, as W is fixed so the formulation is very similar to a traditional linear regression with an additional linear transform. The difference is that x is a binary vector with probabilities. These probabilities are optimized over time. From the Equations 1), 2) and 3), the probabilities are updated in a way to minimize the loss. This is kind of similar to back-propagation. Then the probabilities are updated and thus x is changed as well. In my understanding, this is still gradient-based optimization. I do not see it fundamental different to back-propagation. This is my main concern about this work. I did not check the details of Theorem 1. Could the authors please comment what is the purpose of Theorem 1 before proving it? This part is unclear to me in this paper. One more question, for the W matrix, the authors choice beta = 0.0025 in the experiment. Is there any particular reason for this choice? Or does it matter what value to choice as it is fixed anyway?"}
{"id": "iclr2020_386", "title": "WaveFlow: A Compact Flow-based Model for Raw Audio | OpenReview", "abstract": "Abstract:###In this work, we present WaveFlow, a small-footprint generative flow for raw audio, which is trained with maximum likelihood without complicated density distillation and auxiliary losses as used in Parallel WaveNet. It provides a unified view of flow-based models for raw audio, including autoregressive flow (e.g., WaveNet) and bipartite flow (e.g., WaveGlow) as special cases. We systematically study these likelihood-based generative models for raw waveforms in terms of test likelihood and speech fidelity. We demonstrate that WaveFlow can synthesize high-fidelity speech and obtain comparable likelihood as WaveNet, while only requiring a few sequential steps to generate very long waveforms. In particular, our small-footprint WaveFlow has only 5.91M parameters and can generate 22.05kHz speech 15.39 times faster than real-time on a GPU without customized inference kernels.", "review": "Review:##### Updated review I have read the rebuttal. The new version of the paper is definitely clearer, especially the contribution section and the experimental results. The new version addresses all my concerns, hence I am upgrading my rating to Accept. ## Original review This paper presents the WaveGlow model, a generative model for raw audio. The model is based on a 2D-matrix approach, which allows to generate the audio with a fixed amount of step. The model is shown to be a generalization of the two main approaches for raw audio generation, autoregressive flow and bipartite flow. The model is evaluated and compared with related work on an objective evaluation (Log-likelihood) and a subjective evaluation (MOS), and is shown to be a trade-off between memory footprint, generation speed and quality. I think this paper should be accepted, for the following reasons: - The theoretical framework presented is novel and significant, as it provides a unified view of the two main approaches for neural waveform generation. - The experiments are reasonably convincing, although they could be improved. Detailed comments: - In the subjective evaluation section (5.2), Table 5 is hard to decipher, especially given that there are three measurements to take into account, so it*s not easy to see the benefit of the approach. Maybe the results should be organised differently, for instance grouping them according to one measurement could help, typically showing what speed and MOS each of the three models can achieve for a given model size. Maybe plotting speed vs MOS for the same model size could also be interesting. - In the same section, is the WaveNet model the original one, or the Parallel WaveNet ? if it*s the original, why not include Parallel WaveNet in the table ? - Typo at the end of Section 1: *We orgnize* -> *organize*"}
{"id": "iclr2020_387", "title": "Scalable Generative Models for Graphs with Graph Attention Mechanism | OpenReview", "abstract": "Abstract:###Graphs are ubiquitous real-world data structures, and generative models that approximate distributions over graphs and derive new samples from them have significant importance. Among the known challenges in graph generation tasks, scalability handling of large graphs and datasets is one of the most important for practical applications. Recently, an increasing number of graph generative models have been proposed and have demonstrated impressive results. However, scalability is still an unresolved problem due to the complex generation process or difficulty in training parallelization. In this paper, we first define scalability from three different perspectives: number of nodes, data, and node/edge labels. Then, we propose GRAM, a generative model for graphs that is scalable in all three contexts, especially in training. We aim to achieve scalability by employing a novel graph attention mechanism, formulating the likelihood of graphs in a simple and general manner. Also, we apply two techniques to reduce computational complexity. Furthermore, we construct a unified and non-domain-specific evaluation metric in node/edge-labeled graph generation tasks by combining a graph kernel and Maximum Mean Discrepancy. Our experiments on synthetic and real-world graphs demonstrated the scalability of our models and their superior performance compared with baseline methods.", "review": "Review:###This paper presents a formulation of graph generative models based on graph attention aimed at scalability of these methods. The paper is generally written well and I like the overall theme of the paper, however, there are a few key issues with this work and I don*t think the paper as it stands is ready for publication: 1) The main motivation expressed in the paper is that graph generative models are generally not scalable and they identify three main areas: (a) graph size (i.e. num nodes); (b) data scalability (i.e. num training samples); (c) label scalability (i.e. num of node or edge types). However, the paper doesn*t follow on why the proposed method actually addresses these issues. The derivation doesn*t talk about scale until we reach section 3.5 and then we find out that actually the proposed model is O(n^3) in reality. Then there are approximations to make it scale. So for me there is a massive disconnect between the main motivation of the paper and the suggested model. Why not study approximation methods for already existing graph generator models? 2) Following on the theme of scale, the only experimental result discussing this is the time column reported for the training time. So that partially addresses the data scalability. Other baselines as well have reasonable training times specially when it comes to large datasets (e.g. ZINC is that the largest dataset studied with 250K samples and GraphRNN is 2x slower and GraphRNN-S only about 20%). What I was looking for was when you really can train on real-world datasets that other methods basically can*t be trained. The datasets chosen all are small hence there*s not much issue with scale there. The question about the scalability w.r.t. other aspects (i.e. num nodes and num labels) has not been studied or reported. 3) The approximations suggested in section 3.5 also don*t seem to have much impact on the training time. These approximations were motivated by the scale while looking at the training times they barely make any difference. However, they make a big difference in performance metrics specially in smaller datasets. So the question that comes to mind is that what is the role of these approximations w.r.t. the quality of the models? Again this question needs further study. 4) Comparing GraphRNN and GraphRNN-S*s modifications with the results from the original paper, it seems they are performing much worse (e.g. deg for the original GraphRNN-S is 0.057 while the reported num here is 0.523 for Protein dataset). The same is true for other metrics. Why is that? 5) As pointed out by an observer, it seems that there are nuances to generation of the graph needing seeds of arbitrary size to be provided, explained deep down in the appendix. If this is the case for generation then it should be discussed in the main part of the paper and contrasted with methods that can start from scratch. 6) In the training configuration part of the appendix, A.7.2 it seems there are discrepancies in number of GPUs as well kinds of GPUs used for each method. When reporting training times in the main section, do you normalise against these? 7) It seems that many hyperparameters mentioned in A.7.2 are chosen in an ad-hoc manner without proper model selection and seem to vary across each different versions of GRAM for each different dataset. How sensitive is the model to these hyperparameters? I suspect if the model was insensitive, you could*ve fixed them for many of these experiments, but seems that is not the case. So without proper model selection routines, the results may not be representative of what the model discussed. Minor comment: The model suggested has some similarities to DEFactor model from Assouel et al 2019 in terms of formulation of the problem for labelled graphs (nodes as a matrix and adj as a tensor), though the underlying models are very different, that paper as well targets arbitrary size graph generation and efficiency w.r.t. model parameters."}
{"id": "iclr2020_388", "title": "EMS: End-to-End Model Search for Network Architecture, Pruning and Quantization | OpenReview", "abstract": "Abstract:###We present an end-to-end design methodology for efficient deep learning deployment. Unlike previous methods that separately optimize the neural network architecture, pruning policy, and quantization policy, we jointly optimize them in an end-to-end manner. To deal with the larger design space it brings, we train a quantization-aware accuracy predictor that fed to the evolutionary search to select the best fit. We first generate a large dataset of <NN architecture, ImageNet accuracy> pairs without training each architecture, but by sampling a unified supernet. Then we use these data to train an accuracy predictor without quantization, further using predictor-transfer technique to get the quantization-aware predictor, which reduces the amount of post-quantization fine-tuning time. Extensive experiments on ImageNet show the benefits of the end-to-end methodology: it maintains the same accuracy (75.1%) as ResNet34 float model while saving 2.2× BitOps comparing with the 8-bit model; we obtain the same level accuracy as MobileNetV2+HAQ while achieving 2×/1.3× latency/energy saving; the end-to-end optimization outperforms separate optimizations using ProxylessNAS+AMC+HAQ by 2.3% accuracy while reducing orders of magnitude GPU hours and CO2 emission.", "review": "Review:###This paper proposes an end-to-end design method for architecting mixed-precision model. The proposed method directly searches for the mixed-precision architectures, unlike previous methods that separately optimize the neural network architectures, pruning policy, and quantization policy. The experimental results show that the proposed method can find better models than the state-of-the-art models. - It is not clear how the evolutionary algorithm applies to the framework of the proposed method, i.e., which parts are optimized by the evolutionary algorithm? Also, the details about the evolutionary algorithm part are not provided, for example, what is a genotype and a fitness function used in this paper? - It would be better to provide a comparison with a random search to make the contribution of the evolutionary algorithm clear. - It is not clear about the procedure of the proposed method during training and testing. Please elaborate on it by providing an algorithm table of the proposed method. - Please add an explanation about N in Table 2."}
{"id": "iclr2020_389", "title": "CAN ALTQ LEARN FASTER: EXPERIMENTS AND THEORY | OpenReview", "abstract": "Abstract:###Differently from the popular Deep Q-Network (DQN) learning, Alternating Q-learning (AltQ) does not fully fit a target Q-function at each iteration, and is generally known to be unstable and inefficient. Limited applications of AltQ mostly rely on substantially altering the algorithm architecture in order to improve its performance. Although Adam appears to be a natural solution, its performance in AltQ has rarely been studied before. In this paper, we first provide a solid exploration on how well AltQ performs with Adam. We then take a further step to improve the implementation by adopting the technique of parameter restart. More specifically, the proposed algorithms are tested on a batch of Atari 2600 games and exhibit superior performance than the DQN learning method. The convergence rate of the slightly modified version of the proposed algorithms is characterized under the linear function approximation. To the best of our knowledge, this is the first theoretical study on the Adam-type algorithms in Q-learning.", "review": "Review:###This paper claims to propose a method to train q-based agents that use “alternating” Q-learning. However, the alternating approach given in the paper appears to be the normal Bellman update implemented in most versions of DQN. Furthermore, the citation given for AltQ (Mnih et al. 2016) makes no mention of the term “Alternating Q learning”. The novelty here would be that the authors propose incorporating an Adam-like optimizer and periodically resetting the ADAM parameters. I would not consider using Adam to be sufficiently novel for publication in this venue, and the results from using parameter resetting are not so spectacular or convincing that they qualify, either. Since no ablations are given, I suspect some of the improvement could have come from just using Adam. Finally, the convergence proofs given seem to hold only in the tabular case--not in the case when the Q function is an approximation. Generally, proofs only show that Q-learning converges in the tabular case. If these proofs held in the function approximation case, this would be a surprising breakthrough."}
{"id": "iclr2020_390", "title": "Neural Stored-program Memory | OpenReview", "abstract": "Abstract:###Neural networks powered with external memory simulate computer behaviors. These models, which use the memory to store data for a neural controller, can learn algorithms and other complex tasks. In this paper, we introduce a new memory to store weights for the controller, analogous to the stored-program memory in modern computer architectures. The proposed model, dubbed Neural Stored-program Memory, augments current memory-augmented neural networks, creating differentiable machines that can switch programs through time, adapt to variable contexts and thus fully resemble the Universal Turing Machine or Von Neumann Architecture. A wide range of experiments demonstrate that the resulting machines not only excel in classical algorithmic problems, but also have potential for compositional, continual, few-shot learning and question-answering tasks.", "review": "Review:###= Summary A variation of Neural Turing Machines (and derived models) storing the configuration of the controller in a separate memory, which is then *softly* read during evaluation of the NTM. Experiments show moderate improvements on some simple multi-task problems. = Strong/Weak Points + The idea of generalising NTMs to *universal* TMs is interesting in itself ... - ... however, the presented solution seems to be only half-way there, as the memory used for the *program* is still separate from the memory the NUTM operates on. Hence, modifying the program itself is not possible, which UTMs can do (even though it*s never useful in practice...) - The core novelty relative to standard NTMs is that in principle, several separate programs can be stored, and that at each timestep, the *correct* one can be read. However this read mechanism is weak, and requires extra tuning with a specialized loss (Eq. (6)) ~ It remains unclear where this is leading - clearly NTMs and NUTMs (or their DNC siblings) are currently not useful for interesting tasks, and it remains unclear what is missing to get there. The current paper does not try show the way there. - The writing is oddly inconsistent, and important technical details (such as the memory read/write mechanism) are not documented. I would prefer the paper to be self-contained, to make it easier to understand the differences and commonalities between NTM memory reads and the proposed NSM mechanism. = Recommendation Overall, I don*t see clear, actionable insights in this submission, and thus believe that it will not provide great value to the ICLR audience; hence I would recommend rejecting the paper to allow the authors to clarify their writing and provide more experimental evidence of the usefulness of their contribution. = Minor Comments + Page 6: *As NUTM requires fewer training samples to converge, it generalizes better to unseen sequences that are longer than training sequences.* - I don*t understand the connecting between the first and second part of the sentence. This seems pure speculation, not a fact."}
{"id": "iclr2020_391", "title": "Sentence embedding with contrastive multi-views learning | OpenReview", "abstract": "Abstract:###In this work, we propose a self-supervised method to learn sentence representations with an injection of linguistic knowledge. Multiple linguistic frameworks propose diverse sentence structures from which semantic meaning might be expressed out of compositional words operations. We aim to take advantage of this linguist diversity and learn to represent sentences by contrasting these diverse views. Formally, multiple views of the same sentence are mapped to close representations. On the contrary, views from other sentences are mapped further. By contrasting different linguistic views, we aim at building embeddings which better capture semantic and which are less sensitive to the sentence outward form.", "review": " Overview: This work proposes to learn sentence embeddings using both contrastive learning and multiple *views* of sentences. This work largely builds off of [1], including using the same objective, but uses a multi-view approach to modeling. - They apply the concept of multi-view models, specifically combining tree and linear LSTMs to learning sentence representations. - They prepare a new, large-scale book dataset, which is useful because the previously commonly used book dataset was taken down for legal reason. - They provide a fairly broad set of analyses on their model, both quantitative and qualitative, performance-driven and analysis-driven. - Review: The ideas and models presented in this paper are not new, while the supporting experiments are not very well done or convincing. Overall, I recommend rejecting this work. - The models are contrastively learned in that they are trained to embed *similar* sentences nearby in the embedding space, and *dissimilar* sentence far away, where *similar* sentences are defined as consecutive sentences. This method of learning textual representations is well-established in the NLP literature, mostly prominently in recent years with word embedding models like Skip-Gram and in sentence embedding models like in [1], [2], [3] (the next sentence prediction task), and several more. - In practice, the multiple views of each sentence that this paper considers boils down to encoding the sentence with a bidirectional LSTM and a TreeLSTM and concatenating the representations from each encoder. This idea again has been established in the literature ([4], [5], [6]). - The experiments don*t seem setup to demonstrate that the multiple views are beneficial over a single view. In Table 1, there are rows for just an LSTM or just a TreeLSTM, but they seem to be trained with labeled data whereas the proposed method is trained self-supervised. A more informative comparison to demonstrate the value of using multiple views would be to train the LSTM and TreeLSTM with the same objective (and ideally model size). Overall, I don*t think the claims in the paper are well-supported by the model proposed or the experiments. - I have a number of concerns about the experiments. - *Models are trained on a single epoch on the entire corpus without any train-test split*: so there is no early stopping? Why stop training after one epoch? Was there any indication you were overfitting the data? - *The training phase was stopped after 33 hours of training*: Why stop there? Computational constraints? Later comments suggest this is quite premature (*training phase was completed on only 4.6M sentences among the 78M available*). - The results seem to indicate that this method underperforming recent work significantly. Areas of improvement - Some of the language in the introduction and conclusion are a bit of a stretch. Using a linear and tree LSTM (based on dependency parses) doesn*t really represent a *diversity of linguistic structures*. - Related work: There*s no mention of pretained language models, which could be seen as a form of representation learning for language, and have been hugely impactful in NLP. - Method - Missing negative in the log likelihood - Why do you use inner product if other works *report excellent results* with other scoring functions? - *assumes the underlying structure of the sentence to be a sequence, while allowing for long term dependencies*: If anything, the treeLSTM more easily allows for long-term dependencies than the linear LSTM. - *Negative examples are obtained using the dependency Tree LSTM*: I*m not totally sure how the negatives are obtained here. - *The target sequence is encoded using the sequential Tree LSTM, while the positive and negative samples are encoded using the ChildSum Tree LSTM*: why are the sentences not all encoded with the same encoder? - It looks really odd that most of Table 1 is empty. Given your model, I imagine it can*t have been that difficult to evaluate more baselines (BiLSTM and TreeLSTM) on the rest of the tasks. - It*d be nice if you could clearly indicate in Table 1 which method is yours. - Results and Analysis - The standard evaluation setting for sentence embeddings would be GLUE or SuperGLUE. - A glaringly missing baseline is BERT (or any of its relatives), which is also self-supervised. - The results are underwhelming, and as the author admits, somewhat premature as training didn*t seem to finish. - 5.2: what are the contrastive LSTM and Tree LSTM? Are those the learned encoders from the *Contrastive Tree* in Table 1, or are they trained from scratch? - I don*t think the analyses in Sections 5.2 and 5.3 or Figure 2 are particularly useful. - There are a noticeable number of typos. For example, in the abstract: *this linguist[ic] diversity* and *better capture semantic[s]*. It*d be worthwhile to look over the paper closely for typos. [1] AN EFFICIENT FRAMEWORK FOR LEARNING SENTENCE REPRESENTATIONS. Lajanugen Logeswaran and Honglak Lee [2] Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning. Yacine Jernite, Samuel R. Bowman, David Sontag [3] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova [4] Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference. Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang. [5] Enhanced LSTM for Natural Language Inference. Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang, Diana Inkpen. [6] Improving Sentence Representations with Consensus Maximisation. Shuai Tang, Virginia R. de Sa."}
{"id": "iclr2020_392", "title": "Implicit competitive regularization in GANs | OpenReview", "abstract": "Abstract:###Generative adversarial networks (GANs) are capable of producing high quality samples, but they suffer from numerous issues such as instability and mode collapse during training. To combat this, we propose to model the generator and discriminator as agents acting under local information, uncertainty, and awareness of their opponent. By doing so we achieve stable convergence, even when the underlying game has no Nash equilibria. We call this mechanism emph{implicit competitive regularization} (ICR) and show that it is present in the recently proposed emph{competitive gradient descent} (CGD). When comparing CGD to Adam using a variety of loss functions and regularizers on CIFAR10, CGD shows a much more consistent performance, which we attribute to ICR. In our experiments, we achieve the highest inception score when using the WGAN loss (without gradient penalty or weight clipping) together with CGD. This can be interpreted as minimizing a form of integral probability metric based on ICR.", "review": "Review:###https://openreview.net/pdf?id=SkxaueHFPB The paper has some interesting ideas but I don’t think any of them are fully fleshed out. I find the reporting of Inception Score highly suspect. The authors choose WGAN-GP as a baseline and report scores of ~4.5 vs ~5.5 with their modification. However the WGAN-GP paper reports an IS of 7.86 on CIFAR. Furthermore, current GAN SOTA on CIFAR is approaching IS=9. I am not making the argument that the authors ought to demonstrate SOTA results, however they should at least present results which are consistent with the published results of their chosen baseline. The authors then make this statement: “Thus, its superior performance supports the claim that ICR is the appropriate form of regularization for GANs. We emphasize that in our experiments we did not perform any architecture or hyperparameter tuning, and instead use a model intended to be used with WGAN gradient penalty” This does not hold, since the numbers reported are far below the actual baseline. Besides this major point, I am unconvinced by some of the mathematical statements in the paper. Much of the mathematical details are deferred to the original CGD paper. It is not really particularly reader-friendly to defer that to the CGD paper since they are seemingly crucial to the discussion here. Relative to the CGD paper some signs have been flipped and some definitions appear to be used in subtly different ways which makes for a very difficult read. I feel that far too much has been left as an exercise to the reader. Concretely my concerns refer to the main discussion of the effect of the CGD as a regularizer: The authors state: “If some of the singular values of Dxy are very large, this amounts to approximately restricting the update to the orthogonal complement of the corresponding singular vectors” I don’t see how this is the case. The terms Dxy/Dyx aren’t really introduced or defined anywhere in this work. Assuming is the transpose of the other (?) then the update direction is: A + B where A=inv(S) grad_x and B = inv(S)Dxy grad_y (and S = I + Dxy Dyx). So we have a term which is being affected by the smallest singular values of S and a term which is the orthogonal projection of grad_y onto Dxy, alternatively the ridge-regression fit of grad_y on Dxy which would attenuate directions corresponding to the *small* singular values (as is well known from the theory of ridge regularizers). I feel like there is much more to say here than what is discussed in the paper in very vague terms. Of course the effective rank of S, or the rate of decay of its singular values is crucially important. In practise I would assume the smaller SVs of Dxy to be difficult to estimate or the matrix to be rank deficient in which case they would simply be unity in the inverse whereas the directions corresponding to large singular values would be attenuated. So in this case it is the regularized orthogonal complement but its not clear (if the matrix is not full rank) that it is a meaningful direction (and again this is all highly dependent on the effective rank, too). Further on it is mentioned: “For smoothly varying singular vectors, this can be though of as approximately constraining the trajectories to a manifold of robust play”. First it is not at all clear to me what “smoothly varying singular vectors” are. Varying with respect to what? Secondly, the “manifold of robust play” has not been defined anywhere. Finally, figure 3 is quite bizarre to me. None of the quantities have been rigorously defined and so it seems like the relative effect of each of the arrows and the manifold have been drawn arbitrarily in order to fit the story, rather than to actually illuminate the true behaviour in an intuitive manner. B (defined above) has a very clear interpretation as a least-squares fit so I figure that any geometric interpretation of the CGD update direction could start from there."}
{"id": "iclr2020_393", "title": "Boosting Ticket: Towards Practical Pruning for Adversarial Training with Lottery Ticket Hypothesis | OpenReview", "abstract": "Abstract:###Recent research has proposed the lottery ticket hypothesis, suggesting that for a deep neural network, there exist trainable sub-networks performing equally or better than the original model with commensurate training steps. While this discovery is insightful, finding proper sub-networks requires iterative training and pruning. The high cost incurred limits the applications of the lottery ticket hypothesis. We show there exists a subset of the aforementioned sub-networks that converge significantly faster during the training process and thus can mitigate the cost issue. We conduct extensive experiments to show such sub-networks consistently exist across various model structures for a restrictive setting of hyperparameters (e.g., carefully selected learning rate, pruning ratio, and model capacity). As a practical application of our findings, we demonstrate that such sub-networks can help in cutting down the total time of adversarial training, a standard approach to improve robustness, by up to 49% on CIFAR-10 to achieve the state-of-the-art robustness.", "review": "Review:###The paper presents some follow-up work on the lottery ticket hypothesis (LTH). The iterative magnitude-based pruning (IMP) used in the original LTH paper by Frankle and Carbin (FC19) failed to find winning tickets on large-scale vision networks without modifying the training procedure. This paper describes a change in the learning rate schedule that allows the authors to find subnetworks that outperform randomly reinitialized winning tickets. The authors name these subnetworks “boosting tickets”, and demonstrate that boosting tickets train faster. ?The authors then use these boosting tickets to speed up adversarial training. They propose to use FGSM-based adversarial training to find the boosting tickets (since this part is computationally intensive) and then train these obtained boosting tickets using the more expensive and better performing PGD-based adversarial training. The idea of using winning tickets for adversarial training is quite interesting. However, given the speedup times (49%), the simplicity of the idea (combining two existing things, adversarial training and pruning via IMP) and the limited number of experiments, the overall contributions seem to be minor. The findings presented regarding the slight change in the learning rate schedule are not very novel, nor give any new surprising results. A lot of the things that are presented in the paper also appeared in the original paper FC19. Further, it looks like the authors misunderstood what the LTH says and what the definition of a winning ticket is (see below). Overall, the paper presents very incremental and low-impact work. The experiments should be expanded considerably (and corrected accordingly, given the correct definition of a winning ticket) in order for me to recommend acceptance. DETAILED FEEDBACK WINNING TICKETS, Section 3: “In particular, we show that boosting tickets are winning tickets, in the sense that they outperform the randomly initialized models”. Based on this sentence, I believe that the authors seem to think that a winning ticket (subnetwork + initialization) is the one that outperforms the same subnetwork when it is randomly reinitialized. However, this is incorrect. Frankle and Carbin define it as a subnetwork+initialization that can be trained at least as fast to the same or higher accuracy as the original (!) network. TRAINING TIME. FC19 (v3 on arxiv) contains graphs demonstrating that winning tickets train faster. This submission does not have an explicit comparison how their new learning rate schedule affects the performance of winning tickets (!) compared to the learning rate warmup done in FC19. RANDOM TICKETS. There is no experimental evidence that boosting tickets do much better than random subnetworks when trained in an adversarial way (potentially even more impressive computational savings...). LEARNING RATE SCHEDULE, Section 3. The only difference in the learning rate schedule is that a different schedule is used for pruning and for training. One of the key properties of the LTH is that the subnetworks found via IMP can be trained under *the same* training procedure. In my opinion, introducing some unprincipled/theoretically unjustified learning rate tricks has none to little impact on future research. SCALE, Introduction: “Although FC19 show that winning tickets converge faster than the full models, it is only observed on small networks, such as a convolutional neural network..”. While it is true that in FC19 the experiments were performed on relatively small networks, Frankle et al. 2019 have another paper containing experimental results on large-scale networks (all the way up to inception on imagenet). Further, I would like to highlight that this submission only contains experiments on wide Resnets and VGG-16. If one of the suggested contributions is doing IMP at scale, why are the empirical results limited to these relatively small networks? PRUNING RATIO, Section 3.3. It looks like the pruning ratios are tested only for one shot pruning. Could the authors please explain their choice, and elaborate on the reason for this particular experiment. UNSTRUCTURED PRUNING, Section 2: “One of the limitations of the LTH…winning tickets are found by unstructured pruning.” I cannot see how the submission addresses this. Same holds for the rest of the paragraph. Other minor comments: - “We observe the standard technique introduced in FC19 for .. does not always find boosting tickets”. I am not sure how this is different from what was already pointed out in FC19, and “we observe” suggests that the contribution of observing was done by the authors in this submission. - “boosting effect” in the third paragraph in the introduction is undefined. - none of the plots contain error bars. - In section 5 under “Accelerate adversarial training” a future research direction is suggested: combine “recycling the gradients” idea with the idea proposed in this paper to speed up overall training time. This seems like a trivial application rather than a research idea. What did the authors have in mind here? - Calling the model “Madry’s” in all the tables seems extremely unfair to all of his (more junior) collaborators. - The comment under Figure 5, “While a wider model always boosts faster..”, seems unsupported by the figures."}
{"id": "iclr2020_394", "title": "Encoder-decoder Network as Loss Function for Summarization | OpenReview", "abstract": "Abstract:###We present a new approach to defining a sequence loss function to train a summarizer by using a secondary encoder-decoder as a loss function, alleviating a shortcoming of word level training for sequence outputs. The technique is based on the intuition that if a summary is a good one, it should contain the most essential information from the original article, and therefore should itself be a good input sequence, in lieu of the original, from which a summary can be generated. We present experimental results where we apply this additional loss function to a general abstractive summarizer on a news summarization dataset. The result is an improvement in the ROUGE metric and an especially large improvement in human evaluations, suggesting enhanced performance that is competitive with specialized state-of-the-art models.", "review": " This paper introduces an encoder-decoder as a differentiable loss function for sequential autoregressive generation tasks and more specifically for summarization. This is done by adding a recorder network that that takes the decoded sequence from the summarizer as input and is trained to output the reference summary. I see a fundamental issue with this work: * During inference, authors decode from the probability distribution of the seq2seq model using beam search. * But for training (original seq2seq + recorder) authors backpropagate the NLL loss (which is fully differentiable) of the recorder on reference summaries through the softmax probabilities of outputs from the seq2seq model. >> This whole architecture can be seen as a traditional end-to-end seq2seq model with non-linearity and normalization (softmax) in the middle. Additionally: >> *backpropagating through the softmax weights during training and using the argmax during inference* falls into a long line of work for propagating non-differential objective functions through continuous relaxations of categorical latent variables, more specifically the *straight through* and *gumbel-softmax* (see refs.) These methods have proven to be a strong alternative to reinforcement learning to train non-differential objectives and have been implemented quite a lot for sequence generation mainly for SeqGANs and even for text summarization connections to this line of work must be established in this paper. references - Estimating or propagating gradients through stochastic ´neurons for conditional computation. Bengio et al. 2013 arXiv preprint arXiv:1308.3432, 2013. - CATEGORICAL REPARAMETERIZATION WITH GUMBEL-SOFTMAX Jang et al. 2018 https://arxiv.org/pdf/1611.01144.pdf - GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution https://arxiv.org/pdf/1810.05739.pdf - Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders https://arxiv.org/pdf/1805.04843.pdf - MeanSum : A Neural Model for Unsupervised Multi-Document Abstractive Summarization https://arxiv.org/pdf/1810.05739.pdf"}
{"id": "iclr2020_395", "title": "Barcodes as summary of objective functions* topology | OpenReview", "abstract": "Abstract:###We apply canonical forms of gradient complexes (barcodes) to explore neural networks loss surfaces. We present an algorithm for calculations of the objective function*s barcodes of minima. Our experiments confirm two principal observations: (1) the barcodes of minima are located in a small lower part of the range of values of objective function and (2) increase of the neural network*s depth brings down the minima*s barcodes. This has natural implications for the neural network learning and the ability to generalize.", "review": "Review:###This paper introduces the notion of barcodes as a topological invariant of loss surfaces that encodes the *depth* of local minima by associating to each minimum the lowest index-one saddle. An algorithm is presented for the computation of barcodes, and some small-scale experiments are conducted. For very small neural networks, the barcodes are found to live at small loss values, and the authors argue that this suggests it may be hard to get stuck in a suboptimal local minimum. I believe the concept of barcodes will be new to most members of the ICLR community (at least it was to me), and I appreciate the authors* effort to convey the ideas through multiple definitions in Section 2. I wasn*t able to fully appreciate the importance of Definition 3, and Definitions 1 and 2 were tough to digest owing to imprecise language, but I think I got the main point. I was also unable to fully comprehend the definitions of *birth* and *death* in this context. I*d strongly encourage the authors to improve the readability of this section so that non-experts can follow the story. It seems like the main contribution is a new algorithm for computing barcodes of minima. I am unfamiliar with prior work in this direction, and I was also unable from the paper to infer what the main improvements were relative to the existing algorithms. I*d encourage the authors to state their explicit algorithmic improvements, and to demonstrate empirically that the new algorithm outperforms the prior ones in the expected ways. The main experiments are on extremely tiny neural networks, presumably owing to computational restrictions. The authors state that *it is possible to apply it to large-scale modern neural networks*, but it*s not clear to me how that would work or what additional algorithmic improvements (if any) would need to be made in order to do so. I don*t think that the results on tiny neural networks have much relevance to practice, so I think the empirical data presented in this paper will have very limited impact. If there were results for practical models, it would be a different story. So I*d encourage the authors to devote additional effort to scaling up the method for use on practical neural network architectures. Overall, I think there may be some really nice ideas in this paper that could help shape our understanding of neural network loss surfaces, but the current paper does not explore those ideas fully and does not convey them in a sufficiently clear manner. I hope to see an improved version of this paper at a future conference, but I cannot recommend acceptance of this version to ICLR."}
{"id": "iclr2020_396", "title": "Pruning Depthwise Separable Convolutions for Extra Efficiency Gain of Lightweight Models | OpenReview", "abstract": "Abstract:###Deep convolutional neural networks are good at accuracy while bad at efficiency. To improve the inference speed, two kinds of directions are developed, lightweight model designing and network weight pruning. Lightweight models have been proposed to improve the speed with good enough accuracy. It is, however, not trivial if we can further speed up these “compact” models by weight pruning. In this paper, we present a technique to gradually prune the depthwise separable convolution networks, such as MobileNet, for improving the speed of this kind of “dense” network. When pruning depthwise separable convolutions, we need to consider more structural constraints to ensure the speedup of inference. Instead of pruning the model with the desired ratio in one stage, the proposed multi-stage gradual pruning approach can stably prune the filters with a finer pruning ratio. Our method achieves 1.68 times speedup with neglectable accuracy drop for MobileNetV2.", "review": "Review:###This paper presents a technique to gradually prune the depthwise separable convolution networks, such as MobileNet, for further improving the speed. By imposing more structural constraints and using multi-stage iterative pruning, the proposed pruning algorithm can achieve roughly 2x speedup with little accuracy drop on standard benchmarks. In my opinion, this paper is a borderline paper because it lacks novelty in terms of the algorithm itself. Particularly, multi-stage gradual pruning has long been used in network pruning for better performance. Nevertheless, applying the technique of network pruning to lightweight architectures (so as to handle depthwise separable convolution) seems to be new and promising. Given that, I*ve given a score of 3 and I*m willing to increase the score if the authors can resolve my concerns below. Concerns: - All experiments are done with MobileNet (v1 and v2), I wonder if the algorithm works well on other architectures. I suggest the authors to conduct an extra set of experiments with a new architecture. - The scope of the paper seems to be a little narrow. I wonder if the authors can include a few other lightweight operators. Minor Comments: - I think a recent paper [1] is quite relevant, though they focused on pruning standard convolution kernel. In particular, the paper utilizes filter pruning to get depthwise separable convolution from standard convolution kernel. I wonder if the technique in this paper can been applied to depthwise separable convolution operator to further reduce parameters. For example, you can first reparameterize the original depthwise separable convolution with three consecutive layers with the first and third layers 1x1 convolution (the first and third layers serve as eigenbasis to whiten the middle layer). Reference: [1] EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis. ICML, 2019."}
{"id": "iclr2020_397", "title": "Distributionally Robust Neural Networks | OpenReview", "abstract": "Abstract:###Overparameterized neural networks trained to minimize average loss can be highly accurate on average on an i.i.d. test set, yet consistently fail on atypical groups of the data (e.g., by learning spurious correlations that do not hold at test time). Distributionally robust optimization (DRO) provides an approach for learning models that instead minimize worst-case training loss over a set of pre-defined groups. We find, however, that naively applying DRO to overparameterized neural networks fails: these models can perfectly fit the training data, and any model with vanishing average training loss will also already have vanishing worst-case training loss. Instead, the poor worst-case performance of these models arises from poor generalization on some groups. As a solution, we show that increased regularization---e.g., stronger-than-typical weight decay or early stopping---allows DRO models to achieve substantially higher worst-group accuracies, with 10% to 40% improvements over standard models on a natural language inference task and two image tasks, while maintaining high average accuracies. Our results suggest that regularization is critical for worst-group performance in the overparameterized regime, even if it is not needed for average performance. Finally, we introduce and provide convergence guarantees for a stochastic optimizer for this group DRO setting, underpinning the empirical study above.", "review": "Review:###To the best of my knowledge, this is the first paper to carefully address and propose an algorithm (with guarantees) for distributionally robust learning in the overparametrized regime, which is typical of modern large deep neural networks. Following other work, the paper formalizes distributionally robust learning as the minimization of a worst-case loss over a set of possible distributions. The main message of the paper is that for distributionally robust learning, regularization plays an important role by avoiding perfect fitting to the training data, at the cost of poor generalization (thus lack of robustness) in some of the possible distributions. Furthermore, the paper proposes a new stochastic optimization algorithm to minimize the loss that corresponds to distributionally robust learning and gives convergence guarantees for the proposed algorithm. I think this is an interesting and solid paper, with a clear presentation style, and well-supported contributions. To the best of my knowledge, this is novel work and, in my opinion, it is relevant work, both in terms of applicability as well as in terms of contribution to the understanding of the generalization behavior of overparameterized deep neural networks."}
{"id": "iclr2020_398", "title": "Hindsight Trust Region Policy Optimization | OpenReview", "abstract": "Abstract:###As reinforcement learning continues to drive machine intelligence beyond its conventional boundary, unsubstantial practices in sparse reward environment severely limit further applications in a broader range of advanced fields. Motivated by the demand for an effective deep reinforcement learning algorithm that accommodates sparse reward environment, this paper presents Hindsight Trust Region Policy Optimization (HTRPO), a method that efficiently utilizes interactions in sparse reward conditions to optimize policies within trust region and, in the meantime, maintains learning stability. Firstly, we theoretically adapt the TRPO objective function, in the form of the expected return of the policy, to the distribution of hindsight data generated from the alternative goals. Then, we apply Monte Carlo with importance sampling to estimate KL-divergence between two policies, taking the hindsight data as input. Under the condition that the distributions are sufficiently close, the KL-divergence is approximated by another f-divergence. Such approximation results in the decrease of variance and alleviates the instability during policy update. Experimental results on both discrete and continuous benchmark tasks demonstrate that HTRPO converges significantly faster than previous policy gradient methods. It achieves effective performances and high data-efficiency for training policies in sparse reward environments.", "review": "Review:###The paper proposes an extension to TRPO that makes use of hindsight experience replay. The technique follows previous work on HER for policy gradient methods, augmenting these previous approaches with a constraint based on the average squared difference of log-probabilities. Experiments show that the method can provide significant improvements over previous work. Overall, the paper provides a convincing demonstration of trust region principles to goal-conditioned HER learning, although I think the paper could be improved in the following ways: -- Bounding KL by the squared difference of log-probabilities seems loose. The original TRPO objective is based on a TV-divergence (and before that, based on a state-occupancy divergence). Is it possible to directly bound the TV-divergence (either of actions or state occupancies) by a squared difference of log-probabilities? -- The use of WIS greatly biases the objective. Is there a way to quantify or motivate this bias? -- What is the method in which the alternative goal g* is proposed? I believe this can have a great effect on the bias and variance of the proposed method."}
{"id": "iclr2020_399", "title": "StructPool: Structured Graph Pooling via Conditional Random Fields | OpenReview", "abstract": "Abstract:###Learning high-level representations for graphs is of great importance for graph analysis tasks. In addition to graph convolution, graph pooling is an important but less explored research area. In particular, most of existing graph pooling techniques do not consider the graph structural information explicitly. We argue that such information is important and develop a novel graph pooling technique, know as the StructPool, in this work. We consider the graph pooling as a node clustering problem, which requires the learning of a cluster assignment matrix. We propose to formulate it as a structured prediction problem and employ conditional random fields to capture the relationships among assignments of different nodes. We also generalize our method to incorporate graph topological information in designing the Gibbs energy function. Experimental results on multiple datasets demonstrate the effectiveness of our proposed StructPool.", "review": "Review:###The authors here introduces a novel graph pooling technique called StructPool that uses the underlying graph’s structural information to behave as a node clustering algorithm and learns a node clustering matrix. Graph level classification requires learning good graph level representation, especially for aggregating low level information for high . Recent work in pooling does not take advantage of important structural information of the relationship between different nodes. Here, the authors formulate graph pooling as a structured prediction problem, control clique set in the CRFs and use mean filed approximation to calculate assignments. A cluster assignment matrix assigns each node in the original graph to a cluster in the new graph. The assignment not only depends on the node features but also on the cluster assignment of the other nodes. The authors therefore draw connection with finding the optimal assignment to minimizing the Gibbs energy. The authors propose to learn clustering assignment via CRF conditioned on the global feature representation of the nodes. The unary potentials of the cliques are computed used the GCN to measure energy of each node. The novelty in accommodating topology information is in using l hop connectivity based on adjacency A to define pairwise cliques thus building pairwise relationship between pairs of nodes thus allowing the Gibbs energy formulation of the cluster assignment thereby using GCN to also compute this pairwise energy. I have a few questions as below: I think the authors can better elucidate the motivation for using the attention matrix over Gaussian kernels to measure pairwise energy in section 3.3; an empirical experiment for drawing comparison wrt to the computational time and number of feature dimensions on a toy problem seems important. How is the computation of the unary potential and pairwise energy influenced by the connectivity of the graph G for the datasets considered? It would be interesting to see how the pairwise energy, unary energy varies over different layers of GCNs. Further, how is the cluster assignment affected by the l-hop connectivity? Is there a notion of the minimum value of ‘k’ in the context of convergence? What happens in case of very different graph features, or structural assumptions where the cliques are not enforced? Is there a notion of how the method performs on datasets with a high percentage of isomorphism bias: repeating instances or repeating instances with different labels? It will be interesting to see a discussion on how the performance varies with respect to the depth of the overall architecture, positioning of the structpool and some results on how effective they are on hierarchical features and multiple pooling ops as in architectures such as Graph UNet. Avoid repetition in 2.2 Related work section and in other sections throughout. Otherwise, the paper is rather well written and has clarity."}
{"id": "iclr2020_400", "title": "Scalable Neural Methods for Reasoning With a Symbolic Knowledge Base | OpenReview", "abstract": "Abstract:###We describe a novel way of representing a symbolic knowledge base (KB) called a sparse-matrix reified KB. This representation enables neural modules that are fully differentiable, faithful to the original semantics of the KB, expressive enough to model multi-hop inferences, and scalable enough to use with realistically large KBs. The sparse-matrix reified KB can be distributed across multiple GPUs, can scale to tens of millions of entities and facts, and is orders of magnitude faster than naive sparse-matrix implementations. The reified KB enables very simple end-to-end architectures to obtain competitive performance on several benchmarks representing two families of tasks: KB completion, and learning semantic parsers from denotations.", "review": "Review:###The paper provides a way to represent symbolic KBs called sparse matrix reified. Relations and entities* types are modelled using sparse matrices. This modelization allows distributing the computation on many GPUs. A neural model is used to manage these matrices. The trained model can be used to perform multi-hop queries. The proposed system has been compared with related systems. The results show that it achieves hits@1 that are comparable with the others. The proposed approach seems promising, however, I feel that the paper is not ready for publication. The experiments lack a scalability test with related systems, the scalability test included in the paper only takes into account other definitions of the system. Also, comparison on the run time should be performed to see if the lower performance in terms of hits@1 (which are good anyway) is balanced by a better run time. Therefore, it is difficult to see whether the proposed system is good or not. As for the description of the system, it seems to me to be quite foggy. In my opinion, the neural model should be better described to show how the sparse matrices are mapped into the model. Also, how are the training and testing sentences created? How would the output of a query look? After reading the paper I have the feeling that it was written in a bit of a hurry, without working on the details. There are several typos, parentheses not correctly opened or closed, out of place commas that make some sentences seem unrelated to the rest of the paragraph. However, here last problems are easily fixable. To sum up, I am conflicted about the choice of the final score. On the one hand, the approach is interesting, on the other hand the article seems to me not mature enough and strong enough from the point of view of organization, contents and experimental results shown. Minor problems On page 4, the size of the matrices XM_k uses the factor b that is introduced later. In the introduction, next to footnote 1, I suggest specifying that A is the first query, the one about Tarantino*s movies. Also, his surname is misspelt throughout the paper. The correct one is Tarantino."}
{"id": "iclr2020_401", "title": "A Unified framework for randomized smoothing based certified defenses | OpenReview", "abstract": "Abstract:###Randomized smoothing, which was recently proved to be a certified defensive technique, has received considerable attention due to its scalability to large datasets and neural networks. However, several important questions still remain unanswered in the existing frameworks, such as (i) whether Gaussian mechanism is an optimal choice for certifying -normed robustness, and (ii) whether randomized smoothing can certify -normed robustness (on high-dimensional datasets like ImageNet). To answer these questions, we introduce a {em unified} and {em self-contained} framework to study randomized smoothing-based certified defenses, where we mainly focus on the two most popular norms in adversarial machine learning, {em i.e.,} and norm. We answer the above two questions by first demonstrating that Gaussian mechanism and Exponential mechanism are the (near) optimal options to certify the and -normed robustness. We further show that the largest radius certified by randomized smoothing is upper bounded by , where is the dimensionality of the data. This theoretical finding suggests that certifying -normed robustness by randomized smoothing may not be scalable to high-dimensional data. The veracity of our framework and analysis is verified by extensive evaluations on CIFAR10 and ImageNet.", "review": "Review:###Summary. The authors propose a new definition for robustness of random functions. This definition is ideal for analyzing the certified robustness under randomized smoothing techniques. They analyze and show that the Gaussian smoothing is near optimal for ell_2 smoothing as the mean maximum error is only off by a factor of log d where d is the dimension from the optimal mean maximum energy. This is the case even under a more strict definition of robustness defined as D_infty. Moreover, the authors show that indeed smoothing with an exponential family is optimal under D_infty robustness metric with radius measured in ell_infty. I find the paper very interesting and the approach is novel and generic. I do not have any major criticism. Minor comments. 1) Equation 3 *D(A(x*),A(x))* >> *D_infty(A(x*),A(x))* 2) Page 6 third line below Theorem 16. Reference of Theorem 11 should be Corollary 11. 3) The authors should report the certified accuracy of the undefended baseline classifier over varying radius in Figures 1 and 2 and 3. 4) Running experiments on ImageNet following Cohen et al. should make the paper stronger. 4) Can the authors comment on is the certified accuracy for sigma=0.5 at radius = 0 is better than the unsmoothned classifier a sigma 1.0. I expect that the radius of certification is larger for larger sigma. 5) The authors should explain how does the new definition of robustness relate to the common robustness definitions as the one by Cohen et al. More discussion is necessary for this and more justification. 6) Why is the D_MR defined as maximum over ? It seems it is only sufficient to define it as the ratio over . It seems that this is only needed for Theorem 8 to hold. ------------------------------------------------------------------------ After further careful read of several relevant papers, e.g. Bun et. al 2016 and the work of Dwork *Concentrated Differential Privacy*, I have several questions I would like to ask for some further clarifications. 1) Showing that a network is robust under robustness, implies very strong results. The type of results that are common in the literature. This is since robustness, implies DP networks (see Lemma 3.2 and proposition 3.3 of Bun et al.). Once DP is guaranteed identical results of Lecurer et al. can be derived immediately as this implies separation in expectation (Lecurer et al.) where one can study directly the deterministic classifier and not the random studied in this work. 2) The authors rely on the lower bounds of Bun et al. to find the average maximum energy that preserves the robustness (Thm 15 and 16). Authors show that indeed exponential smoothing is optimal. This is significant but the analysis was intensively based on Bun et al. 3) The relaxation to robustness results into improvement of the dependency on the dimension to instead of for under . This should not be surprising at all and in fact is identical to the results of Bun et al. Note that the zCDP proposed by Bun et al, is a relaxed version of DP where -DP for some radius implies zCDP with radius . See proposition 3.3. Therefore, Theorem 6 and 17 are not surprising nor are they new. 4) My major concern was with the results relating to Gaussian smoothing. I do understand that since Gaussian smoothing only implies high probability result of DP which is often referred to as ( , )-DP which happens to be a equivalent to zCDP proposed by Bun et. al. Therefore, I have no issues of using to analyzing the robustness for Gaussian smoothing since it was always analyzed in the DP community with the -DP and not the stronger -DP. However, the statement of the result (Theorem 12) confused me vastly. Let me clarify. Theorem 12 seems to be too good to be true. How is it possible that one can guarantee robustness without any dimensionality dependence. Using Gaussian smoothing the can depend on . While may seem small; improving this to a constant in dimension is still a very big gap from . This may raise several questions whether one can actually find this optimal smoothing distribution. However, with a careful read of Theorem 12, the range of the input decreases as a function of . That is for a given range of input (independent from d), the energy in fact is NOT constant but scales with . In such a case, the Gaussian smoothing is now of order . Now, the factor is still , but now this is very different as indeed improving the Gaussian to may not be of significant interest as the energy still depends in the optimal sense on which does not allow it to scale for larger problems. Moreover, Cohen et al results show that with Gaussian smoothing the energy of the noise scales since the noise energy where is std of Gaussian. Therefore, it seems that there is nothing surprising about such a result at all. The statement of the Theorem is very misleading and confusing. Overall, I like this new approach of analyzing the random smoothed classifier; however, the poor presentation of the work and the mis-represented Theorems that seem to over claim are a major reason for my rating. In addition, the paper should be self-contained in which one should not need to read 2-3 other works to figure out the details in this work and the meaning of the several robustness metrics and their direct relations to DP and Lecuer et al. results. The statement of constant in dimension lower bound on the energy of the noise under was to me the major contribution; however, I found now that the statement is misleading and that in fact it is reduces the contribution of the paper particularly after learning that such lower bounds are already derived in Bun et al."}
{"id": "iclr2020_402", "title": "Implementing Inductive bias for different navigation tasks through diverse RNN attrractors | OpenReview", "abstract": "Abstract:###Navigation is crucial for animal behavior and is assumed to require an internal representation of the external environment, termed a cognitive map. The precise form of this representation is often considered to be a metric representation of space. An internal representation, however, is judged by its contribution to performance on a given task, and may thus vary between different types of navigation tasks. Here we train a recurrent neural network that controls an agent performing several navigation tasks in a simple environment. To focus on internal representations, we split learning into a task-agnostic pre-training stage that modifies internal connectivity and a task-specific Q learning stage that controls the network*s output. We show that pre-training shapes the attractor landscape of the networks, leading to either a continuous attractor, discrete attractors or a disordered state. These structures induce bias onto the Q-Learning phase, leading to a performance pattern across the tasks corresponding to metric and topological regularities. Our results show that, in recurrent networks, inductive bias takes the form of attractor landscapes -- which can be shaped by pre-training and analyzed using dynamical systems methods. Furthermore, we demonstrate that non-metric representations are useful for navigation tasks.", "review": "Review:##### Overview This paper explores how pre-training a recurrent network on different navigational objectives confers different benefits when it comes to solving downstream tasks. First, networks are pretrained on an objective that either emphasizes position (path integration) or landmark memory (identity of the last wall encountered). This pretraining generates recurrent networks of two classes, called PosNets and MemNets (in addition to no pre-training, called RandNets). Surprisingly, the authors found that pre-training confers different benefits that manifests as differential performance of PosNets and MemNets across the suite. Some evidence is provided that this difference has to do with the requirements of the task. Moreover, the authors show how the different pretraining manifests as different dynamical structures (measured using fixed point analyses) present in the networks after pre-training. In particular, the PosNets contained a 2D plane attractor (used to readout position), whereas the MemNets contained clusters of fixed points (corresponding to the previously encountered landmark). Overall, I thought this was a very interesting paper--it is one of the first papers I have seen that demonstrates how different pre-training requirements both change network dynamics (as measured by fixed points), and how those differences can yield different benefits on downstream navigational tasks. ## Major comments/concerns - I think the presentation the pretraining objective (eq 3) could be clearer. Is eq 3 what is minimized during pre-training? How are alpha, _x0008_eta, and gamma chosen? alpha is used to separate the two types of networks (MemNet from PosNet), which is the critical difference studied in the paper, so it would helpful to go into more detail about what alpha controls and how it was chosen. - For the first task, I am surprised that the agent is able to navigate the environment using only the eight neighboring locations. What is the size of the arena? What fraction of the states are simply surrounded on all sides by empty space? It would be informative to show some trajectories of agents solving the basic task. - For Fig 3A and 3B, it would be nice to show the other network*s performance (i.e. show the PosNet on the scaling task in 3A, and the MemNet on the bar task in 3B). - How come there are no networks that are able to solve both sets of tasks? That is, how come there are no networks in the upper right region of Fig 3C? Does this suggest that an agent needs to combine two separate RNNs to solve the whole suite of tasks? - What happens if you train recurrent networks with more sophisticated cell architectures (e.g. a GRU or an LSTM)? These are typically easier to train (and using automatic differentiation techniques are also amenable to fixed point analysis). ## Minor comments - In eq. (1), use `left(` and `\right)` to make the first set of parentheses have an appropriate height. - Typo on the first line after eq. (6) (matrices) - Relevant reference on comparing networks using dynamics around approximate fixed points: https://arxiv.org/abs/1907.08549."}
{"id": "iclr2020_403", "title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs | OpenReview", "abstract": "Abstract:###We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al., 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction—membership classification and API watermarking—which while successful against some adversaries can also be circumvented by more clever ones.", "review": "Review:###This paper studies the effectiveness of model extraction techniques on large pretrained language models like BERT. The core hypothesis of the paper is that using pretrained language models, and pretrained contextualized embeddings, has made it easier to reconstruct models using model stealing/extraction methods. Furthermore, this paper demonstrates that an attacker needn*t have access to queries from the training set, and that using random sequences of words as a query to the *victim* model is an effective strategy. They authors also show that their model stealing strategies are very cost effective (based on Google Cloud compute cost). The basic set up of their experiments has a fine-tuned BERT model as the victim model, and a pre-trained BERT model as a the attacker model. The attacker model is assumed to not have access to the training set distribution and the queries are randomly generated. There are 2 strategies for query generation (with additional task specific heuristics): 1) randomly selecting words from WikiText-103, and 2) randomly selecting sentences or paragraphs from WikiText-103. The victim model is passed a generated query and the attacker model is fine-tuned using the output from the victim model. Overall, this paper find that this simple strategy for query generation is effective on 4 different datasets: SST2, MNLI, SQuAD 1.1 and BooolQ. The method is also cost-effective, a few hundred dollars depending on the dataset and the number of queries used to train the attacker model. The paper also present some analysis. They find that queries with higher agreement across victim models (5 BERTs with different random seeds) also leda to better results for the attacker model. The authors also run some experiments with humans to test the interpretability of the queries they generated. They collect annotations on SQuAD using questions that were generated with the WIKI and RANDOM strategies (they also compare highest agreement and lowest agreement queries), and also collect a control with the original SQuAD questions. While this is an interesting analysis to present, showing that most of the generated queries are nonsensical to humans and there is low inter-annotator agreement, I have an issue with the experimental procedure here: the victim model is fine-tuned on the original data, therefore it has picked up some of the data heuristics used to generate the queries, the annotators are not trained on, or shown any of the original examples (there is a control run, but these are presumably a separate set of annotators). Through interviews, the authors learn that the annotators were using word overlap heuristics, but perhaps training the annotators on a small set of the original data would draw a closer example to the victim model. Either way, while this is an interesting result, it seems a bit misplaced in this paper. I*m not sure this human annotation experiment is contributing in any real way to the core thesis of the paper. The authors also test the results of having a mismatch between the victim and attacker model. They consider the mismatch of BERT-base and BERT-large models. They conclude that *attackers can maximize their accuracy by fine-tuning more powerful language models, and that matching architectures is a secondary concern.* This conclusion feels like a bit of a stretch. I would suggest that the authors add another few rows of experiments comparing less similar model architectures. The paper*s finding that a model trained from scratch, QANet on SQuAD, suffers significantly without access to the training set inputs is strong supportive evidence for their hypothesis that using pretrained language models has made model extraction easier. The authors also present a few defense strategies, membership inference, implicit membership classification, and watermarking. They also discuss the limitations of these strategies and do not claim to have solved the problem at hand. Overall, I think this paper makes a useful contribution to the field and I would accept this paper. While I have a couple of issues with some of the experiments (human evaluation and architecture mismatch), I think this paper is thorough and the experiments are well presented. This is the first paper, to the best of my knowledge, showing the efficacy of model extraction of large pretrained language models using rubbish/nonsensical inputs."}
{"id": "iclr2020_404", "title": "Keep Doing What Worked: Behavior Modelling Priors for Offline Reinforcement Learning | OpenReview", "abstract": "Abstract:###Off-policy reinforcement learning algorithms promise to be applicable in settings where only a fixed data-set (batch) of environment interactions is available and no new experience can be acquired. This property makes these algorithms appealing for real world problems such as robot control. In practice, however, standard off-policy algorithms fail in the batch setting for continuous control. In this paper, we propose a simple solution to this problem. It admits the use of data generated by arbitrary behavior policies and uses a learned prior -- the advantage-weighted behavior model (ABM) -- to bias the RL policy towards actions that have previously been executed and are likely to be successful on the new task. Our method can be seen as an extension of recent work on batch-RL that enables stable learning from conflicting data-sources. We find improvements on competitive baselines in a variety of RL tasks -- including standard continuous control benchmarks and multi-task learning for simulated and real-world robots.", "review": "Review:###This paper present a novel approach to reinforcement learn from batched data that come from very different sources. To achieve this, they propose to learn a prior model and then constrain the RL policy to a trust-region that does not deviate from the domain where the current policy is close to. The paper is clearly written. Strength: the paper is very novel and tries to solve a very challenging problem. The success of this approach shows the potential that collecting large-scale data without distinguishment is possible. With that, the data efficiency will be much improved! The experiment itself shows superior performance than other methods. This method also works for real robots. Weakness: There is no algorithm box so it can be hard to fully reproduce. The paper mentions stableness for the method however the analysis on this aspect is limited."}
{"id": "iclr2020_405", "title": "Learning Neural Surrogate Model for Warm-Starting Bayesian Optimization | OpenReview", "abstract": "Abstract:###Bayesian optimization is an effective tool to optimize black-box functions and popular for hyper-parameter tuning in machine learning. Traditional Bayesian optimization methods are based on Gaussian process (GP), relying on a GP-based surrogate model for sampling points of the function of interest. In this work, we consider transferring knowledge from related problems to target problem by learning an initial surrogate model for warm-starting Bayesian optimization. We propose a neural network-based surrogate model to estimate the function mean value in GP. Then we design a novel weighted Reptile algorithm with sampling strategy to learn an initial surrogate model from meta train set. The initial surrogate model is learned to be able to well adapt to new tasks. Extensive experiments show that this warm-starting technique enables us to find better minimizer or hyper-parameters than traditional GP and previous warm-starting methods.", "review": " Learning from past experience to quickly adapt to a new task has been an important and fast-growing issue in machine learning. Such technique facilitates Bayesian optimization as well, warm-starting Bayesian optimization. Recently a few methods have been developed along this direction, from designing handcrafted meta-features to learning meta-features. The current paper takes a similar step, learning neural surrogate model from related tasks to warm-start Bayesian optimization. The main idea is to replace the mean function of GP by neural surrogate model, so that parameterized models are used for meta-training, in the framework of RETILE. An idea of weighted REPTILE is another contribution in this paper, where parameter updates are done with weighs defined by rewards. ---Strength--- - Learning initialization for a surrogate model to warm-start Bayesian optimization is a sound approach. ---Weakness--- - While mean function of GP is replaced by a neural surrogate model, the posterior variance of GP should be calculated. In other words, GP regression should be run in addition to updating the neural surrogate model. One can use the conditional neural process (instead of GP regression). Have a look at the ICML18 paper: Marta Garnelo et al. (2018), *Conditional neural processes,* ICML. ---Comments--- - You can also learn an initial mean function of GP. Any comparison? - There is also interesting work on meta Bayesian optimization: Zi Wang et al. (2018), *Regret bounds for meta Bayesian optimization,* NeurIPS. - Ranking loss is used to train neural surrogate models. It is not clear why minimizing ranking loss makes sense in this case. It will be different from the mean function of GP regression, so it is not clear what is the behavior of the acquisition function constructed by the neural surrogate model as the posterior variance of GP."}
{"id": "iclr2020_406", "title": "Learning vector representation of local content and matrix representation of local motion, with implications for V1 | OpenReview", "abstract": "Abstract:###This paper proposes a representational model for image pair such as consecutive video frames that are related by local pixel displacements, in the hope that the model may shed light on motion perception in primary visual cortex (V1). The model couples the following two components. (1) The vector representations of local contents of images. (2) The matrix representations of local pixel displacements caused by the relative motions between the agent and the objects in the 3D scene. When the image frame undergoes changes due to local pixel displacements, the vectors are multiplied by the matrices that represent the local displacements. Our experiments show that our model can learn to infer local motions. Moreover, the model can learn Gabor-like filter pairs of quadrature phases.", "review": "Review:###The authors propose a model for learning local pixel motions between pairs of frames using local image representations and relative pixel displacements between agents and objects. The model learned is compared to the ability of the primary visual cortex where adjacent simple cells share quadrature relationships and capture local motion. *The representation theory underlies much of modern mathematics and holds the key to the quantum theory (Zee, 2016).* Can the relevance of this claim be elaborated on? *Figure 1 illustrates the scheme of representation.* Please provide more detail here on what is happening in the figure. The caption and reference here are not informative to what the figure is representing. *We obtain the training data by collecting static images for (It) and simulate the displacement field ... We refer to this method as self-supervised learning* This is not self-supervised learning. In self-supervised learning the training label/signal is generated by the system. In this case artificial data is being generated as the displacement between images is sampled. Since the motion between images is artificially generated what guarantees are there that the model is learning to capture realistic motion behavior? Why not use adjacent video frames? *Note that those methods train deep and complicated neural networks with large scale datasets to predict optical flows in supervised manners, while our model can be treated as a simple one-layer network, accompanied by weight matrices representing motions.* Is there a comparison on execution times of the different approaches? *by obtaining the pre-trained models and testing on V1Deform testing data* Is this a fair comparison if the proposed approach was trained on V1Deform training data and the comparison methods were not. A more appropriate comparison would be to apply all the methods to infer the displacement fields between video frames which is also a more natural application. This can be controlled to contain small motions if needed. Why nt use the MUG dataset here? *Displacements at image border are leaved out* -> left out Sections 5.4, 5.5 and 5.6 show only qualitative results with no comparison methods. Can the authors provide reasons that other methods could not be used for evaluation? I am not sure I understand the motivation for the approach. Why do we need this over other methods that can better capture larger motions. This needs to be more clear from the introduction. Why do we care if the approach captures aspects of V1 for the tasks presented? The work is sensible and the approach is clear but I found the evaluation and motivation lacking in key areas that I mention above. The authors should revise and make it clear to the reader why we should care about this problem. Aligning with V1 is interesting but it does not come into play in the applications of the approach or the analysis so I am not sure why I should care. The evaluation also needs to be much more convincing before I could recommend acceptance."}
{"id": "iclr2020_407", "title": "Fairness with Wasserstein Adversarial Networks | OpenReview", "abstract": "Abstract:###Quantifying, enforcing and implementing fairness emerged as a major topic in machine learning. We investigate these questions in the context of deep learning. Our main algorithmic and theoretical tool is the computational estimation of similarities between probability, ```a la Wasserstein**, using adversarial networks. This idea is flexible enough to investigate different fairness constrained learning tasks, which we model by specifying properties of the underlying data generative process. The first setting considers bias in the generative model which should be filtered out. The second model is related to the presence of nuisance variables in the observations producing an unwanted bias for the learning task. For both models, we devise a learning algorithm based on approximation of Wasserstein distances using adversarial networks. We provide formal arguments describing the fairness enforcing properties of these algorithm in relation with the underlying fairness generative processes. Finally we perform experiments, both on synthetic and real world data, to demonstrate empirically the superiority of our approach compared to state of the art fairness algorithms as well as concurrent GAN type adversarial architectures based on Jensen divergence.", "review": "Review:###This paper proposes a variant of adversarial learning to achieve some of the popular group fairness definitions. The main novelty is the idea of minimizing Wasserstein distance between the conditional distributions of classifier predictions given different values of the protected attribute. My main concern is the approximation of a simple 1d Wasserstein distance with a neural network. Wasserstein distance between two discrete distributions in 1d can be computed in closed form (simple function of order statistics). That is, eq. (1) is simple to evaluate for two empirical distributions. There is no need to use a neural network for approximation, and even if authors choose to do so, some discussion on how well it approximates actual Wasserstein distance is needed. I think the proposed algorithm could be more interesting if authors can work out the optimization problem with the actual Wasserstein distance. On the theoretical/motivation side, it is not enough to say that demographic parity is achieved when the corresponding Wasserstein distance is 0. What is needed is that demographic parity difference is bounded from above by the corresponding Wasserstein distance (I don*t know if it is true or not, but would like to know). Then minimizing Wasserstein distance to achieve demographic parity could be justified. Finally, the paper is quite poorly written. The description of fairness in the introduction is very vague. Authors essentially describe demographic parity as fairness, while it is simply one of the several definitions of group fairness. There is also individual fairness (the paper by Dwork et al. is cited, but not properly discussed) and prior work emphasizing certain deficiencies of group fairness [1] along with several recent papers studying individual fairness [2,3], some also utilizing Wasserstein distance [4]. Authors also provided incorrect definition of disparate impact. Equation in the bottom of page 2 corresponds to statistical parity difference, while disparate impact is the ratio. *Equality of opportunity* on the top of page 3 seems to be a typo *the mathematical properties of the disparate impact measure are not favorable, in particular it lacks robustness and smoothness features which would be necessary to blend algorithmic practice and mathematical theory* - I don*t think this claim makes sense. There are many prior works studying disparate impact and proposing algorithms to achieve it, e.g. the cited work of Feldman et al. Authors should be more specific regarding what mathematical properties they consider not favorable. There are a lot of typos and grammatical mistakes, e.g. in the 1st paragraph of section 2.2, the sentence *Hence the aim in this case is to” is unfinished. in the 1st paragraph of section 3, the first sentence seems to be unfinished. [1] Kleinberg, J., Mullainathan, S., & Raghavan, M. (2016). Inherent trade-offs in the fair determination of risk scores. [2] Kearns, M., Roth, A., & Sharifi-Malvajerdi, S. (2019). Average Individual Fairness: Algorithms, Generalization and Experiments. [3] Jung, C., Kearns, M., Neel, S., Roth, A., Stapleton, L., & Wu, Z. S. (2019). Eliciting and Enforcing Subjective Individual Fairness. [4] Yurochkin, M., Bower, A., & Sun, Y. (2019). Learning fair predictors with Sensitive Subspace Robustness."}
{"id": "iclr2020_408", "title": "Using Objective Bayesian Methods to Determine the Optimal Degree of Curvature within the Loss Landscape | OpenReview", "abstract": "Abstract:###The efficacy of the width of the basin of attraction surrounding a minimum in parameter space as an indicator for the generalizability of a model parametrization is a point of contention surrounding the training of artificial neural networks, with the dominant view being that wider areas in the landscape reflect better generalizability by the trained model. In this work, however, we aim to show that this is only true for a noiseless system and in general the trend of the model towards wide areas in the landscape reflect the propensity of the model to overfit the training data. Utilizing the objective Bayesian (Jeffreys) prior we instead propose a different determinant of the optimal width within the parameter landscape determined solely by the curvature of the landscape. In doing so we utilize the decomposition of the landscape into the dimensions of principal curvature and find the first principal curvature dimension of the parameter space to be independent of noise within the training data.", "review": "Review:###The paper conjectures that the so called Jeffreys prior over the parameters of a neural network is the prior leading to the best generalization performance. The authors test this conjecture on an artificial task using a small neural network, and investigate the sensitivity of the results to noise. I like the general idea of the paper and appreciate the very detailed exposition placing it in the context of other works. In particular, I enjoyed the summary showing the sometimes conflicting evidence for better generalization in either broader or sharper minima, and how it relates to the Jeffreys prior. However, as I understood the paper, the main claim in page 5 Equation 4 “Thus we conjecture that a correct prior for a model would be:” is an *assertion* that Jeffreys prior is the correct prior to use over the parameter space of neural networks. While it is a possibility, the amount of empirical evidence presented does not (at least to me) provide strong enough justification. On page 7, you say “This model was a neural network composed of one, 5 neuron, hidden layer which utilized a sigmoid activation function in its hidden layer and a linear activation in its scalar output layer.“, describing your experiment. I don*t think this experiment is sufficiently large to convince me. Furthermore, in Figure 1 values outside the optical cluster at 0.0 appear nonetheless. I am not sure how to judge the amount of spread I see, and what effect they have on the performance of the network. In general I would like to see experiments on datasets and with architectures that are at least somewhat close to what people use in practice (at least in terms of the size of the task and the capacity of the net). That would give me more confidence that your conjecture is true. While I appreciate your detailed theoretical exposition, I think the amount of empirical evidence you provide is insufficient to back the claims. Considering the explicit instruction to judge papers exceeding 8 pages with a higher standard, I believe that the lack of a greater amount of empirical evidence is a significant deficiency of your otherwise very interesting work. I encourage you to expand this paper and resubmit to another venue -- I believe it has a great potential."}
{"id": "iclr2020_409", "title": "Learning Temporal Abstraction with Information-theoretic Constraints for Hierarchical Reinforcement Learning | OpenReview", "abstract": "Abstract:###Applying reinforcement learning (RL) to real-world problems will require reasoning about action-reward correlation over long time horizons. Hierarchical reinforcement learning (HRL) methods handle this by dividing the task into hierarchies, often with hand-tuned network structure or pre-defined subgoals. We propose a novel HRL framework TAIC, which learns the temporal abstraction from past experience or expert demonstrations without task-specific knowledge. We formulate the temporal abstraction problem as learning latent representations of action sequences and present a novel approach of regularizing the latent space by adding information-theoretic constraints. Specifically, we maximize the mutual information between the latent variables and the state changes. A visualization of the latent space demonstrates that our algorithm learns an effective abstraction of the long action sequences. The learned abstraction allows us to learn new tasks on higher level more efficiently. We convey a significant speedup in convergence over benchmark learning problems. These results demonstrate that learning temporal abstractions is an effective technique in increasing the convergence rate and sample efficiency of RL algorithms.", "review": " Summary: This paper studies the hierarchical reinforcement learning (HRL) problem. It proposes a framework TAIC that learns temporal abstraction from past experience or expert demons without task-specific knowledge. The method is to formulate the problem by a temporal abstraction problem. That is, they assume that the action sequence is generated by a latent variable o. By regularizing the latent space by adding information-theoretic constraints, they are able to learn the representation. The paper later uses visualization to demonstrate the effectiveness of the learning. I would think this paper is slightly below the borderline. It is an interesting method of encoding the option sequence by a continuous variable. Therefore, the action space becomes continuous rather than discrete. However, I found it not convincing why continuous option space is better than discrete ones. It appears to me that the experiment section does not provide a comparison with previous discrete option based methods as well. Comments: * 4th line of related work: Parr --> *Parr & Russel* * Page 2, problem formulation: in beta(s,o), s is not defined. Maybe you can denote it as beta_o(.). * It appears to be that the*option* is a sequence of actions? This can only happen in the deterministic environment. What will you do if applying pi does not give the same sequence of actions? For instance, from (s1,a1) -> (s2, a2), where s2 is generated from a random distribution, and a2 is based on s2. * the paper is overlength"}
{"id": "iclr2020_410", "title": "ADAPTING PRETRAINED LANGUAGE MODELS FOR LONG DOCUMENT CLASSIFICATION | OpenReview", "abstract": "Abstract:###Pretrained language models (LMs) have shown excellent results in achieving human like performance on many language tasks. However, the most powerful LMs have one significant drawback: a fixed-sized input. With this constraint, these LMs are unable to utilize the full input of long documents. In this paper, we introduce a new framework to handle documents of arbitrary lengths. We investigate the addition of a recurrent mechanism to extend the input size and utilizing attention to identify the most discriminating segment of the input. We perform extensive validating experiments on patent and Arxiv datasets, both of which have long text. We demonstrate our method significantly outperforms state-of-the-art results reported in recent literature.", "review": " The paper presents a methods to combine multiple pre-trained language models using an attention mechanism in order to take the whole document into account. The effectiveness of the methods is evaluated on two long document classification tasks (Arxiv publication and patent classification) with state-of the art results. Pros: - the effectiveness of the methods is experimentally demonstrated using two relevant datasets - the inverted wireless experiment clearly shows the interest of the attention combination strategy Cons: - the methods is very simple (combining multiple segment prediction to perform document classifications) making the contribution of this paper quite weak. In the own terms of the authors, the *main contribution [...] is to investigate the effectiveness of different combination strategies*. I am not sure that it is sufficient for the ICLR standards. - the paper is difficult to read and should be proofread to improve readability Minor issues: - Socher et al (2011) uses recursive neural networks (ReNN) and not recurrent neural networks (RNN). RNN are ReNN but restrained to a linear chain structure - In Introduction: ... in the domain extremely complex data that is language ... -> I*m not sure the sentence is correct - In Introduction: the last sentence should be shorten and rephrased. - many typos"}
{"id": "iclr2020_411", "title": "Generalized Inner Loop Meta-Learning | OpenReview", "abstract": "Abstract:###Many (but not all) approaches self-qualifying as *meta-learning* in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, unnamedlib, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.", "review": "Review:###This work presented a general formulation of a wide class of existing meta-learning approaches, and proved the requirements that must be satisfied for such approaches to be possible. Half of the work is focused on describing the unnamedlib library, which extends PyTorch to enable the easy and natural implementation of such meta-learning approaches. The early sections are interesting, especially section 2, which gives some great insights to the existing inner loop pattern in meta-learning. However, from section 3, the paper has turned to examples and related works, where I was hoping the author would give more detailed analysis of the pattern. My concern is the authors have spent too much space on the unnamedlib library. So http://www.jmlr.org/mloss/ might be a more suitable place for publication."}
{"id": "iclr2020_412", "title": "Representation Quality Explain Adversarial Attacks | OpenReview", "abstract": "Abstract:###Neural networks have been shown vulnerable to adversarial samples. Slightly perturbed input images are able to change the classification of accurate models, showing that the representation learned is not as good as previously thought. To aid the development of better neural networks, it would be important to evaluate to what extent are current neural networks* representations capturing the existing features. Here we propose a way to evaluate the representation quality of neural networks using a novel type of zero-shot test, entitled Raw Zero-Shot. The main idea lies in the fact that some features are present on unknown classes and that unknown classes can be defined as a combination of previous learned features without representation bias (a bias towards representation that maps only current set of input-outputs and their boundary). To evaluate the soft-labels of unknown classes, two metrics are proposed. One is based on clustering validation techniques (Davies-Bouldin Index) and the other is based on soft-label distance of a given correct soft-label. Experiments show that such metrics are in accordance with the robustness to adversarial attacks and might serve as a guidance to build better models as well as be used in loss functions to create new types of neural networks. Interestingly, the results suggests that dynamic routing networks such as CapsNet have better representation while current deeper DNNs are trading off representation quality for accuracy.", "review": "Review:###Summary: This paper aims at revealing the relationship between the quality of deep representations and the attack susceptibility of deep classification models. To this end, they propose the zero-shot test to investigate the *quality* of learned representations for unknown classes. Specifically, they leverage two kinds of quality metrics on data of unknown classes. The first one is based on clustering named Davies-Bouldin Index which measures the compactness of intra-cluster. The second one is based on the difference of soft-label histogram distributions with/without unknown classes during training, which may describe the generalization for unknown classes or bias towards known classes of learned features. Finally, with these two metrics, they rank the quality of different models and compare such ranking results with the attack robustness obtained by different attack techniques on CIFAR-10 dataset. +Strengths: 1. This paper shows satisfactory related works about recent advances in adversarial attacks and defenses. -Weaknesses: 1. The writing of this paper contains many mistakes, especially for the issue of using singular and plural. I strongly recommend the authors to polish the paper carefully. Take the first two pages as an example, “results suggests” in Abstract, “Networks’s” in Introduction, “a DNNs”, “perturbations causes” in Sec1.1, “methods… which uses” in Sec.1.2, etc. 2. The idea of representations quality on unknown classes determining attack robustness is not reasonable. Since the adversarial attack is defined on the known classes, the reasons for that are in two aspects. The first is the training dataset. If the collected training data cannot represent/fulfil the whole continuous distribution/manifold of the categories or even biased, the models are of course easily fooled by unseen modes during test. The second is the mapping for classification is from high to low, which is naturally many to one. In a word, I think the quality of models on unknown classes is not directly related to the problem of adversarial attack. 3. This paper only conducts experiments on CIFAR-10 dataset, which is not convincing enough. It would be better to evaluate their method on more challenging benchmarks and also give more validation about their idea. E.g. on ImageNet, if enough number of categories has been seen for models, whether they would become more robust to adversarial attack. Evaluating features of more layers rather than the softmax outputs is also needed. 4. What is amalgam proportion? Please explain it in detail or give a reference paper. Otherwise the readers cannot understand the motivation of the second metric in Sec3.2. Besides, Fig.2 contains little information and few captions for readers to understand their method. 5. In Tab.1 and 2, the meanings of Attack Accuracy and Average Amount of Perturbations (typo in caption for Tab.2) are not introduced. Tab.5 shows 7 methods but its caption says “five”. Fig.4 is also confused, for AM, which histogram is with unknown classes and which one is without? 6. The title says *representation quality explain adversarial attacks*. After reading this paper, I haven’t found the mechanism leading to the adversarial attacks of DNNs."}
{"id": "iclr2020_413", "title": "Learning Calibratable Policies using Programmatic Style-Consistency | OpenReview", "abstract": "Abstract:###We study the important and challenging problem of controllable generation of long-term sequential behaviors. Solutions to this problem would impact many applications, such as calibrating behaviors of AI agents in games or predicting player trajectories in sports. In contrast to the well-studied areas of controllable generation of images, text, and speech, there are significant challenges that are unique to or exacerbated by generating long-term behaviors: how should we specify the factors of variation to control, and how can we ensure that the generated temporal behavior faithfully demonstrates diverse styles? In this paper, we leverage large amounts of raw behavioral data to learn policies that can be calibrated to generate a diverse range of behavior styles (e.g., aggressive versus passive play in sports). Inspired by recent work on leveraging programmatic labeling functions, we present a novel framework that combines imitation learning with data programming to learn style-calibratable policies. Our primary technical contribution is a formal notion of style-consistency as a learning objective, and its integration with conventional imitation learning approaches. We evaluate our framework using demonstrations from professional basketball players and agents in the MuJoCo physics environment, and show that our learned policies can be accurately calibrated to generate interesting behavior styles in both domains.", "review": "Review:###The paper proposes a weak supervision method to obtain labels from functions that are easily programmable, and propose to use this for learning policies that can be *calibrated* for specific style. The paper demonstrates some experiments on a basketball environment and a halfcheetah environment, showing that the agent will perform according to corresponding styles. My main concern here is the technical novelty of the proposed method: it seems that once we have the labels (which are limited to programmable functions), all we need to do is to learn a policy that conditions on the labels. In this case, we are not concerned with the latent variables whatsoever, therefore it seems that the CTVAE baselines are overkill for the task (learning latent variables that are not actually needed). Maybe more interesting baselines is to see how the two terms in (8) affect self-consistency performance, and not consider any methods that use unsupervised latent variables? Minor questions: - The method*s name, CTVAE-style is a bit confusing, since the policy does not depend on any latent variable z? At least from how the policy is described pi(cdot |y) does not depend on unsupervised latent variables z. - Table 4, KL and NLL results do not seem to match? I wonder if the basketball kl should be multiplied by 10 and the cheetah ctval-style NLL is a typo? - Is it possible to extend this to continuous labels? This seems technically viable but unclear empirically."}
{"id": "iclr2020_414", "title": "Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View | OpenReview", "abstract": "Abstract:###The Transformer architecture is widely used in natural language processing. Despite its success, the design principle of the Transformer remains elusive. In this paper, we provide a novel perspective towards understanding the architecture: we show that the Transformer can be mathematically interpreted as a numerical Ordinary Differential Equation (ODE) solver for a convection-diffusion equation in a multi-particle dynamic system. In particular, how words in a sentence are abstracted into contexts by passing through the layers of the Transformer can be interpreted as approximating multiple particles* movement in the space using the Lie-Trotter splitting scheme and the Euler*s method. Given this ODE*s perspective, the rich literature of numerical analysis can be brought to guide us in designing effective structures beyond the Transformer. As an example, we propose to replace the Lie-Trotter splitting scheme by the Strang-Marchuk splitting scheme, a scheme that is more commonly used and with much lower local truncation errors. The Strang-Marchuk splitting scheme suggests that the self-attention and position-wise feed-forward network (FFN) sub-layers should not be treated equally. Instead, in each layer, two position-wise FFN sub-layers should be used, and the self-attention sub-layer is placed in between. This leads to a brand new architecture. Such an FFN-attention-FFN layer is *Macaron-like*, and thus we call the network with this new architecture the Macaron Net. Through extensive experiments, we show that the Macaron Net is superior to the Transformer on both supervised and unsupervised learning tasks. The reproducible code can be found on http://anonymized", "review": " Contributions: This paper builds an ad-hoc connection between the Transformer and the numerical ODE solver (the Lie-Trotter splitting scheme and the Euler*s method) for a convection-diffusion equation in a multi-particle dynamic system. Then, the author(s) developed an ad-hoc Strang-Marchuk splitting style architecture, named Macaron Net. Finally, this paper provides some experiments to verify the performance of the proposed architecture. However, the comparisons with the benchmark results are questionable. I have listed my concerns in the Experiment section. Motivation: This paper developed the Macaron Net based on a locally third-order operator splitting scheme for the convection-diffusion equation. However, there is no theoretical interpretation of why third-order splitting corresponding to better architecture. Theorem 1 in the paper is a known result, and it is irrelevant to the paper, I highly recommend the author to remove it from the main text. I also suggest the author explore more operator splitting schemes and do a systematic comparison between them. Moreover, I think it will be a real contribution if the author can analyze the error between the numerical scheme and architectures. Reformulate Transformer Layers ans an ODE solver for Multi-Particle Dynamic System: There is a big gap between Eqns 3, 4 and 5. Why F represents a diffusion term, why G represents a convection term? From a statistical mechanics point of view, this comparison does not make sense. I do not buy this model. Related Work: There is no related-work section that discusses the related work, and all the referenced papers are generic. For instance, the efforts in developing language models, the application of convection-diffusion equation, and String-Marchuck and other operator splitting schemes in machine learning. The author should better position the paper to exist work. Experiments: This section is extremely questionable. My initial thought after reading the reported results is that the architecture proposed in this paper easily outperforms the existing work. However, after I do a cross-check with the existing work, I found the author did not compare with the best results reported in the benchmark work and hide much information. After simply checking two existing papers, I found that the author ignored the comparison with BERT large. Also, the author ignored the most important result reported by Wu et al. 2019b. To be fair, the author should perform an apple-to-apple comparison with the existing work and report the uncertainties in their results. Moreover, the author should report the parameters used in all their experiments. I think this heuristic study might be a contribution to ICLR if all my concerns are addressed, and I am willing to raise my rating to accept."}
{"id": "iclr2020_415", "title": "Exploring Model-based Planning with Policy Networks | OpenReview", "abstract": "Abstract:###Model-based reinforcement learning (MBRL) with model-predictive control or online planning has shown great potential for locomotion control tasks in both sample efficiency and asymptotic performance. Despite the successes, the existing planning methods search from candidate sequences randomly generated in the action space, which is inefficient in complex high-dimensional environments. In this paper, we propose a novel MBRL algorithm, model-based policy planning (POPLIN), that combines policy networks with online planning. More specifically, we formulate action planning at each time-step as an optimization problem using neural networks. We experiment with both optimization w.r.t. the action sequences initialized from the policy network, and also online optimization directly w.r.t. the parameters of the policy network. We show that POPLIN obtains state-of-the-art performance in the MuJoCo benchmarking environments, being about 3x more sample efficient than the state-of-the-art algorithms, such as PETS, TD3 and SAC. To explain the effectiveness of our algorithm, we show that the optimization surface in parameter space is smoother than in action space. Further more, we found the distilled policy network can be effectively applied without the expansive model predictive control during test time for some environments such as Cheetah. Code is released.", "review": "Review:###Summary This work provides a novel model-based reinforcement learning algorithm for continuous domains (Mujoco) dubbed POPLIN. The presented algorithm is similar in vein to the state-of-the-art PETS algorithm, a planning algorithm that uses state-unconditioned action proposal distributions to identify good action sequences with CEM in the planning routine. The important difference compared to PETS is the incorporation of a parametric state-conditioned policy (trained on real data) in the planning routine to obtain better action-sequences (CEM is used to learn the *offset* from the parametric policy). The paper presents two different algorithmic ablations where CEM either operates in action space or parameter space (POPLIN-A and POPLIN-P respectively), in combination with different objectives to learn the parametric policy. The method is evaluated on 12 continuous benchmarks and compared against state-of-the-art model-based and model-free algorithms, indicating dominance of the newly proposed method. Quality The quality of the paper is high. This is an experimental study and the number of benchmarks and baselines is far above average compared to other papers in that field. One minor point is that averaging experiments over 4 seeds only is usually not optimal in these environments, but in light of the sheer amount of baselines and benchmarks excusable. While the experimental results are impressive, the authors mention that asymptotic performance in Walker2d and Humanoid might not match the asymptotic performance of model-free baselines. This could be stated more clearly. Also, there are no Humanoid experiments in the paper despite mentioned in the text (2nd paragraph in Section 5.1)? Clarity The clarity of the paper can be in parts improved upon. For example, how does the *policy control* ablation (mentioned in Section 4.3) work precisely, i.e. the interplay between executing the parametric policy in the real world and harnessing the environment model? I assume the policy distillation techniques in Section 4.4 are different alternatives for the second-to-last lines in the pseudocodes from the appendix? Which one is the default used in Section 5.1? On a minor note, above Equation (7), a target network is mentioned---where does the target network occur in Equation (7)? There are some plots that do not mention the name of the environment, e.g. in Figure (4), but also some in the appendix. Furthermore, it could be stated more clearly that the reward function is assumed to be known. If the authors improve the clarity of their paper significantly, I am willing to increase my score further (presupposing that no severe issues arise in the discussion phase). Originality Adding a parametric policy to PETS is not the most original idea, but clearly a gap in the current literature. Significance The experiments and the empirical results make the paper quite significant. Update The authors improved the clarity of the paper. I therefore increase to 8. Section 4.4 paragraph *Setting parameter average (AVG)* can still be improved---does this go together with POPLIN-P-Uni from Section 4.2?"}
{"id": "iclr2020_416", "title": "Quantifying Point-Prediction Uncertainty in Neural Networks via Residual Estimation with an I/O Kernel | OpenReview", "abstract": "Abstract:###Neural Networks (NNs) have been extensively used for a wide spectrum of real-world regression tasks, where the goal is to predict a numerical outcome such as revenue, effectiveness, or a quantitative result. In many such tasks, the point prediction is not enough: the uncertainty (i.e. risk or confidence) of that prediction must also be estimated. Standard NNs, which are most often used in such tasks, do not provide uncertainty information. Existing approaches address this issue by combining Bayesian models with NNs, but these models are hard to implement, more expensive to train, and usually do not predict as accurately as standard NNs. In this paper, a new framework (RIO) is developed that makes it possible to estimate uncertainty in any pretrained standard NN. The behavior of the NN is captured by modeling its prediction residuals with a Gaussian Process, whose kernel includes both the NN*s input and its output. The framework is justified theoretically and evaluated in twelve real-world datasets, where it is found to (1) provide reliable estimates of uncertainty, (2) reduce the error of the point predictions, and (3) scale well to large datasets. Given that RIO can be applied to any standard NN without modifications to model architecture or training pipeline, it provides an important ingredient for building real-world NN applications.", "review": "Review:#### Summary The authors propose a method for post-hoc correction and predictive variance estimation for neural network models. The method fits a GP to the model residuals, and learns a composite kernel that combines two kernels defined on the input space and the model’s output space (called RIO, R for residual, and IO for the input-output kernel). The authors suggest that residual means and variances at test points can then be calculated explicitly using predictive distributions from the GP. The authors run a large panel of experiments across a number of datasets, and compare to a number of methods that draw connections between neural networks and GP’s. In addition, the full method is compared to a number of methods that utilize only some components of the full RIO method. In these experiments, the RIO method generally shows strong performance in both RMSE and NLPD compared to these baselines. # Feedback Overall, this is a neat method. It has the flavor of a number of other composite ML methods that have worked well in the past---e.g., boosting and platt scaling---but is different enough to stand on its own. The experimental results are quite promising. However, I am torn about the paper, because the theoretical discussion of the method is quite convoluted and seems either irrelevant or incorrect. I wish that the authors had spent more time with small demonstrations of what the procedure does in some simple settings. This would give practitioners considering the method far more intuition about when they would expect it to work and fail than the current theoretical discussion. ## Uncertainty Discussion is Lacking The motivation and discussion sell this method as an uncertainty quantification method, but almost all of the theoretical development revolves around prediction correction. The methods properties as an uncertainty quantification tool are underdeveloped. The only theoretical point made about uncertainty estimation is Theorem 2.7, which states that the scalar variance of the GP “nugget” is positively correlated with the variance of the NN’s residuals. Providing a scalar summary of noise is not particularly compelling for a method advertised as a point-prediction uncertainty quantification method. In addition, it is not clear what probability distribution the “correlation” is defined over. The argument made in the proof seems quite obvious: if a GP is used to model a noisier process (i.e., residuals with a larger variance), it will in some cases classify that variability as independent noise. If the authors wanted to focus on the properties of their method as an uncertainty quantification tool, they could discuss the assumptions underlying the GP error estimates, and when they would be likely to diverge from practical properties like predictive interval coverage. For example, because the base NN predictor is treated as fixed, it seems that this method ignore uncertainty that stems from the NN fit due to random initialization. Likewise, it seems that this method would not quantify uncertainty from resampling the data and obtaining a new NN predictor. )The coverage experiments in the appendix seem to confirm this -- generally, the predictive intervals generally under-cover the predicted values.) It’s fine if the method doesn’t quantify these types of uncertainty, but discussion of these types of issues would be far more welcome than the current convoluted theory in Section 2. This discussion might not yield theorems, but it would give practitioners useful guidelines for deciding whether the particular scheme would likely work for this application. ## Problems with the Error Correction Argument The theory section, especially 2.2, was very difficult to parse. First, as a matter of style, a sequence of Lemma and Theorem statements are given without defining most of the notation used therein, and with almost no prose providing context or intuition. In the buildup to the theorems, it is also unclear which assertions about the decompositions of y_i are assumptions about the true data generating process, and which assertions are specifications of a particular GP model. The substance also has some issues. I think the intention in this section is to get to a rather simple variance decomposition of the labels y. The question is how much variation in y or the residual is represented in the posterior predictive mean of a particular GP. It seems reasonable that in some cases, the structure in the residual may be more amenable to modeling with a stationary GP than the structure in the raw labels y. It is not clear that all of the theoretical complexity here is necessary to make this point. Instead, the authors make a convoluted argument that attempts to establish that the errors from the NN + GP approach will be smaller under very general circumstances. The argument is phrased somewhat ambiguously (it is not clear exactly what is being assumed, and what is corresponds to the specification of a working model), but depending on how one reads this section, the argument makes statements that are either too broad to be correct, or too narrow to be relevant. The argument decomposes for the raw labels and the residuals into pieces that a GP can “capture” or “represent”, and parts that it cannot. The two equations are: y_i = f(x_i) + g(x_i) + xi_i R_i = (y_i - h_NN(x_i)) = r_f(x_i) + r_g(x_i) + xi_i f(.) and r_f(.) represent the portions of the label and residual processes, respectively, that the GP *captures*. It is assumed that the GP will model this portion correctly, and leave the “epsilon-indistinguishable” portion g(.) or r_g(.) untouched. The argument then assumes that f(.) and r_f(.) will have proportional kernels, and so it is possible to show that the predictions of residuals based on r_f(.) will have smaller predictive variance than predictions based on f(.) as long as the variation represented by r_g(.) is smaller than the variation represented by g(.). On its face, this argument raises some red flags. Because h_NN(.) is allowed to be an arbitrary function, the argument here should be symmetric. Why can’t we also get a guaranteed variance reduction by adding h_NN(.) to y rather than subtracting it? Perhaps some of this is captured in the parameter delta, which quantifies the reduction in variation represented in r_g(.) vs g(.), but the argument that the kernel of r_f(.) can be no larger than the kernel f(.) in terms of trace (that is, the proportionality constant alpha is not greater than 1) does not make sense. If h_NN(x_i) is simply -f(x_i), then these arguments would not go through. At the very least, conditions need to be articulated about the properties of h_NN(.). Some of the strangeness comes from the fact that this is a poor model of most prediction problems, where the main issue with fitting a GP is not “indistinguishability”, but misspecification. Consider a process y_i that is non-stationary; say g(.) has a linear trend in some component of x. A GP with a stationary covariance kernel fit to this process (such as RBF) will attempt to explain the variation due to the linear trend with a variance kernel that encodes long-range dependence. On the other hand, if this trend were removed by a base model like an NN, the residuals would have a very different structure (perhaps they would be stationary), and in this case, the GP would fit the data with a very different covariance kernel. Unfortunately, it does not seem like the formalism here can express a notion of misspecification at all. In the theory, it is assumed that the GP will only model the portion of the labels y_i for which it is property specified (in this case, f(.)). This generally does not occur in practice, as in the example above. It might be possible for this to apply in some circumstances, but the authors give no conditions (e.g., that the process y_i be stationary). Based on this assumption, the authors assert that the fitted GP to f(.) and r_f(.) will have the same covariance kernel parameters up to some proportionality constant alpha. Much of their theoretical argument depends on this proportionality. But this proportionality cannot apply in general, and again, no conditions are given for when we might expect this to hold. It would be far more compelling if the authors proposed the very standard approach to modeling data via covariance kernels, where one first models non-stationary portions of the data with a base model, then models the correlation in the residuals with something like a GP. This is the bread-and-butter approach in, say, timeseries analysis (see, e.g., the Shumway and Stoffer textbook https://www.stat.pitt.edu/stoffer/tsa4/tsa4.htm), and the approach in this paper could be framed similarly. ## Demos I Wish I Had Seen I wish the authors had presented some demonstrations of what the GP does to the fitted values of an NN. Giving a demonstration of how the output kernel modifies predicted values, for example, would give some nice intuition the value added by this portion. I suspect that this step essentially performs something like Platt scaling, but for continuous outcomes, by shrinking predictions together so that they better match the overall distribution of observed labels. Perhaps the mechanism is different. At any rate, it would be useful to understand where the information gain is coming from, and this would be far better expressed concretely in terms of a toy data example than the theoretical arguments that are given. ## Coverage Experiments I wish the coverage experiments evaluating predictive intervals were included in the main text. As far as uncertainty quantification evaluations go, coverage is one of the few assessments that does not rely on the model itself (unlike NPLD, which uses the model’s own log-likelihood), and can be phrased as a concrete performance guarantee. Here, the goal for predictive intervals is to cover the true prediction value _at least as often_ as the nominal rate (95% intervals should cover the truth _at least_ 95% of the time), not merely that coverage be “close” to the nominal rate. This asymmetric evaluation gives you a concrete guarantee that the uncertainty estimate is conservative. The coverage experiments show that this method quite systematically under-covers compared to the end-to-end SVGP method, which generally satisfies this coverage property. I think this is important information to include about the model, and generally I think this behavior results from the fact that uncertainty is not propagated from the NN fit. This should be presented clearly in the main text."}
{"id": "iclr2020_417", "title": "Scheduled Intrinsic Drive: A Hierarchical Take on Intrinsically Motivated Exploration | OpenReview", "abstract": "Abstract:###Exploration in sparse reward reinforcement learning remains an open challenge. Many state-of-the-art methods use intrinsic motivation to complement the sparse extrinsic reward signal, giving the agent more opportunities to receive feedback during exploration. Commonly these signals are added as bonus rewards, which results in a mixture policy that neither conducts exploration nor task fulfillment resolutely. In this paper, we instead learn separate intrinsic and extrinsic task policies and schedule between these different drives to accelerate exploration and stabilize learning. Moreover, we introduce a new type of intrinsic reward denoted as successor feature control (SFC), which is general and not task-specific. It takes into account statistics over complete trajectories and thus differs from previous methods that only use local information to evaluate intrinsic motivation. We evaluate our proposed scheduled intrinsic drive (SID) agent using three different environments with pure visual inputs: VizDoom, DeepMind Lab and DeepMind Control Suite. The results show a substantially improved exploration efficiency with SFC and the hierarchical usage of the intrinsic drives. A video of our experimental results can be found at https://gofile.io/?c=HpEwTd.", "review": "Review:###Summary: This paper proposes the use of a controller that selects whether to act according to a policy trained to maximize an intrinsic reward or a different policy trained to maximize the extrinsic reward of a task. The two policies are trained jointly and off-policy. However, the controller is not trained for their experiments and instead randomly (with equal probability) picks one of the two policies every N steps (with N fixed). They also introduce a new kind of . intrinsic reward based on successor features, that is supposed to capture trajectory statistics for a fixed policy. They name their method scheduled intrinsic drive (SID). Main Comments: While this paper proposes some interesting ideas, I am concerned about the soundness of the method, some of the precise implementation details and I believe the empirical evaluation could be greatly improved. One of my main concerns is the soundness of using the SFC as intrinsic reward while training off-policy. SFs are defined for a fixed policy so they capture statistics of future states if that policy is being used for control. Can you provide more explanation for why the SFC should still be a useful signal for the agent in the case in which it will follow a very different policy (which seems likely given that the replay buffer not only contains a mix of the exploration and exploitation policies, but also policies at different points during training with potentially very different state visitation distributions). Are the SFs trained with data from both the exploration and the exploitation policy? How can we expect the SFC to have useful signal since it is trained using such a wide range of policies? Is there any guarantee that the arbitrary feature embeddings (which are not learned in your experiments if I understood correctly) and thus the successor features (SFs) will contain meaningful information about the kinds of states a policy will visit in the future? An ablation using hand-designed feature embeddings that contain relevant information about the state (i.e. in a gridworld) might be useful to understand how it compares to a randomly initialized network, which is what you used for the state embeddings as I understand it. I am also concerned by the novelty of this work and the fact that it is missing references and discussion to prior work that proposes very similar ideas. For example, [1] proposed the optimization of different losses at the same time: one for exploitation and one or more for exploration. Can you please discuss what is the difference between your method and theirs (other than the intrinsic reward used for the exploration policy)? Similarly, [2] attempts to decouple exploration and exploitation in RL. This reference (and perhaps others that I have missed) should be included and discussed in the paper. The empirical validation is missing important statistics such as variance across runs. Experiments on AppleDistractions and Cartpole only have 3 random seeds which I do not think is enough for drawing conclusions confidently. Moreover, on the simpler and standard tasks, SID does not seem to be significantly better than other baselines. It is only on carefully designed tasks (e.g. FlytrapEscape or AppleDistractions) that are not regularly used as benchmarks that the method seems to perform better. The experiments section could be improved by including other (more powerful) baselines such as count/pseudocount exploration methods which have been shown to be more effective than ICM / RND for certain benchmarks, the paper using an intrinsic reward based on successor representations [3] or even Go-Explore [4] that is specifically designed to deal with distractor objects for the AppleDistractions task. Additionally, evaluating SID on harder exploration tasks that are generally considered to be good benchmarks by the community would be helpful (e.g. Montezuma Revenge, Pitfall, sparser versions of DoomMyWayHome etc.) would also strengthen the experimental section. Other Questions / Comments: 1. There is no measure of the variance / standard deviation across the random seeds in any of the plots. I find it necessary to be included in the plots, along with the mean across runs. 2. What is the reasoning behind using the number of updates (instead of e.g. number of frames / steps / episodes) in the plots? How exactly do you measure the number of updates that appears in the plots? Is that the total number of updates used for the control policy, the exploration policy, and the successor features or is it only the number of updates used for the control policy? 3. I find the use of the term *hierarchical* in the title and throughout the paper to be misleading since this term is usually used with a different meaning in the RL literature (i.e. to refer to options/subpolicies that a higher-level policy might choose to pursue at a given time). In your case, the control policy is one of the subpolicies and the other subpolicy is only used for exploration. 4. The paper also contains claims which I find unsubstantiated by the results / analytical formulation such as: * our proposed SFC reward implicitly captures statistics over the full distribution of policies that have been followed, since the successor features are learned using states sampled from all past experiences* on page 2 or *Another valuable property of SFC is that it adapts in very meaningful ways that lead to efficient non-stationary exploration policies, when the transitions gathered by a policy maximizing the SFC reward is used to update the SF itself* on page 5. Please provide more intuition or theoretical / empirical evidence to support such claims. 5. What do you use for the fixed interval (N) at which the meta-controller is choosing which policy to follow? Have you tried training the meta-controller? It would be interesting to see how the results change as N varies. Is N = 1 better than N = length of episode or the other way around or does the choice of N not matter that much? References: [1] Beyer, Lucas, et al. *MULEX: Disentangling Exploitation from Exploration in Deep RL.* arXiv preprint arXiv:1907.00868 (2019). [2] Cédric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer. GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms. In Proceedings of the International Conference on Machine Learning (ICML), 2018. [3] Marlos C Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, and Murray Campbell. Eigenoption discovery through the deep successor representation. arXiv preprint arXiv:1710.11089, 2017. [4] Ecoffet, Adrien, et al. *Go-explore: a new approach for hard-exploration problems.* arXiv preprint arXiv:1901.10995 (2019)."}
{"id": "iclr2020_418", "title": "FreeLB: Enhanced Adversarial Training for Language Understanding | OpenReview", "abstract": "Abstract:###Adversarial training, which minimizes the maximal risk for label-preserving input perturbations, has proved to be effective for improving the generalization of language models. In this work, we propose a novel adversarial training algorithm - FreeLB, that promotes higher robustness and invariance in the embedding space, by adding adversarial perturbations to word embeddings and minimizing the resultant adversarial risk inside different regions around input samples. To validate the effectiveness of the proposed approach, we apply it to Transformer-based models for natural language understanding and commonsense reasoning tasks. Experiments on the GLUE benchmark show that when applied only to the finetuning stage, it is able to improve the overall test scores of BERT-based model from 78.3 to 79.4, and RoBERTa-large model from 88.5 to 88.8. In addition, the proposed approach achieves state-of-the-art test accuracies of 85.39\\% and 67.32\\% on ARC-Easy and ARC-Challenge. Experiments on CommonsenseQA benchmark further demonstrate that FreeLB can be generalized and boost the performance of RoBERTa-large model on other tasks as well.", "review": "Review:###In this paper, the authors present a new adversarial training algorithm and apply it to the fintuning stage large scale language models BERT and RoBERTa. They find that with FreeLB applied to finetuning, both BERT and RoBERTa see small boosts in performance on GLUE, ARC, and CommonsenseQA. The gains they see on GLUE are quite small (0.3 on the GLUE test score for RoBERTa) but the gains are more substantial on ARC and CommonsenseQA. The paper also presents some ablation studies on the use of the same dropout mask across each ascent step of FreeLB, empirically seeing gains by using the same mask. They also present some analysis on robustness in the embedding space, showing that FreeLB leads to greater robustness than other adversarial training methods This paper is clearly presented and the algorithm shows gains over other methods. I would recommend that the authors try testing their method on SuperGLUE because it*s possible they*re hitting ceiling issues with GLUE, suppressing any gains the algorithm may yield. Questions, - In tables 4 and 5, why are only results on RTE, CoLA, and MRPC presented? If this is because there was not noticeable difference on the other GLUE datasets, please mention it in the text. - I realize that this method is meant to increase robustness in the embedding space, but did you do any error analysis on the models? Did they make different types of errors than models fine-tuned the vanilla way? Couple typos, - Section 2.2, line 1: many -> much - Section 4.2, GLUE paragraph: 88 -> 88.8"}
{"id": "iclr2020_419", "title": "Anomaly Detection and Localization in Images using Guided Attention | OpenReview", "abstract": "Abstract:###Anomaly detection and localization is a popular computer vision problem which involves detecting anomalous images and localizing anomalies within them. However, this task is challenging due to small sample size and pixel coverage of the anomaly in real-world scenarios. Previous works have a drawback of using anomalous images to compute a threshold during training to detect and localize anomalies. To tackle these issues, we propose AVAGA - the first end-to-end trainable convolutional adversarial variational autoencoder (CAVAE) framework using guided attention which localizes the anomaly with the help of attention maps. AVAGA detects an image as anomalous from the large pixel-wise difference between the input and reconstructed image. In an unsupervised setting, we propose a guided attention loss, where we encourage AVAGA to focus on all non-anomalous regions in the image without using any anomalous images during training. Furthermore, we also propose a selective gradient backpropagation technique for guided attention, which enhances the performance of anomaly localization while using only 2% anomalous images in a weakly supervised setting. AVAGA outperforms the state-of-the-art (SoTA) methods by 10% and 18% on localization and 8% and 15% on classification accuracy in unsupervised and weakly supervised settings respectively on Mvtec Anomaly Detection (MvAD) dataset and by 11% and 22% on localization and 10% and 19% on classification accuracy in unsupervised and weakly supervised settings respectively on the modified ShanghaiTech Campus (STC) dataset", "review": "Review:###This paper describes an approach to detect and localize anomalies in images. A VAE based architecture is used with some fittings to 1) Sharpen blurry output of VAE (with an adversarial loss) 2) Detect and localize anomalies with what they call the guided attention loss (this is the principal novelty it appears) The guided attention loss is obtained from an externally trained network (Grad-CAM, Selvaraju et al). They consider two cases, one with *normal* (non anomalous) images, which is totally unsupervised, while the other case is weakly supervised with some images with anomalies in them, where the attention loss is designed appropriately to handle the two cases. Comparisons are made using 4 different datasets and metrics (e.g. IoU, accuracy of correctly classified anomalous images), and it is shown that their attention based model outperforms the state of the art. My thoughts: It makes sense, loosely speaking, that an attention based model would perform better than one without, enabling it to focus on regions of interest. The paper generally makes this point clear with results. However, I feel that the prior work is not very well described, and the basic idea that an AE architectures are usable for the anomaly detection task because its performance would be worse on images it has not seen (anomalies) is not immediately obvious. I am ambivalent about accepting this paper until we have a better coverage of related work."}
{"id": "iclr2020_420", "title": "A?MCTS: SEARCH WITH THEORETICAL GUARANTEE USING POLICY AND VALUE FUNCTIONS | OpenReview", "abstract": "Abstract:###Combined with policy and value neural networks, Monte Carlos Tree Search (MCTS) is a critical component of the recent success of AI agents in learning to play board games like Chess and Go (Silver et al., 2017). However, the theoretical foundations of MCTS with policy and value networks remains open. Inspired by MCTS, we propose A?MCTS, a novel search algorithm that uses both the policy and value predictors to guide search and enjoys theoretical guarantees. Specifically, assuming that value and policy networks give reasonably accurate signals of the values of each state and action, the sample complexity (number of calls to the value network) to estimate the value of the current state, as well as the optimal one-step action to take from the current state, can be bounded. We apply our theoretical framework to different models for the noise distribution of the policy and value network as well as the distribution of rewards, and show that for these general models, the sample complexity is polynomial in D, where D is the depth of the search tree. Empirically, our method outperforms MCTS in these models.", "review": "Review:###This paper proposes A*MCTS, which combines A* and MCTS with policy and value networks to prioritize the next state to be explored. It further establishes the sample complexity to determine optimal actions. Experimental results validate the theoretical analysis and demonstrate the effectiveness of A*MCTS over benchmark MCTS algorithms with value and policy networks. Pros: This paper presents the first study of tree search for optimal actions in the presence of pretrained value and policy networks. And it combines A* search with MCTS to improve the performance over the traditional MCTS approaches based on UCT or PUCT tree policies. Experimental results show that the proposed algorithm outperform the MCTS algorithms. Cons: However, there are several issues that should be addressed including the presentation of the paper: • The algorithm seeks to combine A* search with MCTS (combined with policy and value networks), and is shown to outperform the baseline MCTS method. However, it does not clearly explain the key insights of why it could perform better. For example, what kind of additional benefit will it bring when integrating the priority queue into the MCTS algorithms? How could it improve over the traditional tree policy (e.g., UCT) for the selection step in MCTS? These discussions are critical to understand the merit of the proposed algorithms. In addition, more experimental analysis should also be presented to support why such a combination is the key contribution to the performance gain. • Many design choices for the algorithms are not clearly explained. For example, in line 8 of Algorithm 2, why only the top 3 child nodes are added to the queue? • The complexity bound in Theorem 1 is hard to understand. It does not give the explicit relations of the sample complexity with respect to different quantities in the algorithms. In particular, the probability in the second term of Theorem 1 is hard to parse. The authors need to give more discussion and explanation about it. This is also the case for Theorems 2-4. The authors give some concrete examples in Section 6.2 for these bounds. However, it would be better to have some discussion earlier right after these theorems are presented. • The experimental results are carried out under the very simplified settings for both the proposed algorithm and the baseline MCTS. In fact, it is performed under the exact assumption where the theoretical analysis is done for the A*MCTS. This may bring some advantage for the proposed algorithm. It is not clear whether such assumptions hold for practical problems. More convincing experimental comparison should be done under real environment such as Atari games (by using the simulator as the environment model as shown in [Guo et al 2014] “Deep learning for real-time atari game play using offline monte-carlo tree search planning”). Other comments: • It is assumed that the noise of value and policy network is zero at the leaf node. In practice, this is not true because even at the leaf node the value could still be estimated by an inaccurate value network (e.g., AlphaGo or AlphaZero). How would this affect the results? • In fact, the proof of the theorems could be moved to appendices. • In the first paragraph of Section 6.2, there is a typo: V*=V_{l*}=eta should be V*-V_{l*}=eta ?"}
{"id": "iclr2020_421", "title": "Universal Adversarial Attack Using Very Few Test Examples | OpenReview", "abstract": "Abstract:###Adversarial attacks such as Gradient-based attacks, Fast Gradient Sign Method (FGSM) by Goodfellow et al.(2015) and DeepFool by Moosavi-Dezfooli et al. (2016) are input-dependent, small pixel-wise perturbations of images which fool state of the art neural networks into misclassifying images but are unlikely to fool any human. On the other hand a universal adversarial attack is an input-agnostic perturbation. The same perturbation is applied to all inputs and yet the neural network is fooled on a large fraction of the inputs. In this paper, we show that multiple known input-dependent pixel-wise perturbations share a common spectral property. Using this spectral property, we show that the top singular vector of input-dependent adversarial attack directions can be used as a very simple universal adversarial attack on neural networks. We evaluate the error rates and fooling rates of three universal attacks, SVD-Gradient, SVD-DeepFool and SVD-FGSM, on state of the art neural networks. We show that these universal attack vectors can be computed using a small sample of test inputs. We establish our results both theoretically and empirically. On VGG19 and VGG16, the fooling rate of SVD-DeepFool and SVD-Gradient perturbations constructed from observing less than 0.2% of the validation set of ImageNet is as good as the universal attack of Moosavi-Dezfooli et al. (2017a). To prove our theoretical results, we use matrix concentration inequalities and spectral perturbation bounds. For completeness, we also discuss another recent approach to universal adversarial perturbations based on (p, q)-singular vectors, proposed independently by Khrulkov & Oseledets (2018), and point out the simplicity and efficiency of our universal attack as the key difference.", "review": " This paper studied the problem of universal adversarial attack which is an input-agnostic perturbation. The authors proposed to use the top singular vector of input-dependent adversarial attack directions to perform universal adversarial attacks. The authors evaluated the error rates and fooling rates for three attacks on standard benchmark datasets. - The paper is generally well-written and easy to follow. My main concern towards this paper is about the experiments part from several aspects. First, the proposed method needs quite large L2 norm (50 on ImageNet) to work, while common adversarial attack experiments on ImageNet are usually conducted with L2 perturbation strength of 5 or less. I totally understand that performing universal attack would be much more difficult, yet having such loose L2 norm constraint still seems impractical. Second, the authors did not compare with any other baselines such as (Moosavi-Dezfooli et al. 2017a) arguing that their universal attack is different for different perturbation strength and pixels are normalized. I do not think normalized pixel will be a problem as you can simply scale the perturbation strength accordingly. And because (Moosavi-Dezfooli et al. 2017a) uses different attack vectors for different perturbation strength, some comparison between these two types of universal attacks should be presented in order to mark the difference and demonstrate your advantages. I would suggest the authors to compare with several mentioned baselines in the paper to show the superiority of the proposed method. - Theorem 1 seems interesting, yet it needs a special assumption. The authors argue that this is a reasonable assumption in a small neighborhood of x. I wonder if the authors could conduct some demonstrative experiments to verify this? Because the definition of S_x depends on the attack function, does it mean that the assumption need to be held for any attack function? Also regarding the choice of delta, it seems that delta is different for different x? If so, since u is also depend on delta, this attack vector seems not universal? Detailed comments: - In proof of Theorem 1, all S should be G? - In proof of Theorem 2, how to get |v - hat v|_2 leq epsilon/(gamma - epsilon)? Directly applying the Theorems seems to get epsilon / (gamma) only? Depending on whether the authors can address my concerns, I may change the final rating. ====================== after the rebuttal I thank the authors for their response but I still feel that the assumption is not well-justified and there is still a lot to improve in terms of experiments. Therefore I decided to keep my score unchanged."}
{"id": "iclr2020_422", "title": "Vid2Game: Controllable Characters Extracted from Real-World Videos | OpenReview", "abstract": "Abstract:###We extract a controllable model from a video of a person performing a certain activity. The model generates novel image sequences of that person, according to user-defined control signals, typically marking the displacement of the moving body. The generated video can have an arbitrary background, and effectively capture both the dynamics and appearance of the person. The method is based on two networks. The first maps a current pose, and a single-instance control signal to the next pose. The second maps the current pose, the new pose, and a given background, to an output frame. Both networks include multiple novelties that enable high-quality performance. This is demonstrated on multiple characters extracted from various videos of dancers and athletes.", "review": "Review:###The paper presents an approach to extract a character from a video and then maneuver that character in the plane, optionally with other backgrounds. The character is then redrawn into the background with a neural net, and all of this is done in real time. All in all, this paper was well structured and extensively detailed wrt how it engineered this solution (and why). If I had a complaint, it would be that I did not learn anything scientifically from the paper. There isn*t a tested hypothesis, but rather it*s a feat of engineering to get this to work. Those are important as well for the field, and I suspect that this direction could be pushed a lot more. For example, it*s not close to getting realistic spatial movement relative to the plane nor is the control that impressive wrt limbs. However, as a next-contribution, this work deserves to be seen more widely. Hence, I rate it as a weak accept."}
{"id": "iclr2020_423", "title": "Towards Understanding the Regularization of Adversarial Robustness on Neural Networks | OpenReview", "abstract": "Abstract:###The problem of adversarial examples has shown that modern Neural Network (NN) models could be rather fragile. Among the most promising techniques to solve the problem, one is to require the model to be {it -adversarially robust} (AR); that is, to require the model not to change predicted labels when any given input examples are perturbed within a certain range. However, it is widely observed that such methods would lead to standard performance degradation, i.e., the degradation on natural examples. In this work, we study the degradation through the regularization perspective. We identify quantities from generalization analysis of NNs; with the identified quantities we empirically find that AR is achieved by regularizing/biasing NNs towards less confident solutions by making the changes in the feature space (induced by changes in the instance space) of most layers smoother uniformly in all directions; so to a certain extent, it prevents sudden change in prediction w.r.t. perturbations. However, the end result of such smoothing concentrates samples around decision boundaries, resulting in less confident solutions, and leads to worse standard performance. Our studies suggest that one might consider ways that build AR into NNs in a gentler way to avoid the problematic regularization.", "review": "Review:###===== Summary ===== The paper presents new theory to develop understanding about why adversarially robust neural networks show lower test performance compared to their standard counterparts despite being more robust to perturbations in the data. The main hypothesis is that the degradation in performance in adversarially robust networks is due to many samples being concentrated around the decision boundary, which makes the network less confident about its decisions. The paper studies this hypothesis by deriving a bound on the generalization error based on the margin between the samples in the training set and the decision boundary. The paper then presents empirical demonstrations that aim to illustrate the theoretical findings. Contributions: 1. Derive a generalization bound on the performance of adversarially robust networks that depends on the margin between training examples and the decision boundary. 2. Provide empirical evaluations that aim to illustrate the theoretical results. ===== Review ===== The problem that the paper addresses is very significant to the robust optimization field and the study of adversarial robustness in neural networks. Thus, I believe that the results could represent a significant contribution. However, due to the way that the information is presented, it is difficult to validate the correctness of the theory and the insights from the paper. Consequently, I consider that the paper should be rejected. ===== Detailed Comments ===== - First, and foremost, the paper should be proof-read for English grammar and writing style. In its current form, it is difficult to follow the main argument of many of the paragraphs. This is exceedingly important because the main subject of the paper is already difficult to digest as is. - In the related work section, a lot of previous work is referenced without any context about what the contribution of each of those papers is. Each of these papers should be mentioned along with their corresponding contributions. Otherwise, it is difficult to frame the paper within the context of the current literature. Moreover, not providing context makes it difficult to determine which parts of the paper are original and which are the result from previous work. - The first item in Section 1.1 is difficult to follow because there are many gaps in logic that are left to the reader to fill in. It is reasonable to expect the reader to fill in some of the details, but since the sole purpose of this section is to build intuition about what is about to be presented in the paper, then each step in the explanation should follow as seamlessly as possible. - The motivation for studying the margins between the training set and the decision boundary is not clear until Section 5.2.1 where it is mentioned that this is a widely used tool in learning theory. This should be presented earlier since not every reader will be completely familiar with learning theory. Moreover, it would also be useful to provide some intuition about how margins relate to the confidence of a classifier. - After presenting the main result of the paper — Theorem 4.1 — very little intuition is provided about each of the terms in the bounds. It would greatly increase the clarity of the result if each term was explained intuitively, so that the readers can gain the main insight of the paper before reading the proofs. This would also help motivate better the empirical evaluations in the following section. - The plots in FIgure 3 and Figure 4 are very difficult to understand and unintuitive. For Figure 3, the main reason the plots are difficult to read is because different colors are used for different networks. For Figure 4, it is difficult to understand what is happening because the tallest curves are plotted on top of the shortest ones. Hence, the information from the other curves is mostly lost. This seems like a very minor comment, but this graphs are not very complicated, so they should be easy to understand; yet it takes several minutes to take in what is happening in the graph. - For the proof of Lemma 4.1it is not clear how to get from Equation (7) to the main result of the lemma even after referencing Theorem 3 of Sokolic et. al. (2017) and Jia et. al. (2019). This could also be my lack of expertise on the topic; however, since the proof is already in the appendix, the proof should not be sparse in the amount of detail that it provides. ===== References ===== Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel R. D. Rodrigues. Robust Large Margin Deep Neural Networks. IEEE Transactions on Signal Processing, 65(16):4265–4280, aug 2017. ISSN 1053-587X. doi: 10.1109/TSP.2017.2708039. Kui Jia, Shuai Li, Yuxin Wen, Tongliang Liu, and Dacheng Tao. Orthogonal Deep Neural Networks. Technical report, 2019. URL http://arxiv.org/abs/1905.05929."}
{"id": "iclr2020_424", "title": "Towards Verified Robustness under Text Deletion Interventions | OpenReview", "abstract": "Abstract:###Neural networks are widely used in Natural Language Processing, yet despite their empirical successes, their behaviour is brittle: they are both over-sensitive to small input changes, and under-sensitive to deletions of large fractions of input text. This paper aims to tackle under-sensitivity in the context of natural language inference by ensuring that models do not become more confident in their predictions as arbitrary subsets of words from the input text are deleted. We develop a novel technique for formal verification of this specification for models based on the popular decomposable attention mechanism by employing the efficient yet effective interval bound propagation (IBP) approach. Using this method we can efficiently prove, given a model, whether a particular sample is free from the under-sensitivity problem. We compare different training methods to address under-sensitivity, and compare metrics to measure it. In our experiments on the SNLI and MNLI datasets, we observe that IBP training leads to a significantly improved verified accuracy. On the SNLI test set, we can verify 18.4% of samples, a substantial improvement over only 2.8% using standard training.", "review": "Review:###This work is an application of interval bound propagation on evaluating the robustness of NLI model. This work is well-motivated, assuming that the confidence of a neural model should be lower when part of the sentence is missed. However, the application of vanilla IBP is quite limited in certain model architectures. In this work, the author considers specifically the decomposable attention model, which is a very shallow network, and not a state-of-the-art model anymore. It is non-trivial to adapt the proposed method to other more advanced models, such as the ones based on the Transformer model. Hence, this work does not make enough contribution to be accepted."}
{"id": "iclr2020_425", "title": "Adapt-to-Learn: Policy Transfer in Reinforcement Learning | OpenReview", "abstract": "Abstract:###Efficient and robust policy transfer remains a key challenge in reinforcement learning. Policy transfer through warm initialization, imitation, or interacting over a large set of agents with randomized instances, have been commonly applied to solve a variety of Reinforcement Learning (RL) tasks. However, this is far from how behavior transfer happens in the biological world: Humans and animals are able to quickly adapt the learned behaviors between similar tasks and learn new skills when presented with new situations. Here we seek to answer the question: Will learning to combine adaptation reward with environmental reward lead to a more efficient transfer of policies between domains? We introduce a principled mechanism that can \textbf{``Adapt-to-Learn*}, that is adapt the source policy to learn to solve a target task with significant transition differences and uncertainties. We show through theory and experiments that our method leads to a significantly reduced sample complexity of transferring the policies between the tasks.", "review": "Review:###Summary This paper tackles the problem of transferring a policy from source to target MDP, which differ in the state transition function. The idea is to add an additional cost that is the KL divergence between the trajectory likelihood under target policy (being learned) and target dynamics and the trajectory likelihood under the source policy (assumed optimal and deterministic) and source dynamics. The intuition is that the target policy will learn to match the state distribution of the optimal source policy. Results on MuJoCo locomotion robots with varying physics show that the proposed method performs better on target than warm-started RL or learning from scratch. I think the problem of transferring knowledge from one task to another in RL is very important for RL to be applicable to more real-world scenarios. Concerns / Questions Line 7 of Alg1 is confusing because it refers to a “target task model”, but in Assumption 2, it says only a model of the source transition function is needed. I think it makes sense that only the source transition model is needed because the target next state is given by experience. I think the combined assumptions of a) access to expert behavior (same as DAGGER) and b) that the MDPs differ only in dynamics functions and c) access to the source transition model are rather strong. I think (b) is a special case of transfer learning - a lot of transfer learning is concerned with changing reward functions as well, which this method wouldn’t apply to. I think this could be made more clear in the paper. It would be good if all these assumptions were made clear and discussed. I think the related work section is missing important areas of research in imitation learning and meta-reinforcement learning. For imitation learning, the approach strikes me as bearing similarity to PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings (Rhinehart et al.), and the topic of imitation learning should be discussed in general. For meta-RL, mentioning that it shares the same goal of transfer and citing a few main works (e.g. Duan et al. 2016, Wang et al. 2016, Finn et al. 2017 etc) would be good. Writing Suggestions Some terms used throughout the paper are quite unclear (e.g., “unsupervised RL”, “intrinsic adaptation reward”, “supervised reference trajectory tracking”). I suggest standardizing and defining terms early to avoid unnecessary confusion. Writing the Bellman operator and the value function equations in Section 2 don’t seem very relevant as they are I think never used again? Sections 3.1 and 3.2 are quite difficult to understand on first read (e.g., what does “point-wise local trajectories” mean?). I find Section 3.2.1 a bit misleading, “The optimization is more akin to supervised learning” - I agree the KL minimization is essentially imitation learning, but you are still doing policy search in addition to it?"}
{"id": "iclr2020_426", "title": "Model-free Learning Control of Nonlinear Stochastic Systems with Stability Guarantee | OpenReview", "abstract": "Abstract:###Reinforcement learning (RL) offers a principled way to achieve the optimal cumulative performance index in discrete-time nonlinear stochastic systems, which are modeled as Markov decision processes. Its integration with deep learning techniques has promoted the field of deep RL with an impressive performance in complicated continuous control tasks. However, from a control-theoretic perspective, the first and most important property of a system to be guaranteed is stability. Unfortunately, stability is rarely assured in RL and remains an open question. In this paper, we propose a stability guaranteed RL framework which simultaneously learns a Lyapunov function along with the controller or policy, both of which are parameterized by deep neural networks, by borrowing the concept of Lyapunov function from control theory. Our framework can not only offer comparable or superior control performance over state-of-the-art RL algorithms, but also construct a Lyapunov function to validate the closed-loop stability. In the simulated experiments, our approach is evaluated on several well-known examples including classic CartPole balancing, 3-dimensional robot control and control of synthetic biology gene regulatory networks. Compared with RL algorithms without stability guarantee, our approach can enable the system to recover to the operating point when interfered by uncertainties such as unseen disturbances and system parametric variations to a certain extent.", "review": "Review:########### Rebuttal Response: Thanks for the thorough response. Q2: The title still hasn’t changed on the current draft Q4: To be more precise: ‘a novel data-based approach for analyzing the stability of the closed-loop system is proposed by constructing a Lyapunov function parameterized by deep neural network’ - this alone is not novel, you would need to specify how your method of doing this is new ‘a practical learning algorithm is designed to search the stability guaranteed controller’ - this is a natural consequence of contribution 1, some further justification is needed as to why this could be viewed as an interesting contribution (i.e. the Lagrangian approach, if this is novel) ‘ the learned controller is able to stabilize the system when interfered by uncertainties such as unseen disturbance and system parameters variations of certain extent’ - this is not a contribution, but an experimental result. Q5: The review believes that model-free control and stability-guarantees are fundamentally orthogonal ideas, rather than just under-studied work as the authors have been suggesting in the script and rebuttal. Given that discrete-time Lyapunov stability is defined through expressions along the lines of L(f(x)) - L(x) < 0, for Lyapunov function L and closed-loop dynamics f, claiming that stability is being ‘analyzed’ without f is disingenuous. Instead, by making the value function a Lyapunov function, the goal is that the *converged* value function should produce a stable policy, and still, this is surely only assured within the space of samples. Moreover, the use of the discount factor gamma, popular in MFRL, essentially acts as a time horizon, so I’m not convinced a Lyapunov function learned with a gamma < 1 can be called stable in the pure infinite-horizon sense. With this in mind, I think the work would benefit from a revised central claim: that the use of Lyapunov value functions (as an inductive bias) provides more *robust* model-free controllers. I believe this message highlights the value of this work for MFRL, without making false assertions. This, in particular, would highlight the fact that many MFRL algorithms are benchmarked on deterministic environments, and therefore incredible brittle as the experimental results suggest. Q9: This remark was aimed at earlier in the paper, either the introduction or main section, rather than the experimental section. The fact that a value function can be viewed as a Lyapunov function makes sense but I’m not sure it is a well-known fact in the wider community. Basically, an introduction to the intersection of Lyapunov stability and optimal control would improve the paper. Q:10 The fact that the clipping of the multiplier corresponds to unstable policies during learning demonstrates that this pitfall needs to be expressed explicitly. Whether the stability guarantees apply to the converged policy or also intermediate policies is not clear on the initial reading of the paper. For me, this highlights another weakness in the paper. This initial theorems talk of L(s), which relates to the critic L_c by L(s) = E_{usimpi(s)} [L_c(s,u)], however in the subsequent objectives (eg Eq 2), this marginalization never occurs, therefore I don’t feel like you can say Theorem 2 applies to your resultant algorithm. Moreover, with the alpha_3 c term in Equation 3, c should be c(s, a) with a marginalized, which it doesn’t appear to be, and the hyperparameter alpha_3 is never discussed nor tuning explained. Assuming this not done in the code, the experiments need to be re-evaluated with Theorem 2 properly enforced through a sample approximation of the marginalization. Q11/Q12: Thank you for the Markov jump experiments. I’m not sure I understand why LAC is able to learn the task while SAC cannot. To me, this suggests perhaps a lack of hyperparameter tuning for SAC or further investigation. Moreover, there are typos in captions Fig 1, e and f. A note of figures: Please ensure all axes should be labeled and should be of sufficient size. Many are too small and unreadable. Figure 2 looks like it could be 1 plot (though perhaps requires normalization). Given that the strength of this method is the added robustness upon convergence, I think it would be valuable to focus less on time-domain results (Figure 3) (these can be added to the appendix for clarity), but instead show how each parameter/noise variation affects the mean and variance of the episodic return. I would expect that, while LAC provides significant robustness, it is still limited. The results don’t demonstrate this. It would also be interesting to know which hyperparameter controls this limit. I imagine there is a robustness/performance tradeoff. In conclusion, while I appreciate the efforts the authors put into the rebuttal, the extended discussions made me rethink my rating and I have decreased my rating to reject. I believe fixing the issues highlighted above and redrafting the central message must be done before this paper is ready for publication. ######## Review: This paper investigates the use of Lyanpunov theory as an inductive bias for improving the stability / robustness of policies in a model-free actor-critic reinforcement learning setting. Through viewing the Critic as a Lyapunov function, optimizing the policy with a Lyapunov-based constraint is meant to ensure the stability of the policy through a ‘cost stability’ metric.. Experimental results show that Lyapunov-based Soft-Actor Critic (LAC) is more robust than SAC on some linear and nonlinear environments. The reviewer believes that the study of intersections between Control Theory and RL to be immensely valuable and the authors outline a principled formulation. However, the implementation, experiments and general manuscript suggest that paper requires further work before it is conference-ready. As the author understands it, the current state of the literature of Lyapunov methods for Deep Reinforcement Learning can be summarized as: Richards et al, 2018, Classify stable region and learn neural Lyapunov function for a safe exploration strategy Berkenkamp et al, 2018: Classify the stable region via GP, move there for exploration Chow et al 2018 Constrained MDPs for discrete gridworld environments Chow et al 2019 Constrained MDPs for continuous environments through a projection on the policy This work: Actor-Critic constrained policy optimization with a lyapunov-based value function critic In the introduction and the related work, too much emphasis is put on explaining stability and discussing methods like Model Predictive Control (MPC) which do not benefit the rest of the paper. Additionally, the three contributions listed do not seem particularly novel given the past literature. The premise of the formulation also presents several unquestioned assumptions and design decisions: Why model-free RL, as the authors also state that many samples are required to validate stability? How does the requirement of stability inform the search strategy in this work? Especially as SAC uses a maximum entropy stochastic policy to aid exploration. Do you really get ‘guarantees’ with sample-based methods? I would expect bounds based on the number of samples The cost-based measure of stability seems open to abuse - i.e. for the half-cheetah environment only the centre-of-mass horizontal velocity in covered in the cost function, the stability of the embodiment (joint angles and velocities) are ignored. For the Fetch Reacher, a cost function in cartesian space ignores instabilities from kinematic singularities in joint space. I would image the cost function needs to be a measure on the entire dynamic state. The notion of a Value function as a Lyapunov function is very interesting, and since it was the basis of the work, would have benefitted from more discussion, i.e. for which cost/reward function families the equivalence is valid for, and how it compares to other Lyapunov candidate functions. With the RL formulation, the requirement of clipping with the lagrangians is suspicious, as it suggests the objective and/or its numerics are not well posed. With the choice of experiments, they do not seem to question the central problem outlined by the paper. Rather than show environments SAC returns unstable trajectories during learning, the experiments aim to demonstrate instead a general robustness. The reviewer appreciates that stability is difficult to assess; however, while stability is heavily linked to robustness, a paper title promising stability guarantees should demonstrate some strong empirical evidence stability. Additionally, the choice of environments do not seem to be ideal test beds for stability - i.e, the half-cheetah is stabilized via interactions with the ground. The reviewer would prefer to see simpler nonlinear environments, such as Markov Jump Processes / Switching Linear Dynamics, where SAC clearly demonstrates instability during learning which LAC is sufficiently regularized against. Additionally, while the `repressilator’ is an interesting application to the domain of bioengineering, its addition does not seem to be especially motivated by the central goal of the paper, so just adds to confuse the reader with unnecessary theoretical content. Moreover, a brief literature review uncovered some relevant earlier work which was not cited: Construction of neural network based Lyapunov functions, Petridis et al, 2006 Generation of Lyapunov functions by neural networks, Noroozi et al, 2008 Lyapunov Design for Safe Reinforcement Learning, Perkins et al, 2002 Some of the references also appear incorrectly formatted or incorrect, i.e. the reference for Spencer et al, 2018 should be the CoRL 2018 version rather than arxiv. Also, the general use of grammar in the manuscript would benefit from another draft. In particular, the title could be improved, i.e. Model-free Control of Nonlinear Stochastic Systems with Stability Guarantees"}
{"id": "iclr2020_427", "title": "Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack | OpenReview", "abstract": "Abstract:###The evaluation of robustness against adversarial manipulations of neural networks-based classifiers is mainly tested with empirical attacks as the methods for the exact computation, even when available, do not scale to large networks. We propose in this paper a new white-box adversarial attack wrt the -norms for aiming at finding the minimal perturbation necessary to change the class of a given input. It has an intuitive geometric meaning, yields quickly high quality results, minimizes the size of the perturbation (so that it returns the robust accuracy at every threshold with a single run). It performs better or similarly to state-of-the-art attacks which are partially specialized to one -norm.", "review": "Review:###Authors extend deepFool by adding extra steps and constraints to find closer points to the source image as the adversarial image. They both project onto the decision boundary. Deepfool does and adhoc clipping to keep the pixel values in (0,1) but the new proposed method respects the constraints during the steps. Also during the steps they combine projection of last step result and original image to keep it closer to the original image. Moreover, at the end of the optimization they perform extra search steps to get closer to the original image. Also they add random restarts. Rather than considering the original image, they randomly choose an image in the half ballpark of the total delta. According to the results in fig.2 the backward steps has the highest impact in comparison to deepfool. But mixing with original projection always helps a little and random restarts help a little too. Without the backward steps there is almost no gain from mixing the projections. Considering the full results in the appendix, the results are mixed with no obvious advantage in comparison to PGD specially."}
{"id": "iclr2020_428", "title": "Emergent Tool Use From Multi-Agent Autocurricula | OpenReview", "abstract": "Abstract:###Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.", "review": "Review:###1. Summary The authors report on an empirical study of emergent behavior of multiple RL agents learning to play hide-and-seek (a sparse reward task). The main point of this paper is that RL agents learning at scale (large number of samples, batch-size 64000). can learn to solve tasks with strategies that are human-interpretable (e.g., using ramps, boxes). Scale also requires various simplifications (e.g., keeping the learning setup as close as possible to a single-agent problem as possible). Agents are grouped in 2 teams (seekers, hiders). Each agent receives a team reward, e.g., it can be punished for events that it did not participate in, e.g., if a team-mate is seen by an opponent. If hiders are hidden, seekers also automatically see reward. The first 40% of the episode there is no reward to let hiders hide. There is one actor model, all agents share weights. Hence this is self-play: hiders and seekers use the same agent model. Also, all agents use a central value function that can see the entire state (decentralized execution, centralized learning). This makes the setting basically a single-agent problem, with the only decentralized aspect being each actor model only receiving its own observation. Note that a large body of multi-agent RL work in fact uses agents that do not share weights, etc. Other features described: - Auto-curricula: e.g. agents find new strategies (using ramps, boxes) that other agents have to counteract. - Human-relevant skills: They report that the agent model learns multiple ways to interact with (objects in) the environment that are semantically interesting (resembles something humans might do). - Authors compare with policies learning via intrinsic motivation. - Evaluation through transfer learning shows some benefit of transfer of hide-seek agents to auxiliary tasks. However, it is not so clear how this evaluation informs future work on transfer learning (e.g., how would you pick evaluation tasks for a given train-task?) 1. Decision (accept or reject) with one or two key reasons for this choice. Reject. The main point of the paper is empirical RL at scale. Although the learned behaviors are human-interpretable, this does not seem surprising given the fact that in many (large-scale) RL applications (Atari games, Go, DotA 2, Starcraft), it has been observed that RL agents can learn to manipulate and use their environment (which includes other agents!) in unexpected ways / find creative ways to exploit the reward function (see e.g. demos in https://www.alexirpan.com/2018/02/14/rl-hard.html). There has also been work on object-level RL [Agnew, Domingos 2018], which involves agents interacting with objects in the environment. Compared to this, the observation that RL agents learn human-interpretable uses of objects does not seem surprising. The paper also does not give new insights in how to make large-scale RL ``*work*. For instance, there are no significant differences in algorithm / model structure from DotA / Starcraft agents that can inform future large-scale experiments. The paper also does not introduce new concrete evaluation metrics that can apply to other tasks / RL problems, skill detection / segmentation methods to learn the structure of auto-curricula. Furthermore, the setup is very close to a single-agent problem (see above), and is far simpler in the multi-agent assumptions from other decentralized multi-agent work (Foerster 2018, Jacques 2019, etc)."}
{"id": "iclr2020_429", "title": "Classification-Based Anomaly Detection for General Data | OpenReview", "abstract": "Abstract:###Anomaly detection, finding patterns that substantially deviate from those seen previously, is one of the fundamental problems of artificial intelligence. Recently, classification-based methods were shown to achieve superior results on this task. In this work, we present a unifying view and propose an open-set method to relax current generalization assumptions. Furthermore, we extend the applicability of transformation-based methods to non-image data using random affine transformations. Our method is shown to obtain state-of-the-art accuracy and is applicable to broad data types. The strong performance of our method is extensively validated on multiple datasets from different domains.", "review": " UPDATE: I acknowledge that I‘ve read the author responses as well as the other reviews. I appreciate the clarifications, additional experiments, and overall improvements made to the paper. I updated my score to 6 Weak Accept. #################### This paper proposes a deep method for anomaly detection (AD) that unifies recent deep one-class classification [6] and transformation-based classification [3, 4] approaches. The proposed method transforms the data to subspaces via random affine transformations and identifies with each such transformation a cluster centered around some centroid (set as the mean of the respectively transformed samples). The training objective of the method is defined by the triplet loss [5] which learns to separate the subspaces via maximizing the inter-class as well as minimizing the intra-class variation. The anomaly score for a sample is finally given by the sum of log-probabilities, where each transformation-/cluster-probability is derived from the distance to the cluster center. Using random affine transformations, the proposed method is applicable to general data types in contrast to previous works that only consider geometric transformations (rotation, translation, etc.) on image data [3, 4]. The paper conclusively presents experiments on CIFAR-10 and four tabular datasets (Arrhythmia, Thyroid, KDD, KDD-Rev) that indicate a superior detection performance of the proposed method over baselines and deep competitors. I think this paper is not yet ready for acceptance due to the following main reason: (i) The experimental evaluation needs clarification and should be extended to judge the significance of the empirical results. (i) I think the comparison with state-of-the-art deep competitors [6, 4] should consider at least another image dataset besides CIFAR-10, e.g. Fashion-MNIST or the recently published MVTec [1] for AD. On CIFAR-10, do you also consider geometric transformations however using your triplet loss or are the reported results from random affine transformations? I think reporting both would be insightful to see the difference between image-specific and random affine transformations. On the tabular datasets, how do deep networks perform in contrast to the final linear classifier reported on most datasets? Especially when only using a final linear classifier, the proposed method is very similar to ensemble learning on random subspace projections. Figure 1 (right) shows an error curve that is also typical for ensemble learning (decrease in mean error and reduction in overall variance). I think this should be discussed and ensemble baselines [2] should be considered for a fair comparison. Table 2 also seems incomplete with the variances missing for some methods? Further clarifications are needed. How many transformations do you consider on the specific datasets? How is hyperparameter chosen? Finally, I think the claim that the approach is robust against training data contamination is too early from only comparing against the DAGMM method on KDDCUP (Is Figure 1 (left) wrong labeled? As presented DAGMM shows a lower classification error). Overall, I think the paper proposes an interesting unification and generalization of existing state-of-the-art approaches [6, 4], but I think the experimental evaluation needs to be more extensive and clarified to judge the potential significance of the results. The presentation of the paper also needs some polishing as there are many typos and grammatical errors in the current manuscript (see comments below). #################### *Additional Feedback* *Positive Highlights* 1. Well motivated anomaly detection approach that unifies existing state-of-the-art deep one-class classification [6] and transformation-based classification [3, 4] approaches that indicates improved detection performance and is applicable to general types of data. 2. The work is well placed in the literature. All relevant and recent related work is included in my view. *Ideas for Improvement* 3. Extend and clarify the experimental evaluation as discussed in (i) to infer statistical significance of the results. 4. I think many details from the experimental section could be moved to the Appendix leaving space for the additional experiments. 5. Maybe add some additional tabular datasets as presented in [2, 7]. 6. Maybe clarify “Classification-based AD” vs. “Self-Supervised AD” a bit more since unfamiliar readers might be confused with supervised classification. 7. Improve the presentation of the paper (fix typos and grammatical errors, improve legibility of plots) 8. Some practical guidance on how to choose hyperparameter would be good. This may just be a default parameter recommendation and showing that the method is robust to changes in s with a small sensitivity analysis. *Minor comments* 9. The set difference is denoted with a backslash not a forward slash, e.g. . 10. citet vs citep typos in the text (e.g. Section 1.1, first paragraph “ ... Sakurada & Yairi (2014); ...”) 11. Section 1.1: “ADGMM introduced by Zong et al. (2018) ...” » “DAGMM introduced by Zong et al. (2018) ...”. 12. Eq. (1): in the first denominator as well. 13. Section 2, 4th paragraph: . 14. , , and are used somewhat inconsistently in the text. 15. Section 3: “Note, that it is defined everywhere.”? 16. Section 4: *If is chosen deterministicaly ...* >> *If is chosen deterministically ...* 17. Section 5, first sentence: “... to validate the effectiveness our distance-based approach ...” » “... to validate the effectiveness of our distance-based approach ...”. 18. Section 5.1: “We use the same same architecture and parameter choices of Golan & El-Yaniv (2018) ...” » “We use the same architecture and parameter choices as Golan & El-Yaniv (2018) ...” 19. Section 5.2: “Following the evaluation protocol of Zong et al. Zong et al. (2018) ...” » “Following the evaluation protocol of Zong et al. (2018) ...”. 20. Section 5.2: “Thyroid is a small dataset, with a low anomally to normal ratio ...” » “Thyroid is a small dataset, with a low anomaly to normal ratio ...”. 21. Section 5.2, KDDCUP99 paragraph: “Tab. ??” reference error. 22. Section 5.2, KDD-Rev paragraph: “Tab. ??” reference error. #################### *References* [1] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger. Mvtec ad–a comprehensive real-world dataset for unsupervised anomaly detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9592–9600, 2019. [2] J. Chen, S. Sathe, C. Aggarwal, and D. Turaga. Outlier detection with autoencoder ensembles. In SDM, pages 90–98, 2017. [3] S. Gidaris, P. Singh, and N. Komodakis. Unsupervised representation learning by predicting image rotations. In ICLR, 2018. [4] I. Golan and R. El-Yaniv. Deep anomaly detection using geometric transformations. In NIPS, 2018. [5] X. He, Y. Zhou, Z. Zhou, S. Bai, and X. Bai. Triplet-center loss for multi-view 3d object retrieval. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1945–1954, 2018. [6] L. Ruff, R. A. Vandermeulen, N. Görnitz, L. Deecke, S. A. Siddiqui, A. Binder, E. Müller, and M. Kloft. Deep one-class classification. In International Conference on Machine Learning, pages 4393–4402, 2018. [7] L. Ruff, R. A. Vandermeulen, N. Görnitz, A. Binder, E. Müller, K.-R. Müller, and M. Kloft. Deep semi-supervised anomaly detection. arXiv preprint arXiv:1906.02694, 2019."}
{"id": "iclr2020_430", "title": "Bayesian Inference for Large Scale Image Classification | OpenReview", "abstract": "Abstract:###Bayesian inference promises to ground and improve the performance of deep neural networks. It promises to be robust to overfitting, to simplify the training procedure and the space of hyperparameters, and to provide a calibrated measure of uncertainty that can enhance decision making, agent exploration and prediction fairness. Markov Chain Monte Carlo (MCMC) methods enable Bayesian inference by generating samples from the posterior distribution over model parameters. Despite the theoretical advantages of Bayesian inference and the similarity between MCMC and optimization methods, the performance of sampling methods has so far lagged behind optimization methods for large scale deep learning tasks. We aim to fill this gap and introduce ATMC, an adaptive noise MCMC algorithm that estimates and is able to sample from the posterior of a neural network. ATMC dynamically adjusts the amount of momentum and noise applied to each parameter update in order to compensate for the use of stochastic gradients. We use a ResNet architecture without batch normalization to test ATMC on the Cifar10 benchmark and the large scale ImageNet benchmark and show that, despite the absence of batch normalization, ATMC outperforms a strong optimization baseline in terms of both classification accuracy and test log-likelihood. We show that ATMC is intrinsically robust to overfitting on the training data and that ATMC provides a better calibrated measure of uncertainty compared to the optimization baseline.", "review": "Review:###The authors propose the adaptive thermostat Monte Carlo sampler for feedforward neural networks. The proposed approach dynamically adjust the amount of momentum and noisy applied to each model parameter during updates. ResNet++ (ResNet without batchnorm/dropout but adding SELU, fixup and weight normalization) is introduced. Further, the authors claim that the need for hyperparameter setup is reduced provided that early stopping, stochastic regularization and carefully tuned learning rate schedules are not required. The authors highlight some practical issues with the Nose-hoover thermostat, however, recognize its mathematical soundness. When ATMC is described (temperature stages), a motivation is provided but not justified theoretically. In (10) and (11), gamma_1(), gamma_2(), eta_t and a are introduced without definition or further explanation. The authors claim that the need for hyperparameter setup is reduced, however, in the experiments they use a cyclic step size with length n=50 (20 for ImageNet), a Laplace prior with parameter b=5, momentum noise with parameter 0.9, pre-conditioner parameter 0.0003 and c parameter 0.001. The impact of these choices on performance is not described. Further, the number of filters is doubled relative to ResNet-56 without explanation. The calibration curves in Figure 3 are underwhelming. ATMC is better than SGD but not necessarily well calibrated. Also, note that x and y scales are heavily biased toward 1. In summary, the proposed approach needs to be described in more detail and the experiments are not very satisfying given the claims made by the authors in the Introduction. Minor: - In (1) W is not defined. - In (1) the dimensionality of D, Q and Gamma is not defined but their elements are used. - In (2) m is only defined after (3), in fact, only called by its name, pre-conditioner, in Algorithm 1. - In Section 2.3 there is a reference to the step size, though not introduced until discretization later in Section 3. - In (7) _x0008_eta() is a function of p, but not in other instances, e.g., (4), (10) and (11). - Move Algorithm 1 closer to definition. - In (13), d is not defined."}
{"id": "iclr2020_431", "title": "Distilling Neural Networks for Faster and Greener Dependency Parsing | OpenReview", "abstract": "Abstract:###The carbon footprint of natural language processing (NLP) research has been increasing in recent years due to its reliance on large and inefficient neural network implementations. Distillation is a network compression technique which attempts to impart knowledge from a large model to a smaller one. We use teacher-student distillation to improve the efficiency of the Biaffine dependency parser which obtains state-of-the-art performance with respect to accuracy and parsing speed (Dozat & Manning, 2016). When distilling to 20% of the original model’s trainable parameters, we only observe an average decrease of ?1 point for both UAS and LAS across a number of diverse Universal Dependency treebanks while being 2.26x (1.21x) faster than the baseline model on CPU (GPU) at inference time. We also observe a small increase in performance when compressing to 80% for some treebanks. Finally, through distillation we attain a parser which is not only faster but also more accurate than the fastest modern parser on the Penn Treebank.", "review": "Review:###This paper applies model distillation to the Biaffine dependency parser (Dozat & Manning, 2016), showing that parsing speed and model size can be substantially improved without significant loss in performance. The paper is generally clearly written (though see comments below), and the results are generally convincing. On the other hand, there is not a lot of content, and not much novelty. Overall I am slightly positive towards this paper, mostly because of the important topic and the clean experimentation, in particular with multiple languages. Comments: 1. While the writing is fairly clear, the tables and graphs can be improved: - the upper part of table 1 is misleading. As the authors rightfully note, the speed is not comparable between different reported numbers due to architecture differences, etc. I would recommend showing only the two bottom parts, or reproduce some of the other papers on the same architecture. - The fonts are very small in all graphs, making them hard to understand. - Figure 3 is confusing (especially given the small font). It might be preferable to make the baseline parser the baseline numbers on which the proposed approach and the small model are compared against, instead of making the proposed method the baseline. 2. I would appreciate more details about how the model was compressed to X%. The authors mention reducing the number of dimensions of each layer, but more details would be helpful. 3. As the authors rightfully notice, their favorable results in Figure 2 are almost exclusively attributed to improved performance on Tamil. Adding standard deviations to Figure 2 would help appreciate the observed trends. Moreover, while experimenting with 8 languages is beyond standard practice in NLP, it seems adding more languages, in particular some with small datasets such as Tamil, would help better appreciate the nature of the proposed approach."}
{"id": "iclr2020_432", "title": "Learning to Learn Kernels with Variational Random Features | OpenReview", "abstract": "Abstract:###Meta-learning for few-shot learning involves a meta-learner that acquires shared knowledge from a set of prior tasks to improve the performance of a base-learner on new tasks with a small amount of data. Kernels are commonly used in machine learning due to their strong nonlinear learning capacity, which have not yet been fully investigated in the meta-learning scenario for few-shot learning. In this work, we explore kernel approximation with random Fourier features in the meta-learning framework for few-shot learning. We propose learning adaptive kernels by meta variational random features (MetaVRF), which is formulated as a variational inference problem. To explore shared knowledge across diverse tasks, our MetaVRF deploys an LSTM inference network to generate informative features, which can establish kernels of highly representational power with low spectral sampling rates, while also being able to quickly adapt to specific tasks for improved performance. We evaluate MetaVRF on a variety of few-shot learning tasks for both regression and classification. Experimental results demonstrate that our MetaVRF can deliver much better or competitive performance than recent meta-learning algorithms.", "review": "Review:###The paper focuses on the topic of meta-learning for few-shot learning and explores kernel approximation with random fourier features for this problem. The authors propose to learn adaptive kernels by meta variational random features, and evaluate their approach on different few-shot learning tasks, comparing it against recent meta-learning algorithms. The paper is well-motivated and well-written. On page 8, the authors mention related works that were not included for comparison because they rely on pre-trained embeddings or large-scale deep architectures. It would have been interesting to see the difference in performance."}
{"id": "iclr2020_433", "title": "From English to Foreign Languages: Transferring Pre-trained Language Models | OpenReview", "abstract": "Abstract:###Pre-trained models have demonstrated their effectiveness in many downstream natural language processing (NLP) tasks. The availability of multilingual pre-trained models enables zero-shot transfer of NLP tasks from high resource languages to low resource ones. However, recent research in improving pre-trained models focuses heavily on English. While it is possible to train the latest neural architectures for other languages from scratch, it is undesirable due to the required amount of compute. In this work, we tackle the problem of transferring an existing pre-trained model from English to other languages under a limited computational budget. With a single GPU, our approach can obtain a foreign BERT-base model within a day and a foreign BERT-large within two days. Furthermore, evaluating our models on six languages, we demonstrate that our models are better than multilingual BERT on two zero-shot tasks: natural language inference and dependency parsing.", "review": "Review:###This paper presents a method to efficiently transfer pre-trained english language model to bilingual language model. The obtained representations are evaluated on downstream NLP task (natural language inference and dependency parsing) with state-of-the-art performances. Pros: - Experiments clearly show that, using the proposed method, stronger pre-trained English embedding leads to stronger bilingual language model and thus to better performances for downstream foreign tasks. Cons: While it is generally intelligible, some structural modifications could be done to improved the clarity of the paper. For instance, the method used to align foreign word vectors with English word vectors, when no aligned corpus is available, should appear sooner. It is described in 3.1 but should probably appear in 2.1 subsection Learning from Monolingual Corpus. Minor issues: - in section 3: RoBERA -> RoBERTa - in section 5.1: the third sentence is syntactically incorrect - in Conclusion: our approach produces better than -> our approach performs better than"}
{"id": "iclr2020_434", "title": "Feature-Robustness, Flatness and Generalization Error for Deep Neural Networks | OpenReview", "abstract": "Abstract:###The performance of deep neural networks is often attributed to their automated, task-related feature construction. It remains an open question, though, why this leads to solutions with good generalization, even in cases where the number of parameters is larger than the number of samples. Back in the 90s, Hochreiter and Schmidhuber observed that flatness of the loss surface around a local minimum correlates with low generalization error. For several flatness measures, this correlation has been empirically validated. However, it has recently been shown that existing measures of flatness cannot theoretically be related to generalization: if a network uses ReLU activations, the network function can be reparameterized without changing its output in such a way that flatness is changed almost arbitrarily. This paper proposes a natural modification of existing flatness measures that results in invariance to reparameterization. The proposed measures imply a robustness of the network to changes in the input and the hidden layers. Connecting this feature robustness to generalization leads to a generalized definition of the representativeness of data. With this, the generalization error of a model trained on representative data can be bounded by its feature robustness which depends on our novel flatness measure.", "review": "Review:###This paper describes a connection between flatness of minima and generalization in deep neural networks. The authors define a concept called *feature-robustness* and show that it is related to flatness. This is derived through a straightforward observation that perturbations in feature space can be recast as perturbations of the model in parameter space. This allows the authors to define a (layerwise) flatness measure for minima in deep networks (this layerwise flatness measure is also invariant to rescalings of the layers in neural networks with positively homogenous activations). The authors combine their notion of feature robustness with epsilon representativeness of a function to connect flatness to generalization. They present a few empirical evaluations on CIFAR10 and MNIST. I believe this paper is able to once again confirm the relationship between flatness and generalization in an empirical manner with their layerwise measure of flatness. I am not so convinced about the theoretical justification that they claim to provide and thus do not recommend acceptance. Theory - The key theorem relating generalization and flatness is Theorem 10 which says that if a compositional model is feature robust and the output of the first component is an epsilon-representative for the second component, then the compositional model will generalize. While this is interesting, it is not clear to me that this guarantees generalization for deep neural networks. This result only talks about feature robustness and representativeness for a particular layer. If a deep network has many layers, will the feature robustness layers closer to the input guarantee feature robustness at deeper layers? That might require a further unit operator norm constraint on the layer operator, which is a restriction on the types of weights that can be used. If a sample is epsilon representative at one layer, what is required for the next layer to be epsilon representative for the rest of the deep network? This seems to be a missing step in relating flatness/feature robustness of a layer to the generalization of the whole network. Another idea that I think arises from Theorem 10 is that the flatness of loss landscapes is important when you have learning problems where the hypothesis class is compositional. While flatness is only spoken of in the case of deep neural networks, can we identify the same phenomenon in other problems? I would encourage the authors to try and identify another model in which the flatness-generalization relationship exists (even empirical evidence would suffice for now). This would strengthen the case for studying flatness and biasing optimization towards flatter solutions in the case of deep networks. Experiments - This section seems to be pretty rudimentary, I would like to see more results on different kinds of network architectures (VGG? Inception? AlexNet?), more datasets (KMNIST? Fashion MNIST? SVHN?), and possibly more repetitions. At one point the authors mention that they declare a minimum has been reached if the training loss is < 0.07. Atleast on CIFAR10 and MNIST it is possible to achieve training loss <1e-4 so am not sure if the networks that the authors are testing are minima at all (It is important for them to be minima since the flatness measure is only defined at minima). Can the authors also identify more situations other than large batch vs small batch training that would lead them to obtain flatter/sharper minima? The authors also claim that measuring generalization using test error is flawed, but do not provide details about their method of measuring generalization. I would want to see these details and a more thorough discussion of why measuring generalization through test error is flawed. While this is an interesting paper, I do not believe it is ready for acceptance at ICLR 2020."}
{"id": "iclr2020_435", "title": "Picking Winning Tickets Before Training by Preserving Gradient Flow | OpenReview", "abstract": "Abstract:###Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time. Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels.", "review": "Review:###This paper introduces a method to prune networks at initialization in a way that (mostly) preserves the gradient flow through the resulting pruned network. This is a direct improvement over previous methods (e.g. SNIP) which have no guarantees that pruned connections will break the gradient flow and thereby harm learning. I quite like this paper, the motivation and results are convincing and it is well presented. The writing is excellent for most of the paper. From section 5 onwards the writing does need quite a bit of editing, as its quality is significantly reduced from what came before. Some detailed comments: - Figure 1 is very nice and really clarifies the idea! - In paragraph below Equation (8): what does *can be computed by backward twice* mean? - Please specify where the equalities in equation (9) are coming from. - Table 3 & 4: Why are the pruning ratios different for each model? - Table 3: Why are values missing for the baseline for 80% and 90%? - Section 5.2: *We observed that, the main bottleneck or pruned... when deriving the pruning criteria*: it*s not clear where this conclusion is coming from. - Table 5 has no batch size results, even though you*re referencing them in the text. And some minor comments to help with the writing: - Intro: *As shown in Dey et al. (2019) that with pre-specified sparsity, they can achieve* would read better as *As shown by Dey et al. (2019), with pre-specified sparsity one can achieve* - Equation (3): Clarify that this is a function of - Sentence below Equation (6): *of the pruned network, and thus our goal* remove the *and thus* - Table 1: Specify that you*re reporting accuracy. - Section 4.1: *e.g. wide ResNet (Zagaruyko & Komodakis, 2016), and thus we can regard* remove the *and thus* - Sentence below equation (9): *encouraging the eigenspace of Theta align* add a *to* before *align* - Sentence before section 5: *it will encourage the eigenspace of the NTK distributing large eigenvalues in the direction of Y, which will in turn accelerates the decrease of the loss (Arora et al., 2019) and benefits to the optimization in A* would read better as *it will encourage the eigenspace of the NTK to distribute large eigenvalues in the direction of Y, which in turn accelerates the decrease of the loss (Arora et al., 2019) and benefits the optimization in A* - Throughout section 5, write it in present tense rather than past tense. e.g. *In this section, we conduct various experiments* instead of *In this section, we conducted various experiments* - Sentence below table 2: you have *the the* - Second paragraph of section 5.1: *We can observe GraSP outperform random pruning clearly* would read better as *We can observe GraSP clearly outperforms random pruning* - Second paragraph of section 5.1: *In the next, we further compared* remove *In the next* - Second paragraph of section 5.1: *Besides, we further experimented with the late resetting* remove *Besides* - Paragraph above section 5.2: *GraSP surpassing SNIP* use *surpasses* instead - Paragraph above section 5.2: *investigate the reasons behind in Section 5.2 for promoting better understanding* would read better as *investigate the reasons behind this in Section 5.2 for obtaining a better understanding* - Section 5.2: *We observed that, the main bottleneck* -> *We observe that the main bottleneck* - Section 5.2: *Besides, we also plotted the the gradient norm of the pruned*, remove *Besides* and the extra *the* - Section 5.2: *the average of the gradients of the entire dataset* use *over the entire dataset* - Section 5.2: *hopefully more training progress can make as evidenced* would read better as *hopefully more training progress can be made as evidenced* - Section 5.3 title would be better using *Visualizing* instead of *Visualize* - Section 5.3: Join the first two sentences with a comma into a single sentence. - Section 5.3: *In contrast, SNIP are more likely* -> In contrast, SNIP is more likely* - Section 5.4: *for ablation study* would read better as *via ablations* - Section 5.4: *we tested GraSP with three different initialization methods;* use a *:* instead of *;* - Section 6: *Besides, readers may notice that*, remove the *Besides* - Section 6: *traditional pruning algorithms while still enjoy the cheaper training cost. As an evidence,* would read better as *traditional pruning algorithms while still enjoying cheaper training costs. As evidence,* - Your citation for Evci et al. (2019) is missing the publication venue/arxiv ID."}
{"id": "iclr2020_436", "title": "Mutual Information Maximization for Robust Plannable Representations | OpenReview", "abstract": "Abstract:###Extending the capabilities of robotics to real-world complex, unstructured environments requires the capability of developing better perception systems while maintaining low sample complexity. When dealing with high-dimensional state spaces, current methods are either model-free, or model-based with reconstruction based objectives. The sample inefficiency of the former constitutes a major barrier for applying them to the real-world. While the latter present low sample complexity, they learn latent spaces that need to reconstruct every single detail of the scene. Real-world environments are unstructured and cluttered with objects. Capturing all the variability on the latent representation harms its applicability to downstream tasks. In this work, we present mutual information maximization for robust plannable representations (MIRO), an information theoretic representational learning objective for model-based reinforcement learning. Our objective optimizes for a latent space that maximizes the mutual information with future observations and emphasizes the relevant aspects of the dynamics, which allows to capture all the information needed for planning. We show that our approach learns a latent representation that in cluttered scenes focuses on the task relevant features, ignoring the irrelevant aspects. At the same time, state-of-the-art methods with reconstruction objectives are unable to learn in such environments.", "review": "Review:###The authors propose a model-based reinforcement algorithm in which the model is a sequential latent variable model and the actions planned with a cross-entropy method (CEM) planner. The model is learnt by maximizing a lower bound on the mutual information between the latent states and their successor observations (instead of the classical sequential ELBO). The authors argue that the latter objective function yield robustness to distraction in visual scenes. The algorithm, named MIRO, is experimented on 4 simulated environments. --- Overall I did not find the paper particularly clear and easy to read. The method is only introduced in the 5th page and no ablation study is conducted. It is still not obvious to me why maximizing the MI in the objective function would reduce the influence of potential distractors.` Furthermore, the paper overlooks a good part of the related work on extending VAEs to sequence data, published in the last 3 years and does not draw links to similar architectures. The experiments are in my opinion not convincing, as the approach is only experimented on 2 non trivial -yet not particularly challenging- environments (Finger and Half Cheetah). Minor: in the equation of the ELBO, page 3, the parameters \theta and phi are swapped."}
{"id": "iclr2020_437", "title": "The divergences minimized by non-saturating GAN training | OpenReview", "abstract": "Abstract:###Interpreting generative adversarial network (GAN) training as approximate divergence minimization has been theoretically insightful, has spurred discussion, and has lead to theoretically and practically interesting extensions such as f-GANs and Wasserstein GANs. For both classic GANs and f-GANs, there is an original variant of training and a *non-saturating* variant which uses an alternative form of generator gradient. The original variant is theoretically easier to study, but for GANs the alternative variant performs better in practice. The non-saturating scheme is often regarded as a simple modification to deal with optimization issues, but we show that in fact the non-saturating scheme for GANs is effectively optimizing a reverse KL-like f-divergence. We also develop a number of theoretical tools to help compare and classify f-divergences. We hope these results may help to clarify some of the theoretical discussion surrounding the divergence minimization view of GAN training.", "review": "Review:########### Updated Review ######### I would like to thank the author(s) for their rebuttal, which I have carefully read. I also appreciate the effort made to improve the paper. My overall evaluation of the paper stands unchanged. ############################# #### My review is based on the updated paper downloaded from the anonymous link. #### This paper discusses alternative training strategies for f-GANs. While the discussion has some interesting points, the presentation needs to be much improved. It is not easy to follow this paper in its current form, and the main results are not properly emphasized. As such, I am not certain of its real contribution. f-GANs are not routinely used in practice (except for the vanilla JSD and RKL), and as far as I can tell the saturating gradient issue is no longer a central concern (it has been well addressed years ago, with, e.g. WGANs). I am voting to reject this submission, but I am willing to re-evaluate this paper if the author(s) significantly improves their writing. - In Section 2, what does it mean by *the Fisher metric of the family*? The concept of Fisher metric is defined anywhere in the text, and there is no reference to it. - It is not is intuitive why the second derivative of f_R takes the form (given above Eqn (2)). Please elaborate. - Please avoid the use of subjective phrases such as *imagining*, *get a feel*, etc. I am guessing the author(s) are trying to suggest taking a visual inspection of the discrepancies projected on the log-likelihood ratio axis (which is 1D) and figure out which f-div might be more appropriate. - Fig. 3 needs legends. There are two solid (dotted, resp) lines in the Figure, and I am guessing one of them is for the saturating and the other for the non-saturating gradient. This needs to be specified because the line specs are identical. - After going through the entire paper, I would highly recommend the author(s) to take a course in academic writing. The main contributions are not highlighted and some of the key concepts are not even properly defined. For example, analysis of the non-saturating gradient, which is supposedly the main result of this submission, appeared in pp. 7, by which time most readers have exhausted their patience. The vanilla version of the non-saturating scheme never appeared in the main text. The notation system is also non-standard, where the notation , normally reserved for average/expectation, has been used to denote the gradient wrt . The writing can be very unprofessional at times, for example, *the answers to these questions are yes, yes and no respectively*. - I do not know why the author(s) inserted one toy experiment in the paper, as it serves no purpose. Each model converges to their respectively optimal, as expected. There is no discussion of how to choose an appropriate f-div or (f,g) hybrid in practice. - I totally agree with the author(s) on the point that this is a note (sec 7, second paragraph), rather than an academic paper. There is a lot of derivations in the paper, but insightful discussions are limited in the sense that it is not clear how to connect these results to improve practice. The section titles also read like bullet points in a note. - Not certain about the significance of this paper. The derivations are fairly standard and I can not find any useful proposals that might benefit the practice of f-GAN training. There is a number of papers discussing non-saturating GAN training (see sec 8), and I do not think this paper adds too much value to this discussion."}
{"id": "iclr2020_438", "title": "Characterizing Missing Information in Deep Networks Using Backpropagated Gradients | OpenReview", "abstract": "Abstract:###Deep networks face challenges of ensuring their robustness against inputs that cannot be effectively represented by information learned from training data. We attribute this vulnerability to the limitations inherent to activation-based representation. To complement the learned information from activation-based representation, we propose utilizing a gradient-based representation that explicitly focuses on missing information. In addition, we propose a directional constraint on the gradients as an objective during training to improve the characterization of missing information. To validate the effectiveness of the proposed approach, we compare the anomaly detection performance of gradient-based and activation-based representations. We show that the gradient-based representation outperforms the activation-based representation by 0.093 in CIFAR-10 and 0.361 in CURE-TSR datasets in terms of AUROC averaged over all classes. Also, we propose an anomaly detection algorithm that uses the gradient-based representation, denoted as GradCon, and validate its performance on three benchmarking datasets. The proposed method outperforms the majority of the state-of-the-art algorithms in CIFAR-10, MNIST, and fMNIST datasets with an average AUROC of 0.664, 0.973, and 0.934, respectively.", "review": "Review:###The authors present an interesting idea: creating representations based on gradients with respect to the weights to supplement information missing from the training dataset. The idea if very well motivated and gets the reader excited. Their explanation is clear from a high-level, however, the paper as a whole lacks rigorous justification. A lot of space is spent reviewing the role of gradients in training, what reconstruction loss is, and the basics of back-propagation --- however, many of their own propositions (e.g. why constrain gradients?) are given without reasoning. The paper could be made more clear by giving details and by changing the flow. For instance, Section 4 first gives a high-level explanation, then begins discussing specific experiments, and then returns to the method and provides some details. Additionally, it isn*t until page 6 (Section 5) that the authors introduce GradCon, one of the more promising ideas shared, this should be a focal point in the paper and introduced early (and given more than two sentences for explanation). The main proposal appears to be a modification of the loss function, but the paper may benefit from discussing implementation details (for example, during training vs. testing). Finally, the Figures (2 and 3 in particular) are not clear and need more explanation given in the captions. Figure 3a and 3b tell an interesting story, but they*re not easily digestable, nor is the key take-away clear to the reader just by looking at the figure and caption. The experimental results look reasonable and thorough, however the methods are sold on the idea of better representations for data missing from the training set, whereas the results are focused on anomaly detection. The method looks particularly promising at anomaly detection tasks --- the authors may have a more clear paper if they focus on this aspect. Overall, the paper presents a promising idea but it needs a more clear and rigorous presentation."}
{"id": "iclr2020_439", "title": "Variance Reduction With Sparse Gradients | OpenReview", "abstract": "Abstract:###Variance reduction methods which use a mixture of large and small batch gradients, such as SVRG (Johnson & Zhang, 2013) and SpiderBoost (Wang et al., 2018), require significantly more computational resources per update than SGD (Robbins & Monro, 1951). We reduce the computational cost per update of variance reduction methods by introducing a sparse gradient operator blending the top-K operator (Stich et al., 2018; Aji & Heafield, 2017) and the randomized coordinate descent operator. While the computational cost of computing the derivative of a model parameter is constant, we make the observation that the gains in variance reduction are proportional to the magnitude of the derivative. In this paper, we show that a sparse gradient based on the magnitude of past gradients reduces the computational cost of model updates without a significant loss in variance reduction. Theoretically, our algorithm is at least as good as the best available algorithm (e.g. SpiderBoost) under appropriate settings of parameters and can be much more efficient if our algorithm succeeds in capturing the sparsity of the gradients. Empirically, our algorithm consistently outperforms SpiderBoost using various models to solve various image classification tasks. We also provide empirical evidence to support the intuition behind our algorithm via a simple gradient entropy computation, which serves to quantify gradient sparsity at every iteration.", "review": "Review:###Summary: This paper introduces a sparse variant to SpiderBoost which reduces the complexity cost of updating gradient estimates by way of sparse updates. The authors prove that this variant incurs a negligible increase in worst case complexities as soon as certain assumptions are satisfied, and that when their algorithm captures sparsity correctly, they improve upon SpiderBoost*s complexity. This paper is clearly, and the experiments support the theoretical contributions. In Figure 1, you report results as a function of gradient queries/N. Given Theorem 2, I assume that the graphs would look similar as a function of wall-clock time; can you confirm this? Recommendation: Accept. Minor comments and questions for the author: - I am slightly confused by the introduction of the rtop operator. Specifically, 1) What is the relation between k1, k2, and k? 2) You write that S is a random subset of size k. Should this be k2? 3) In your first example, should we have rtop(x,y) = (0, 16, 0, 0, 1), since for ell = 2, y_ell = 4, d-k1 = 4, k2=1? Am I missing something? More generally, my understanding is that the rtop(x,y) operator randomly sparsifies y based on x, which essentially provides indication of where sparsity would be least harmful; when not sparsifying, rtop applies a rescaling that guarantees unbiased estimates. If this is correct, I would recommend making that intuition more clear early on in the paper, in order to improve upon the clarity of the paper. - You state that rtop is linear in y; since rtop depends on the random variable S, is the claim that E[rtop(x, y+y*)] = E[rtop(x, y)]+E[rtop(x,y*)] (which follows from unbiasedness)? - For your experiments, could you discuss how your choice of hyperparameters relates to the constraints in Theorem 1 and 2? - I believe Table 1 would be more impactful if it also included the initial entropy ratios at the beginning of training, rather than reporting those values below. - Other variance reduction techniques for minibatching focus on choosing the minibatches themselves with non-uniform sampling. Under such a sampling mechanism, do you foresee any complications to using SpiderBoost with Sparse Gradients, eg., decrease in overall sparsity?"}
{"id": "iclr2020_440", "title": "Frequency Analysis for Graph Convolution Network | OpenReview", "abstract": "Abstract:###In this work, we develop quantitative results to the learnablity of a two-layers Graph Convolutional Network (GCN). Instead of analyzing GCN under some classes of functions, our approach provides a quantitative gap between a two-layers GCN and a two-layers MLP model. Our analysis is based on the graph signal processing (GSP) approach, which can provide much more useful insights than the message-passing computational model. Interestingly, based on our analysis, we have been able to empirically demonstrate a few case when GCN and other state-of-the-art models cannot learn even when true vertex features are extremely low-dimensional. To demonstrate our theoretical findings and propose a solution to the aforementioned adversarial cases, we build a proof of concept graph neural network model with stacked filters named Graph Filters Neural Network (gfNN).", "review": "Review:###The authors extend the existing work SGC [1] to a nonlinear version, which addresses the limitations of dealing with nonlinear feature. It further extends the theoretical finding in [1] about the low-pass filtering functionality of graph convolutional networks and shows its advantage in dealing with graph signal processing problem. Evaluation of the proposed method is performed on 7 datasets for node classification task. Pros: 1. This work goes into detail of the theoretical finding of SGC. 2. Authors conduct extensive experiments on multiple datasets. Cons: 1. The proposed graph neural network mode is an extension of the existing model SGC, which address the limitation of SGC to model the node feature nonlinearity. It is a good extension, but the novelty is limited. 2. Authors further extend the theoretical finding in [1] and verify the fact that low-pass filtering functionality of graph neural network provide better noise robustness than two layer MLP. It has some novelty, but the novelty is incremental. 3. In the experiment, the performance gap between SGC and gfNN is very small on noise robustness study and traditional node classification. The performance gap between GCN and gfNN is also small on noise robustness study is also small, which can not support the conclusion *GCN has a risk of overfitting to the noise* 4. The paper has the problem of notation missing, e.g on page 2, Fig1 is never mentioned; on page 7, the notation of *LG* is missed in Fig 5. 5. The code link is given but no code is missing. [1]Wu, Tianyi Zhang, Amauri Holanda de Souza Jr., Christopher Fifty, Tao Yu, and Kilian Q.Weinberger. Simplifying graph convolutional networks. ICML2019"}
{"id": "iclr2020_441", "title": "Guiding Program Synthesis by Learning to Generate Examples | OpenReview", "abstract": "Abstract:###A key challenge of existing program synthesizers is ensuring that the synthesized program generalizes well. This can be difficult to achieve as the specification provided by the end user is often limited, containing as few as one or two input-output examples. In this paper we address this challenge via an iterative approach that finds ambiguities in the provided specification and learns to resolve these by generating additional input-output examples. The main insight is to reduce the problem of selecting which program generalizes well to the simpler task of deciding which output is correct. As a result, to train our probabilistic models, we can take advantage of the large amounts of data in the form of program outputs, which are often much easier to obtain than the corresponding ground-truth programs.", "review": " = Summary A method for a refinement loop for program synthesizers operating on input/ouput specifications is presented. The core idea is to generate several candidate solutions, execute them on several inputs, and then use a learned component to judge which of the resulting input/output pairs are most likely to be correct. This avoids having to judge the correctness of the generated programs and instead focuses on the easier task of judging the correctness of outputs. An implementation of the idea in a tool for synthesizing programs generating UIs is evaluated, showing impressive improvements over the baseline. = Strong/Weak Points + The idea is surprisingly simple and applies to an important problem in program synthesis. + The experiments show that the method works very well in UI-generation domain - The paper repeatedly claims general applicability to program synthesizers, but is only evaluated in the specific domain of UI-generating programs. I have substantial doubts that the approach would work as well in the domains of, e.g., string manipulation, Karel, or data structure transformations. My doubts are based on the fact that there are easily generalizable rules for UIs (no overlaps, symmetry, ...), whereas other domains are less easily described. This creates a substantial gap between paper claims and empirical results. - The writting is somewhat sloppy (see below), which makes it sometimes hard to understand. Names such as *views* are used without explanation, and it*s not explained how a device is an input to a program (yes, I get what this means, but it makes in unnecessarily hard to follow the paper) = Recommendation I would ask the authors to rewrite their paper to make less general claims, but believe that the general idea of judging the correctness of a program (or policy) by evaluating it on different inputs is a powerful concept that would be of substantial value to the wider ICLR audience. Improving the readability of the paper would make me improve my rating to a full accept. = Minor Comments * page 1, par *Generalization challenge*: The second sentence here is 4 lines long and very hard to follow. Please rephrase. * page 2, par 2: *no large real-word datasets exists* -> exist * page 2, par 3: *even when both optimizations of InferUI are disabled*: at this point, the reader doesn*t know about any optimizations of InferUI. * page 4, par 1: *i.e., * - is undefined here (will be defined later on the page) * page 4, par 2: *Generate a candidate program * - in step 2, there are suddenly also , which are never explicitly generated. Either adapt this step, or explicitly generate them in step 2 based on the distinguishing input * page 7 par 2: *We use one screen dimension as the input specification , the second as the distinguishing input* - this confused me, as the paper discussed discovering the distinguishing input (page 4, paragraph *Finding a distinguishing input*), whereas it sounds here like that input is manually selected."}
{"id": "iclr2020_442", "title": "Avoiding Negative Side-Effects and Promoting Safe Exploration with Imaginative Planning | OpenReview", "abstract": "Abstract:###With the recent proliferation of the usage of reinforcement learning (RL) agents for solving real-world tasks, safety emerges as a necessary ingredient for their successful application. In this paper, we focus on ensuring the safety of the agent while making sure that the agent does not cause any unnecessary disruptions to its environment. The current approaches to this problem, such as manually constraining the agent or adding a safety penalty to the reward function, can introduce bad incentives. In complex domains, these approaches are simply intractable, as they require knowing apriori all the possible unsafe scenarios an agent could encounter. We propose a model-based approach to safety that allows the agent to look into the future and be aware of the future consequences of its actions. We learn the transition dynamics of the environment and generate a directed graph called the imaginative module. This graph encapsulates all possible trajectories that can be followed by the agent, allowing the agent to efficiently traverse through the imagined environment without ever taking any action in reality. A baseline state, which can either represent a safe or an unsafe state (based on whichever is easier to define) is taken as a human input, and the imaginative module is used to predict whether the current actions of the agent can cause it to end up in dangerous states in the future. Our imaginative module can be seen as a ``plug-and-play** approach to ensuring safety, as it is compatible with any existing RL algorithm and any task with discrete action space. Our method induces the agent to act safely while learning to solve the task. We experimentally validate our proposal on two gridworld environments and a self-driving car simulator, demonstrating that our approach to safety visits unsafe states significantly less frequently than a baseline.", "review": "Review:###This paper presents a model-based approach to safety in RL, where the agent uses a transition model to plan ahead to avoid actions that can lead it to unsafe states. They call the planning component an imaginative module. The agent takes the baseline state as input - that can be used to define either a safe or unsafe state, that is used in the planning component. The authors claim that using these two techniques they can tackle both the safe exploration (not violating safety constraints during learning) and irreversible side-effects (unintended irreversible behavior due to poorly designed reward-function). They validate their approach on two grid world environments and self-driving car simulators. This paper should be rejected because of the assumptions it makes goes against the very task they are trying to solve. In the sense, the task is trivial given the assumptions they have. 1) The inconsistent assumption regarding the access to trajectories to learn a model. The authors start with the assumption that the agent does not have access to the model (Sec 3) , and they explicitly learn the model. However, in the very next section (Sec 4), the authors assume that they can deploy a number of agents that interact with the environment randomly and collect that data to learn a complete transition model. Note that this assumption is wrong because: If the random data agents are “safe”, i.e., don’t violate any safety constraint or cause any harmful behavior in the environment, then it is equivalent to assuming the agent having access to all the data to learn the model. This is a very big assumption that essentially says the agent has access to the model, which defeats the purpose of the safe-exploration problem. If the random agents are “unsafe”, i.e., they can violate the safety constraint, then it goes against the very claim made about their method being able to respect the constraints throughout the learning process. 2) The assumption about the baseline state(s). This is also a pretty big assumption to have, that is not acknowledged in the paper. If the agent already has the set of all the states it needs to avoid (or the set of states that are safe), then along with the assumption regarding access to the model, solving reversibility is significantly easier task then the general safe exploration problem [1, 2] 3) The results reported in Figure 4 are not statistically significant. The experiments are only run over 3 random seeds [3] 4) Can you give a few more details about the assumptions? In terms of how realistic they are or how essential they are to the method. Things to improve the paper that did not impact the score: - The negative side-effects problem that this work address is only based on reversibility criteria. Claim about learning the dynamics model is sample efficient is unsupported. References: [1] Berkenkamp, Felix, et al. *Safe model-based reinforcement learning with stability guarantees.* Advances in neural information processing systems. 2017. [2] Dalal, Gal, et al. *Safe exploration in continuous action spaces.* arXiv preprint arXiv:1801.08757 (2018). [3] Henderson, Peter, et al. *Deep reinforcement learning that matters.* Thirty-Second AAAI Conference on Artificial Intelligence. 2018."}
{"id": "iclr2020_443", "title": "Accelerating First-Order Optimization Algorithms | OpenReview", "abstract": "Abstract:###Several stochastic optimization algorithms are currently available. In most cases, selecting the best optimizer for a given problem is not an easy task. Therefore, instead of looking for yet another ’absolute’ best optimizer, accelerating existing ones according to the context might prove more effective. This paper presents a simple and intuitive technique to accelerate first-order optimization algorithms. When applied to first-order optimization algorithms, it converges much more quickly and achieves lower function/loss values when compared to traditional algorithms. The proposed solution modifies the update rule, based on the variation of the direction of the gradient during training. Several tests were conducted with SGD, AdaGrad, Adam and AMSGrad on three public datasets. Results clearly show that the proposed technique, has the potential to improve the performance of existing optimization algorithms.", "review": "Review:###Summary: This paper presents an adaptive method which can be used alongside existing accelerated gradient methods. The paper is difficult to read due to mistakes and poorly defined mathematical notation. I believe that the paper is missing reference to related methods. The theoretical analysis in the paper is difficult to follow and provides little insight into the benefits of the proposed approach. Overview: There are many mistakes throughout the paper which have made it difficult to read. Overall, I felt that this paper was missing a discussion of the effect of stochasticity on the proposed method. The issue with measuring the variation in the gradient direction is that in regimes where the gradient noise is dominating the signal the gradient direction at each time step is poorly correlated with overall optimization progress --- thus it seems intuitively ineffective to rely on the gradient direction to adjust the algorithm. 1) At the bottom of page 2, the authors write *knowing that we do not have any knowledge of what this function looks like*. While minor, I would point out that we are able to compute local statistics of the function and so certainly we have _some_ knowledge. 2) The authors claim that no techniques exist which use the variation of the direction of the gradient. One such example is in [1] which uses (in one case) the variation of the gradient direction to determine and appropriate time to restart the momentum computation. 3) In section 3.2, the Adam moment computation is missing a *diag*. Assuming that AMRSGrad is AMSGrad (mistyped), then this term is incorrect and matches Adam. 4) There are many mistakes in the Algorithm 1 box. - The wrong is used in the input (should be ). - The algorithm takes as input a sequence of functions ( ) which are not used. - Within the if statement, . I believe this should be an . It is not clear what the vector is exactly, and then is used afterwards which is also not defined. - The algorithm checks for while the text uses . 5) The first line of section 3.3 is quite worrying: *We assume that if we are able to prove that modifying one optimizer with the proposed method does not alter its convergence, then the same applies for the other optimizers*. This seems like a dangerous assumption to make and should at the very least be carefully verified empirically. Following this, I am not sure what the authors mean by *deterministic* and *non-deterministic* methods. 6) I do not understand the claim above Theorem 2 that . Under what conditions does this hold and how is computed? If I understand correctly, the bound provided in Theorem 2 is worse than that given for gradient descent. Moreover, the bound does not depend on the hyperparameter introduced in Algorithm 1 and provides limited insights into the method. I could not find a proof of Theorem 2 in the paper or appendix. 7) There are serious flaws with the experimental evaluation in this paper. a) There is no tuning over hyperparameter settings for any of the optimizers. b) The basic problems are very limited, even for toy problems. The 1D deterministic quadratic tells us very little about the performance of the optimizer. And the 1D cubic problem is particularly confusing. Unless I am mistaken, the gradient will always have the same sign (3x^2) and thus the acceleration condition will never be triggered. c) I believe that Figure 2 explores stochastic optimization problems which as discussed at top is a crucial evaluation. Unfortunately, due to lack of parameter tuning it is difficult to infer much about the comparison between the methods. d) Figure 4 compares performance variation over changing the threshold. The y-axis scale across each plot changes making the comparison unnecessarily difficult --- the scale should be the same. Minor: - TYPO Line 2, *minimize ---,* - End of intro, MNIST and CIFAR not cited while IMDB is. Citation uses citet not citep. - Bottom of page 2, ** - Top of section 3.2, *The pseudo code of our the method* References: [1] Adaptive Restart for Accelerated Gradient Schemes, Brendan O*Donoghue and Emmanuel Candes"}
{"id": "iclr2020_444", "title": "A Data-Efficient Mutual Information Neural Estimator for Statistical Dependency Testing | OpenReview", "abstract": "Abstract:###Measuring Mutual Information (MI) between high-dimensional, continuous, random variables from observed samples has wide theoretical and practical applications. Recent works have developed accurate MI estimators through provably low-bias approximations and tight variational lower bounds assuming abundant supply of samples, but require an unrealistic number of samples to guarantee statistical significance of the estimation. In this work, we focus on improving data efficiency and propose a Data-Efficient MINE Estimator (DEMINE) that can provide a tight lower confident interval of MI under limited data, through adding cross-validation to the MINE lower bound (Belghazi et al., 2018). Hyperparameter search is employed and a novel meta-learning approach with task augmentation is developed to increase robustness to hyperparamters, reduce overfitting and improve accuracy. With improved data-efficiency, our DEMINE estimator enables statistical testing of dependency at practical dataset sizes. We demonstrate the effectiveness of DEMINE on synthetic benchmarks and a real world fMRI dataset, with application of inter-subject correlation analysis.", "review": "Review:###The paper proposes a neural-network-based estimation of mutual information, following the earlier line of work in [A]. The main focus has been to develop an estimator that can reliably work with small dataset sizes. They first reduce the sample complexity of estimating mutual information by decoupling the network learning problem and the estimation problem by creating a training and validation set and then using the validation set for estimating mutual information. Of course, there is still the problem of learning the network with smaller sized data. For this, they propose the strategy of creating multiple tasks from the same dataset, where the dataset is run through transformations that do not affect mutual information. I am inclined to accept (weak) the paper for the following reasons: 1. The paper uses some nice ideas to reduce the variance of the MI estimates and to be able to work with smaller dataset sizes. Both splitting data into training and validation and then using task augmentation to make learning robust are pretty nice ideas. 2. The results on the synthetic datasets show that the resulting estimator does have low variance and the estimates are less than or equal to the true MI value, which is consistent with the fact that it is a lower bound estimation. 3. The results on fMRI dataset were interesting and showed that the method gives improvements over baselines were the estimates were made on a smaller sized dataset. Some improvement suggestions: 1. I don*t see why MINE cannot be applied to the fMRI dataset and results be reported. I know that the variance in estimation is large, but it would still be useful to look at the performance of MINE in comparison to DEMINE. 2. There are several errors in the writing - hyperparamters in Abstract, repetition of the word *Section* in fMRI experiment section etc. - which needs to be fixed. [A] Mutual Information Neural Estimation, ICML 2018"}
{"id": "iclr2020_445", "title": "Adjustable Real-time Style Transfer | OpenReview", "abstract": "Abstract:###Artistic style transfer is the problem of synthesizing an image with content similar to a given image and style similar to another. Although recent feed-forward neural networks can generate stylized images in real-time, these models produce a single stylization given a pair of style/content images, and the user doesn*t have control over the synthesized output. Moreover, the style transfer depends on the hyper-parameters of the model with varying ``optimum* for different input images. Therefore, if the stylized output is not appealing to the user, she/he has to try multiple models or retrain one with different hyper-parameters to get a favorite stylization. In this paper, we address these issues by proposing a novel method which allows adjustment of crucial hyper-parameters, after the training and in real-time, through a set of manually adjustable parameters. These parameters enable the user to modify the synthesized outputs from the same pair of style/content images, in search of a favorite stylized image. Our quantitative and qualitative experiments indicate how adjusting these parameters is comparable to retraining the model with different hyper-parameters. We also demonstrate how these parameters can be randomized to generate results which are diverse but still very similar in style and content.", "review": "Review:###The paper proposed a generative model for image style transfer in real time. In particular, comparing to the existing work, the proposed method is able to generate a series of transferred images instead of one, and more importantly, users can adjust different parameters without re-training the network to control over the synthesized output. The proposed method was evaluated on publicly-available datasets, and achieved convincing experimental results. Pros: - The problems that the paper tried to solve is interesting and important. I think the proposed model could make some contributions to related research communities. - The idea of learning and incorporating *adjustable loss weights* is interesting and reasonable. - The paper is well written and easy to understand. - Experimental results appear to be promising. Cons: - The *adjust loss weights* are able to control the synthesized output, but does each of them have a particular meaning? For example, if I want to generate other outputs more realistic, colorful, or in other styles, is it possible for the users to (roughly) know which parameter should be adjust, and how? - Following the above question, I suggest the authors to provide more qualitative results with different *loss weights*, and give some detailed explanations. - Some typos and missing links could be corrected. I think the proposed approach is reasonable and and experimental results are convincing. I*m not an expert in this area, and I*m not sure about the method*s novelty. If other reviewers find other existing work that proposed very similar ideas, then this would have an impact on my recommendation."}
{"id": "iclr2020_446", "title": "Posterior sampling for multi-agent reinforcement learning: solving extensive games with imperfect information | OpenReview", "abstract": "Abstract:###Posterior sampling for reinforcement learning (PSRL) is a useful framework for making decisions in an unknown environment. PSRL maintains a posterior distribution of the environment and then makes planning on the environment sampled from the posterior distribution. Though PSRL works well on single-agent reinforcement learning problems, how to apply PSRL to multi-agent reinforcement learning problems is relatively unexplored. In this work, we extend PSRL to two-player zero-sum extensive-games with imperfect information (TZIEG), which is a class of multi-agent systems. More specifically, we combine PSRL with counterfactual regret minimization (CFR), which is the leading algorithm for TZIEG with a known environment. Our main contribution is a novel design of interaction strategies. With our interaction strategies, our algorithm provably converges to the Nash Equilibrium at a rate of . Empirical results show that our algorithm works well.", "review": "Review:###Posterior sampling for multi-agent reinforcement learning: solving extensive games with imperfect information ================================================================ This paper investigates the use of Thompson sampling in multi-agent reinforcement learning. They present a natural extension of the PSRL algorithm paired with counterfactual regret minimization, rather than expected reward maximization. They provide support for this algorithm*s efficacy through a theorem that proves polynomial learning rates, together with empirical evaluation where this approach is competitive with state of the art. There are several things to like about this paper: - This paper is definitely *groundbreaking* in that it makes a true extension to the existing literature: PSRL has been relatively well-studied in single-agent RL but never (to my knowledge) in the multi-agent setting. - The extensions from single agent to multi-agent are natural, but also non-trivial, and it seems like this is a genuinely novel piece of work that can be interesting to both side (exploration and multi-agent). - The general structure of the paper and presentation is good. - The support from the theorem is great, and also the empirical evaluation is convincing. There are a few places where the paper might be improved: - It might be helpful to draw the connection to Thompson sampling more explicitly at the start. PSRL is really an application of Thompson sampling principle, but it is important that it doesn*t happen every step but instead on a longer timescale. It might be helpful to cite *a tutorial on thompson sampling* Russo et al. - Do you think there are promising avenues towards PSRL with generalization (rather than tabular)? It feels like actually this should carry over naturally... so maybe you should mention this? - I*m not really an expert on the novelty / impressiveness of this algorithm in the multi-agent setting so cannot fully comment on that. Overall I think this is a really interesting paper that should be of interest to the ICLR community. I can*t say this with full confidence (especially with respect to the multi-agent side) but I do think it*s something that probably would add value to the conference!"}
{"id": "iclr2020_447", "title": "Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors | OpenReview", "abstract": "Abstract:###In this paper, we propose a new loss function for performing principal component analysis (PCA) using linear autoencoders (LAEs). Optimizing the standard L2 loss results in a decoder matrix that spans the principal subspace of the sample covariance of the data, but fails to identify the exact eigenvectors. This downside originates from an invariance that cancels out in the global map. Here, we prove that our loss function eliminates this issue, i.e. the decoder converges to the exact ordered unnormalized eigenvectors of the sample covariance matrix. For this new loss, we establish that all local minima are global optima and also show that computing the new loss (and also its gradients) has the same order of complexity as the classical loss. We report numerical results on both synthetic simulations, and a real-data PCA experiment on MNIST (i.e., a 60,000 x784 matrix), demonstrating our approach to be practically applicable and rectify previous LAEs* downsides.", "review": "Review:###This paper proposes a new loss function for performing principal component analysis (PCA) using linear autoencoders (LAEs). With this new loss function, the decoder weights of LAEs can eventually converge to the exact ordered unnormalized eigenvectors of the sample covariance matrix. The main contribution is to add the identifiability of principal components in PCA using LAEs and. Two empirical experiments were done to show the effectiveness of proposed loss function on one synthetic dataset and the MNIST dataset. Overall, this paper provides a nontrivial contribution for performing principal component analysis (PCA) using linear autoencoders (LAEs), with this new novel loss function. This paper is well presented. There are some issues to be addressed: 1. The output matrix is constrained to be the same size of the input, which is scarcely seen in practical applications. 2. Literature on (denoising) auto-encoder can be reviewed more thoroughly. 3. Comparison with state-of-the-art auto-encoder can be provided to demonstrate the effectiveness of the proposed algorithm. 4. It is better to explain the meaning of each variable when it first appears, e.g., , the projection matrices A and B, and Variable A* in theorem 2. 5. In the experiment part, in both the Synthetic Data or MNIST, the size of each data set is relatively small. It*s better to add experimental results on big data sets with larger dimension. 6. In order to better show the effectiveness of the new loss function, you can add some comparative test for different choice of compressed dimension p. 7. There are some typos, such as ‘faila’ in the second line of the second paragraph in the INTRODUCTION."}
{"id": "iclr2020_448", "title": "Hybrid Weight Representation: A Quantization Method Represented with Ternary and Sparse-Large Weights | OpenReview", "abstract": "Abstract:###Previous ternarizations such as the trained ternary quantization (TTQ), which quantized weights to three values (e.g., {?Wn, 0,+Wp}), achieved the small model size and efficient inference process. However, the extreme limit on the number of quantization steps causes some degradation in accuracy. To solve this problem, we propose a hybrid weight representation (HWR) method which produces a network consisting of two types of weights, i.e., ternary weights (TW) and sparse-large weights (SLW). The TW is similar to the TTQ’s and requires three states to be stored in memory with 2 bits. We utilize the one remaining state to indicate the SLW which is referred to as very rare and greater than TW. In HWR, we represent TW with values while SLW with indices of values. By encoding SLW, the networks can preserve their model size with improving their accuracy. To fully utilize HWR, we also introduce a centralized quantization (CQ) process with a weighted ridge (WR) regularizer. They aim to reduce the entropy of weight distributions by centralizing weights toward ternary values. Our comprehensive experiments show that HWR outperforms the state-of-the-art compressed models in terms of the trade-off between model size and accuracy. Our proposed representation increased the AlexNet performance on CIFAR-100 by 4.15% with only1.13% increase in model size.", "review": " Summary: The paper proposes a hybrid weights representation method where the weights of the neural network is split into two portions: a major portion of ternary weights and a minor portion of weights that are represented with different number of bits. The two portions of weights are differentiated by using the previous unused state of a typical ternary neural network since only three states are used out of the four states given 2-bit representation. The experiments are solid based on the selected baseline model on CIFAR-100 and Imagenet dataset. Pros: • The idea of using the previous unused state in ternary neural network is interesting • Overall, the paper is well written. The proposed method is presented clearly with proper graph illustration. Cons: • The idea of using mixed bit width for neural network quantization is not new. However, the experiments in the paper only compare with basic quantization method which makes the comparison not fair enough. For example, in ABC-net[1], a few full precision coefficients are used to binarize the network. With 3 bit for both weights and activations, it achieves 61% top 1 classification on ImageNet dataset with ResNet-18 as backbone model. This is around 3% higher than the paper’s proposed method with 2/4 bits for weights and 4 bits for activations. • In the paper, it claims that the proposed weight ridge method “can obtain better accuracy than L2 weights decay”. However, there are no experiments or any theoretical supports for it. • After utilizing the forth state of a ternary neural network, it implies that all four states provided by 2 bit representation are used. Hence, the comparison with a quantized neural network of 2 bits should be given in the experiments also. [1] Lin, Xiaofan, Cong Zhao, and Wei Pan. *Towards accurate binary convolutional neural network.* Advances in Neural Information Processing Systems. 2017."}
{"id": "iclr2020_449", "title": "Topology of deep neural networks | OpenReview", "abstract": "Abstract:###We study how the topology of a data set comprising two components representing two classes of objects in a binary classification problem changes as it passes through the layers of a well-trained neural network, i.e., one with perfect accuracy on training set and a generalization error of less than 1%. The goal is to shed light on two well-known mysteries in deep neural networks: (i) a nonsmooth activation function like ReLU outperforms a smooth one like hyperbolic tangent; (ii) successful neural network architectures rely on having many layers, despite the fact that a shallow network is able to approximate any function arbitrary well. We performed extensive experiments on persistent homology of a range of point cloud data sets. The results consistently demonstrate the following: (1) Neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simple one as it passes through the layers. No matter how complicated the topology of the data set we begin with, when passed through a well-trained neural network, the Betti numbers of both components invariably reduce to their lowest possible values: zeroth Betti number is one and all higher Betti numbers are zero. Furthermore, (2) the reduction in Betti numbers is significantly faster for ReLU activation compared to hyperbolic tangent activation --- consistent with the fact that the former define nonhomeomorphic maps (that change topology) whereas the latter define homeomorphic maps (that preserve topology). Lastly, (3) shallow and deep networks process the same data set differently --- a shallow network operates mainly through changing geometry and changes topology only in its final layers, a deep network spreads topological changes more evenly across all its layers.", "review": " ## Summary of the Paper This paper studies changes in topology of two-class neural networks. Specifically, the *Betti numbers*, a measure of topological complexity, are monitored during the training process for different architectures and different activation functions. Results are collected in large empirical study and three main observations are stated: 1. Topological changes are robust across repetitions of the training. 2. Smooth activation functions slow down the topological changes. 3. Initial layers contribute less to topological changes than deeper ones. ## Summary of the Review This is a highly interesting paper and I enjoyed reading about this topic. However, two issues prevent me from accepting this paper in its current form: 1. It is unclear how the stated observations generalise beyond the simple data sets and architectures that are used in the paper. 2. I have some concerns about the technical correctness of the complexity calculations in the paper. Moreover, there are some errors in the description of key concepts in the paper. Given that the claims in this paper are relatively large and somewhat vague---for example the claim that initial layers are responsible for geometric changes, while deeper layers are responsible for topological changes---I suggest revising this paper. Despite my verdict, I want to stress that I very much like this line of research and this type of analysis. This has the potential to become an extremely strong contribution. In the following, I will comment on the afore-mentioned issues and some minor ones. # Issue 1: Generalisation All experiments in this paper are performed for extremely simple architectures and simple data sets. Nonetheless, the paper appears to make very general claims about, for example, the simplification of topology. While the last experiment briefly deals with MNIST, the setup does not go into sufficient details here. Moreover, as the data set is subjected to a transformation before (which is apparently required for computational purposes), this experiment does not exactly deal with the same input manifold as before, making the observations into a stretch. I would suggest experiments on other FCN architectures here and at least one other (image) data set. The employed architecture also seems relatively sparse (10 layers, of 10 neurons each); is the predictive performance of this network for a binary classification task sufficiently good? To add to this: I am perfectly aware that any paper with empirical observations has to start somewhere; I would be perfectly fine with seeing experiments dealing with small FCN architectures on a few well-selected data sets only. But in this case, the claims of the paper need to be slightly rephrased---right now, there is large *leap of faith* from the performed experiments to the generality of the claims. This should be fixed not only by rewriting but also by considering a more detailed setup. For example: do the observations hold for *other* binary problems on MNIST? Are there certain data sets to which the observations do not apply? # Issue 2: Technical correctness The use of Betti numbers as complexity measures is well-known in algebraic topology. In this case, we are dealing with *samples*, though, that are not necessarily as well behaved as a simplicial complex of a known object: Betti numbers of real-world data sets are known to be notoriously unstable; persistent homology exists for this reason. While the paper describes their usage in the appendix, I see some potential issues with the algorithm: - Why is no *witness complex* used for the point clouds? If computational issues are a concern, I would suggest first to try this algorithm, whose stability is known. - The construction of the Vietoris--Rips complex strikes me as slightly ad-hoc. Is there a reason for not using a standard construction first, i.e. Euclidean distance + scale parameter? I am raising this point because the complex construction is *the* crucial step in homology approximation---which is essentially what is happening here. I am worried that the approximation is not stable enough, and I would recommend rather looking at persistence barcodes and use something like *total persistence*; (*Diffusion Runs Low on Persistence Fast, Chen & Edelsbrunner, 2011). In fact, all experiments in this paper are also explainable by noticing that the Vietoris--Rips complex, if sufficiently many connections exist, simply describes the full -dimensional simplex, which has trivial topology, i.e. Betti numbers 1,0,0,...,1. Depending on the algorithm used, simplices in the top dimension might not be used to increase the highest Betti number, leading to the described profile of 1,0,0,...,0. This needs to be carefully check and analysed in order to make it clear that the results are to be trusted! Another issue with this setup is that the Betti numbers that are *possible* to be reported also typically depend on the filtration that is used in the complex. It appears that an integer-based filtration based on the number of edges is used. Does the paper employ the scale values to decide what the Betti numbers are? The provided explanation does not make this sufficiently clear to me. At the very least, I would recommend explaining the choices in filtration and distance in more detail. In particular the parameter of the KNN graph used to calculate geodesic distances needs to be explained more---how stable is this choice? Does it affect the results in any way? Why is the *ordinary* distance filtration, potentially with scale factors to ensure comparability between layers, not sufficient? - To add to the points above: what if the topological changes during training happen at different scales? The way the Vietoris--Rips complex is constructed, this cannot be detected. - In Figure 13, the *reduction algorithm* is not explained in a sufficient amount of detail. Which one is used to ensure that the topology does not change? # Clarity The paper is lacking clarity at several places and uses erroneous description of some concepts: - while homeomorphic maps preserve topology, the statement in the abstract only holds if the network is sufficiently wide to encompass the intrinsic dimension of the manifold. Or am I misunderstanding this sentence? It seems to rule out a lot of architectures---in particular any architecture with a small bottleneck; even the MNIST reduction from 50D to 10D technically does *not* qualify as a homeomorphism any more---unless the concept of intrinsic dimensionality is invoked, in which case more details are required. - I slightly disagree with muddling the terms *topology* and *shape*. Technically, topological information is coordinate-free and only defined up to homotopy. For the sake of precision, I would suggest using only topology here. - The Betti numbers provided in many examples are wrong. The standard torus has Betti numbers (1,2,1), while the solid torus is , i.e. the product of a circle and a disk. But the disk is homotopy equivalent to a point, which means that the solid torus has Betti numbers (1,1,0), _not_ (1,2,0) as claimed multiple times in the paper. This strikes me as somewhat worrisome, to be honest, because the Betti numbers are an essential concept to understand the rest of the paper. - The terms *geometric* vs. *topological* are used without defining what is meant by *geometry*. To my understanding, the geometry cannot be measured by Betti numbers alone. The claim that initial layers thus perform geometrical changes needs to be defended somewhat better. - The digression of floating point precision strikes me as a somewhat unnecessary tangent; I understand that homeomorphism are restricted, but even if all activation functions are homeomorphisms, the network architecture *itself* could account for topological changes---or am I misunderstanding something here? Is the claim about homeomorphism to be taken in terms of each individual neuron*s value? Nevertheless, I like the discussion from a technical point of view. - The formulation *is far from a homeomorphism* strikes me as slightly odd; I would rephrase that; the paper is not discussing homotopies, after all. - The sentence *We posit that...* on p. 3 is hard to understand---why does it invalidate the approximation point of view? The paper is *only* looking at the topology, but neglecting the geometry of the data entirely. It could very well be that topology---or at least Betti numbers---is too coarse to describe the changes in the network. - Dimension is a topological invariant, but it should be made clear if one is talking about *intrinsic* or *ambient* dimension. The whole idea of the manifold hypothesis is that the intrinsic dimension is low. - A dense + uniform sampling does not necessarily guarantee that Betti numbers can be recovered correctly (p. 4); more details are required, maybe a brief link to a section in the appendix - On p. 5, the Betti numbers for the filled torus are wrong (see above); they should be (1,1,0). Likewise, the Betti numbers for the genus two surface are wrong; they should be (1,4,1). - The claim about the scale for the Vietoris--Rips complex is factually correct, but the statement is misleading insofar as the paper only extracts the complex at a single threshold, whereas persistent homology integrates the multi-scale aspect of real-world data sets. - I do not understand the choice of showing *half* a standard deviation in the plots. Is it not customary to show the mean plus/minus one standard deviation? - Why is the variance between different runs in Figure 6 so high? - In Figure 7, the -axis should be scaled to be the same for all plots in order to make the comparison easier - In Table 1, multiple values are already shown; why not show persistence barcodes? - On p. 17, homeomorphism should be replaced by homomorphism - On p. 17, boundary operators are not required to satisfy Eq. 4, but rather Eq. 4 is the *fundamental lemma* of simplicial homology. I would suggest rewriting this in terms of the standard boundary operator as it will simplify the exposition. # Related work Some related work on topology and deep neural networks is missing. I would suggest citing: - On Characterizing the Capacity of Neural Networks using Algebraic Topology, https://arxiv.org/abs/1802.04443 - Neural Persistence: A Complexity Measure for Deep Neural Networks Using Algebraic Topology, https://openreview.net/forum?id=ByxkijC5FQ Moreover, there is a lot of ongoing research with respect to the information bottleneck principle that should be at least briefly mentioned. Here is a good starting point: - Deep Learning and the Information Bottleneck Principle, https://arxiv.org/abs/1503.02406 # Minor style issues The paper is well-written for the most part. Here are some minor style issues that I found: - *citep* should be used; the citations are currently intermingled with the text - *on training set* --> *on a training set* - The union of and is supposed to be disjoint; the paper could employ a disjoint operator here to make this more explicit; I would recommend *cupdot*. - *reduction is computation* --> *reduction in computation* - *it is a routine to verify* --> *it is a routine proof* - *take quotient* --> *take the quotient*"}
{"id": "iclr2020_450", "title": "Continual Learning using the SHDL Framework with Skewed Replay Distributions | OpenReview", "abstract": "Abstract:###Human and animals continuously acquire, adapt as well as transfer knowledge throughout their lifespan. The ability to learn continuously is crucial for the effective functioning of agents interacting with the real world and processing continuous streams of information. Continuous learning has been a long-standing challenge for neural networks as the repeated acquisition of information from non-uniform data distributions generally lead to catastrophic forgetting or interference. This work proposes a modular architecture capable of continuous acquisition of tasks while averting catastrophic forgetting. Specifically, our contributions are: (i) Efficient Architecture: a modular architecture emulating the visual cortex that can learn meaningful representations with limited labelled examples, (ii) Knowledge Retention: retention of learned knowledge via limited replay of past experiences, (iii) Forward Transfer: efficient and relatively faster learning on new tasks, and (iv) Naturally Skewed Distributions: The learning in the above-mentioned claims is performed on non-uniform data distributions which better represent the natural statistics of our ongoing experience. Several experiments that substantiate the above-mentioned claims are demonstrated on the CIFAR-100 dataset.", "review": "Review:###The paper suggest to use the previously proposed ScatterNet Hybrid Deep Learning (SHDL) network in a continual learning setting. This is motivated by the fact that the SHDL needs less supervised data, so keeping a small replay buffer can be enough to maintain performance while avoiding catastrophic forgetting. My main doubt is the benchmark and evaluation of the proposed method. The metrics reported are all relative to a baseline value (which I could not find reported), and make it difficult to understand how the model is performing in absolute term. This is particularly a problem when comparing with existing state of the art method (Fig. 3, Table 4), since this does not exclude that they may have an overall much better accuracy in absolute terms. Also concerning the comparison with the previous literature, I could find no details about the architecture and the training algorithm used. Notice that this may in particular affect some the reported metrics, since they depend on the shape of the training curve (reporting the training curves for all methods may also be useful). Also, since SHDL uses a small replay buffer, are EWC and the other method modified to use the replay buffer and make the comparison fair? While several standard tests for continual learning exists (for example the split CIFAR10/100 in Zenke et al., 2017), those are not used, and rather a simpler test is used which only attempt to learn continually two datasets. It would be helpful to also report a direct comparison on those tests. Regarding the line: *The autoencoder is jointly trained from scratch for classes of both phases to learn mid-level features*, does this mean that the auto-encoder is trained using data of the two distributions at the same time rather than one after the other? If it is the former case, while it is unsupervised training, it would be a deviation from the standard continual learning framework and should clearly be stated."}
{"id": "iclr2020_451", "title": "Fairness with Wasserstein Adversarial Networks | OpenReview", "abstract": "Abstract:###Quantifying, enforcing and implementing fairness emerged as a major topic in machine learning. We investigate these questions in the context of deep learning. Our main algorithmic and theoretical tool is the computational estimation of similarities between probability, ```a la Wasserstein**, using adversarial networks. This idea is flexible enough to investigate different fairness constrained learning tasks, which we model by specifying properties of the underlying data generative process. The first setting considers bias in the generative model which should be filtered out. The second model is related to the presence of nuisance variables in the observations producing an unwanted bias for the learning task. For both models, we devise a learning algorithm based on approximation of Wasserstein distances using adversarial networks. We provide formal arguments describing the fairness enforcing properties of these algorithm in relation with the underlying fairness generative processes. Finally we perform experiments, both on synthetic and real world data, to demonstrate empirically the superiority of our approach compared to state of the art fairness algorithms as well as concurrent GAN type adversarial architectures based on Jensen divergence.", "review": "Review:###The authors propose a method for adding an approximate disparate impact loss to a classification objective, and show that optimizing a classifier for this loss leads to *fairer* predictions with little or no accuracy loss. The authors first formulate two notions of fairness in terms of earth mover distance between the distribution of scores conditioned on either value of a protected variable. They then show that the dual formulation of the earth mover distance can be approximated (specifically, lower-bounded) by optimizing the parameters of a neural network under spectral norm constraints. This leads to a min-max global optimization scheme to learn a fair classifier. Strengths: The proposed method does better on the considered fairness metric than a GAN model with similar accuracy. Weaknesses: The paper is difficult to read, glosses over some important details, and contains some inaccuracies. -- Clarity: --- The authors need to better describe the assumptions (or lack thereof) made on the joint distribution of X, S, and Y. --- Measures such as the quantiles or probability laws need to be formally defined before they are used in definitions. --- The mathcal{L} notation is overloaded (it is used for probability laws, conditional and unconditional, as well as marginals, with mathcal{L}_1 referring to both!), leading to potential confusion. --- The domain of X and Y in equation (2) is not defined anywhere, neither is the distance. --- Similar lack of consistency with the use of F / mathcal{F} / hat{f}, without any explicit parameterization --- Figure 2 needs to be in Section 4, and Table 1 needs to be trimmed to size -- Overlooked problems: --- In the dual formulation, the optimization is done over a sub-set of Lipschitz function, hence approximation of mathcal{W} is a lower bound at every step. Minimizing a lower bound on a loss can be justified, but requires more discussion --- The trade-off inherent in the choice of n_w in algorithm 1 needs to be further discussed, especially in the case of large datasets where a full epoch of SGD in the inner loop of the optimization process is impractical -- Inaccuracies: The graphical models in Figures 1 and 2 and conditional independences written in the text are not consistent: --- In Figure 1, X is NOT independent of S given Y (neither is Y*) (see: V structures in a directed graphical model) --- In Figure 2, X* is independent of S regardless of conditioning on Y Considering all of the above issues, the paper is not currently ready for publication"}
{"id": "iclr2020_452", "title": "Toward Understanding Generalization of Over-parameterized Deep ReLU network trained with SGD in Student-teacher Setting | OpenReview", "abstract": "Abstract:###To analyze deep ReLU network, we adopt a student-teacher setting in which an over-parameterized student network learns from the output of a fixed teacher network of the same depth, with Stochastic Gradient Descent (SGD). Our contributions are two-fold. First, we prove that when the gradient is zero (or bounded above by a small constant) at every data point in training, a situation called emph{interpolation setting}, there exists many-to-one emph{alignment} between student and teacher nodes in the lowest layer under mild conditions. This suggests that generalization in unseen dataset is achievable, even the same condition often leads to zero training error. Second, analysis of noisy recovery and training dynamics in 2-layer network shows that strong teacher nodes (with large fan-out weights) are learned first and subtle teacher nodes are left unlearned until late stage of training. As a result, it could take a long time to converge into these small-gradient critical points. Our analysis shows that over-parameterization plays two roles: (1) it is a necessary condition for alignment to happen at the critical points, and (2) in training dynamics, it helps student nodes cover more teacher nodes with fewer iterations. Both improve generalization. Experiments justify our finding.", "review": "Review:###This paper studies the learning of over-parameterized neural networks in the student-teacher setting. More specifically, this paper assumes that there is a fixed teacher network providing the output for student network to learn, where the student network is typically over-parameterized (i.e., wider than teacher network). This paper first investigates the properties of critical points of student networks in the ideal case, i.e., assuming we have infinite number of training examples. Then the results have been generalized to a practical case (the gradient is smaller than some small quantity). Moreover, this paper further studies the training dynamics via gradient flow, and proves some convergence results of GD. Overall, this paper is somewhat difficult to follow and understand. The notation system is kind of complicated and some assumptions seem to be unrealistic. Detailed comments are as follows: It is a little bit difficult to get insightful understandings towards the critical points of deep neural networks from the theorems provided in this paper. I would like to see clearer properties of the critical points learned by student network rather than some intermediate results. The title is not consistent with the content of the paper. From the title of this paper looks like a characterization on the student network trained by SGD. However, throughout the paper, the authors somehow investigate the critical points under a stronger condition, i.e., all stochastic gradient is zero, rather than the widely used one, the expectation of stochastic gradient is zero. I don’t think the critical points considered in this paper can be guaranteed to be found by SGD. Besides, when analyzing the training dynamics, as provided in Section 5, the authors resort to gradient descent, because in (5) the dynamics of rely on the expectation of stochastic gradients. Many statements should be elaborated in detail. For example, in the paragraph before Corollary 1, why is a convex polytope? In Theorem 2, what’s ? What’s the meaning of alignment? In the paragraph after Theorem 4, why Theorem 4 suggests a picture of bottom-up training? I believe the authors should provide a more detailed explanation. This paper studies the over-parameterized student network, is there any condition on its width? In Theorem 5, the assumption seems rather unrealistic, typically this bound can only hold in expectation or with high probability. Besides, why there is no condition on the sample size n in Theorem 5? It looks like Theorem 5 aims to tackle the case of finite number of training samples. ----------------------------------- Thanks for your response and revision. The current title is clearer and the definition of SGD critical points is more accurate. The observations regarding the alignment between teacher and student networks are indeed interesting. However, I still feel that this result is somehow difficult to parse, as I am not clear why this can be interpreted as the learning of the teacher network. Therefore I would like to keep my score."}
{"id": "iclr2020_453", "title": "Robust saliency maps with distribution-preserving decoys | OpenReview", "abstract": "Abstract:###Saliency methods help to make deep neural network predictions more interpretable by identifying particular features, such as pixels in an image, that contribute most strongly to the network*s prediction. Unfortunately, recent evidence suggests that many saliency methods perform poorly when gradients are saturated or in the presence of strong inter-feature dependence or noise injected by an adversarial attack. In this work, we propose a data-driven technique that uses the distribution-preserving decoys to infer robust saliency scores in conjunction with a pre-trained convolutional neural network classifier and any off-the-shelf saliency method. We formulate the generation of decoys as an optimization problem, potentially applicable to any convolutional network architecture. We also propose a novel decoy-enhanced saliency score, which provably compensates for gradient saturation and considers joint activation patterns of pixels in a single-layer convolutional neural network. Empirical results on the ImageNet data set using three different deep neural network architectures---VGGNet, AlexNet and ResNet---show both qualitatively and quantitatively that decoy-enhanced saliency scores outperform raw scores produced by three existing saliency methods.", "review": "Review:###The authors tackle the important problem of generating saliency maps. First, a decoy image is defined (in short, it is a perturbed version of the image that leads to very similar activations) and then, the method that leverages decoy images is proposed. The method can be understood as a improvement that can be applied to enhance an existing saliency extractor. For a given image, the method generates a bunch of decoys images, and then a given saliency extractor is applied to not only the original image but all generated decoy images. The resulting saliency maps are aggregated to output the final saliency map. I found this idea technically sound. However, there are two keys reasons why this paper should be rejected. (1) The idea of using decoy images is interesting but computationally expensive. In practice (as showed in the paper) using blurred images leads to comparable results. (2) The paper misses one very important experiment which is a quantitative comparison with the existing works on Imagenet ILSVRC’14 localization task. This is the standard experiment and is used in a few works cited (Fong & Vedaldi, 2017; Dabkowski & Gal, 2017). Even though the idea of using decoy images is technically sound, I find the algorithm for generating these decoy images very complex and computationally expensive. First, under Eq.6 one can find *Our strategy is to set c to a small value initially and run the optimization. If it fails, then we double c and repeat until success.* And then below Eq.8 there is *After each iteration, if the second term in Equation 8 is zero, indicating that ? is too large, then we reduce ? by a factor of 0.95 and repeat*. Hence, the algorithm is nested and the paper does not provide any details how long the procedure is in practice. On top of that, since a population of decoy images is needed (12 in the experiments), this expensive procedure has to be run a few times. When decoy images are replaced with blurry images the results are almost the same and hence, unfortunately, using decoy images is not justified. In general, the experimental results are decent and prove the possible benefits of using the population of images (decoys or blurred ones). However, I think that Imagenet ILSVRC’14 localization task is the standard experiment that provides the quantitative view and should be performed. The paper evaluates on Imagenet data set already, and hence adding this experiment should be straightforward. The paper does a good job motivating the problem and covering related work. The paper states that the key limitation of existing saliency maps is that they *evaluate pixel-wise importance in an isolated fashion by design, implicitly assuming that other pixels are fixed* and *the presence of gradient saturation*. While it is valid for gradient-based methods, it is not for perturbation-based methods. The latter are just briefly mentioned and then it is stated that *for any perturbation-based method, a key challenge is ensuring that the perturbations are effective yet preserving the training distribution*. However, I do not find this reason to be strong enough to exclude these work from consideration, because there are works that train saliency extractor and the classifier simultaneously and then the argument mentioned does not hold (Fan et al., 2017; Zolna et al., 2018). There is one more thing that I would like to ask about. In Eq.3, Z_j is defined as max(E˜_j ) ? min(E˜_j ). Hence, if a given pixel is very important for all decoy images, all elements of E˜_j will be high and then Z_j will be very small. As a result, a saliency value assigned to this pixel will be low which seems to be counter-intuitive. Can you please elaborate on that? (Fong & Vedaldi, 2017): Ruth C Fong, and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturbation (Dabkowski & Gal, 2017): Piotr Dabkowski, and Yarin Gal. Real time image saliency for black box classifiers (Fan et al 2017): Lijie Fan, Shengjia Zhao, and Stefano Ermon. Adversarial localization network (Zolna et al 2018): Konrad Zolna, Krzysztof J. Geras, and Kyunghyun Cho. Classifier-agnostic saliency map extraction"}
{"id": "iclr2020_454", "title": "Attention Privileged Reinforcement Learning for Domain Transfer | OpenReview", "abstract": "Abstract:###Applying reinforcement learning (RL) to physical systems presents notable challenges, given requirements regarding sample efficiency, safety, and physical constraints compared to simulated environments. To enable transfer of policies trained in simulation, randomising simulation parameters leads to more robust policies, but also in significantly extended training time. In this paper, we exploit access to privileged information (such as environment states) often available in simulation, in order to improve and accelerate learning over randomised environments. We introduce Attention Privileged Reinforcement Learning (APRiL), which equips the agent with an attention mechanism and makes use of state information in simulation, learning to align attention between state- and image-based policies while additionally sharing generated data. During deployment we can apply the image-based policy to remove the requirement of access to additional information. We experimentally demonstrate accelerated and more robust learning on a number of diverse domains, leading to improved final performance for environments both within and outside the training distribution.", "review": " Overall the method is interesting but I am not overly convinced the method provides a significant improvement over simpler methods that are not compared to the work here. Also, the generalization and extrapolation experiments need more description. As well, the work talks about how this method can be used to accelerate learning for transfer to real-robotic systems but does not have an example of this. More detailed comments: - When you are referencing a paper that is very similar to your method and you build off of it is good to link to the published version ( the reference for Asymmetric DDPG was published at RSS2018.) - While the method is very interesting I feel that one of the obvious baselines that would be good to try is to train a model that maps o -> s (image to state). This is a very simple method and appears to accomplish the goals for the authors. A discussion on why this would not be good enough and some results to show that this performs poor would be good addition. - Related to the last point why are object-specific segmentation maps needed? It seems like the method alone was not capable enough to learn from pixels alone. Also, Can these segmentation maps be visually described? It is not very clear. Where do the segmentation maps come from? - Can you describe the shared replay buffer in more detail? What does no shared replay buffer mean? - The caption in Figure 2 is rather sparse and comes before the explication of the different methods in the figure. This figure needs more explanation. What are the other versions of April? What is the baseline? There does not appear to be a significant improvement? Can you show more of a qualitative improvement via videos of the policy performance? - It appears the method does not work as well on the Atari games can the authors provide some insights into why the method does not offer as much of an improvement? Is there no Attentive DQN for Pong, natural? - I do not understand what experiment is being performed in section 4.3.2. Maybe it would help if you explained what a canonical image is? Where do you get the compressed state information? - In 4.4 you show that APRiL does not do better than the baseline? What is the baseline? - In section 4.5 you describe how the method generalizes to the task with additional distractors. Can you describe specifically how this experiment is designed? How do the distractors compare to the other objects in the scene? are their colors changed? How many are there? This section needs a lot more detail to understand how much the reader can surmise about the methods ability to generalize. - In the same section, the authors say the image segmentation is crucial. Does this imply that maybe we should just put an image segmentation network in between the observation and the policy inputs? What is using this additional attention mechanism better than that simpler method? - For Table 1: It is interesting that the baseline does better than the APRiL no share or no sup. It might be important to perform an exploration as to why having those features reduces performance and the baseline appears more robust. - It would be nice to see this work applied to more tasks. For now the reacher and walking are the most interesting but those tasks are also somewhat basic. How would this method work on the walker 3d where there is even more partial observation? - Can the authors explain more how they have deduced from the attention map that the attention has figured out how to indirectly discover where the Jaco links are without explicit attention? I do not see this. - For figure 4, how the attention is visualized could be explained better. Are the white regions receiving all the attention? Also, What makes the difference between a Interpolation example and a Extrapolation example? Is it just more objects in the scene? - The authors should also reference *Sim-to-Real Transfer of Robotic Control with Dynamics Randomization*."}
{"id": "iclr2020_455", "title": "QXplore: Q-Learning Exploration by Maximizing Temporal Difference Error | OpenReview", "abstract": "Abstract:###A major challenge in reinforcement learning is exploration, especially when reward landscapes are sparse. Several recent methods provide an intrinsic motivation to explore by directly encouraging agents to seek novel states. A potential disadvantage of pure state novelty-seeking behavior is that unknown states are treated equally regardless of their potential for future reward. In this paper, we propose an exploration objective using the temporal difference error experienced on extrinsic rewards as a secondary reward signal for exploration in deep reinforcement learning. Our objective yields novelty-seeking in the absence of extrinsic reward, while accelerating exploration of reward-relevant states in sparse (but nonzero) reward landscapes. This objective draws inspiration from dopaminergic pathways in the brain that influence animal behavior. We implement the objective with an adversarial Q-learning method in which Q and Qx are the action-value functions for extrinsic and secondary rewards, respectively. Secondary reward is given by the absolute value of the TD-error of Q. Training is off-policy, based on a replay buffer containing a mix of trajectories sampled using Q and Qx. We characterize performance on a set of continuous control benchmark tasks, and demonstrate comparable or faster convergence on all tasks when compared with other state-of-the-art exploration methods.", "review": "Review:###The paper proposes an exploration method based on the TD-error as an adversarial reward signal. The authors show that this reward signal has interesting exploration properties. They compare it empirically to RND and epsilon-greedy, showing better performance. Besides, they perform additional ablation studies to better investigate the properties of their approach. Though the paper proposes an interesting concept, it suffers from many weaknesses, some easy to fix and some which will require more work. Here are some remarks, in random order: - at the end of the introduction, the authors mention inspiration from computational neuroscience models, but they do not come back to this aspect in their work. To me, these remarks should be removed from the paper and kept for another paper about the biological significance of the model. In the conclusion, the authors come back to the *biological concepts of curiosity boredom, and exploration*, I would rather say they are psychological concept, and the authors should have a look at developmental psychology and developmental robotics if they really want to contribute in this respect (but not for this paper). - some related work references are dispersed in the introduction, in Section 2, in the beginning of Section 3.2, in the end of Section 3.3 and in a few other places. The authors should build a proper *related work* section. Globally, the paper is poorly organized, e.g. Section 3.2 refers to Section 3.4 etc. - given that Q_x*s reward function is the unsigned TD-error, I would like to see whether QXplore can deal with problems taking both some positive and negative rewards. - the authors mention RND and DORA as baselines, but only compare to RND. What about DORA? Despite the excitement it generated when published, I suspect RND is a rather weak baseline. There are many other exploration frameworks, the paper would be much stronger if the comparison was with respect to many other methods, such as GEP-PG (Colas et al. , ICML 18), Novelty search approaches, etc. To me, the weak comparison is the biggest weakness of this work. - the authors compared themselves to approaches based on TRPO while they were using TD3 (this information is hidden in Appendix A and should be moved in the main paper. But then, is the difference in performance due to using TRPO which is known to be less sample efficient? Again, this makes the comparison very weak, the authors should rely on the same algorithm from both sides. - the caption of Table 1 should be explicit about which algorithm comes from which paper. - I found Section 4.5 very weak, it looks like mere *handwaving* and would deserve a proper quantitative study if the authors want to keep it. In my opinion, the authors should remove it for now and move Appendices E and F to the main paper instead. As is, Appendix E is very poor (by the way, the caption and the figure legend do not match, so we don*t know which is which) and I had to look for the number of seeds until I found it hidden in Appendix F. - Appendix C shows that, though the authors try to minimize the importance of this fact, their algorithm is very sensitive to initialization. It is very honest of them to have kept this study in their paper, but I*m afraid it strongly speaks against the algorithm. - As described, SparseHaflCheetah is not that sparse (-1 everywhere and 0 when you succeed, as this reward scheme already favors exploration). It would be more informative to use 0 everywhere and 1 when successful. typos: Eq (1) and (3) should finish with a dot as it is the end of a sentence. p3: MDP*s => MDPs Section 3.3, second sentence: avoid starting a sentence with a symbol. Section 4, the second sentence is not a sentence (no main verb) p14: is also no subject to it => not"}
{"id": "iclr2020_456", "title": "A Closer Look at Deep Policy Gradients | OpenReview", "abstract": "Abstract:###We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: surrogate rewards do not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the *true* gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods.", "review": "Review:###[Summary] This paper empirically studies the behavior of deep policy gradient algorithms during the optimization. The conclusion is that, while these methods generally improve the policy, their behavior does not comply with the underlying theoretical framework. First, sample gradients obtained with a reasonable batch size have little correlation with each other and with the true gradient. Second, a larger batch size requires a smaller step-size. Third, the value baseline is far from true values and only marginally reduces variance, yet it considerably helps with optimization. Finally, the optimization landscape highly varies with the choice of objective function and the number of samples used to estimate it. [Decision] I vote for acceptance. To the best of my knowledge, the findings of this paper are new and not predictable by the current theory. These negative results have some merit as they call for theory that explains the behavior of these algorithms, or an algorithm whose behavior is predictable by the current theory. The paper is well-written, with a few small issues in presentation that should to be addressed in the final revision. [Comments] In Fig. 4 (b) it does not look like that the value error is high. It is said that *the learned value function is off by about 50% w.r.t. the underlying true value function.* This sentence should be clarified or visualized. What is pi in Eq (13) in A1? If it is the agent*s current policy, how is it different than pi_\theta? If pi corresponds to the distribution of state-action pairs in the replay buffer, how can one obtain a policy pi that has led to this distribution of states in order to construct the importance sampling ratio? In 2.2, the claim that a learned value baseline results in significant improvement in performance should be supported by results or reference to previous work. Figs. 6 and 7 compare the loss surface with different objectives and sample regimes. Do these factors (objective and sample size) affect the part of the parameter space that is visualized (by changing the origin and the update direction), or are they only used to evaluate the values on the z-axis for the same area in the parameter space? Observing a different landscape in a different part of the parameter space is not surprising. [Minor comments] - Is V_\theta_{t-1} in Eq (4) a function of state? If so, a (s_t) is missing before the plus sign."}
{"id": "iclr2020_457", "title": "UNIVERSAL MODAL EMBEDDING OF DYNAMICS IN VIDEOS AND ITS APPLICATIONS | OpenReview", "abstract": "Abstract:###Extracting underlying dynamics of objects in image sequences is one of the challenging problems in computer vision. On the other hand, dynamic mode decomposition (DMD) has recently attracted attention as a way of obtaining modal representations of nonlinear dynamics from (general multivariate time-series) data without explicit prior knowledge about the dynamics. In this paper, we propose a convolutional autoencoder based DMD (CAE-DMD) that is an extended DMD (EDMD) approach, to extract underlying dynamics in videos. To this end, we develop a modified CAE model by incorporating DMD on the encoder, which gives a more meaningful compressed representation of input image sequences. On the reconstruction side, a decoder is used to minimize the reconstruction error after applying the DMD, which in result gives an accurate reconstruction of inputs. We empirically investigated the performance of CAE-DMD in two applications: background/foreground extraction and video classification, on publicly available datasets.", "review": "Review:###This paper presents an application of convolutional autoencoder networks and a nonlinear dynamic systems analysis method known as extended dynamic mode analysis (EDMD) to a data-driven analysis of multivariate time series. The DMD method appears to be well-known in the physics community but is outside my area of expertise and unfortunately I have limited time to make a quick study of it. However, from what I gather, it involves empirical approximation of a nonlinear dynamical system as a high-dimensional linear dynamical system, which in turn enables analysis in terms of eigendecomposition of the resulting linear operator, revealing basic modes of the dynamics. In the proposed method, DMD is used in the latent representations of a convolutional autoencoder for image sequences. The DMD objective is incorporated into the autoencoder training loss to minimize its reconstruction error. The DMD is also used, by conditioning on the eigenvalues, to split the reconstruction into high frequency (quickly varying) foreground modes and low-frequency (slowly varying) background modes. Although end-to-end training is mentioned, it is not made clear how the derivatives of the DMD decomposition are implemented, especially considering that the DMD involves an SVD, which can have unstable/ singular derivatives when two or more singular values are close to the same / exactly the same. The resulting methods are applied to a foreground extraction and a classification tasks, and compared with numerous baselines. It is not clear to me what the state of the art is on these tasks, but the proposed methods compare favorably to reported baselines, and the images of results look convincing. However the experimental results seem a little thin and I would expect a more thorough study. Overall the method looks very interesting. Some complaints: - the tables are a bit sloppy and should be formatted to fit in the document with normal sized fonts, - the images are too small to see well."}
{"id": "iclr2020_458", "title": "Test-Time Training for Out-of-Distribution Generalization | OpenReview", "abstract": "Abstract:###We introduce a general approach, called test-time training, for improving the performance of predictive models when test and training data come from different distributions. Test-time training turns a single unlabeled test instance into a self-supervised learning problem, on which we update the model parameters before making a prediction on the test sample. We show that this simple idea leads to surprising improvements on diverse image classification benchmarks aimed at evaluating robustness to distribution shifts. Theoretical investigations on a convex model reveal helpful intuitions for when we can expect our approach to help.", "review": "Review:###The authors propose a method for adapting model parameters by doing self-supervised training on each individual test example. They show striking improvements in out-of-domain performance across a variety of image classification tasks while preserving in-domain performance; the latter is a marked difference from other robustness procedures which tend to sacrifice in-domain performance for out-of-domain (or adversarial) performance. These results are exciting, and I believe that this proposed test-time training method will spur a significant amount of further research into similar approaches. The paper is well-written and the experiments are thorough, so I have no major concerns. Some remaining questions about the proposed approach are: 1) How sensitive is test-time training to hyperparameters like splitting the parameters at the right location (i.e., the particular partitioning of into , , and ), or to the learning rate? Is there a good way to pick these hyperparameters, given that evaluation is on an out-of-domain distribution that we assume we do not have access to at training time? The paper proposes a particular split of parameters and a particular learning rate and number of steps (which differs for standard vs. online training). How were those chosen? 2) How does test-time training compare to methods that assume access to the test distribution? I understand that a big benefit is that test-time training does not need to see the entire distribution (unlike standard domain adaptation approaches). But in cases where we do get to see parts of the test distribution -- say some unlabeled examples from it, or even some labeled examples -- how does test-time training compare? For example, should we see test-time training as providing the benefits of domain adaptation even when we*re unable to access the unlabeled test distribution; or should we see it as doing something beyond what standard domain adaptation methods do, even when we have access to the unlabeled test distribution? Minor comments, no need to respond: a) There are several minor typos in the paper, e.g.: p1, *prediciton*; eqn 8, v; p8, *address*; p9, *orcale*; appendix A, missing ref. b) The discussion in Appendix A seems a bit speculative and opinionated. For example, it is not obvious that one has to fall back on the space of all possible models in order to represent test-time training with a single gradient step as a fixed model. The discussion is useful but in my subjective opinion could be toned down; the experiments and discussion in the main paper are strong and less speculative. === Edit: Thank you for the response. The discussion about hyper-parameters makes sense. My rating edit comes from the realization that the performance improvements obtained are almost entirely from the *online* version, which gets to see the test distribution. So I think the baselines are in lacking in that sense: as a straw man baseline for example, one could simply run the normal model on half of the test set, and then use those observed test examples to do some other sort of domain adaptation training."}
{"id": "iclr2020_459", "title": "CURSOR-BASED ADAPTIVE QUANTIZATION FOR DEEP NEURAL NETWORK | OpenReview", "abstract": "Abstract:###Deep neural network (DNN) has rapidly found many applications in different scenarios. However, its large computational cost and memory consumption are barriers to computing restrained applications. DNN model quantization is a widely used method to reduce the DNN storage and computation burden by decreasing the bit width. In this paper, we propose a novel cursor based adaptive quantization method using differentiable architecture search (DAS). The multiple bits’ quantization mechanism is formulated as a DAS process with a continuous cursor that represents the possible quantization bit. The cursor-based DAS adaptively searches for the desired quantization bit for each layer. The DAS process can be solved via an alternative approximate optimization process, which is designed for mixed quantization scheme of a DNN model. We further devise a new loss function in the search process to simultaneously optimize accuracy and parameter size of the model. In the quantization step, based on a new strategy, the closest two integers to the cursor are adopted as the bits to quantize the DNN together to reduce the quantization noise and avoid the local convergence problem. Comprehensive experiments on benchmark datasets show that our cursor based adaptive quantization approach achieves the new state-of-the-art for multiple bits’ quantization and can efficiently obtain lower size model with comparable or even better classification accuracy.", "review": " This paper is about using quantization to compress the DNN models. The main idea is to use NAS to obtain the mixed precision model. More specifically, it adaptively chooses the number of quantization bit for each layer using NAS by minimizing the cross-entropy loss and the total number of bits (or model size) used to compress the model. The experiment is on CIFAR and ImageNet, and compared with other quantization methods showing better accuracy. I have somes questions on this paper: 1) Is Eq(1) standard for quantization optimization? Any reference? Normally, we aim to minimize the loss on the training set, not the validation set, or sometimes generalization loss on data from the data distribution. 2) For experiment, it is interesting to see how the compression ratio changes over the accuracy--- that is a curve with x-axis on compression ratio, and y axis accuracy so that we can have a sense of how the accuracy and compression rate trade-offed. 3) What is the training time for NAS for finding the *optimal* bits for each layer? Although we might not care much about the training time for compression task, I just want to have a sense of how training works. Also what is the reason for not compressing the first and last layers? Do these two layers taken into account in the final compression ratio computation?"}
{"id": "iclr2020_460", "title": "Scalable Neural Learning for Verifiable Consistency with Temporal Specifications | OpenReview", "abstract": "Abstract:###Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, we extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, our verified training method produces models that both perform well (in terms of test error or reward) and can be shown to be provably consistent with specifications.", "review": "Review:###This paper focuses on verifying sequential properties of deep beural networks. Linear Temporal Logic (LTL) is a natural way to express temporal properties, and has been extensively studied in the formal methods community. Signal temporal logic (STL) is a natural extension, of LTL. STL specifications provide a rich set of formulations to encode intent for real valued signal over time. Formally proving STL formulae is intractable. But, it is possible to falsify such properties. This has been the main goal for various tools like Breach, and S-Taliro. Pros : A very interesting avenue explored in this paper, is using the syntax of STL to formulate properties about multiple-MNIST, Safe RL and NLP applications. Even though the conversion from STL specifications to scalar valued function is a very well known technique. Cons : In my opinion, the paper lacks sufficient contributions in itself to be accepted at this conference. The idea of training for robustness using intervals, has been well known for a while. The authors extend that to get conservative estimates of the level of satisfaction of the STL formula, and use that in the training process. Though training for robustness is an interesting idea in itself, but the general opinion about using interval propagation to train networks is negative. Overall : Though the direction of this work is interesting but lacks sufficient technical novelty."}
{"id": "iclr2020_461", "title": "Learning to Prove Theorems by Learning to Generate Theorems | OpenReview", "abstract": "Abstract:###We consider the task of automated theorem proving, a key AI task. Deep learning has shown promise for training theorem provers, but there are limited human-written theorems and proofs available for supervised learning. To address this limitation, we propose to learn a neural generator that automatically synthesizes theorems and proofs for the purpose of training a theorem prover. Experiments on real-world tasks demonstrate that synthetic data from our approach significantly improves the theorem prover and advances the state of the art of automated theorem proving in Metamath.", "review": "Review:###This paper proposes a generative model for proofs in Metamath, a language for formalizing mathematics. The model includes neural networks, which provide guidance about which fact to try to prove next and how to prove the fact from the facts derived so far. The parameters of these networks are learned from existing proofs or theorem statements. The main purpose of this model is to generate synthetic theorems and proofs that can be used to train the neural networks of a data-driven search-based theorem prover. The experiments with the Metamath set.mm knowledge base show the benefits of the synthetically generated proofs for building a data-driven theorem prover. I think that the paper studies an important problem and contains interesting ideas. The idea of using a language model for theorem statements (so that a generated theorem can be meaningfully compared with a given theorem even when they are not the same) looks sensible. Also, the conjecture that a good proof generator is likely to lead to a good theorem prover sounds plausible. I find the description of the training of the generative model in the experiments slightly confusing. Adding some clarification may help some readers. More specifically, here are some questions that I couldn*t answer for myself. What theory is formalized by set.mm? Set theory? Among the proofs of 29337 theorems, which ones are used during the training of the generative model? Here are some minor comments. * p1: positive awards ===> positive rewards * p2: A citation is missing in the first sentence of Section 2. * AddNode, Algorithm1, p5: Merge h_q to h* ===> Merge h_q to h * p6: uses a_v as a precondition ===> uses a_u as a precondition * p6: and has been ===> has been * p7: which demonstrate ===> which demonstrates * p7: from these the relevance ===> from the relevance * p7: wiht ===> with * p9: languagee ===> language"}
{"id": "iclr2020_462", "title": "Disentanglement through Nonlinear ICA with General Incompressible-flow Networks (GIN) | OpenReview", "abstract": "Abstract:###A central question of representation learning asks under which conditions it is possible to reconstruct the true latent variables of an arbitrarily complex generative process. Recent breakthrough work by Khemakhem et al. (2019) on nonlinear ICA has answered this question for a broad class of conditional generative processes. We extend this important result in a direction relevant for application to real-world data. First, we generalize the theory to the case of unknown intrinsic problem dimension and prove that in some special (but not very restrictive) cases, informative latent variables will be automatically separated from noise by an estimating model. Furthermore, the recovered informative latent variables will be in one-to-one correspondence with the true latent variables of the generating process, up to a trivial component-wise transformation. Second, we introduce a modification of the RealNVP invertible neural network architecture (Dinh et al. (2016)) which is particularly suitable for this type of problem: the General Incompressible-flow Network (GIN). Experiments on artificial data and EMNIST demonstrate that theoretical predictions are indeed verified in practice. In particular, we provide a detailed set of exactly 22 informative latent variables extracted from EMNIST.", "review": "Review:###This paper builds upon the recent theoretical framework on nonlinear ICA, put forward in recent work Khemakhem et al. (2019) that draw a lot of attention. The latter work provides an extension of the basic nonlinear ICA that is closely related to a VAE with a conditional factorized prior, essentially introducing side information (with assumed extra observables u) to resolve non-identifiability issues. This paper proposes an invertible architecture related to RealNVP and NICE, coined as GIN: the General Incompressible-flow network. On the positive side, A key feature of the proposed methodology is model selection (such as determining the model order) that is in general a hard problem even in linear latent variable models. The performance is illustrated on the EMNIST dataset, as well as carefully constructed synthetic experiments. The semantic descriptions of each captured dimension, as detailed in the appendix, is particularly interesting. The experimental section is quite extensive. On the negative side, The paper is largely based on the results of a recent technical report (Khemakhem et al. (2019)) and is not self contained, hence rather hard to digest. Even the proofs in the appendix and the intuition requires the reading of this longer technical report. It is also not at all clear where the side information (variables u) is coming from. On EMNIST a natural candidate is the digit labels (and this turns out to be is indeed the case in the experimental section) but it is not very clear what conditions need to be satisfied. What prevents us selecting simply a subset of observations x as u? Lacking an explicit motivation, the exercise of writing the canonical parameters of a multivariate Gaussian in (2) - (4) is not very informative. This needs to be better motivated. The prior structure resembles quite closely the general hierarchical priors SVAE proposed in https://arxiv.org/abs/1603.06277. It would be also informative to discuss the links with this approach. Arguably, the key contribution of the current paper is in 3.1. and 3.2, but these are rather hastily written and quite short. Overall, the balance of known results and new contributions"}
{"id": "iclr2020_463", "title": "Asymptotics of Wide Networks from Feynman Diagrams | OpenReview", "abstract": "Abstract:###Understanding the asymptotic behavior of wide networks is of considerable interest. In this work, we present a general method for analyzing this large width behavior. The method is an adaptation of Feynman diagrams, a standard tool for computing multivariate Gaussian integrals. We apply our method to study training dynamics, improving existing bounds and deriving new results on wide network evolution during stochastic gradient descent. Going beyond the strict large width limit, we present closed-form expressions for higher-order terms governing wide network training, and test these predictions empirically.", "review": "Review:###The paper investigates the asymptotic behavior of correlation functions of wide networks. The main part of the work revolves around Conjecture 1 which assesses how the correlation function scales depending on the number of connected components of even/odd size of the constructed cluster graph. The authors then study training dynamics of wide networks under gradient flow and (stochastic) gradient descent and present empirical evidence to support their claims. While I am no expert in this particular area, the paper is fairly well written and the main concepts and ideas are outlined nicely in most places. I do have some comments, though, wrt. the notation and the relevance of certain results: (1) In 2.1. the authors state the def. of a deep LINEAR network, however, the activation functions are non-linear. Could you be more clear what you mean by linear in your setting? (2) What are the vertices in Conjecture 1? v_i = T(x_i), right? I think it*s clear from the context (i.e., the definition of edges), but this could be written more precisely. (3) What is the actual relevance of the ORDER of the correlation function in Conjecture 1 and the relevance of the cluster graph. Can the authors motivate this more clearly, or provide some more intuition on this construction? (4) What does the notation x_1 leftrightarrow x_2 in Eq. 8 mean? (5) Thm. 3 - Apparently, the expression for C only depends on the #loops in gamma. Is this independent of the number of connected components (especially, since Conjecture 1 explicitly hinges on #connected components)? (6) Does the analysis equally hold if you consider affine maps (Ax + b ) instead of linear operations Ax? Overall, the paper presents some quite interesting results, but the authors could be more clear on the relevance of those results and its implications. Non-expert readers are left with having to figure this out on their own. In my point of view, this would make the paper much stronger."}
{"id": "iclr2020_464", "title": "Observational Overfitting in Reinforcement Learning | OpenReview", "abstract": "Abstract:###A major component of overfitting in model-free reinforcement learning (RL) involves the case where the agent may mistakenly correlate reward with certain spurious features from the observations generated by the Markov Decision Process (MDP). We provide a general framework for analyzing this scenario, which we use to design multiple synthetic benchmarks from only modifying the observation space of an MDP. When an agent overfits to different observation spaces even if the underlying MDP dynamics is fixed, we term this observational overfitting. Our experiments expose intriguing properties especially with regards to implicit regularization, and also corroborate results from previous works in RL generalization and supervised learning (SL).", "review": " Review: This paper considers the problem of overfitting in RL through a specific model of overfitting, namely one where noise is introduced in the observation space, independently of the controllable dynamics. The paper provides both theoretical and empirical insights into the various manifestations of overfitting in this class of domains. A strength of this paper is to deepen our understanding of the phenomenon of overfitting in RL. To get a deep understanding of hard problems it is worthwhile to look at sub-classes of problems, and as such the identification of the observational overfitting setting is interesting. The paper provides insightful findings such as: - Explicit separation of f and g_theta. - Theoretical properties of generalization for the specific case of LQR and linear policies. There are also several empirical results, on three contrasting domains which illustrates various aspects of the interaction between parameterization vs generalization. But overall, though I read the paper in detail, and am knowledgeable of the topic, I am still struggling with extracting very specific new findings from this work. Here are a few examples: p.6: “We observe empirically that the underlying state dynamics has a significant effect on generalization performance as the policy nontrivially increased test performance” -> I see where you draw this analysis from the results. But is this finding new or surprising? I would have been very comfortable saying (without your results) that the underlying state dynamics can have a significant effect on generalization performance. p.6: “This suggests that the Rademacher complexity and the weight perturbation bound for rewards vary highly for different environments.” -> Again, how is this new knowledge? p.7: “this also suggests that the RL generalization quality of a convolutional architecture is not limited to real world data” -> Same question, what new knowledge have we gained? Perhaps the key message is that “there are generalization benefits to overparameterization” and that “implicit generalization” plays a key role in controlling this? If that is the main message, than it is somewhat obfuscated by all the material on LQR, which doesn’t really go in this direction (p.4: “We being with a theorem which implies that a high dimensional observational space directly contributes to overfitting”). I must say I found most of the material in 3.1 to distrct from the main message – but perhaps it is because I am not very interested in the LQR setting, and did not understand how the findings there supported those in latter sections. Overall, I would say the paper suffers from some clarity issues. There are some minor typos, then some key terms of are not sufficiently explained/defined (e.g. g_\theta in Sec.2.2 is said to project “the latent data to unimportant features”, but then there is discussion of “in settings where does matter” – I’m confused.) It’s not clear whether Thm 3.1 is new, or adaptation of existing results. More broadly, I found Sec.3.1 difficult to follow. The last paragraph of Sec.4 seems superfluous. I would recommend some good editing throughout. Another concern with the work is the fact that several aspects of RL are ignored (e.g. exploration, entropy, gamma, noise, stochastic gradients, etc. – as per p.4), yet are known to have an effect on overfitting, perhaps much more than the depth and width of the neural network. If that’s the case, it is perhaps dangerous to ignore them in the analysis; it is not described in the paper, for the domains studied in the empirical results, whether this is the case or not. As a result, it’s not clear how far the current analysis carries. I’m also not clear on the analysis at bottom p.7 “IMPALA-LARGE (…) memorizes less than IMPA due its inherent inductive bias” -> Seems to me IMPALA-LARGE might simply be in underfitting regime. ====== Update post-rebuttal: Thank you for the thoughtful responses. Given the other strong reviews, and your careful characterization of various aspects of the work, I am favorable to accepting. Please try to incorporate elements of this discussion into the paper, which I think will give a richer and more nuanced understanding of the work to future readers."}
{"id": "iclr2020_465", "title": "Empirical Studies on the Properties of Linear Regions in Deep Neural Networks | OpenReview", "abstract": "Abstract:###A deep neural networks (DNN) with piecewise linear activations can partition the input space into numerous small linear regions, where different linear functions are fitted. It is believed that the number of these regions represents the expressivity of a DNN. This paper provides a novel and meticulous perspective to look into DNNs: Instead of just counting the number of the linear regions, we study their local properties, such as the inspheres, the directions of the corresponding hyperplanes, the decision boundaries, and the relevance of the surrounding regions. We empirically observed that different optimization techniques lead to completely different linear regions, even though they result in similar classification accuracies. We hope our study can inspire the design of novel optimization techniques, and help discover and analyze the behaviors of DNNs.", "review": "Review:###This work presents an array of analytical tools to characterize linear regions of deep neural networks (DNNs). Using the tools the work analyzes the effect of dropout and batch normalization (BN) on the linear regions of trained DNNs; namely, by assessing the properties such as inspheres, orientation of hyperplanes, decision boundaries and relevance of surrounding regions, the authors highlight the differences and similarities of linear regions induced by vanilla SGD as compared to SGD with dropout or BN. The paper is clearly written and is easy to follow for the most part. The paper indeed presents a number properties for analyzing the nature of linear regions in DNNs; however it falls short of connecting them with an improvement in the optimization or interpretability of DNNs. Even with respect to providing support for general applicability, the work does not go very far: without enough variation in data (not just image benchmarks), tasks and architecture, it is hard to determine if the analysis tools presented in the paper generalize beyond the chosen setup. For instance just the optimization techniques compared in the paper have their own hyperparameters and it is not clear how the results might vary with them. I am not sure what to take away from figure 1 since it*s only a two-dimensional slice of very high-dimensional input space. Maybe the authors could instead choose an example with a low-dimensional input space for illustration purposes. Also, how much can be perceived from distributions shown in figure 2, since inradius (Eq. 5) may turnout to be a very coarse representation of linear regions, especially for deeper networks. Can the authors clarify this? Moreover, how would the figures look if we were using a different objective, dataset or architecture? In figure 3, is it not possible to show the average results instead of just one example? I would further like to know how the authors would deal with scalability issues if their analysis were to applied to more realistic (i.e. large) network architectures."}
{"id": "iclr2020_466", "title": "Imitation Learning of Robot Policies using Language, Vision and Motion | OpenReview", "abstract": "Abstract:###In this work we propose a novel end-to-end imitation learning approach which combines natural language, vision, and motion information to produce an abstract representation of a task, which in turn can be used to synthesize specific motion controllers at run-time. This multimodal approach enables generalization to a wide variety of environmental conditions and allows an end-user to influence a robot policy through verbal communication. We empirically validate our approach with an extensive set of simulations and show that it achieves a high task success rate over a variety of conditions while remaining amenable to probabilistic interpretability.", "review": " The paper addresses the problem of using multiple modalities for learning from demonstration. Approaches that take in task or joint space data to learn a policy for replicating that task are numerous. Doing the same with multiple modalities involved, in particular vision, language and motion, has only been recently considered, so this is a timely paper. The core contribution is pretty well summarised by the architecture in figure 1, which involves a combination of encodings of the words and sentences, images and parameters of a DMP in order to generate movement commands from a high level instruction. Unless I have missed something in the experimental setup, all of the considered task variations are movement commands of the form <Move> to <Object>. The network setup allows for synonyms of two kinds, so <Move> can be replaced by numerous verbal synonyms such as advance and go, and the object can be specified in terms of shapes, colors and so on, but otherwise this is the only specification of the task. This has been addressed in the recent literature using neural network architectures similar to the one being proposed here, e.g., see the following papers. These papers already solve the proposed problem and provide similar explanations. It would be helpful to see comparative discussion with respect to those methods and a clear statement of novelty with respect to such prior work: [R1] M. Burke, S. Penkov, S. Ramamoorthy, From explanation to synthesis: Compositional program induction for learning from demonstration, Robotics: Science and Systems (R:SS), 2019. [R2] Y. Hristov, D. Angelov, A.Lascarides, M. Burke, S. Ramamoorthy, Disentangled Relational Representations for Explaining and Learning from Demonstration, Conference on Robot Learning (CoRL), 2019. An interesting feature in R2 that the authors do not explicitly address here is the issue of relational specifications in the language, e.g., in addition to saying *move to the red bowl*, we may also wish to say *place on top of red block*. In the way that MPN is currently set up to map from the language input directly to hyperparameters of the DMP, and considering the embedding structure, it is not clear if MPN is capable of handling such specifications. If so, the claim of generalisation on the language input should be stated more clearly. The ablation study is setup somewhat differently than what I would have expected. The authors consider the effect of changing the training set size and if the language input includes synonyms or not. Those two aspects seem to produce the expected results. It would also be interesting to see an ablation study in the sense of replacing or removing aspects of the architecture to see its relative effect on the overall model performance. So, for instance, if one did not have a DMP with the hyperparameters being estimated by a network and instead had a more straightforward encoding of where to move to - does it make a difference and how much? Likewise, how much performance benefit, if any, is being derived from an uninterpreted image I being combined as described in the embedding as opposed to an alternative that detects an objects and combines that position differently. The paper would have been stronger if such architectural choices were better justified and also demonstrated in the experiments."}
{"id": "iclr2020_467", "title": "Non-Autoregressive Dialog State Tracking | OpenReview", "abstract": "Abstract:###Recent efforts in Dialogue State Tracking (DST) for task-oriented dialogues have progressed toward open-vocabulary or generation-based approaches where the models can generate slot value candidates from the dialogue history itself. These approaches have shown good performance gain, especially in complicated dialogue domains with dynamic slot values. However, they fall short in two aspects: (1) they do not allow models to explicitly learn signals across domains and slots to detect potential dependencies among (domain, slot) pairs; and (2) existing models follow auto-regressive approaches which incur high time cost when the dialogue evolves over multiple domains and multiple turns. In this paper, we propose a novel framework of Non-Autoregressive Dialog State Tracking (NADST) which can factor in potential dependencies among domains and slots to optimize the models towards better prediction of dialogue states as a complete set rather than separate slots. In particular, the non-autoregressive nature of our method not only enables decoding in parallel to significantly reduce the latency of DST for real-time dialogue response generation, but also detect dependencies among slots at token level in addition to slot and domain level. Our empirical results show that our model achieves the state-of-the-art joint accuracy across all domains on the MultiWOZ 2.1 corpus, and the latency of our model is an order of magnitude lower than the previous state of the art as the dialogue history extends over time.", "review": "Review:###[Contribution summary] Authors propose a new model for the DST task that (1) reduces the inference time complexity with an non-autoregressive decoder, and (2) obtains the competitive DST accuracy (49.04% joint accuracy on MultiWoZ 2.1). [Comments] - The proposed model is well motivated and well structured. Empirical results show improvement over other baselines, with the main gain coming from delexicalization, slot gating, fertility output, etc. - Some of the details are not entirely provided - e.g. please provide the loss hyper-parameter values (e.g. Eq.23) and optimizer parameters for the training. - Overall presentation, notations, figures, etc. could improve. - There have been recent work on DST with new SOTA results (e.g. “Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset” by Rastogi et al.) -- please consider comparing the approaches."}
{"id": "iclr2020_468", "title": "Domain Adaptive Multiflow Networks | OpenReview", "abstract": "Abstract:###We tackle unsupervised domain adaptation by accounting for the fact that different domains may need to be processed differently to arrive to a common feature representation effective for recognition. To this end, we introduce a deep learning framework where each domain undergoes a different sequence of operations, allowing some, possibly more complex, domains to go through more computations than others. This contrasts with state-of-the-art domain adaptation techniques that force all domains to be processed with the same series of operations, even when using multi-stream architectures whose parameters are not shared. As evidenced by our experiments, the greater flexibility of our method translates to higher accuracy. Furthermore, it allows us to handle any number of domains simultaneously.", "review": " After Discussion Period: I stick to my original score. My issues are largely resolved. ---- The submission is using adaptive computation graphs for domain adaptation. Multi-flow network is the main architectural element proposed in the submission. And, it is composed of parallel blocks of computations which aggregated using weighted summation with learnable weights. The domain adaptation is performed by setting different weights for source and target dataset. The adaptive weights and network parameters are all learned jointly by minimizing the combination of classification loss and domain difference loss. Although the idea of adaptive computation is not novel and has been explored, their application to the domain adaptation problem is novel to the best of my knowledge. Moreover, the proposed method is sensible and technically sound. The submission talks about different amount of computation needed per domain as an intuition behind the method. This is sensible and intuitive; however, it has not been experimented. The paper uses the same amount of layers for all domains making the amount of computation exactly same. It would be interesting to see the performance when different paths actually lead different computations. For example, parallel blocks can have different number of layers etc. The submission only provides result for RPT and DANN. These are clearly not state-of-the-art domain adaptation methods. Proposed method does not necessarily need to have state-of-the-art adaptation results to be accepted, but not reporting what state-of-the-art performance is makes the experimental results incomplete. Figure 4 suggests that there is no real parameter sharing at the end of the training. And, all domains have different computations. Authors should try to explain this behaviour since it is quite counter-intuitive. In summary, proposed method is somewhat novel, interesting and seems to be working well. Improved discussion on the experimental study is definitely needed."}
{"id": "iclr2020_469", "title": "Unifying Question Answering, Text Classification, and Regression via Span Extraction | OpenReview", "abstract": "Abstract:###Even as pre-trained language encoders such as BERT are shared across many tasks, the output layers of question answering, text classification, and regression models are significantly different. Span decoders are frequently used for question answering, fixed-class, classification layers for text classification, and similarity-scoring layers for regression tasks. We show that this distinction is not necessary and that all three can be unified as span extraction. A unified, span-extraction approach leads to superior or comparable performance in supplementary supervised pre-trained, low-data, and multi-task learning experiments on several question answering, text classification, and regression benchmarks.", "review": "Review:###This paper introduces a method for converting sentence pair classification tasks and sentence regression tasks into span into span extraction tasks, by listing all the possible classes (entailment, contradiction, neural) or the discretized scores (0.0, 0.25 ...) and concatenating them with the source text. With this formulation, one can train a BERT-based span-extraction model (SpEx-BERT) on classification, regression, and QA tasks without introducing any task-specific parameters. The purposed SpEx-BERT model achieves moderate improvement (0.3 points) over the BERT-large baseline on the GLUE test set when fine-tuned on intermediate STILTs tasks (Phang et al., 2018). Strengths: - Extensive finetuning/intermediate-finetuning experiments on a range of NLP tasks. - The paper is mostly well-written and easy to follow. Weaknesses: - This paper presents a lot of experiments. But it seems that the most useful / head-to-head comparison against the BERT model are the last 2 rows in Table 2 with the GLEU results, where the improvement is moderate. - The idea of expressing various NLP tasks (including textual entailment and text classification) as question-answer has been well-explored in decaNLP (McCann et al., 2018). It would be nice if the authors could elaborate more on how the proposed method differs from theirs. Other comments/suggestions: - Likely typo in abstract: *fixed-class, classification layers for text classification*"}
{"id": "iclr2020_470", "title": "GRASPEL: GRAPH SPECTRAL LEARNING AT SCALE | OpenReview", "abstract": "Abstract:###Learning meaningful graphs from data plays important roles in many data mining and machine learning tasks, such as data representation and analysis, dimension reduction, data clustering, and visualization, etc. In this work, we present a scalable spectral approach to graph learning from data. By limiting the precision matrix to be a graph Laplacian, our approach aims to estimate ultra-sparse weighted graphs and has a clear connection with the prior graphical Lasso method. By interleaving nearly-linear time spectral graph sparsification, coarsening and embedding procedures, ultra-sparse yet spectrally-stable graphs can be iteratively constructed in a highly-scalable manner. Compared with prior graph learning approaches that do not scale to large problems, our approach is highly-scalable for constructing graphs that can immediately lead to substantially improved computing efficiency and solution quality for a variety of data mining and machine learning applications, such as spectral clustering (SC), and t-Distributed Stochastic Neighbor Embedding (t-SNE).", "review": " In this paper, the authors present a method that transforms data into graph. They emphasize on the fact that the proposed method is scalable, using a spectral embedding to construct the graph. We think that the paper is not of enough quality to be accepted in ICLR. Without going in detail in the derivations, we give below some major issues in this submitted paper. The studied problem has been widely investigated in the literature. Many methods have been proposed within the same objective, including taking care of the scalability issue. The authors fail to provide the state of the art, as well as describe the contributions with respect to previous work. As a consequence, the contributions are not clear. Maybe the proposed framework is original, but the there has been plenty of methods that have considered the same problem. Experiments are poor and not convincing. The authors compare the proposed method to only two spectral clustering methods, which as the standard kNN and the Consensus kNN from 2013. These two methods are pretty old and many more recent methods have been introduced in the literature. Moreover, the results in Table 1 are somehow misleading, as the standard kNN is faster that the proposed method on 3 out of 4 datasets. Experiments in graph recovery are not clear, starting from the fact that the datasets are not defined (what are the Gaussian graph and ER graph?), neither the experimental setting (what is the problem at hand?). The same goes to the application of t-SNE which is also very weak. -------------- Reply to Rebuttal The authors have modified the paper to take into consideration our previous comments and suggestions. However, we think that it is still of not sufficient quality. We give below some elements, without providing a thorough review. It is pretty pretentious to say that *this is the first work that introduces a spectral method for learning ultra-sparse (tree-like) graphs from data*, while not comparing to the state of the art. There have been many spectral methods in graph learning for large-scale datasets. In experiments, the only added method is the one of Kalofolias and Perraudin (submitted in 2017 to ArXiv). However, results show that this method is the worst of all methods. It is even the worst compared to the simple standard knn. It is not clear how the authors get such results; It looks like something is wrong in experiments, or they are cherrypicking."}
{"id": "iclr2020_471", "title": "The Usual Suspects? Reassessing Blame for VAE Posterior Collapse | OpenReview", "abstract": "Abstract:###In narrow asymptotic settings Gaussian VAE models of continuous data have been shown to possess global optima aligned with ground-truth distributions. Even so, it is well known that poor solutions whereby the latent posterior collapses to an uninformative prior are sometimes obtained in practice. However, contrary to conventional wisdom that largely assigns blame for this phenomena on the undue influence of KL-divergence regularization, we will argue that posterior collapse is, at least in part, a direct consequence of bad local minima inherent to the loss surface of deep autoencoder networks. In particular, we prove that even small nonlinear perturbations of affine VAE decoder models can produce such minima, and in deeper models, analogous minima can force the VAE to behave like an aggressive truncation operator, provably discarding information along all latent dimensions in certain circumstances. Regardless, the underlying message here is not meant to undercut valuable existing explanations of posterior collapse, but rather, to refine the discussion and elucidate alternative risk factors that may have been previously underappreciated.", "review": "Review:###This paper tries to establish an explanation for the posterior collapse by linking the phenomenon to local minima. If I am understanding correctly, the final conclusion reads along the lines of *if the reconstruction error is high, then the posterior distribution will follow the prior distribution*. They also provide some experimental data to suggest that when the reconstruction error is high, the distribution of the latents tend to follow the prior distribution more closely. Although I really liked section 3 where authors establish the different ways in which `posterior collapse* can be defined, overall I am not sure if I can extract a useful insight or solution out of this paper. When the reconstruction error is large, the VAE is practically not useful. Also, I don*t think I am on board with continuing to use the standard Gaussian prior. Several papers (such as the cited Vampprior paper) showed that one can very successfully use GMM like priors, which reduces the burden on the autoencoder. Even though I liked the exposition in the first half of the paper, I don*t think I find the contributions of this paper very useful, as one can actually learn the prior and get good autoencoder reconstructions while obtaining a good match between the prior and the posterior, without having degenerate posterior distribution which is independent from the data distribution. All in all, I think using a standard gaussian prior is not a good idea, and that fact renders the explanations provided in this paper obsolete in my opinion. Is there any reason why we would want to utilize a simplistic prior such as the standard Gaussian prior? Do you have any insights with regards to whether the explanations in this paper would still hold with more expressive prior distributions? In general, I have found section 5 hard follow. And to reiterate, the main arguments seem to be centered around autoencoders which cannot reconstruct well, as the authors also consider the deterministic autoencoder. If the autoencoder can not reconstruct well, it is not reasonable to expect a regularized autoencoder such as VAE to reconstruct, better, and therefore the VAE is already is a regime where it is not useful anyhow. I think the authors should think about the cases where the reconstruction error is low, and see if there is an issue of posterior collapse in those setups."}
{"id": "iclr2020_472", "title": "End-to-End Multi-Domain Task-Oriented Dialogue Systems with Multi-level Neural Belief Tracker | OpenReview", "abstract": "Abstract:###It has been an open research challenge for developing an end-to-end multi-domain task-oriented dialogue system, in which a human can converse with the dialogue agent to complete tasks in more than one domain. First, tracking belief states of multi-domain dialogues is difficult as the dialogue agent must obtain the complete belief states from all relevant domains, each of which can have shared slots common among domains as well as unique slots specifically for the domain only. Second, the dialogue agent must also process various types of information, including contextual information from dialogue context, decoded dialogue states of current dialogue turn, and queried results from a knowledge base, to semantically shape context-aware and task-specific responses to human. To address these challenges, we propose an end-to-end neural architecture for task-oriented dialogues in multiple domains. We propose a novel Multi-level Neural Belief Tracker which tracks the dialogue belief states by learning signals at both slot and domain level independently. The representations are combined in a Late Fusion approach to form joint feature vectors of (domain, slot) pairs. Following recent work in end-to-end dialogue systems, we incorporate the belief tracker with generation components to address end-to-end dialogue tasks. We achieve state-of-the-art performance on the MultiWOZ2.1 benchmark with 50.91% joint goal accuracy and competitive measures in task-completion and response generation.", "review": " This paper presents a deep neural architecture for dialogue systems. It relies on a novel belief state tracking approach based on a transformer architecture. The method is tested on multiWOZ dataset. The paper is very descriptive of the architecture and doesn*t motivate very well the choices that were made. I feel the authors were very much influenced by the dataset they will test their method on which led to very specific design choices. The dialog state is supposed to be representable by a set of key-value pairs which is a strong assumption. It comes from a long tradition of slot-filling applications in the field of spoken dialogue systems but I feel this choice makes the work very ad hoc. Especially, the response generator is strongly dependent on that structure. I also feel that belief tracking makes more sense in the case of a spoken dialogue system that can introduce additional ambiguity because of speech recognition errors. Here, the input text is supposed to represent quite accurately the intent of the user. It comes again from the chosen test data set. All in all, I feel the proposed architecture would not serve the general purpose of developing end-to-end dialogue systems for multi-domain applications. It only solves the targeted task (multiWOZ). Thus, I*m not sure what is the scientific/technical contribution of the paper. I*d like the authors to comment on that."}
{"id": "iclr2020_473", "title": "Situating Sentence Embedders with Nearest Neighbor Overlap | OpenReview", "abstract": "Abstract:###As distributed approaches to natural language semantics have developed and diversified, embedders for linguistic units larger than words (e.g., sentences) have come to play an increasingly important role. To date, such embedders have been evaluated using benchmark tasks (e.g., GLUE) and linguistic probes. We propose a comparative approach, nearest neighbor overlap (N2O), that quantifies similarity between embedders in a task-agnostic manner. N2O requires only a collection of examples and is simple to understand: two embedders are more similar if, for the same set of inputs, there is greater overlap between the inputs* nearest neighbors. We use N2O to compare 21 sentence embedders and show the effects of different design choices and architectures.", "review": "Review:###The paper proposes N2O, a tool for probing the similarity among sentence embedders. Given two sentence embedders, N2O measures the amount of overlap of the k-nearest neighbor sets reported by the two embedders, averaged over a sample of probing queries. Cosine similarity is used as the similarity metric. The paper computes all-pair N2O scores for common sentence embedders and analyzes the results. Overall, while the idea of probing similarity between embedders is interesting, the paper has the following weaknesses: - The use of neighbor overlap to compare embedders has precedence in the context of word embeddings [https://www.aclweb.org/anthology/C16-1262/ | https://hal.archives-ouvertes.fr/hal-01806468/ | https://www.aclweb.org/anthology/Q18-1008/]. The methods in these works are mostly identical to N2O with some minor variations (e.g., using Jaccard distance). - Using an existing method is justified if it provides new insights for the new setting. However, the results do not offer a lot of new insights. The fact that static embeddings, ELMo, and BERT give different neighbors is unsurprising. (The paper even admitted this.) Moreover, since sentence embedders are usually used as features when fine-tuned on downstream tasks, the importance of observations based on the pre-fine-tuned models is unclear. Contrast this with the work on word embedding similarity. Since some research areas (e.g., social science) directly use the similarity of word embeddings to conclude findings, the insights of how different word embeddings behave are more directly applicable. - The paper does address some design choices, such as the number of neighbors and the number of probing queries, showing that different choices have little effects on the scores. However, the use of cosine similarity is problematic. As noted in the paper, some embedders such as ELMo were not trained on similarity objectives. BERT does have the next-sentence task, but the sparse attention-based architecture does not necessarily push similar sentences to have low cosine similarity. Additional comments and questions: - Instead of using the amount of overlap for a fixed k, it would be nice to have a metric that captures the whole distribution. For instance, maybe two embedders disagree on the first 20 neighbors, but end up retrieving the same set when considering 50 neighbors. The current overlap-based metric cannot capture such a phenomenon. - Page 2: The method is technically not task-agnostic --- the task is sentence similarity with respect to a specific corpus. - The paper tests variants of the same embedders by training on different corpora, with the conclusion that mismatched corpora give lower N2O scores. What is the N2O between two embedders of the same type trained on the same corpora but with different seeds?"}
{"id": "iclr2020_474", "title": "Amharic Negation Handling | OpenReview", "abstract": "Abstract:###User generated content contains opinionated texts not only in dominant languages (like English) but also less dominant languages( like Amharic). However, negation handling techniques that supports for sentiment detection is not developed in such less dominant language(i.e. Amharic). Negation handling is one of the challenging tasks for sentiment classification. Thus, this work builds negation handling schemes which enhances Amharic Sentiment classification. The proposed Negation Handling framework combines the lexicon based approach and character ngram based machine learning model. The performance of framework is evaluated using the annotated Amharic News Comments. The system is outperforming the best of all models and the baselines by an accuracy of 98.0. The result is compared with the baselines (without negation handling and word level ngram model).", "review": " While this paper tackles an interesting problem. The technical approach is unfortunately too outdated and obvious and not quite the level of ICLR. The dataset is likely too easy given the high accuracy."}
{"id": "iclr2020_475", "title": "SCELMo: Source Code Embeddings from Language Models | OpenReview", "abstract": "Abstract:###Continuous embeddings of tokens in computer programs have been used to support a variety of software development tools, including readability, code search, and program repair. Contextual embeddings are common in natural language processing but have not been previously applied in software engineering. We introduce a new set of deep contextualized word representations for computer programs based on language models. We train a set of embeddings using the ELMo (embeddings from language models) framework of Peters et al (2018). We investigate whether these embeddings are effective when fine-tuned for the downstream task of bug detection. We show that even a low-dimensional embedding trained on a relatively small corpus of programs can improve a state-of-the-art machine learning system for bug detection.", "review": "Review:###The paper proposes an embedding method for source code tokens, which is based on contextual word representation, particularly is based on the method of ELMo. The learned representation is evaluated on the task of bug detection, with promising performance. Strengths: The paper addresses an important and impactful problem. The solution designed for this problem seems very reasonable. Experiments are useful and reasonable and the experimental results are promising and in the favor of the paper. The paper is well written and clear. Weaknesses: - The data used (in particular the method of buggy code generation applied) seems very specific. It would be interesting to know the performance of the method on real bugs. - The paper is a bit low in technicality. Decision: Accept I think this paper is overall a good work and can open direction of research even beyond the scope of the paper, for example in combining learning and reasoning, or in source code generation with adversarial models. Minor: - Since compilers can spot errors in code completely, it would be useful to motivate the advantage of learning for bug detection - The table referrals in the body of the paper contains wrong table numbers in Sections 6.1, 6.2, 6.3. - The incorrect Binary Operator example in Listing 2 does not seem to be a well justified bug. It could be a correct piece of code for a different purpose. - which use -> which we use"}
{"id": "iclr2020_476", "title": "On the Relationship between Self-Attention and Convolutional Layers | OpenReview", "abstract": "Abstract:###Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as powerful as any convolutional layer. Our numerical experiments then show that the phenomenon also occurs in practice, corroborating our analysis. Our code is publicly available.", "review": "Review:###The paper claims that 1. multi-head self-attention(MHSA) is at least as powerful as convolutions by showing that a CONV can be cast as a special case of MHSA and 2. that in practice, MHSA often mimic convolutional layers. These claims are interesting and timely, given that there has been a fair amount of recent work that have explored the use of self-attention(SA) on image tasks, either by composing SA with convolutions or replacing convolutions altogether with self-attention (examples of each are referenced in the paper). So should these claims be true, they would give theoretical evidence that SA can completely replace convolutions. However, I think that the claims are exaggerated and misleading. 1. The theory shows an arguably contrived link between self-attention and convolution. Theorem 1 says that a convolution can be seen as a special case of MHSA, and the constructive proof (that chooses SA parameters to derive a convolution) shows a correspondence between the output of each head of MHSA and a D_out by D_in linear transform applied to the D_in features of a single pixel, with attention weight given entirely to this pixel (i.e. hard attention). The derivation relies heavily on the use of a relative encoding scheme that sets W_qry=W_key = 0 (usually referred to as W^Q, W^K in the self-attention literature, the linear maps applied to the queries and keys) i.e. the attention weights do not depend on the key/query values, but only their relative positions. Moreover, the softmax temperature (an interpretation of 1/alpha) is set arbitrarily close to 0 to make the softmax saturate and attain hard-attention. With these two constraints, I am sceptical as to whether you can really say that you are implementing self-attention. In standard practice when MHSA is used, W^Q and W^K are never set to zero, and the scale of the logits for the self-attention weights are controlled by normalising them with sqrt(D_k) (or sqrt(D_k/N_h), depnding on how you choose to deal with multiple heads). Furthermore, the derivation only holds for when stride=1 and padding=“SAME”, such that the spatial dimensions of the input (H & W) remain unchanged. In fact the padding is not really dealt with in the derivation, and it is unclear whether the result can generalise to convolutions with stride > 1, making the claim “MHSA layer … is at least as powerful as any convolutional layer” problematic. Hence although I think the derivation is mathematically correct, I think that the link that the derivation makes between convolutions and MHSA is somewhat contrived and not a useful observation in practice. I expect MHSA with learned W_qry and W_key will behave differently to when they are set to 0, and it would be much more interesting/relevant to see how their behaviour compares with convolutions in this more realistic setting. 2. The heavy dependence of the experiments on the quadratic encoding, the aforementioned contrived form of MHSA that was used to derive the link between convolutions and MHSA, makes the results not very relevant and the claim that *MHSA often mimic convolutional layers* rather misleading. It could be more relevant if quadratic encoding can replace standard MHSA parametererisations with learned W_qry and W_key, but I’m not convinced that this is the case. Although Figure 4 suggests that this SA with quadratic encoding gives similar test performance to ResNet18, I think that CIFAR-10 classification is too simple a task to claim that quadratic encoding can replace standard SA with learned W_qry and W_key, and I think results can look very different for harder problems e.g. ImageNet, MSCOCO - explored in Ramachandran et al - made possible because they use local SA as opposed to full SA. Experiments on these problems would be much more interesting and relevant. Note that the experiments using the learned relative positional encoding have “attention scores (are) computed using only the relative positions of the pixels and not the data” (I’m guessing this means W_qry=W_key=0 again). Hence the qualitative similarities between MHSA and convolutions only hold for the rather restricted case where I get the impression that self-attention has been unrealistically constrained only to increase its chance of behaving similarly to convolutions. Also the comparison in Figure 4 and Table 1 is being used to support the claim that self-attention can be as “powerful” as convolutions, but I think this is misleading because both quadratic and learned SA uses full SA, where each pixel attends to all pixels - this means the time & memory complexity of the algorithm is O((HW)^2), whereas for convolutions it is O(HW). So the expressiveness of SA that matches convolutions for this particular problem comes at a significant cost, to the extent that for bigger problems (ImageNet, MSCOCO) full SA is not feasible due to its quadratic memory requirement, whereas convolutions don’t face this problem. I think this should be pointed out more explicitly in the text, and think the claim that “self-attention is at least as powerful as convolutions” should be replaced with a more moderate statement such as “self-attention defines a family of functions that contains convolutions (of stride 1)” Summary: Although the writing of the paper is clear and the derivation is mathematically correct as far as I can see, the link between self-attention and convolutions in the paper are fairly contrived, hence the contribution of the paper to the field is not so significant in my honest opinion. ******************** I appreciate the authors* response, and understand that the maths suggests a single head of MHA (in the original form) cannot exactly emulate a general convolution. But empirically, the localised attention patterns do seem to suggest that each head can behave similarly to a restricted form of convolution, where similar weights are given to the receptive field (the local patch) in the neighbourhood each input pixel. Perhaps an analysis of what special case of convolution each head can emulate would be interesting, given the empirically observed similarities in the qualitative behaviour. With the more justified nuance of the findings of the paper, and together with the authors* significant efforts to make the evaluation more relevant and thorough, I will increase my score to *weak accept*."}
{"id": "iclr2020_477", "title": "Diverse Trajectory Forecasting with Determinantal Point Processes | OpenReview", "abstract": "Abstract:###The ability to forecast a set of likely yet diverse possible future behaviors of an agent (e.g., future trajectories of a pedestrian) is essential for safety-critical perception systems (e.g., autonomous vehicles). In particular, a set of possible future behaviors generated by the system must be diverse to account for all possible outcomes in order to take necessary safety precautions. It is not sufficient to maintain a set of the most likely future outcomes because the set may only contain perturbations of a dominating single outcome (major mode). While generative models such as variational autoencoders (VAEs) have been shown to be a powerful tool for learning a distribution over future trajectories, randomly drawn samples from the learned implicit likelihood model may not be diverse -- the likelihood model is derived from the training data distribution and the samples will concentrate around the major mode of the data. In this work, we propose to learn a diversity sampling function (DSF) that generates a diverse yet likely set of future trajectories. The DSF maps forecasting context features to a set of latent codes which can be decoded by a generative model (e.g., VAE) into a set of diverse trajectory samples. Concretely, the process of identifying the diverse set of samples is posed as DSF parameter estimation. To learn the parameters of the DSF, the diversity of the trajectory samples is evaluated by a diversity loss based on a determinantal point process (DPP). Gradient descent is performed over the DSF parameters, which in turn moves the latent codes of the sample set to find an optimal set of diverse yet likely trajectories. Our method is a novel application of DPPs to optimize a set of items (forecasted trajectories) in continuous space. We demonstrate the diversity of the trajectories produced by our approach on both low-dimensional 2D trajectory data and high-dimensional human motion data.", "review": "Review:###Caveat: I am very familiar with DPPs, but unfamiliar with the literature of trajectory forecasting, autonomous driving, etc. Summary: This work introduces a generative model for diverse sequences based on a DPP, with the goal of providing likely yet non-overlapping possible future trajectories to models that require such information for safety concerns (eg, autonomous driving). Rather than using the DPPs negative log-likelihood as a measure of the trajectory set*s diversity, the authors use the DPP*s expected sample size as a proxy for a diversity metric. Experimentally, the authors show on two tasks that this approach generates more diverse, relevant trajectories than several competing baselines. Recommendation: I recommend this paper be accepted, with the caveat that I am not well equipped to evaluate the experimental contribution of this paper, which in my opinion is the most important part of this work. I would have liked to see more extensive experiments, in particular (a) other baselines (DPP NLL as a diversity loss, different DPP kernels) and (b) experiments on larger datasets. High level comments and questions: - When generating the trajectory set (Alg. 3), your algorithm may generate sets of variable size (as the DPP NLL is log-submodular but not necessarily increasing). Did you also consider generating sets of fixed size, or of a minimal size? - Previous work has also looked at using DPPs to alleviate mode collapse in GANs (Elfeki et al., ICML 2019). Could you comment on how you expect such a method would perform on your chosen tasks? - The intuition that the DPP negative log-likelihood will overwhelmingly penalize subsets that contain a few similar trajectories seems very reasonable. However, I would have liked to see that intuition verified through an experiment that used the NLL as a diversity loss. - The training sets for both experiments seem fairly small (1100 and 9400 training examples). Could you comment on this choice, and on whether you expect your experimental results to generalize to much larger datasets? - Your motivating example is autonomous vehicles and safety concerns; with that example in mind, could you comment on how to select the radius R (or, equivalently, \rho) in Eq. 9? It seems like this choice would have significant downstream implications on the trade-off between accuracy and robustness to unpredictability. - The idea of using the expected subset size as a metric for diversity is compelling. The authors may want to take a look at (Gillenwater et al., NeurIPS*18), which uses a similar metric as a proxy for user engagement. Minor comments: - after Eq. 2: please consider defining the acronym ELBO before using it. - Eq. 8: if you are using an exponentiated quadratic, I believe the distance should be squared. - Eq. 10: consider citing the relevant proposition from (Kulesza & Taskar)."}
{"id": "iclr2020_478", "title": "Deep automodulators | OpenReview", "abstract": "Abstract:###We introduce a novel autoencoder model that deviates from traditional autoencoders by using the full latent vector to independently modulate each layer in the decoder. We demonstrate how such an *automodulator* allows for a principled approach to enforce latent space disentanglement, mixing of latent codes, and a straightforward way to utilize prior information that can be construed as a scale-specific invariance. Unlike GANs, autoencoder models can directly operate on new real input samples. This makes our model directly suitable for applications involving real-world inputs. As the architectural backbone, we extend recent generative autoencoder models that retain input identity and image sharpness at high resolutions better than VAEs. We show that our model achieves state-of-the-art latent space disentanglement and achieves high quality and diversity of output samples, as well as faithfulness of reconstructions.", "review": "Review:###The paper makes the following contribution: - using the AdaIn architecture proposed by Karras et al., 2019 with the autoencoding architecture of AGE/PIONEER; - a cyclic loss to enforce disentangling between different layers; - a method to enforce invariances at specific layers. The adaptation of the AdaIn architecture in an autoencoding fashion (a la AGE/PIONEER) is sensible and well motivated, combining state-of-the-art generator while allowing inference in a compact setting (i.e. not requiring an additional discriminator). The other contribution are harder to read and the writing should be improved. The cyclic loss should be better described. The notation of the KL divergence is confusing if you are using the KL divergence defined by AGE/PIONEER and will need to be explained. I will also assume that d_cos is the cosine loss as defined by the PIONEER paper. This should be mentioned as well. The method to enforce invariance is also not clear to me. While the authors introduce F as a *known invariance*, it is unclear what role it plays in the cost function. Is F an invariant on which we measure this reconstruction loss d? What is d? Explaining that might shed light on the result Figure 5, e.g. why the images become blurry when doing this rotation. The experiment demonstrates the sampling quality of the model and the transfer of features at different level (coarse-medium-fine) Figure 4. It is unclear what was the contribution of the layer specific loss metric to allow that feature transfer. It seems from Figure 5 the invariance objective has been roughly satisfied but at the cost of a significant drop in image quality. The clearest contribution from this paper is definitely the AGE/PIONEER approach to train the AdaIn architecture. The two other contributions are unclear, both in their explanation and in what they contribute: the layer specific loss not compared to an architecture just trained in an AGE way, and the enforcing of invariance, although filling its objective, might deteriorate other desirable properties of the model (e.g. sample quality)."}
{"id": "iclr2020_479", "title": "Meta-Graph: Few shot Link Prediction via Meta Learning | OpenReview", "abstract": "Abstract:###We consider the task of few shot link prediction, where the goal is to predict missing edges across multiple graphs using only a small sample of known edges. We show that current link prediction methods are generally ill-equipped to handle this task---as they cannot effectively transfer knowledge between graphs in a multi-graph setting and are unable to effectively learn from very sparse data. To address this challenge, we introduce a new gradient-based meta learning framework, Meta-Graph, that leverages higher-order gradients along with a learned graph signature function that conditionally generates a graph neural network initialization. Using a novel set of few shot link prediction benchmarks, we show that Meta-Graph enables not only fast adaptation but also better final convergence and can effectively learn using only a small sample of true edges.", "review": "Review:###This paper presents a new link prediction framework in case of having seen only a small fraction of the graph. The premise is that the predictor is trained on other similar graphs. The predictor (a variational graph autoencoder) performs transfer learning through gradient-based metalearning with a combination of global and local parameters. The paper is conceptually solid, however, it is hard to claim novelty in individual ideas in the paper. The major concern I have is with the benchmarks. It seems like the (meta) training data is very similar to the validation data. So, even though it is supposed to be a few shot prediction problem, it seems like the meta-training phase has already provided more than a *few* training points. Please correct me if this observation is incorrect. My rating is neutral so far (not an option in the system) and ready to change upon comments from the authors."}
{"id": "iclr2020_480", "title": "UNPAIRED POINT CLOUD COMPLETION ON REAL SCANS USING ADVERSARIAL TRAINING | OpenReview", "abstract": "Abstract:###As 3D scanning solutions become increasingly popular, several deep learning setups have been developed for the task of scan completion, i.e., plausibly filling in regions that were missed in the raw scans. These methods, however, largely rely on supervision in the form of paired training data, i.e., partial scans with corresponding desired completed scans. While these methods have been successfully demonstrated on synthetic data, the approaches cannot be directly used on real scans in absence of suitable paired training data. We develop a first approach that works directly on input point clouds, does not require paired training data, and hence can directly be applied to real scans for scan completion. We evaluate the approach qualitatively on several real-world datasets (ScanNet, Matterport3D, KITTI), quantitatively on 3D-EPN shape completion benchmark dataset, and demonstrate realistic completions under varying levels of incompleteness.", "review": "Review:###This paper proposes a new method for making 3D point clouds by automatically completing 3D scans. It does not require paired data samples for training which makes it possible to train it on real data instead of synthetic data. The authors use a generative adversarial network (GAN) to “generate” complete point clouds from noisy or partial point clouds obtained by 3D scanning. The generator learns to perform mapping from point set manifold of scanned noisy and partial input X_r to manifold of clean shapes X_c. The discriminator tries to tell between encoded clean shapes (synthetic data point clouds) and mappings of noisy input (point clouds from real-life data 3D scans). An encoder-decoder network similar to those in Achlioptas et al. (2018) and Qi et al. (2017a) is trained to transform original point clouds to a low-dimensional latent space prior to training the GAN. The authors find that using the encoder-decoder trained on clean shape data even for noisy input yields better results. One of the issues of completing noisy and partial scans is that the desired complete scan can have a very different shape compared to the noisy input. The generator can map latent vectors to any points on the target manifold which allows it to generate shapes that are far different from the original inputs. In order not to generate random clean shapes, the authors add a reconstruction loss to the generator which encourages it to preserve the partial shape of the input end reconstruct it in the completed clean shape. The choice of Hausdorff distance for reconstruction loss is sound and the ablation study confirms it. The authors perform rather extensive experimental evaluation of their proposed method. They perform qualitative and quantitative analysis on several datasets, both real-life and synthetic ones. The proposed method outperforms existing methods in real-life data scenario. On the synthetic dataset (3D-EPN), the quantitative results are not as good as those of PCN (which is a supervised method, unlike the proposed method), but the qualitative results look plausible and comparable to PCN results. In terms of plausibility score, the proposed method outperforms existing methods in all experiments, which is probably thanks to its objective - map the input into the latent space of clean and complete shapes. However, plausible looking point clouds do not necessarily have to precisely match the input, which is the objective of 3D scan point cloud completion. The ablation study also confirms the effectiveness of individual parts in the proposed method. The main contribution of this paper is training with unpaired data, which enables training on real-life data, leading to better results on real-life scans. While I understand the difficulties, I believe it would be better to try to focus more on real-life data in the evaluation. There is only one experiment with quantitative analysis on real-life data in the paper. Supporting that with a visual Turing test would have been great. Questions raised: In 4.3, you say that “ground truth complete scans are not available for training.” How do you then train the supervised methods? Are they trained on 3D-EPN? In that case, is your proposed method also trained on 3D-EPN? If your method is the only one trained on your synthetic data (dataset D), which is also used for testing, then I do not think you could claim that your method is better at dealing with different data distributions. Please make clear in the comments what data you use for training in this experiment. The meaning of section 4.4 is not very clear to me. If my understanding is correct, when completing noisy or partial point clouds, low diversity in results is better than high diversity because it is a task of repairing the input, which only has one correct result. Are you trying to say in this section that your method yields more consistent results with lower diversity than the method from previous work? If that is the case, you should consider rewriting that part to make it clearer to readers. The PCN paper (Yuan et al., 2018) shows that PCN can generalize and complete point clouds of objects unseen during training very well. Unfortunately, this paper does not discuss performance on objects of unseen classes. Were such experiments considered? It would be beneficial to do a qualitative analysis of performance on unseen objects. The description of the left table in Table 1 first says that it shows performance on real-life scans, but performance on synthetic data also appears there. Shouldn’t it rather say that it is plausibility comparison on real-life scans and synthetic data? Summary: The proposed method is easy to understand, exploits recent progress in generative models, and allows training on real-life scans as it does not require paired training data. The paper does not contain much algorithmical novelty and mostly combines existing methods to solve the problem of obtaining clean and complete point clouds from real 3D scans. However, the ablation study shows that adding a good reconstruction loss to the generator is crucial for the performance. The main strengths of this paper are extensive experimental evaluation using both quantitative and qualitative analysis, and significant improvement in performance on real-life data over previous works. The main weakness is limited novelty in terms of the techniques used in the proposed method. Additional comments that do not affect the review decision: Citations in the text have to be revised and correctly put into parentheses where necessary. E.g., on page 2: “Since its introduction, GAN Goodfellow et al. (2014) has been used...” should be rewritten to “Since its introduction, GAN (Goodfellow et al., 2014) has been used...” Table 2 is the only table where the best results are not highlighted by bold text. It is also the only table where the proposed method is not the best performing method as it loses to PCN. I apologize if it is just a pure coincidence but it seems as if the authors did not want to draw attention to the fact that their proposed method loses to an existing method in that experiment. I believe that you should be fair and highlight best results in all tables."}
{"id": "iclr2020_481", "title": "Hierarchical Summary-to-Article Generation | OpenReview", "abstract": "Abstract:###In this paper, we explore \textit{summary-to-article generation}: the task of generating long articles given a short summary, which provides finer-grained content control for the generated text. To prevent sequence-to-sequence (seq2seq) models from degenerating into language models and better controlling the long text to be generated, we propose a hierarchical generation approach which first generates a sketch of intermediate length based on the summary and then completes the article by enriching the generated sketch. To mitigate the discrepancy between the ``oracle** sketch used during training and the noisy sketch generated during inference, we propose an end-to-end joint training framework based on multi-agent reinforcement learning. For evaluation, we use text summarization corpora by reversing their inputs and outputs, and introduce a novel evaluation method that employs a summarization system to summarize the generated article and test its match with the original input summary. Experiments show that our proposed hierarchical generation approach can generate a coherent and relevant article based on the given summary, yielding significant improvements upon conventional seq2seq models.", "review": "Review:###This paper explores the task of summary-to-article generation, the task of generating long articles given a short summary. Generating long text as output is challenging, since the seq2seq model often fall into degeneration of language models. To address this issue and have a better control on the long-form text generation, this paper proposes a hierarchical generation approach which first generates an intermediate sketch of the article and then generated the full article. This intermediate sketch can be noisy during inference because of the discrepancy between the training and inference strategies. To this end, this paper proposes a multi-agent reinforcement learning as well. For the evaluation of such long-form text outputs (article), this paper also proposes to use a summarization model to summarize the generated article and measure the ROUGE score between the generated summary and ground-truth. The premise here is that if we can generate a good article, then the output summary generated based on this article as input will also be good. They empirically evaluated the proposed hierarchical model with reinforcement learning, showing significant improvements over conventional seq2seq models. Overall, this paper is interesting with some new ideas, with caveat for some clarifications and missing some important experiments. Arguments: 1) It would be interesting to see how does a fine-tuned GPT-2 model on these summarization datasets perform on this article generation task. 2) ROUGE-rec metric is weird in the sense that if the summary-to-article model just copies the summary with some junk non-related information which is fluent, the summarization model based on this article can still generate a good summary. However, this might not be the case as the human evaluation correlation is good with this metric. Anyway, I would suggest to further analyse this metric. 3) By seeing the outputs generated by the conventional seq2seq model, I was wondering if the repetition can be fixed by simply blocking the n-gram repeats. Further, I would be interesting to see if transformers are better baselines in such long-form text generation tasks."}
{"id": "iclr2020_482", "title": "The Sooner The Better: Investigating Structure of Early Winning Lottery Tickets | OpenReview", "abstract": "Abstract:###The recent success of the lottery ticket hypothesis by Frankle & Carbin (2018) suggests that small, sparsified neural networks can be trained as long as the network is initialized properly. Several follow-up discussions on the initialization of the sparsified model have discovered interesting characteristics such as the necessity of rewinding (Frankle et al. (2019)), importance of sign of the initial weights (Zhou et al. (2019)), and the transferability of the winning lottery tickets (S. Morcos et al. (2019)). In contrast, another essential aspect of the winning ticket, the structure of the sparsified model, has been little discussed. To find the lottery ticket, unfortunately, all the prior work still relies on computationally expensive iterative pruning. In this work, we conduct an in-depth investigation of the structure of winning lottery tickets. Interestingly, we discover that there exist many lottery tickets that can achieve equally good accuracy much before the regular training schedule even finishes. We provide insights into the structure of these early winning tickets with supporting evidence. 1) Under stochastic gradient descent optimization, lottery ticket emerges when weight magnitude of a model saturates; 2) Pruning before the saturation of a model causes the loss of capability in learning complex patterns, resulting in the accuracy degradation. We employ the memorization capacity analysis to quantitatively confirm it, and further explain why gradual pruning can achieve better accuracy over the one-shot pruning. Based on these insights, we discover the early winning tickets for various ResNet architectures on both CIFAR10 and ImageNet, achieving state-of-the-art accuracy at a high pruning rate without expensive iterative pruning. In the case of ResNet50 on ImageNet, this comes to the winning ticket of 75:02% Top-1 accuracy at 80% pruning rate in only 22% of the total epochs for iterative pruning.", "review": "Review:###This paper attempts an in depth study of the lottery ticket hypothesis. The lottery ticket hypothesis holds that sparse sub-networks exist inside dense large models and that the sparse sub-networks achieve at least as good an accuracy as the underlying large model. These sub-networks are discovered by training and iteratively pruning the dense model. This paper investigates the epoch at which pruning should occur as well as the epoch at which weights should be rewound when retraining. Then, the authors conduct experiments with different pruning strategies (one-shot vs. gradual) in an attempt to find such sparse models (or *winning tickets*) earlier than they otherwise would have been found. The experiments conducted by the authors seem to be very extensive, and I think the paper contains useful data to have for researchers interested in better understanding the lottery ticket hypothesis. However, my main issue is with both the originality and significance of this work. This paper gives evidence that winning tickets may be found *early,* although their notion of early still involves quite a lot of training. Although the paper is interested in addressing the structure of the winning tickets, I really didn*t find any of the discussion of structure to give much insight into the lottery ticket hypothesis. Most of the section focuses on analyzing weight magnitude, though I was hoping for something more about the actual structure of the sparse subnetwork -- especially given the title of the paper. Figure 3 is notable, showing that different winning tickets (parameterized by different prune and rewind epochs) can have a large Hamming distance between them. This is very interesting, and I wish the authors had more to say. How is this affected by different initializations? Are these solutions connected on a loss landscape? Is there something invariant about the sparse architecture after symmetries are taken into account? It*s not clear to me that Hamming distance alone is enough. In conclusion, the paper presents a set of nice experiments, but doesn*t really shed too much additional light on the scientific nature of the lottery ticket hypothesis."}
{"id": "iclr2020_483", "title": "The Variational InfoMax AutoEncoder | OpenReview", "abstract": "Abstract:###We propose the Variational InfoMax AutoEncoder (VIMAE), an autoencoder based on a new learning principle for unsupervised models: the Capacity-Constrained InfoMax, which allows the learning of a disentangled representation while maintaining optimal generative performance. The variational capacity of an autoencoder is defined and we investigate its role. We associate the two main properties of a Variational AutoEncoder (VAE), generation quality and disentangled representation, to two different information concepts, respectively Mutual Information and network capacity. We deduce that a small capacity autoencoder tends to learn a more robust and disentangled representation than a high capacity one. This observation is confirmed by the computational experiments.", "review": "Review:###I went over this work multiple times and had a really hard time judging the novelty of this work. The paper seems to be a summary of existing work reinterpreting variational autoencoding objectives from an information theoretic standpoint. In particular, the paper seems to follow the same analysis as in Wasserstein Autoencoders (Tolstikhin et al., 2017) and InfoVAE (Zhao et al., 2017). It is unfair to say that the objectives were *derived independently* since these works are from a couple of years ago. The paper also lacks discussion on two crucial works in this space: 1. https://arxiv.org/abs/1711.00464 shows how to trade off rate-distortion in VAEs. 2. https://arxiv.org/abs/1812.10539 shows how to learn informative representations by eliminating the KL divergence term and at the same time specifying an implicit generative model (Theorem 1). re: disentanglement. Unsupervised disentanglement has been shown to be theoretically impossible and several key challenges have been highlight w.r.t. prior work. Again, the relevant paper: https://arxiv.org/abs/1811.12359 has not even been cited. More importantly, the claims around *more disentangled representation* are imprecise in light of this work. A proper discussion on the contributions of this work as well as discussion on the above related works would be desirable on the author*s end."}
{"id": "iclr2020_484", "title": "High performance RNNs with spiking neurons | OpenReview", "abstract": "Abstract:###The increasing need for compact and low-power computing solutions for machine learning applications has triggered a renaissance in the study of energy-efficient neural network accelerators. In particular, in-memory computing neuromorphic architectures have started to receive substantial attention from both academia and industry. However, most of these architectures rely on spiking neural networks, which typically perform poorly compared to their non-spiking counterparts in terms of accuracy. In this paper, we propose a new adaptive spiking neuron model that can also be abstracted as a low-pass filter. This abstraction enables faster and better training of spiking networks using back-propagation, without simulating spikes. We show that this model dramatically improves the inference performance of a recurrent neural network and validate it with three complex spatio-temporal learning tasks: the temporal addition task, the temporal copying task, and a spoken-phrase recognition task. Application of these results will lead to the development of powerful spiking models for neuromorphic hardware that solve relevant edge-computing and Internet-of-Things applications with high accuracy and ultra-low power consumption.", "review": "Review:###The submission describes an adaptive spiking neuron model that is based on the Laplace transform of the model output. This reformulation allows to train a recurrent neural network of spiking neural network with good performances. Three tasks are proposed to assess the recurrent net, on synthetic data and real signal. I think this contribution could not be accepted for a methodological problem: the main idea is to use neuromorphic chips that use spiking neural networks with an approximation that is basically similar to an artificial neural network. The authors need to introduce a complex rescaling of the time step, repercussion on the computation. This point should be shown experimentally. The authors introduced a modification of the aI&F and are applying low pass filtering the spike trains. As all the results are obtained on a python simulator, it is difficult to assess the interest and the applicability to real neuromorphic chips. As pointed out by the authors, the main interest of these architectures are the lower energy consumption than CPU/GPU-based architecture. Unfortunately, it is not possible to assess if the proposed neuron model is working on energy efficient architecture. [1] Nair, M. V. and Indiveri, G. (2019). An ultra-low power sigma-delta neuron circuit. In 2019 IEEE International Symposium on Circuits and Systems (ISCAS), pages 1–5."}
{"id": "iclr2020_485", "title": "Implicit Generative Modeling for Efficient Exploration | OpenReview", "abstract": "Abstract:###Efficient exploration remains a challenging problem in reinforcement learning, especially for those tasks where rewards from environments are sparse. A commonly used approach for exploring such environments is to introduce some *intrinsic* reward. In this work, we focus on model uncertainty estimation as an intrinsic reward for efficient exploration. In particular, we introduce an implicit generative modeling approach to estimate a Bayesian uncertainty of the agent*s belief of the environment dynamics. Each random draw from our generative model is a neural network that instantiates the dynamic function, hence multiple draws would approximate the posterior, and the variance in the future prediction based on this posterior is used as an intrinsic reward for exploration. We design a training algorithm for our generative model based on the amortized Stein Variational Gradient Descent. In experiments, we compare our implementation with state-of-the-art intrinsic reward-based exploration approaches, including two recent approaches based on an ensemble of dynamic models. In challenging exploration tasks, our implicit generative model consistently outperforms competing approaches regarding data efficiency in exploration.", "review": "Review:###Summary: This paper introduces a new intrinsic reward for aiding exploration. This one is based on learning a distribution on parameters for a neural network which represents the dynamic function. The variance the predictions from this dynamic function serves as the intrinsic reward. Results are compared against several current state-of-the-art approaches. Feedback: Using uncertainty as an instrinc reward to guide exploration is a very active area of research and it would have been more helpful to say how this work differs from Burda et al, Eysenbach et al, Gregor et al. 1. The underlying algorithms are all very similar and differ in only small and subtle ways. The main difference with this work and Pathak et al. seems to be that the variance is all coming from one particular conditional distribution rather than an ensemble of models, but in Pathak et al it is also a distribution over models. Amortized SVGD is used instead of regular SVI in this work, but it is never articulated why to problem benefits from using that framework. This paper would greatly benefit from some explanation. It is mentioned as a novel aspect of the work, but never really justified at all. The experimental results and convincing and do show a substantial improvements over similar approaches in domains in ant maze navigation and robot hand. [1] Gregor, Karol, Danilo Jimenez Rezende, and Daan Wierstra. *Variational intrinsic control.* arXiv preprint arXiv:1611.07507 (2016)."}
{"id": "iclr2020_486", "title": "Adversarial Paritial Multi-label Learning | OpenReview", "abstract": "Abstract:###Partial multi-label learning (PML), which tackles the problem of learning multi-label prediction models from instances with overcomplete noisy annotations, has recently started gaining attention from the research community. In this paper, we propose a novel adversarial learning model, PML-GAN, under a generalized encoder-decoder framework for partial multi-label learning. The PML-GAN model uses a disambiguation network to identify noisy labels and uses a multi-label prediction network to map the training instances to the disambiguated label vectors, while deploying a generative adversarial network as an inverse mapping from label vectors to data samples in the input feature space. The learning of the overall model corresponds to a minimax adversarial game, which enhances the correspondence of input features with the output labels. Extensive experiments are conducted on multiple datasets, while the proposed model demonstrates the state-of-the-art performance for partial multi-label learning.", "review": "Review:###The authors propose a novel GAN-based method to tackle PML problem. By using Encoder-Decoder architecture, the authors combine four neural networks including disambiguator, predictor, generator, and discriminator to implement the integrated model and get an overall good performance in various experiment settings and datasets. Pros: - It*s novel to introduce GAN to solve the PML problem. -The model works better than other state-of-art models overall. Cons: - The novelty of this paper is limited to some extent. It seems that the model just combines ideas of GAN and PML. - I wonder if GAN still works in this architecture in some situations. Under the condition when the dimension of labels is small enough, it will be hard to generate good samples because the information which can be utilized for the generator mightn*t be sufficient. However, when the dimension of labels is large, there will be some combinations of labels that aren*t in the training set, which may harm the performance of the generator. - Because the relations between instances and labels aren*t one-to-one in most cases, I wonder whether the five-layer perceptron still works well as a generator if one setting of labels can correspond to various kinds of instances. - It will be better to compare the generator part proposed in this paper with a simple interpolation method in the process of extending the dataset."}
{"id": "iclr2020_487", "title": "Spectral Nonlocal Block for Neural Network | OpenReview", "abstract": "Abstract:###The nonlocal network is designed for capturing long-range spatial-temporal dependencies in several computer vision tasks. Although having shown excellent performances, it needs an elaborate preparation for both the number and position of the building blocks. In this paper, we propose a new formulation of the nonlocal block and interpret it from the general graph signal processing perspective, where we view it as a fully-connected graph filter approximated by Chebyshev polynomials. The proposed nonlocal block is more efficient and robust, which is a generalized form of existing nonlocal blocks (e.g. nonlocal block, nonlocal stage). Moreover, we give the stable hypothesis and show that the steady-state of the deeper nonlocal structure should meet with it. Based on the stable hypothesis, a full-order approximation of the nonlocal block is derived for consecutive connections. Experimental results illustrate the clear-cut improvement and practical applicability of the generalized nonlocal block on both image and video classification tasks.", "review": "Review:###I have two general concerns, the first is related to the presentation and the second to the relevance of the results. (1) Presentation is confusing at many points, for instance: * It is unclear if theorem in Eq. 4 is original or belongs to Shuman et al 2013. (no proof is given) * Eq. 8 seems an arbitrary decomposition of the original NonLocal operator that could have been proposed without any reference to the Chebyshev expansion (which, on the other hand, is truncated to 1st order with no extra explanation). * The point of Fig. 1 and Fig 4 is not clear. Fig. 1 explains how SpectralNonLocal reduces to NonLocal and NonLocalStage, but we can see this from the formulas. I dont see how this discussion on the Ws relates to the regions highlighted in the bird. The same applies to Fig. 4. What are we supposed to see in Fig. 4 (and, more importantly, why?). * What is the CFL condition? (is it the Courant-Friedrichs–Lewy sampling condition?). How is that related to the values of Ws. Can we take those arbitrarily small as suggested in that proof? * The upper limit in the sums after Eq. 10 is unclear. * First time table 4.2 is cited there is no context to understand it. (actually there is no table labeled as *Table 4.2*) Where do we see the different number of NonLocal units?. This is only clear when you arrive and read text of page 9 (but not when cited the first time from page 6). * Explanation of experiments is a little bit confusing (e.g. what does it mean top1 and top5 in tables?). The only explanation of *top-something* I found in the text has to do with eigenvectors in fig. 5. This also apply to the *topX* in the figures? (2) Nevertheless, the main concern is the scarce relevance of the results: differences of behavior in all tables are about 1%. Then, what is the real advantage of the proposed modification?"}
{"id": "iclr2020_488", "title": "Context-aware Attention Model for Coreference Resolution | OpenReview", "abstract": "Abstract:###Coreference resolution is an important task for gaining more complete understanding about texts by artificial intelligence. The state-of-the-art end-to-end neural coreference model considers all spans in a document as potential mentions and learns to link an antecedent with each possible mention. However, for the verbatim same mentions, the model tends to get similar or even identical representations based on the features, and this leads to wrongful predictions. In this paper, we propose to improve the end-to-end system by building an attention model to reweigh features around different contexts. The proposed model substantially outperforms the state-of-the-art on the English dataset of the CoNLL 2012 Shared Task with 73.45% F1 score on development data and 72.84% F1 score on test data.", "review": "Review:###This paper proposes to use an extra feature (grammatical number) for context-aware coreference resolution and an attention-based weighting mechanism. The approach proposed is built on top of a recent well performing model by Lee et al. The improvement is rather minor in my view: 72.64 to 72.84 in the test set. There is not much in the paper to review. I don*t think the one extra feature warrants a paper at a top conference. The weighting mechanism over the features is also unclear to me why it benefits from attention. Couldn*t we just learn the weights using another layer? It could be context dependent if desired. It is also incorrect to criticise Lee et al. (2018) that they would give the same representation to the same mention every time. Their model is context dependent as they use a BiLSTM over the sentence. Of course the same mentions are likely to get similar representations, but this is desirable."}
{"id": "iclr2020_489", "title": "Scaling Laws for the Principled Design, Initialization, and Preconditioning of ReLU Networks | OpenReview", "abstract": "Abstract:###Abstract In this work, we describe a set of rules for the design and initialization of well-conditioned neural networks, guided by the goal of naturally balancing the diagonal blocks of the Hessian at the start of training. We show how our measure of conditioning of a block relates to another natural measure of conditioning, the ratio of weight gradients to the weights. We prove that for a ReLU-based deep multilayer perceptron, a simple initialization scheme using the geometric mean of the fan-in and fan-out satisfies our scaling rule. For more sophisticated architectures, we show how our scaling principle can be used to guide design choices to produce well-conditioned neural networks, reducing guess-work.", "review": "Review:###I think the topic of the paper is interesting, though I think in its current form the paper is not ready. First of all I find the empirical section quite weak. While the authors attempt to formalize their intuition, as they mention in the work itself, such works are somewhat outside mathematical proof. This is due to the many approximations needed, and assumptions that can not hold in practice. As such the main experimental results in running AlexNet on Cifar-10 and LIBSVM datasets. I think more experimental evidence is needed, e.g. more architectures, more dataset (maybe different data modalities). Is hard to tell from this one main experiment (Cifar 10) to what extend one can trust this initialization. I think is worth noting that KFAC and (all?) cited methods actually use the Fisher Information matrix (hence being forms of natural gradient) and not the Hessian. The extended Gauss-Newton approximation is indeed the Fisher matrix, I think as discussed in Martens* work (which is heavily cited) though in other works as well. Just as an additional note, the extended Gauss-Newton was introduced in Schraudolph, N. N. (2002). *Fast curvature matrix-vector products for second-order gradient descent* where it was presented as an approx to the Hessian, this was used by Martens and later the community (and he himself) observed that actually rather than an approx to Hessian this can be thought of as the Fisher Information Matrix. Relying on the expected squared singular value should be motivated better. The reasoning sounds fine (i.e. minimum and maximum would be too pessimistic) but no data is given. Some statistics over multiple runs. Overall this is a repeating theme in the work. While everything makes sense intuitively, I would have hoped more rigor. More empirical evidence for any such choice. The weight-to-gradient ratio is not a commonly used measure. Maybe show how this ratio progresses over time when training a model. Having multiple runs showing the correlation between things. Table 2 is not referenced in the text. While an average over 10 seeds is provided for Cifar, no error bars. Overall I think the direction of the work is interesting. And definitely we have not heard the last word on initialization. It plays a crucial role (and indeed bad initialization can easily induce bad local minima (https://arxiv.org/abs/1611.06310)). But I think the paper needs to be written more carefully, with a more thorough empirical exploration, showing different architectures, different datasets. Maybe trying to break the usual initialization, and showing you can do considerably better with newer initialization."}
{"id": "iclr2020_490", "title": "Multi-Step Decentralized Domain Adaptation | OpenReview", "abstract": "Abstract:###Despite the recent breakthroughs in unsupervised domain adaptation (uDA), no prior work has studied the challenges of applying these methods in practical machine learning scenarios. In this paper, we highlight two significant bottlenecks for uDA, namely excessive centralization and poor support for distributed domain datasets. Our proposed framework, MDDA, is powered by a novel collaborator selection algorithm and an effective distributed adversarial training method, and allows for uDA methods to work in a decentralized and privacy-preserving way.", "review": " ###Summary### This paper tackles unsupervised domain adaptation in a decentralized setting. The high-level observation is that the conventional unsupervised domain adaptation has two bottlenecks, namely excessive centralization and poor support for distributed domain datasets. The paper proposes Multi-Step Decentralized Domain Adaptation (MDDA) to transfer the knowledge learned from the source domain to the target domain without sharing the data. The paper also explores explore a proposition: in addition to adapting from the labeled source, can uDA leverage the knowledge from other target domains, which themselves may have undergone domain adaptation in the past. The proposed MMDA method contains a feature extractor (E), a domain discriminator (D) and task classifier (C) for each domain. The target domain components are initialized with the respective source components. The source domain discriminator D_s target domain discriminator D_t are synchronized by exchanging and averaging the gradients. The paper also proposes Lazy Synchronization to reduce the communication cost of the algorithm. The paper also proposes Wasserstein distance guided collaborator selection schema to perform the domain adaptation task. The paper performs experiments on five image and audio datasets: Rotated MNIST, Digits, and Office-Caltech, DomainNet and Mic2Mic. The baselines used in this paper include *Labeled Source*, *Random Collaborator*, and *Multi-Collaborator*. The experimental results demonstrate that the proposed method can outperform the baselines on some of the experimental settings. The paper also provides a detailed analysis of the model and experimental results. ### Novelty ### This paper does not propose a new domain adaptation algorithm. However, the paper introduces some interesting tricks to solve the MMDA task such as the lazy synchronization between the source domain discriminator and the target domain discriminator. ###Clarity### Several critical explanations are missing from the paper: 1) When training the source domain discriminator D_s and target domain discriminator D_t, if the features between the source domain and target domain cannot be shared with each other, how to train the D_s and D_t. For example, the D_s cannot get access to the features from the target domain, how to train D_s? 2) How is the target classifier C_t updated when there are no labels for the target domain? 3) As far as I understand, the domain discriminator is this paper is trained adversarially. The detailed adversarial training step is unclear. ###Pros### 1) The paper proposes an interesting transfer learning schema where the data between the source and target domain can not be shared with each other to protect the data-privacy. 2) The paper provides extensive experiments on multiple standard domain adaptation benchmarks, especially the most recent dataset such as the DomainNet. 3) The paper provides detail empirical analysis to demonstrate the effectiveness of the proposed methods. ###Cons### 1) The most critical issue of this paper is that some explanations are missing, e.g. how are D_s, D_t, C_t trained? Refer to the #Clarity. 2) The presentation and writing of this paper need polish. The author should do more relative surveys to motivate the authors. One critical relevant reference of this paper is: *Secure Federated Transfer Learning*, Yang Liu et al https://arxiv.org/pdf/1812.03337.pdf 3) The baselines used in this paper is also trivial. It is desirable to compare the proposed method with state-of-the-art domain adaptation methods. Based on the summary, cons, and pros, the current rating I am giving now is *reject*. I would like to discuss the final rating with other reviewers, ACs. To improve the rating, the author should explain the questions I proposed in the #Clarity"}
{"id": "iclr2020_491", "title": "ODE Analysis of Stochastic Gradient Methods with Optimism and Anchoring for Minimax Problems and GANs | OpenReview", "abstract": "Abstract:###Despite remarkable empirical success, the training dynamics of generative adversarial networks (GAN), which involves solving a minimax game using stochastic gradients, is still poorly understood. In this work, we analyze last-iterate convergence of simultaneous gradient descent (simGD) and its variants under the assumption of convex-concavity, guided by a continuous-time analysis with differential equations. First, we show that simGD, as is, converges with stochastic sub-gradients under strict convexity in the primal variable. Second, we generalize optimistic simGD to accommodate an optimism rate separate from the learning rate and show its convergence with full gradients. Finally, we present anchored simGD, a new method, and show convergence with stochastic subgradients.", "review": "Review:###The paper studies last-iterate convergence of simultaneous gradient descent and related algorithms in (convex-concave) GANs. The experiments are very weak. Figure 4 shows that anchored Adam outperforms Adam and optimistic Adam in terms of FID on MNIST, but not CIFAR-10. The authors cite many of the vast number of algorithms that have been proposed to train GANs (Heusel, Mescheder, Gidel, Gemp, and on and on ... and on…) and discuss some of them in the analysis. However, when it comes to experiments, they only compare against vanilla Adam (!) and Optimism. I would summarize the experiments as *suggesting* that anchoring doesn’t hurt on MNIST or CIFAR-10. The paper doesn’t tune beta and gamma, so it’s hard to be sure. The motivation for the paper is GANs, but GANs are not convex-concave. The analysis is therefore not directly relevant. From the experiments, it is not clear *at all* whether anchored Adam is *actually* an improvement in practice over any of the alternative algorithms that the authors discuss or cite. In short, the contribution is unclear. The analysis is better suited to a venue like COLT. To be a reasonable ICLR submission, the paper needs to compare against more baselines at a bare minimum."}
{"id": "iclr2020_492", "title": "Vid2Game: Controllable Characters Extracted from Real-World Videos | OpenReview", "abstract": "Abstract:###We extract a controllable model from a video of a person performing a certain activity. The model generates novel image sequences of that person, according to user-defined control signals, typically marking the displacement of the moving body. The generated video can have an arbitrary background, and effectively capture both the dynamics and appearance of the person. The method is based on two networks. The first maps a current pose, and a single-instance control signal to the next pose. The second maps the current pose, the new pose, and a given background, to an output frame. Both networks include multiple novelties that enable high-quality performance. This is demonstrated on multiple characters extracted from various videos of dancers and athletes.", "review": " This paper proposes a method to address the interesting task, i.e. controllable human activity synthesis, by conditioning on the previous frames and the input control signal. To synthesis the next frame, a Pose2Pose network is proposed to first transfer the input information into the next frame body structure and object. Then, a Pose2Frame network is applied to generate the final result. The results on several video sequences look nice with more natural boundaries, object, and backgrounds compared to previous methods. Pros: 1. The proposed Pose2Pose successfully transfer the pose conditioned on the past pose and the input control signal. The proposed conditioned residual block, occlusion augmentation and stopping criteria seem to help the Pose2Pose network work well. Besides, the object is also considered in this network, which makes the method generalized well to the videos where human holds some rigid object. 2. The Pose2Frame network is similar to previous works but learns to predict the soft mask to incorporate the complex background and to produce shallow. The mask term in Eq. (7) seems to work well for the foreground (body+object) and the shallow regions. 3. The paper is easy to follow. Cons: 1. Since the method is only evaluated on several video sequences, I am not sure how the method will perform on other different scenes. Results on more scenes will make the performance more convincing. I also wonder if the video data will be released, which could be important for the following comparisons. 2. As to the results of the Pose2Pose network, I wonder if there are some artifacts that will affect the performance of the Pose2Frame network. Then, there will be another question: how the two networks are trained? Are they trained separately or jointly? I assume the authors first train the Pose2Pose network, then use the output to train the Pose2Frame network. Otherwise, the artifacts from Pose2Pose will affect the testing performance of the Pose2Frame network. 3. The mask term seems to work well for the shallow part. I wonder how the straightforward regression term plus the smooth term will perform for the mask. Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?"}
{"id": "iclr2020_493", "title": "Learning Classifier Synthesis for Generalized Few-Shot Learning | OpenReview", "abstract": "Abstract:###Object recognition in real-world requires handling long-tailed or even open-ended data. An ideal visual system needs to reliably recognize the populated visual concepts and meanwhile efficiently learn about emerging new categories with a few training instances. Class-balanced many-shot learning and few-shot learning tackle one side of this problem, via either learning strong classifiers for populated categories or learning to learn few-shot classifiers for the tail classes. In this paper, we investigate the problem of generalized few-shot learning (GFSL) -- a model during the deployment is required to not only learn about *tail* categories with few shots, but simultaneously classify the *head* and *tail* categories. We propose the Classifier Synthesis Learning (CASTLE), a learning framework that learns how to synthesize calibrated few-shot classifiers in addition to the multi-class classifiers of ``head** classes, leveraging a shared neural dictionary. CASTLE sheds light upon the inductive GFSL through optimizing one clean and effective GFSL learning objective. It demonstrates superior performances than existing GFSL algorithms and strong baselines on MiniImageNet and TieredImageNet data sets. More interestingly, it outperforms previous state-of-the-art methods when evaluated on standard few-shot learning.", "review": " This paper proposes a method for generalized few-shot learning (GFSL) where the test data set contains samples of both base and novel classes. This is a natural extension of the dominant conventional FSL setting, under which the test data comes only from the novel classes. The proposed method, termed CASTLE follows a very similar pipeline as the method of Gidaris & Komodakis (2018). Specifically, the synthesized classifier weight vector is a combination of both class prototype (mean of the few shots) and a weighted sum of some basis vectors. The difference is that in Gidaris & Komodakis (2018), the bases are the base class weight vectors, and in this work, the bases are learned from the data. These bases or neural dictionary of classifiers are based on the models in (Changpinyo et al., 2016; 2018) developed for zero-shot learning. GFSL clearly should be the next focus for few-shot learning and this paper does advance the state-of-the-art under this setting. However, I can’t help but notice the similarity between this work and that in Gidaris & Komodakis (2018). The changes are based on borrowing ideas from existing zero-shot learning works in (Changpinyo et al., 2016; 2018). So the contribution is incremental. There are indeed some improvements on performance over Gidaris & Komodakis (2018). However, the multi-classifier learning brings 1-2% from the Appendix. The residual formation in Eq. (5) also helps. So one would expect that adding these two tricks to Gidaris & Komodakis (2018) would bring the performance closer. Section 3.2 is unclear to me: is the base class classifier parameter \thetha_S updated during the episodic training process? Eq. (7) suggests it is, but the text suggests otherwise. Any reason for not using the commonly used low-shot imagenet dataset and CUB for the GFSL experiments? It would also be useful to examine different CNN backbones including the less powerful conv-4-64 and more powerful wide ResNet based architectures. It is well known that it makes a big difference."}
{"id": "iclr2020_494", "title": "Efficient Multivariate Bandit Algorithm with Path Planning | OpenReview", "abstract": "Abstract:###In this paper, we solve the arms exponential exploding issues in multivariate Multi-Armed Bandit (Multivariate-MAB) problem when the arm dimension hierarchy is considered. We propose a framework called path planning (TS-PP) which utilizes decision graph/trees to model arm reward success rate with m-way dimension interaction, and adopts Thompson sampling (TS) for heuristic search of arm selection. Naturally, it is quite straightforward to combat the curse of dimensionality using a serial processes that operates sequentially by focusing on one dimension per each process. For our best acknowledge, we are the first to solve Multivariate-MAB problem using graph path planning strategy and deploying alike Monte-Carlo tree search ideas. Our proposed method utilizing tree models has advantages comparing with traditional models such as general linear regression. Simulation studies validate our claim by achieving faster convergence speed, better efficient optimal arm allocation and lower cumulative regret.", "review": "Review:###In this paper, the authors address the curse of dimensionality in Multivariate Multi-Armed Bandits by framing the problem as a sequential planning problem. Their contributions are the following: 1. They propose to solve the Multivariate MAB sequentially across each dimension, which I think is a reasonable and not very common approach in the literature compared to e.g. structured bandits, even though the authors themselves call this approach *natural* and *quite straightforward*. 2. They propose a framework based on a Monte-Carlo Tree Search procedure to solve this sequential decision problem. 3. They introduce several approximations and heuristics to avoid traversing the entire tree and trade-off performance for computational complexity, which they evaluate empirically. The obtained results seem to support their original claim: the tree-based approaches (e.g. FPF) perform better than the considered baselines. However, this claim is at the same time contradicted by the fact that a depth-one planning procedure with heuristic leaf evaluation (PPF2) reaches similar performances, which indicates that the underlying problem can be solved greedily and does not involve long-term (m-way) interations and limits the interest of the main idea in this paper. This is probably due to the design of experiments which only model pairwise interactions, but then this questions the relevance of the proposed empirical evaluation with respect to the claim. Generally, though several good ideas were presented, I felt that they were not properly motivated and that the paper generally lacks rigor and clarity. The many variants and modeling decisions proposed seem quite arbitrary, and in the end fail to provide any valuable insight. Several aspects of the proposed approach and algorithms were not clear or justified. I will list some of them, grouped by the corresponding contribution: 1. Multivariate MAB framed as a planning problem. No intuition is provided regarding why it is expected to perform any better than classical N^D-MAB. In this absence of assumption regarding additional structure to the problem considered (such as, say, the independence assumption in D-MABS), is there any reason to think so? (for instance, all the N^D arms are present at the leaves of the tree) The authors only mention in their conclusion that they leverage a *hierarchy dimension structure*, but this claimed structure was is never defined nor formalized in the paper. I can only guess that such a structure would involve a particular ordering of dimensions such that shallow ones have more influence on the payoff than the deeper ones? But then, it would be unknown in practice, and the choice of the particular ordering used in TS-PP is expected to have a crucial influence on its performance. Yet, the sensitivity of their algorithms to the ordering is never discussed by the authors -they choose it arbitrarily- nor evaluated in their experiments. 2. The proposed TS-PP framework - The authors propose using MCTS with TS instead of UCB as a sampling rule, which they claim is quite novel: *Applying TS as node selection policy is not well investigated from our best knowledge*. However, the authors do not mention important reference (Bai et al, 2014) until the very end of the paper in their conclusion: *We notice some related work dealing with continuous rewards in this area*. Yet, Bai et al. also consider Bernoulli rewards in their experiments and use conjugate Beta priors, exactly like the authors of the present work do. Moreover, the choice of TS rather than UCB is not properly motivated, and the UCT algorithm is not considered as a baseline in the experiments. Hence, there is no basis to assess whether TS is particularly relevant or not for this problem. - They also propose to repeat S times the candidate searching procedure in Algorithm 1 in order not to be *[stuck] in sub-optimal arms*. This property is normally ensured by optimistic sampling rules, it is not clear why it is required here. More importantly, the sensitivity of the Algorithm 1 to S is not evaluated. - I did not fully understand the Candidates Construction Stage in Algorithm 1. For instance, in the case of FPF, for each search c=1..S (l3) there is a call (l4) to Full Path Finding() to generate a candidate A^c. From what I understand, a random ordering is chosen (Algorithm 2, l2), and a single rollout is performed (l2, the dimensions 1->D are iterated from root to leaf) to return a candidate. When we move to the next search c -> c+1, since a new ordering d_1:D is chosen then the previous tree storing the models for p(fd_i | fd_1...fd_i-1) is not relevant anymore and must be discarded. Does that mean that the priors at the nodes are recomputed for each search/ordering/tree given the entire history, instead of being maintained and updated? The authors do state that *FFP utilizes hidden states from the same decision tree*, but I do not see how this is feasible when the dimensions ordering changes at each iteration. - This confusion is related to the fact that the update of the Beta posterior does not appear anywhere in Algorithm 1 and 2. This is unfortunate, especially considering that contrary to TS in a normal MAB setting, the observations for an internal node are not i.i.d. since they depend on the underlying planning procedure, which induces a distributional drift as more sample are collected and probably make the derived posterior invalid. Has this issue been considered by the authors? 3. The variants of FPF The idea behind Partial-PF-m is common in the tree-search literature: to stop at early depths and use a heuristic to estimate the value at the leaf. Here, the suggestion in the particular context of Multivariate-MAB to use TS with an independence assumption (D-MABs) is interesting and sound. However, It requires the knowledge the degree m of interactions, whereas the idea of MCTS is precisely to avoid using such heuristics for leaf evaluation by replacing them without random rollouts. Other methods such as DS start at a leaf and move locally which rather resembles Hill-climbing more closely than actual Tree-Search, and I do not really see how they fit in the TS-PP framework? Finally, I found the manuscript difficult to read due to many language mistakes and typos. Minor comments: - The citation style is incorrect. - What is the point of introducing contextual MABs in the problem formulation when the context is not going to play any part whatsoever in the presented work? This only brings unnecessary complexity and impedes clarity. - In the experiments, H=100 replications are performed, which is appreciated, but unfortunately the corresponding confidence intervals or standard deviations are not shown, only the mean performance. - I found the section 3.1 unclear. Is B_A^T mu a dot product? Where do the coefficients of B_A or its length M appear in (1)? How is M chosen, how is B_A obtained? - Figure 1.a is also unclear, the graph seems to be used to define permutations/orderings of dimensions 1..D. But all paths are not valid (eg. d1 -> d2 -> d1), ad random orderings are used in Algorithm 2, so where is this graph used exactly? ============= After Rebuttal: On one hand, some of the points mentioned are now clearer (e.g. 1 & 2.2, 2.3). On the other hand, most of my concerns remain valid, and some were not addressed. Typically, the dependency to an unknown (and undefined...) optimal ordering and the underlying optimization procedure (random search) are not properly discussed, whereas they seem central to justify the tree-based approach. Other remaining concerns are more generally the lack of justification and clarity of the proposed framework and implemented variants, and the inadequacy of the experiments with respect to the claim (using m=2 basically means only the first depth of the tree is useful). Therefore, I will maintain my initial score."}
{"id": "iclr2020_495", "title": "Semi-Supervised Generative Modeling for Controllable Speech Synthesis | OpenReview", "abstract": "Abstract:###We present a novel generative model that combines state-of-the-art neural text- to-speech (TTS) with semi-supervised probabilistic latent variable models. By providing partial supervision to some of the latent variables, we are able to force them to take on consistent and interpretable purposes, which previously hasn’t been possible with purely unsupervised methods. We demonstrate that our model is able to reliably discover and control important but rarely labelled attributes of speech, such as affect and speaking rate, with as little as 1% (30 minutes) supervision. Even at such low supervision levels we do not observe a degradation of synthesis quality compared to a state-of-the-art baseline. We will release audio samples at https://tts-demos.github.io/.", "review": "Review:###The authors propose to do neural text to speech, conditioned on attributes such as valence and arousal and speech rate. A seq-to-seq network is trained using stochastic gradient variational Bayes. The idea is interesting and new. The method section could be made clearer by giving first some intuition, explaining the formulas in the prose and introducing the terms used. Regarding the crowd sourced MOS: how were the ratings obtained? how many subjects were used for the rating? How were they selected? Was each rater presented with samples from each method? Or was each method assessed by different groups of raters? The experimental setting is a little weak, relying mostly on the Mean Opinion Score. It would be useful to include more evalution, for instance: * run a speech recognition method on the generated speech and measure the error rate * examples could be included in the supplementary (e.g. spectrograms) * For the evaluation of emotional speech to be meaningful, the proposed classifier should be tested, and more detailed given (hyper-parameters, training setting, validation/test splits?, etc). In particular, it would be useful to compare the proposed method for affect classification to an existing (state-of-the-art) methodology. A state-of-the-art method should be ideally be directly used to classify the generated sequences into emotional classes. The authors collected data in studio conditions, as such it is hard to compare. Will the data be released? How much hdata was collected? It seems that what the authors effectively do is condition on discrete emotion classes, not valence and arousal, which are continuous measures of affect."}
{"id": "iclr2020_496", "title": "Are there any *object detectors* in the hidden layers of CNNs trained to identify objects or scenes? | OpenReview", "abstract": "Abstract:###Various methods of measuring unit selectivity have been developed with the aim of better understanding how neural networks work. But the different measures provide divergent estimates of selectivity, and this has led to different conclusions regarding the conditions in which selective object representations are learned and the functional relevance of these representations. In an attempt to better characterize object selectivity, we undertake a comparison of various selectivity measures on a large set of units in AlexNet, including localist selectivity, precision, class-conditional mean activity selectivity (CCMAS), network dissection, the human interpretation of activation maximization (AM) images, and standard signal-detection measures. We find that the different measures provide different estimates of object selectivity, with precision and CCMAS measures providing misleadingly high estimates. Indeed, the most selective units had a poor hit-rate or a high false-alarm rate (or both) in object classification, making them poor object detectors. We fail to find any units that are even remotely as selective as the *grandmother cell* units reported in recurrent neural networks. In order to generalize these results, we compared selectivity measures on a few units in VGG-16 and GoogLeNet trained on the ImageNet or Places-365 datasets that have been described as *object detectors*. Again, we find poor hit-rates and high false-alarm rates for object classification.", "review": "Review:###The paper makes an empirical claim that CNNs for object recognition do not contain hidden neuron which is highly selective to each class, mainly based on three aspects: (a) metrics related to the maximum informedness, (b) jitterplots of activation data, and (c) a user study assessing whether generated images maximizing a given unit is perceptible to the user. The paper point out these results are in contrast to the case of RNN, where it has been reported that many localist hidden units emerge. It is also noticed that the existing metrics for selectivity do not adequately discriminate highly selective units in CNN. In overall, the manuscript is well-written and easy-to-follow. I particularly appreciated a kindly presented overview on the literature and thoroughly conducted experiments including a complete user study. One of my key concerns, however, is that I am still not fully convinced whether the key finding in this paper - the lack of highly selective units in CNNs - is an indeed important problem for ICLR community: Personally, I feel the *existence* of selective units in RNN could be interesting, but the *non-existence* in the case of CNN is not that surprising for some readers, as it seems much likely (at least to me): The final layer of CNN would be surely selective across classes, but it may be not the case for the hidden layers - Nevertheless, some of the units may act selectively, not across classes but in some other concepts: e.g. stripes, orientations, etc. Therefore, I wonder if the paper could further provide a discussion on the importance of the key finding. - In Section 2 - Network and Dataset - *... We selected 233 ...* : Which criteria is actually used to choose the candidate units for the analysis? - Do the overall results mean that the *maximum informedness*-based metrics are superior to the others for assessing selectivity of a unit? Also, are those metrics original to this paper? - Currently, I feel the concept of *selectivity* is presented in somewhat subjectively: the definition could vary across the context. It would be nice if this could be more formalized to support the claims in the paper, e.g. the superiority of the proposed metrics."}
{"id": "iclr2020_497", "title": "PROTOTYPE-ASSISTED ADVERSARIAL LEARNING FOR UNSUPERVISED DOMAIN ADAPTATION | OpenReview", "abstract": "Abstract:###This paper presents a generic framework to tackle the crucial class mismatch problem in unsupervised domain adaptation (UDA) for multi-class distributions. Previous adversarial learning methods condition domain alignment only on pseudo labels, but noisy and inaccurate pseudo labels may perturb the multi-class distribution embedded in probabilistic predictions, hence bringing insufficient alleviation to the latent mismatch problem. Compared with pseudo labels, class prototypes are more accurate and reliable since they summarize over all the instances and are able to represent the inherent semantic distribution shared across domains. Therefore, we propose a novel Prototype-Assisted Adversarial Learning (PAAL) scheme, which incorporates instance probabilistic predictions and class prototypes together to provide reliable indicators for adversarial domain alignment. With the PAAL scheme, we align both the instance feature representations and class prototype representations to alleviate the mismatch among semantically different classes. Also, we exploit the class prototypes as proxy to minimize the within-class variance in the target domain to mitigate the mismatch among semantically similar classes. With these novelties, we constitute a Prototype-Assisted Conditional Domain Adaptation (PACDA) framework which well tackles the class mismatch problem. We demonstrate the good performance and generalization ability of the PAAL scheme and also PACDA framework on two UDA tasks, i.e., object recognition (Office-Home,ImageCLEF-DA, andOffice) and synthetic-to-real semantic segmentation (GTA5?CityscapesandSynthia?Cityscapes).", "review": " This paper proposes to leverage prototypes to solve the mismatch problem in unsupervised domain adaptation. It further imposes intra-class compactness to help ambiguous classes. Experiments show it achieves new state-of-the-art results in several datasets. pros: + intra-class compactness to help ambiguous classes concerns: -- Prototypes does not come from nowhere. They come from predictions. If you worry about the quality of target predictions (pseudo labels), then Eq. 8 and Eq. 9 are questionable. The intra-class compactness relies on p_t, too. The authors should explain why prototypes are superior than pseudo labels in [1]. -- How does the authors select hyper-parameters? There are lots of magic numbers in Section 4.1 about hyper-parameters but no clues about how to tune them. Recently there is a paper [2] about model selection for UDA, maybe the authors should try it. details: - terminology: *intra-class* is better than *within class* - separate citations: e.g. entropy minimization, mean-teacher, and virtual adversarial training, have been successfully applied to UDA (Vu et al., 2019; French et al., 2018; Shu et al., 2018) -> entropy minimization (Vu et al., 2019), mean-teacher (French et al., 2018), and virtual adversarial training (Shu et al., 2018), have been successfully applied to UDA - confusion: At the last of Section 3.2, it says hat{f}=M^{T}p. But in Eq. 9, hat{f} and M^{T}p are concatenated, which is confusing: why do you concatenate two identical vectors? - Implementation Details: Section 4.1, paragraph 4: lambda^{f}_{adv} =5e-3, lambda^{f}_{adv} and lambda^{p}_{adv} increase from 0 to 1. It is confusing that lambda^{f}_{adv} both is a constant and changes continuously. [1] Conditional adversarial domain adaptation, Long et.al, in NeurIPS 2018 [2] Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation, You et.al, in ICML 2019"}
{"id": "iclr2020_498", "title": "A GOODNESS OF FIT MEASURE FOR GENERATIVE NETWORKS | OpenReview", "abstract": "Abstract:###We define a goodness of fit measure for generative networks which captures how well the network can generate the training data, which is necessary to learn the true data distribution. We demonstrate how our measure can be leveraged to understand mode collapse in generative adversarial networks and provide practitioners with a novel way to perform model comparison and early stopping without having to access another trained model as with Frechet Inception Distance or Inception Score. This measure shows that several successful, popular generative models, such as DCGAN and WGAN, fall very short of learning the data distribution. We identify this issue in generative models and empirically show that overparameterization via subsampling data and using a mixture of models improves performance in terms of goodness of fit.", "review": "Review:###This work proposed a new goodness of fit measure for generative network evaluations, which is based on how well the network can generate the training data. The measure is zero if the network could perfectly recover the training data, and would represent how far it is from generating the training set in the average manner of the total least square sense, where the one-to-one mapping between the generated data and the training sample is constructed through latent space optimization. Using the proposed measure, the authors showed an interesting trend present in the DCGAN training and the impact of the residual connection. The authors might want to add some discussion in Section 4.2 regarding why the residual connection is detrimental for covering the support. Increasing the model complexity through larger latent space dimension and learning mixtures is proposed as solutions to improve the measure as well. With all the interesting results presented, I still have the concerns about the sensitivity of the proposed measure: - It is an average over the training data or the selected sample. Above Section 4, the authors argued that *hat{F}(G) > 0 meaning that we do not observe any memorization*. This seems overly assertive. Since the measure is an average over the training data, it has difficulty to differentiate between one network which has almost zero value for part of the training data but large values for the rest, and another network with roughly the same hat{F}(G) value but small values for all training data. The variance could help, but can not resolve this issue. This would be more important when the training data contains noise or outliers. - It only concerns the generation of the training data, but not the sampled data from the network (at least not directly). Therefore it has no direct control of the fidelity of the generated samples. - As shown by the authors, the proposed measure can be considered as the approximation of the true probability support not covered by the generative models, which also defines a necessary condition to avoid mode collapse. But what about the other part? It would have difficulty comparing two models with the same support but different high-density areas. Indeed, there are existing works which consider both the precision and recall of the generative models [1, 2, 3], and directly work with the generated samples instead of the training data. These should be discussed and compared with, not just the FID scores which have already been shown to have issues [3]. Some notations: - In the last equation on Page 2, should it be L_{G} instead of L_{D}? - In the first equation on Page 3, should the denominator be N_{B} instead of N_{N}? - *Optimality* in terms of generative models may depend on the downstream tasks. I do not think there exists a universal definition of *optimality* for generative models. [1] M.S.M. Sajjadi, O. Bachem, M. Lucic, O. Bousquet, and S. Gelly. Assessing generative models via precision and recall. NeurIPS 2018. [2] L. Simon, R. Webster, and J. Rabin. Revisiting precision and recall definition for generative model evaluation. ICML 2019. [3] T. Kynkaanniemi, T. Karras, S. Laine, J. Lehtinen, and T. Aila. Improved precision and recall metric for assessing generative models. Arxiv:1904.06991."}
{"id": "iclr2020_499", "title": "Locally Constant Networks | OpenReview", "abstract": "Abstract:###We show how neural models can be used to realize piece-wise constant functions such as decision trees. Our approach builds on ReLU networks that are piece-wise linear and hence their associated gradients with respect to the inputs are locally constant. We formally establish the equivalence between the classes of locally constant networks and decision trees. Moreover, we highlight several advantageous properties of locally constant networks, including how they realize decision trees with parameter sharing across branching / leaves. Indeed, only neurons suffice to implicitly model an oblique decision tree with leaf nodes. The neural representation also enables us to adopt many tools developed for deep networks (e.g., DropConnect (Wan et al., 2013)) while implicitly training decision trees. We demonstrate that our method outperforms alternative techniques for training oblique decision trees in the context of molecular property classification and regression tasks.", "review": "Review:###In this paper, the authors proposed an approach to fit locally constant functions using deep neural networks (DNNs). The idea is based on the fact that DNN consisting of only linear transformations and ReLU activations is piecewise linear. Thus, the derivative of such a network with respect to the input is locally constant. In the paper, the authors focused on connecting the locally constant network with oblique decision trees. Specifically, they proved that these two models are in some sense equivalent, and one can transform one model to another. This connection enables us to train the oblique decision trees by training the locally constant network instead. Because the locally constant network can be trained using the gradient-based methods, it would be much easier to train than the oblique decision trees. I think the paper is well-written and the idea is clear. Connecting the locally constant network with oblique decision trees looks interesting. I have one concern, however. The authors mention that the training of oblique decision trees is difficult, and the use of the locally constant network is helpful. If I understand correctly, oblique decision tree is one specific instance of the hierarchical mixtures of experts. And, [Ref1] pointed out that the hierarchical mixtures of experts can be trained using EM algorithm, which is another type of the gradient-based training. The current paper misses such a prior study. I am interested in to see if the use of locally constant network is truly effective for training oblique decision trees over the algorithms considered in the literatures of hierarchical mixtures of experts. [Ref1] Hierarchical mixtures of experts and the EM algorithm ### Updated after author response ### The authors have successfully demonstrated that the proposed approach is better than the EM-like classical approaches. I therefore keep my score."}
{"id": "iclr2020_500", "title": "Lagrangian Fluid Simulation with Continuous Convolutions | OpenReview", "abstract": "Abstract:###We present an approach to Lagrangian fluid simulation with a new type of convolutional network. Our networks process sets of moving particles, which describe fluids in space and time. Unlike previous approaches, we do not build an explicit graph structure to connect the particles but use spatial convolutions as the main differentiable operation that relates particles to their neighbors. To this end we present a simple, novel, and effective extension of N-D convolutions to the continuous domain. We show that our network architecture can simulate different materials, generalizes to arbitrary collision geometries, and can be used for inverse problems. In addition, we demonstrate that our continuous convolutions outperform prior formulations in terms of accuracy and speed.", "review": "Review:###This paper proposes a novel technique to perform fluid simulations. Specifically, they promote the idea of using spatial convolutions to model how particles interact with nearby particles. Compared to graph-based models, this approach has several advantages and yields a model that can be conveniently trained end-to-end. The authors also develop a specific type of continuous convolution that yield better and faster inference than the benchmark algorithms. To main contribution in this paper is the idea of using spatial convolutions to model particle interactions. Even though the obtained results contain significant errors compared to ground truth, the paper indicates a promising strategy others may leverage on to develop even more accurate deep learning based simulators. Considering that they have also plausibly argued that their specific algorithm is already state of the art, I view this as a significant contribution. Having said this, the contribution is really to use a well-known technique (spatial convolutions) on a new problem (fluid simulations). My understanding is that ICLR primarily wants to promote general learning techniques and I am not convinced that this paper contains any significant contributions in this field. The authors also develop a specific network architecture that they compare with other deep learning architectures for continuous convolutions. Unfortunately, the design contains a number of questionable choices and I suspect that the main reason that existing architectures for deep learning using continuous convolutions perform worse is that their hyper-parameters have been fine-tuned for a different task. As an example of a questionable choice, why do you “exclude the particle at which we evaluate the convolution” in your convolutions? Minor remarks: * You include a constant 1 in the input feature vectors. Assuming that the neurons in your network have weights and biases, this constant is completely redundant. Of course, this also means that you can include it without ruining your performance, but why would you? * I found the explanation of Lambda in Figure 1 too short to be understandable. * In (7), you seem to be using convolutions between functions that have not been pre-mirrored, and it would be better to then express (5) and (6) on the same form."}
{"id": "iclr2020_501", "title": "Fast is better than free: Revisiting adversarial training | OpenReview", "abstract": "Abstract:###Adversarial training, a method for learning robust deep networks, is typically assumed to be more expensive than traditional training due to the necessity of constructing adversarial examples via a first-order method like projected gradient decent (PGD). In this paper, we make the surprising discovery that it is possible to train empirically robust models using a much weaker and cheaper adversary, an approach that was previously believed to be ineffective, rendering the method no more costly than standard training in practice. Specifically, we show that adversarial training with the fast gradient sign method (FGSM), when combined with random initialization, is as effective as PGD-based training but has significantly lower cost. Furthermore we show that FGSM adversarial training can be further accelerated by using standard techniques for efficient training of deep networks, allowing us to learn a robust CIFAR10 classifier with 45% robust accuracy at epsilon=8/255 in 6 minutes, and a robust ImageNet classifier with 43% robust accuracy at epsilon=2/255 in 12 hours, in comparison to past work based on ``free** adversarial training which took 10 and 50 hours to reach the same respective thresholds.", "review": "Review:###The authors claimed a classic adversarial training method, FGSM with random start, can indeed train a model that is robust to strong PGD attacks. Moreover, when it is combined with some fast training methods, such as cyclic learning rate scheduling and mixed precision, the adversarial training time can be significantly decreased. The experiment verifies the authors* claim convincingly. Overall, the paper provides a novel finding that could significantly change the adversarial training strategy. The paper is clearly written and easy to follow. I recommend the acceptance."}
{"id": "iclr2020_502", "title": "Meta-Learning Deep Energy-Based Memory Models | OpenReview", "abstract": "Abstract:###We study the problem of learning associative memory -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version. Attractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. In such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. In general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast. Thus, most research in energy-based memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing. We present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its weights. We demonstrate experimentally that our EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate.", "review": "Review:###======================================== Update after revisions ============================================ I appreciate the effort the authors have put into the revision and the rebuttal. I*m happy to increase my score and recommend acceptance based on the revised paper. However, I have to say that some of my worries still linger. With respect to non-memory baselines, the authors have responded that the memory based models will outperform non-memory models in cases where prior structure is less important than memory and provided a demonstration of this with an extreme example, i.e. a case with no structure (random binary strings example). I understand the point being made here, but this is a rather pedantic and uninteresting example. The authors have provided another (more interesting) example in Appendix A3 and shown that the memory-based model outperforms some simpler baselines such as the DAE even in this case. But no explanation is given for this result. Why is the memory based model outperforming the DAE in this case, given that this is an example where prior *is* very important? I*m a bit worried that the DAE results may perhaps be due to a non-optimized architecture or training setup (and what exactly is the architecture used for the DAE here)? I would appreciate it if the authors could clarify these issues in the final version. I have also spotted several typos. For the final version, please make sure to go through the paper thoroughly at least once and fix all the typos. ======================================================================================================== This paper proposes a meta-learning approach to learning fast read and write mechanisms in an energy-based model so that a given set of images can be quickly inducted into memory and retrieved from memory with noisy queries. The paper is well-written and the proposed approach seems interesting and novel enough. However, I have some concerns about the paper that need to be addressed. Here are the main issues for me: 1) I am in general not really convinced about the supposed advantages of these attractor memory models (this paper and the earlier Kanerva machine) over more standard and much simpler approaches. For example, for the problem of retrieval from noisy queries, a more standard approach would be a simple autoencoder. Note that in an autoencoder, reading (inference) is already fast. The authors might point out that writing (training) will not be fast, which is correct. However, the meta-learning phase proposed in this paper will also not be fast and perhaps the fair comparison should be between the meta-learning phase of this paper and the standard training phase of an autoencoder. Note that the autoencoder will have additional benefits. For example, with the autoencoder, one is not constrained by memory storage requirements and can make use of a much larger set of images to train the model. This allows the model to learn a richer structure in images. Moreover, with a large enough feedforward net, one can approximate arbitrarily complex dependencies in images. However, in attractor memory models, on the other hand, one necessarily restricts oneself to a particular model class that can be expressed as gradient descent dynamics in an energy landscape both during reading and writing. This seems overly restrictive to me. So, perhaps, the authors can clarify the supposed advantages of these attractor memory models a bit better. For example, I would be interested in seeing some comparative results with, say, a denoising autoencoder model. 2) Currently, the paper only uses a specific type of “block-noise” corruption. One thing that would be nice to see is some results with other noise models. I think this is important to demonstrate that the approach is general enough to handle different kinds of noise. Also, a salt-and-pepper noise will allow the authors to compare their results with the dynamic Kanerva machine (the authors note that the DKM failed to train successfully for the block-noise used here). 3) It would be good to say something about the meta-learned parameters, theta_bar, r, tau. Is there any meaningful structure in these parameters that distinguishes them from their random initial values? Is one of these parameters more important than the others? For example, what happens if you just use generic step size decay rules for gamma and eta (or perhaps no decay at all)?"}
{"id": "iclr2020_503", "title": "Temporal Probabilistic Asymmetric Multi-task Learning | OpenReview", "abstract": "Abstract:###When performing multi-task predictions with time-series data, knowledge learned for one task at a specific time step may be useful in learning for another task at a later time step (e.g. prediction of sepsis may be useful for prediction of mortality for risk prediction at intensive care units). To capture such dynamically changing asymmetric relationships between tasks and long-range temporal dependencies in time-series data, we propose a novel temporal asymmetric multi-task learning model, which learns to combine features from other tasks at diverse timesteps for the prediction of each task. One crucial challenge here is deciding on the direction and the amount of knowledge transfer, since loss-based knowledge transfer Lee et al. (2016; 2017) does not apply in our case where we do not have loss at each timestep. We propose to tackle this challenge by proposing a novel uncertainty- based probabilistic knowledge transfer mechanism, such that we perform knowledge transfer from more certain tasks with lower variance to uncertain ones with higher variance. We validate our Temporal Probabilistic Asymmetric Multi-task Learning (TP-AMTL) model on two clinical risk prediction tasks against recent deep learning models for time-series analysis, which our model significantly outperforms by successfully preventing negative transfer. Further qualitative analysis of our model by clinicians suggests that the learned knowledge transfer graphs are helpful in analyzing the model’s predictions.", "review": " This paper proposes a temporal probabilistic asymmetric multi-task learning model for sequential data. The asymmetric property lies in the feature level. This paper criticize that the task loss used in previous works may be unreliable as a measure of the knowledge from the task. However, the feature uncertainty used in the proposed model can also be unreliable and I cannot see any reliable guarantee. The organization of Section 3 is not good. The logics in different subsections are not so clear. Some notations seem undefined. Eq. (4) defines p(z_d|x,omega). But later it defines z_d is from p(z_d|x,y_d,omega). The difference between these two distributions lies in y_d. I don’t know which one is used. What is p_\theta(z_d|x,omega)? The notations need to be properly defined. Based on Section 3.3, the proposed model seems to require different tasks have the same total time step T and this requirement is a bit strong. It seems that there is no Figure 2c."}
{"id": "iclr2020_504", "title": "Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks | OpenReview", "abstract": "Abstract:###Recent work has revealed that overparameterized networks trained by gradient descent achieve arbitrarily low training error, and sometimes even low test error. The required width, however, is always polynomial in at least one of the sample size n, the (inverse) training error 1/epsilon, and the (inverse) failure probability 1/delta. This work shows that O(1/epsilon) iterations of gradient descent on two-layer networks of any width exceeding polylog(n, 1/epsilon, 1/delta) and Omega(1/epsilon^2) training examples suffices to achieve a test error of epsilon. The analysis further relies upon a margin property of the limiting kernel, which is guaranteed positive, and can distinguish between true labels and random labels.", "review": "Review:###In this paper, the author shows that for a two-layer ReLU network, it only requires a network width that is poly-logarithmic in the sample size n to get good optimization and generalization error bounds, which is better than prior results. Overall, the paper is well written and easy to follow. However I still have some questions about this paper. One of my major concerns is that there might be an important error in the proof of the main theorem. Specifically, in the proof of Theorem 2.2 (page 12), it says that due to lemma 2.5, . However, Lemma 2.5 only shows that is small, and the reason can also be small is not explained in this paper at all. Based on Lemma 2.5, I can roughly get that can be small, but the reason why is small is unclear to me, especially when the network width m is only polylogarithmic in n and varepsilon. Without a clear explanation on this issue, the theoretical results in this paper might be flawed, and the polylog claim might not be correct. Moreover, this paper does not provide sufficient comparison with existing work. For example, Assumption 2.1 looks very similar to the assumption made in Cao & Gu (2019a). The definition of has also been introduced in Cao & Gu (2019a). However these similarities are not mentioned in the paper at all. Moreover, the result of Lemma 2.6, which is also one of the selling points of this paper, is actually very similar to Fact D.4 and Claim D.6 in the following paper: Allen-Zhu, Zeyuan, and Yuanzhi Li. *What Can ResNet Learn Efficiently, Going Beyond Kernels?.* arXiv preprint arXiv:1905.10337 (2019). Finally, the authors’ claim in the title that the width of the network is poly-logarithmic with the sample size n might be misleading. In fact, in Section 5, it has been discussed that in certain settings about the data distribution, is polynomial of n. However, the width is polynomial with , which means the width is poly of n in these settings."}
{"id": "iclr2020_505", "title": "Multi-Scale Representation Learning for Spatial Feature Distributions using Grid Cells | OpenReview", "abstract": "Abstract:###Unsupervised text encoding models have recently fueled substantial progress in Natural Language Processing (NLP). The key idea is to use neural networks to convert words in texts to vector space representations (embeddings) based on word positions in a sentence and their contexts. We see a strikingly similar situation in spatial analysis, which focuses on incorporating both absolute positions and spatial contexts of geographic objects such as Points of Interest (POIs) into models. A general space encoding method is valuable for a multitude of tasks such asPOI search, land use classification, point-based spatial interpolation and locationaware image classification. However, no such general model exists to date beyond simply applying discretizing or feed forward nets to coordinates, and little effort has been put into jointly modeling distributions with vastly different characteristics, which commonly emerges from GIS data. Meanwhile, Nobel Prize-winning Neuroscience research shows that grid cells in mammals provide a multi-scale periodic representation that functions as a metric for encoding space and are critical for recognizing places and for path-integration. Inspired by this research, wepropose a representation learning model called Space2vec to encode the absolutepositions and spatial relationships of places. We conduct experiments on realworld geographic data and predict types of POIs at given positions based on their1) locations and 2) nearby POIs. Results show that because of its multi-scale representations Space2vec outperforms well-established ML approaches such as RBF kernels, multi-layer feed forward nets, and tile embedding approaches.", "review": "Review:###The paper introduces Space2Vec, a space representation learning model. The work is motivated by the biological grid cell’s multi-scale periodic representations and the success of representation learning of NLP. So, the key idea behind the model is two-fold. On one hand, utilize the position information and the context associated with the position. On the other hand, the authors build a multiscale point space encoder based on Theorem 1 (in the paper), which was previously proved by Gao et al. (2019). The multi-scale point feature encoder is novel. The experimental results turn out that the whole model is good at predicting features using only location information but does not outperform the RBF kernel (on validation) in terms of using spatial context modeling. One core selling point of this paper is dealing with location distributions with very different characteristics. This is very well motivated at the beginning. More analysis/statistics would help better understand how the model *theory* wins Table one. See comments on experiments. I have some comments/questions about model architecture and also experimental results/analysis. 1. Regarding Contextual Embedding -- The encoder of this paper is not doing much “contextual embedding”. The encoder typically encodes features of each position independently thus lead to very local embedding. -- The location decoder would reconstruct the same (or similar) type of point features given embeddings of locations of the same type. As the distributions of different types of locations are very different, an encoder capable to deal with multi-scale data is crucial here. -- The Spatial context decoder, like a context-dependent language model, would reconstruct the current position features, given the neighboring information. -- Overall, unlike many existing pre-training models in NLP with deep encoders, the full model of this paper is with very local encoder, while the decoder does the most work of “gathering contextual information*. As you claim your model to be *a general-purpose space representation model*, can you describe/specify how you would use your model for other tasks? Will you take some intermediate output of decoders to be representations? Or will you fine-tune the whole encoder-decoder? 2. For experiments: a. I do not prefer saying your method outperforms RBF in the “spatial context modeling” task when you getting worse validation set performance. It is interesting that RBF is stronger in terms of validation. b. Is it possible for the authors to do some statistics on the different types of locations? For your first task “location modeling”, should we expect to see that your model does not have very bad performance on certain types of locations while other non-multi-scale approaches do? This is trying to provide better support to one of your core contributions. c. Again, to claim the model can be widely applied, try more tasks? To clarify my *experience assessment*, I mean I read many related representation learning papers rather than specific papers related to GIS data. My actual rating for the paper is between weak reject to weak accept (but the system does not have intermediate choices). I would like to hear the author*s feedback to further revise the rating."}
{"id": "iclr2020_506", "title": "Black Box Recursive Translations for Molecular Optimization | OpenReview", "abstract": "Abstract:###Machine learning algorithms for generating molecular structures offer a promising new approach to drug discovery. We cast molecular optimization as a translation problem, where the goal is to map an input compound to a target compound with improved biochemical properties. Remarkably, we observe that when generated molecules are iteratively fed back into the translator, molecular compound attributes improve with each step. We show that this finding is invariant to the choice of translation model, making this a *black box* algorithm. We call this method Black Box Recursive Translation (BBRT), a new inference method for molecular property optimization. This simple, powerful technique operates strictly on the inputs and outputs of any translation model. We obtain new state-of-the-art results for molecular property optimization tasks using our simple drop-in replacement with well-known sequence and graph-based models. Our method provides a significant boost in performance relative to its non-recursive peers with just a simple *``for* loop. Further, BBRT is highly interpretable, allowing users to map the evolution of newly discovered compounds from known starting points.", "review": "Review:###This paper presents a novel approach to generating molecules using Black Box Recurrent Translation. The authors uses existing machine-translation inspired schemes to generate new, similar, molecules with better properties according to some measure. Then, the recursion takes the top K best molecules and runs another iteration to generate even better molecules, ad infinitum. The authors use the newly introduced SELFIES strings as vocabulary for generation. The authors also analyze the decoding strategy, and how the process generates interpretable molecular traces. Relating to wetlab work, having a molecular trace available from the recursive translation scheme is valuable for drug-sythesis. The authors also show that this technique can optimize multiple properties at once. I am leaning towards an accept for this paper, since not only does the technique presented seem general, the authors does in depth analysis into the model and how it affects drug discovery. - Recursive black box translation seems to be widely applicable to new models. - The model seems to reach a significantly better state of the art on the metrics proposed. - None of the baselines seem to use SELFIES as the string of choice. This means it*s difficult to tell how much the *Blackbox recursive* part of the algorithm adds to the model. An ablation experiment without BBRT might inform us of how much of the benefit is due to the molecule representation (Fig 4A reports the mean, but it would be good to have the same metric as Table 1). - The authors provide an in depth discussion about how having molecular traces would hhelp in drug design. This makes the tool seem more widely appealing and useful. A few questions would clear up the strengths of the paper: - Is there a connection to the backtranslation work in Lample 2018? (Phrase-Based & Neural Unsupervised Machine Translation) It seems like a similar idea - except in this domain, the target language and source language are the same. - How can there be multiple scoring functions? Were they combined in one run, or were these separately optimized runs? Are these only used in Figure 4? - Why would beam search do less well than stochastic? Is it because during recursive translation, the beam search variants have low diversity? Then, training with stochastic decoding and generation with a beam search should do even better, right? This would highlight that the advantage of stochastic decoding is really online in the context of recursive translation, not generally. - What is the point of Fig 4A right? Why do we expect that maximizing non-logP properties will increase mean logP?"}
{"id": "iclr2020_507", "title": "SVQN: Sequential Variational Soft Q-Learning Networks | OpenReview", "abstract": "Abstract:###Partially Observable Markov Decision Processes (POMDPs) are popular and flexible models for real-world decision-making applications that demand the information from past observations to make optimal decisions. Standard reinforcement learning algorithms for solving Markov Decision Processes (MDP) tasks are not applicable, as they cannot infer the unobserved states. In this paper, we propose a novel algorithm for POMDPs, named sequential variational soft Q-learning networks (SVQNs), which formalizes the inference of hidden states and maximum entropy reinforcement learning (MERL) under a unified graphical model and optimizes the two modules jointly. We further design a deep recurrent neural network to reduce the computational complexity of the algorithm. Experimental results show that SVQNs can utilize past information to help decision making for efficient inference, and outperforms other baselines on several challenging tasks. Our ablation study shows that SVQNs have the generalization ability over time and are robust to the disturbance of the observation.", "review": "Review:###This paper proposes a new sequential model-free Q-learning methodology for POMDPs that relies on variational autoencoders to represent the hidden state. The approach is generic, well-motivated and has clear applicability in the presence of partial observability. The idea is to create a joint model for optimizing the hidden-state inference and planning jointly. For that reason variational inference is used to optimize the ELBO objective in this particular setting. All this is combined with a recurrent architecture that makes the whole process feasible and efficient. The work is novel and it comes with the theoretical derivation of a variational lower bound for POMDPs in general. This intuition is exploited to create a VAE based recurrent architecture. One motivation comes from maximal entropy reinforcement learning (MERL), but which has the ad hoc objective of maximizing the policy entropy. On the other hand SVQN optimizes both a variational approximation of the policy and that of the hidden state. Here the rest terms of the ELBO objective can be approximated generatively and some of them are conditioned on the previous state which calls for a recurrent architecture. The other parts are modeled by a VAE. The paper also explores two different recurrent models in this context: GRU and LSTM are both evaluated. Besides the nice theoretical derivation the paper presents compelling evidence by comparing this approach to competing approaches on four games of the flickering ATARI benchmark and outperforming the baselines significantly. Also both the GRU and LSTM version outperforms the baseline methods on various tasks of the VIZDoom benchmark as well. In general, I find that this well written paper presents a significant progress in modelling POMDPS in a model-free manner with nice theoretical justification and compelling empirical evidence."}
{"id": "iclr2020_508", "title": "Reinforced Genetic Algorithm Learning for Optimizing Computation Graphs | OpenReview", "abstract": "Abstract:###We present a deep reinforcement learning approach to minimizing the execution cost of neural network computation graphs in an optimizing compiler. Unlike earlier learning-based works that require training the optimizer on the same graph to be optimized, we propose a learning approach that trains an optimizer offline and then generalizes to previously unseen graphs without further training. This allows our approach to produce high-quality execution decisions on real-world TensorFlow graphs in seconds instead of hours. We consider two optimization tasks for computation graphs: minimizing running time and peak memory usage. In comparison to an extensive set of baselines, our approach achieves significant improvements over classical and other learning-based methods on these two tasks.", "review": "Review:###In this work the authors propose a deep RL approach to minimize the makespan and the peak memory usage of a computation graph as produced by MXNet/PyTorch/TensorFlow. This is an increasingly important problem as distributed deep learning is necessary in many cases. The authors aim to minimize the execution time and/or the peak memory usage. For this purpose they generate a training dataset out of a real-world dataset of various TensorFlow computation graphs using simulation software. The proposed RL approach consists of two steps. First a GNN is used to derive representations for computation graphs. Then the authors use a heuristic BRKGA to learn a policy for the placement of computation graphs, that actually works on unseen graphs. Overall this paper is well-written, deals with an important practical problem. While it is not immediately clear to the reader the effect of BRKGA on the mapping of the graph to the resource network and why it works so well, the results are convincing (but still there is space for improvement). That is why I rate it as a *weak accept*. - Can you explain why the beta distribution choices at each node may have a negative impact on the makespan in certain cases? Have you looked into them? - To what extent are the simulations realistic? Can you please comment more on this aspect? - Have you tried Scotch? https://www.labri.fr/perso/pelegrin/scotch/. Since the software aims to achieve a different objective, it serves as a baseline. - Can you obtain insights with respect how you could cluster the TensorFlow computation graphs? - Can you improve the discussion on BRKGA? Since it is a vital component of the proposed framework, it would be informative to read few more self-contained details on how it works in section 2. - Once you obtain a mapping, can you comment on any insights concerning the structure of the partition and schedule?"}
{"id": "iclr2020_509", "title": "Dimensional Reweighting Graph Convolution Networks | OpenReview", "abstract": "Abstract:###In this paper, we propose a method named Dimensional reweighting Graph Convolutional Networks (DrGCNs), to tackle the problem of variance between dimensional information in the node representations of GCNs. We prove that DrGCNs can reduce the variance of the node representations by connecting our problem to the theory of the mean field. However, practically, we find that the degrees DrGCNs help vary severely on different datasets. We revisit the problem and develop a new measure K to quantify the effect. This measure guides when we should use dimensional reweighting in GCNs and how much it can help. Moreover, it offers insights to explain the improvement obtained by the proposed DrGCNs. The dimensional reweighting block is light-weighted and highly flexible to be built on most of the GCN variants. Carefully designed experiments, including several fixes on duplicates, information leaks, and wrong labels of the well-known node classification benchmark datasets, demonstrate the superior performances of DrGCNs over the existing state-of-the-art approaches. Significant improvements can also be observed on a large scale industrial dataset.", "review": "Review:###The paper is out of my research area. I could only provide little recommendation. I have tried to read this paper, but it was rather tedious with heavy notations. It would be more friendly to represent the models in visible way for example using diagrams as I can see that the model is a sequence matrix operators with non-linear transformations after that. The paper states that the proposed DrGCNs can improve the stability of GCN models via mean field theory. The experiments were conducted on benchmark datasets and the proposed method was compared to several GCN variations."}
{"id": "iclr2020_510", "title": "UNIVERSAL MODAL EMBEDDING OF DYNAMICS IN VIDEOS AND ITS APPLICATIONS | OpenReview", "abstract": "Abstract:###Extracting underlying dynamics of objects in image sequences is one of the challenging problems in computer vision. On the other hand, dynamic mode decomposition (DMD) has recently attracted attention as a way of obtaining modal representations of nonlinear dynamics from (general multivariate time-series) data without explicit prior knowledge about the dynamics. In this paper, we propose a convolutional autoencoder based DMD (CAE-DMD) that is an extended DMD (EDMD) approach, to extract underlying dynamics in videos. To this end, we develop a modified CAE model by incorporating DMD on the encoder, which gives a more meaningful compressed representation of input image sequences. On the reconstruction side, a decoder is used to minimize the reconstruction error after applying the DMD, which in result gives an accurate reconstruction of inputs. We empirically investigated the performance of CAE-DMD in two applications: background/foreground extraction and video classification, on publicly available datasets.", "review": "Review:###I found the topic of this paper interesting and I believe I understand what the authors are trying to achieve but I*m afraid this was after several readings and I do think the paper could be presented differently that would make it more accessible. My suggestion would be to explain how the model will be applied first (identify the required properties) to motivate the need for the learned basis and then present the DMD as a method for providing a basis that meets the properties required. I acknowledge that different communities have different styles of presentation so apologies if this is just me. First I would just like to check that I have understood correctly so please could the authors point out if I have missed something or misunderstood in the following? Our goal is to establish a basis invariant to the video dynamics that can then be used, for example, to partition the video into parts with differing dynamics - e.g. foreground/background. To do this we need to identify such a basis from a specific video - we will use the collection of pairs of neighboring frames. The Koopman operator acts on a differential system to identify a function space invariant to the dynamics. If we instantiate this with a finite number of dimensions we can essentially establish the invariance as an eigenvalue problem. From this and our pairs of successive videos we can establish a vector basis for the space and then project the video into this basis. The spectral properties of the coefficients of the projection will determine whether something is static (omega = 0) or transitory in the scene and these can be used to identify foreground and background. Next there is the issue that this method operates in a linear domain with something like Gaussian noise which is not a good fit for image space videos so the authors propose to identify the dynamics in a linear latent space determined by an autoencoder to handle the non-linear mapping to image space. I hope I have understood the main points? If this is the case, I think that much more needs to be said about the second part, which is the essential novelty of the paper, with a discussion of the merits of different approaches and full details - at the moment there is just one small paragraph at the end of 4.2 which contains the majority of the contribution. My main concern about the paper is that I find it very difficult to appreciate the efficacy of the method given the current presentation of the results. There are no error bars to ascertain significance for any of the results and the summarization of multiple experiments to a single percentage gives very little insight into where this method works and where it doesn*t. There are a number of ways that a dynamic prior could be added to a latent space and it is unclear why we would expect this approach to be preferred given the evidence presented in the paper. Other Notes: I found that the notation is not always consistent and sometimes could be simplified - it is unclear whether some operators are convolutions or multiplications (vector or scalar). To me the asterisk does not represent straight forward multiplication but it might be being used for this? Could Table 1 be placed in the experiments section rather than in the middle of page 5? Do the authors mean half the number of pixels or half the edge size (e.g. a quarter of the area) in terms of the latent space? Please can all equations be numbered so that they can be referred to - there are no equation numbers in all of section 2."}
{"id": "iclr2020_511", "title": "Improving Semantic Parsing with Neural Generator-Reranker Architecture | OpenReview", "abstract": "Abstract:###Semantic parsing is the problem of deriving machine interpretable meaning representations from natural language utterances. Neural models with encoder-decoder architectures have recently achieved substantial improvements over traditional methods. Although neural semantic parsers appear to have relatively high recall using large beam sizes, there is room for improvement with respect to one-best precision. In this work, we propose a generator-reranker architecture for semantic parsing. The generator produces a list of potential candidates and the reranker, which consists of a pre-processing step for the candidates followed by a novel critic network, reranks these candidates based on the similarity between each candidate and the input sentence. We show the advantages of this approach along with how it improves the parsing performance through extensive analysis. We experiment our model on three semantic parsing datasets (GEO, ATIS, and OVERNIGHT). The overall architecture achieves the state-of-the-art results in all three datasets.", "review": "Review:###This paper proposes a reranking architecture with a LogicForm-to-NaturalLanguage preprocessing step for semantic parsing. The authors experiment their method on three datasets and get the state of the art results. The proposed method is natural. But using neural models to rank (or rerank) is a long-existing technique, regardless of the chosen parametrization of the reranking model. This paper chose BERT. See section-2.6 of this tutorial for more details about using neural models to rank: https://www.microsoft.com/en-us/research/uploads/prod/2017/06/INR-061-Mitra-neuralir-intro.pdf. Overall, I think the paper is not ready to publish for the following reasons. 1. The method relies much upon manual designs that seem hard to generalize. By converting the logic forms to natural languages, the authors can leverage paraphrase datasets and pre-train the critic as a paraphrase model. However, the way they convert the logic forms is different for each dataset and they have to manually design rules for each logic form. 2. It is not clear how certain experimental designs were made. The authors chose to not rerank if the candidates* scores are too low or high but close. Such choice and associated thresholds seem arbitrary: how were they actually found out? Were they tuned on a development set? How does the method work if the candidate with the highest score is always picked: in the end, this is what the model is supposed to learn, correct? Other designs include beam size, whether or not to use a pretrained model, etc. How were such decisions made? Tuned on a development set? 3. The results are not sound enough. Given the issued pointed out in 1 and 2, I am not sure if the results are really sound as the authors claimed. For example, what if the authors don’t use a LogicForm-to-NaturalLanguage conversion? What is the result if we directly learn to match input and logic forms? Moreover, the authors better answer questions in 2 so I can gauge if their hyper-parameters were chosen in the principled ways. Once those are answered, a significant test had better be done since the improvement seems small. 4. Claiming Shaw et al. 2019 in table-3 as ``our methods’’ is wrong. It is clear that Shaw et al. (2019) didn*t experiment on OVERNIGHT dataset, but setting up the baseline on a dataset should not be classified as ``our method’’. Moreover, I have some comments on the model and experiments. These are not weakness, but I think some work in this direction may help improve the paper. 1. The model architecture should be better justified. In its current form, the two arguments (input query and output sequence translated from a logic form) are interchangeable. Why so? Why isn’t an asymmetric architecture more natural? How can the authors use a pair of logic forms as negative examples (in figure-2)? Why do the authors use the Quora dataset in particular? 2. The error analysis might be better to be a bit more quantitative. Its current form doesn’t seem to give insight on how the proposed method really helps. What the authors can do is: you can sample some sentences from the test/development set and count how many comparative words are misused in the original model, among which how many are corrected by reranking."}
{"id": "iclr2020_512", "title": "Discourse-Based Evaluation of Language Understanding | OpenReview", "abstract": "Abstract:###New models for natural language understanding have made unusual progress recently, leading to claims of universal text representations. However, current benchmarks are predominantly targeting semantic phenomena; we make the case that discourse and pragmatics need to take center stage in the evaluation of natural language understanding. We introduce DiscEval, a new benchmark for the evaluation of natural language understanding, that unites 11 discourse-focused evaluation datasets. DiscEval can be used as supplementary training data in a multi-task learning setup, and is publicly available, alongside the code for gathering and preprocessing the datasets. Using our evaluation suite, we show that natural language inference, a widely used pretraining task, does not result in genuinely universal representations, which opens a new challenge for multi-task learning.", "review": "Review:###The paper presents a new GLUE-like dataset collection called DiscEval, which focuses on discourse and pragmatics. Like GLUE and SuperGLUE, datasets from existing works are collated and formated. The tasks are all classification tasks. The paper also evaluates several baselines including bag-of-words, BiLSTM encodings, and BERT fine-tuned on different types of data, which have shown success in GLUE tasks. Like other NLP benchmarks, the DiscEval benchmark would be a good resource for other researchers to hill-climb their systems on, provided that the data format is standardized and the submission system is easy to use like GLUE. That said, the paper has some rooms for improvement: - With the information in Table 2, it is hard to judge the difficulty and headroom for each task. Only a few tasks have human evaluation scores were estimated from the inter-annotator agreement. Contrast this to the GLUE and SuperGLUE papers which provide human baselines from actual humans. Without these anchoring numbers, it is hard to see if the remaining gap is due to the model*s inability to model discourse, or due to noise in the dataset. - Providing the best single-task result from previous work would also help give a more complete picture. - With the result of fine-tuned BERT almost matching the human performance in several tasks, the argument that BERT is not a universal representation (abstract + introduction) is weakened somewhat. As a valuable resource for other researchers, I am still leaning toward acceptance despite the issues above. Other comments: - The bullet points on Page 5 could be clarified. Currently, the first bullet seems to contain multiple groups, for example. - Sentiment analysis (in GLUE) could be viewed as a discourse task. It would be nice to be a bit more upfront about it."}
{"id": "iclr2020_513", "title": "An Explicitly Relational Neural Network Architecture | OpenReview", "abstract": "Abstract:###With a view to bridging the gap between deep learning and symbolic AI, we present a novel end-to-end neural network architecture that learns to form propositional representations with an explicitly relational structure from raw pixel data. In order to evaluate and analyse the architecture, we introduce a family of simple visual relational reasoning tasks of varying complexity. We show that the proposed architecture, when pre-trained on a curriculum of such tasks, learns to generate reusable representations that better facilitate subsequent learning on previously unseen tasks when compared to a number of baseline architectures. The workings of a successfully trained model are visualised to shed some light on how the architecture functions.", "review": "Review:###This paper presents a network architecture based on the multi-head self-attention module to learn a new form of relational representations. The proposed method is shown to improve data efficiency and generalization ability on a sequence of curriculum learning tasks. + The major novelty of this paper is a new attention-based network architecture that aims to discover objects and the relations between them. The idea of explicitly appending the patch positions to the representations is interesting, though I am not sure whether it can be generalized to real data. - Since the proposed network takes patches of full images as inputs, my major concern is about its effectiveness on high-dimensional images with more realistic objects as in the CLEVR dataset other than 2d-grid objects. It would be better if the authors could extend their method to natural images. - A closely related work is the NLM model [1], which can perfectly generalized to new tasks. Please compare the proposed model with it. - Minor: Figures are understandable, but some of them are too small, especially for the graphical legend. As space is an issue, I would suggest removing some plots and increasing the size of the ones provided. - In Figure 1, is g equal to n? [1] Neural Logic Machines. Dong et al., ICLR 2019."}
{"id": "iclr2020_514", "title": "Match prediction from group comparison data using neural networks | OpenReview", "abstract": "Abstract:###We explore the match prediction problem where one seeks to estimate the likelihood of a group of M items preferred over another, based on partial group comparison data. Challenges arise in practice. As existing state-of-the-art algorithms are tailored to certain statistical models, we have different best algorithms across distinct scenarios. Worse yet, we have no prior knowledge on the underlying model for a given scenario. These call for a unified approach that can be universally applied to a wide range of scenarios and achieve consistently high performances. To this end, we incorporate deep learning architectures so as to reflect the key structural features that most state-of-the-art algorithms, some of which are optimal in certain settings, share in common. This enables us to infer hidden models underlying a given dataset, which govern in-group interactions and statistical patterns of comparisons, and hence to devise the best algorithm tailored to the dataset at hand. Through extensive experiments on synthetic and real-world datasets, we evaluate our framework in comparison to state-of-the-art algorithms. It turns out that our framework consistently leads to the best performance across all datasets in terms of cross entropy loss and prediction accuracy, while the state-of-the-art algorithms suffer from inconsistent performances across different datasets. Furthermore, we show that it can be easily extended to attain satisfactory performances in rank aggregation tasks, suggesting that it can be adaptable for other tasks as well.", "review": "Review:###This paper proposed a novel architecture to tackle the match prediction problem. There are two/three modules in the architecture, the R/P modules and the G module. R/P modules take the current utility estimates of the individuals in a given group comparison as input and produce the current R/P estimates for the individuals as output. The G module takes the final utility estimates of the individuals in a given group comparison as input and produces the winning probability estimate of one group preferred over the other in the given group comparison as output. I would recommend a weak accept for this paper based on the following reasons: * Both the R/P and G modules* input and output dimensions are independent of the number of items. This keeps the architecture scalable when facing large data sets in the real world. * The empirical result looks satisfactory on several data sets across different domains. * The theoretical foundation is sound. I would hope that the authors will make some effort in making the paper more approachable and practical: * A more detailed motivation section for the architecture will make it much easier for the readers to understand. * A conclusion plus future work section could come in handy for future researchers."}
{"id": "iclr2020_515", "title": "Tensorized Embedding Layers for Efficient Model Compression | OpenReview", "abstract": "Abstract:###The embedding layers transforming input words into real vectors are the key components of deep neural networks used in natural language processing. However, when the vocabulary is large, the corresponding weight matrices can be enormous, which precludes their deployment in a limited resource setting. We introduce a novel way of parametrizing embedding layers based on the Tensor Train (TT) decomposition, which allows compressing the model significantly at the cost of a negligible drop or even a slight gain in performance. We evaluate our method on a wide range of benchmarks in natural language processing and analyze the trade-off between performance and compression ratios for a wide range of architectures, from MLPs to LSTMs and Transformers.", "review": "Review:###This paper proposes to use TensorTrain representation to transform discrete tokens/symbols to its vector representation. Since neural networks can only work with numerical numbers, in many NLP tasks, where the raw inputs are in the discrete token/symbol format, the popular technique is to use *embedding* matrices to find a vector representation of those inputs. As the authors point out, the embedding matrices usually require huge number of parameters, since it assigns one vector for each input token for one embedding vector, but to attain a competitive performance in the real world applications, we need to use large number of embedding vectors, which results in a large number of parameters in the neural networks. The paper assumes that those embedding matrices can be compressed by assuming that the low-rank property of embedding matrices. I think this is a valid assumption in many cases, and the paper shows the performance degradation according to this assumption is relatively small compared to the gain, a dramatically reduced size of parameters in the embedding stage, is substantial. I think the paper is well written and proposes a new direction to find a memory efficient representation of symbols. I am not sure the current initialization techniques, nor the training method in the paper are the right way to train a tensor train *embedding* but I expect that the authors would perform the follow up work on those topics."}
{"id": "iclr2020_516", "title": "On importance-weighted autoencoders | OpenReview", "abstract": "Abstract:###The importance weighted autoencoder (IWAE) (Burda et al., 2016) is a popular variational-inference method which achieves a tighter evidence bound (and hence a lower bias) than standard variational autoencoders by optimising a multi-sample objective, i.e. an objective that is expressible as an integral over Monte Carlo samples. Unfortunately, IWAE crucially relies on the availability of reparametrisations and even if these exist, the multi-sample objective leads to inference-network gradients which break down as is increased (Rainforth et al., 2018). This breakdown can only be circumvented by removing high-variance score-function terms, either by heuristically ignoring them (which yields the *sticking-the-landing* IWAE (IWAE-STL) gradient from Roeder et al. (2017)) or through an identity from Tucker et al. (2019) (which yields the *doubly-reparametrised* IWAE (IWAE-DREG) gradient). In this work, we argue that directly optimising the proposal distribution in importance sampling as in the reweighted wake-sleep (RWS) algorithm from Bornschein & Bengio (2015) is preferable to optimising IWAE-type multi-sample objectives. To formalise this argument, we introduce an adaptive-importance sampling framework termed adaptive importance sampling for learning (AISLE) which slightly generalises the RWS algorithm. We then show that AISLE admits IWAE-STL and IWAE-DREG (i.e. the IWAE-gradients which avoid breakdown) as special cases.", "review": "Review:###UPDATE: bumping up my score after the revisions --- Nice connections but novelty and practical takeaways unclear SUMMARY OF THE PAPER: This paper views recent IWAE-based [1] methods (IWAE-STL [2], IWAE-DREG [3], RWS [4, 5]) for training generative models p and inference networks q under a common framework, AISLE. This heavily relies on the *double-reparameterization* property by [2] and is restated in Lemma 1. This framework makes it explicit that we*re interested in 1) maximizing the (log) marginal likelihood wrt p parameters, and 2) minimizing some divergence between the posterior in the learned model to q. In AISLE, IWAE-STL*s q-gradient is viewed as a doubly-reparameterized self-normalized importance sampling (SNIS) estimate of KL(p || q). This is in contrast to viewing it as a biased estimator of the IWAE*s q-gradient. This can potentially explain why it performs well when number of SNIS samples are increased. It is also some evidence against the fact that having no unified objective is bad (because there isn*t evidence of IWAE-STL diverging despite there being no unified objective). IWAE-DREG*s q-gradient is viewed as a doubly-reparameterized SNIS estimate of X-divergence(p || q) (up to multiplicative constant of the number of SNIS samples). This is in contrast to viewing it as an unbiased estimator of the IWAE*s q-gradient. The view on RWS is unchanged: the q-gradient is a SNIS estimate of KL(p || q). For me, the main contribution is viewing IWAE-STL and IWAE-DREG q-gradient estimators as biased gradients of an explicit divergence rather than of the IWAE objective. I also found the observation that the signal-to-noise (SNR) decrease in IWAE*s q-gradient can be proved by noting that it is a SNIS estimator of a zero vector nice. STRUCTURE: The article is well-written and easy to understand. NOVELTY: A different view on IWAE-STL and IWAE-DREG is interesting and novel (as mentioned above). This means that IWAE-STL and IWAE-DREG are good not only because they reduce gradient variance (as previously understood) but also potentially because they directly target a divergence. Viewing generalization of RWS as a main contribution (first bulletpoint of Section 1.2: *...we show that AISLE admits RWS as a special case.*) is a bit of a stretch since this generalization is very straightforward from the way RWS is formulated. The recommendation of using RWS-style algorithms over IWAE as given in the abstract (*we argue that directly optimising the proposal distribution in importance sampling as in the RWS algorithm is preferable to optimising IWAE-type multi-sample objectives) is also not novel since this is also advocated by [5] (section 3.2: *This makes RWS a preferable option to IWAE for learning inference networks because the phi updates in RWS directly target minimization of the expected KL divergences from the true to approximate posterior*). The recommendation as a method for non-reparameterisable latent variables at the end of section 1.2 (*as well as further algorithms which do not require reparameterisations*) is also given in [5]. Are there different adaptive importance sampling algorithms that could be used within AISLE that would improve on IWAE-STL/IWAE-DREG/RWS? EXPERIMENTS: There are no experiments in the main paper. However, experiments that would support/falsify the following points could be good: - RWS and IWAE-STL don*t suffer from non-unified objectives because IWAE-STL has non-unified objectives but doesn*t diverge, - [targeting direct divergence] is more useful than (or as useful as) [lower variance gradient estimators]. CONCLUSION: While I really like the presentation and connections made in the paper, I*m not sure what the practical takeaways are (other than use IWAE-STL, IWAE-DREG, RWS over IWAE which is advocated by [2], [3], [5]). I*m giving this a weak accept due to the former. I*m willing to bump up my score if - the paper is modified to more accurately reflect the contributions or - there are experiments that provide additional support for the [targeting direct divergence] view in addition to [2, 3, 4, 5], or - there is a new practical algorithm that the AISLE generalization would suggest that is better than IWAE-STL, IWAE-DREG, RWS in some respects. [1] Importance Weighted Autoencoders. https://arxiv.org/abs/1509.00519 [2] Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference. https://arxiv.org/abs/1703.09194 [3] Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives. https://arxiv.org/abs/1810.04152 [4] Reweighted Wake-Sleep. https://arxiv.org/abs/1406.2751 [5] Revisiting Reweighted Wake-Sleep for Models with Stochastic Control Flow. https://arxiv.org/abs/1805.10469 [6] Variational Inference via ?-Upper Bound Minimization. https://arxiv.org/abs/1611.00328 [7] Tighter Variational Bounds are Not Necessarily Better. https://arxiv.org/abs/1802.04537"}
{"id": "iclr2020_517", "title": "Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control | OpenReview", "abstract": "Abstract:###In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system from observed state trajectories. To achieve better generalization with fewer training samples, SymODEN incorporates appropriate inductive bias by designing the associated computation graph in a physics-informed manner. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way which can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even when the generalized coordinate data is embedded in a high-dimensional space or we can only access velocity data instead of generalized momentum. This framework, by offering interpretable, physically-consistent models for physical systems, opens up new possibilities for synthesizing model-based control strategies.", "review": "Review:###Update: I have read the author*s response. Thank you! *** This is an excellent paper that integrates inductive biases from physics into learning dynamical models that can then be integrated into deep RL-based control tasks. The model approximates the dynamical function f(q, p, u), where q are the generalised coordinates of the system (mixture of positions in R^1 and angles in S^1), p are the generalised momenta and u is the external control input. Function f can then be integrated by a numerical solver, and used as the dynamical function in a Neural ODE (ordinary differential equation) for modelling the continuous time evolution of the nonlinear dynamical system. The dynamics function is explicitly written as the equations of the Hamiltonian dynamical system, involving the 1) inverse of the mass function, 2) potential energy and 3) control function, in a complex graph (Figure 1) that transforms positional and angular coordinates and momenta x and the external control into f(x, u). The derivation of the method is long but very well written and didactic. The experiments on the control suite of OpenAI Gym are simple (pendulum and cart-pole only) but thorough, and compare the proposed method with a non-inductive bias method (still relying on Neural ODEs), a simpler naive and geometric baselines. Overall, the paper is very easy to follow (even for someone who does not work on control experiments in deep RL) while addressing complex physics. Minor remarks: Can you define g in 2.1? Can you add the derivation of equations (8) through (10) in the appendix? The second to last sentence on page 4 seems unfinished. Can the authors comment on how this model would scale to larger (e.g. multiple joints) dynamical systems?"}
{"id": "iclr2020_518", "title": "Differentiable learning of numerical rules in knowledge graphs | OpenReview", "abstract": "Abstract:###Rules over a knowledge graph (KG) capture interpretable patterns in data and can be used for KG cleaning and completion. Inspired by the TensorLog differentiable logic framework, which compiles rule inference into a sequence of differentiable operations, recently a method called Neural LP has been proposed for learning the parameters as well as the structure of rules. However, it is limited with respect to the treatment of numerical features like age, weight or scientific measurements. We address this limitation by extending Neural LP to learn rules with numerical values, e.g., “People younger than 18 typically live with their parents”. We demonstrate how dynamic programming and cumulative sum operations can be exploited to ensure efficiency of such extension. Our novel approach allows us to extract more expressive rules with aggregates, which are of higher quality and yield more accurate predictions compared to rules learned by the state-of-the-art methods, as shown by our experiments on synthetic and real-world datasets.", "review": "Review:###This paper proposes an extension of NeuralLP that is able to learn a very restricted (in terms of expressiveness) set of logic rules involving numeric properties. The basic idea behind NeuralLP is quite simple: traversing relationships in a knowledge graph can be done by multiplicating adjacency matrices, and which rules hold and which ones don*t can be discovered by learning an attention distribution over rules from data. The idea is quite clever: relationships between numeric data properties of entities, such as age and heigh, can also be linked by relationships such as leq and geq, and those relations can be treated in the same way as standard knowledge graph relationship by the NeuralLP framework. A major drawback in applying this idea is that the corresponding relational matrix is expensive to both materialise, and use within the NeuralLP framework (where matrices are mostly sparse). To this end, authors make this process tractable by using dynamic programming and by defining such a matrix as a dynamic computation graph by means of the cumsum operator. Furthermore, authors also introduce negated operators, also by defining the corresponding adjacency matrices by means of computation graphs. Authors evaluate on several datasets - two real world and two synthetic - often showing more accurate results than the considered baselines. One thing that puts me off is that, in Table 2, AnyBurl (the single one baseline authors considered other than the original NeuralLP) yields better Hits@10 values than Neural-LP-N, but the corresponding bold in the results is conveniently omitted. Another concern I have is that the expressiveness of the learned rules can be somehow limited, but this paper seems like a good star towards learning interpretable rules involving multiple modalities. Missing references - authors may want to consider citing https://arxiv.org/abs/1906.06187 as well in Sec. 2 - it seems very related to this work."}
{"id": "iclr2020_519", "title": "Contrastive Learning of Structured World Models | OpenReview", "abstract": "Abstract:###A structured understanding of our world in terms of objects, relations, and hierarchies is an important component of human cognition. Learning such a structured world model from raw sensory data remains a challenge. As a step towards this goal, we introduce Contrastively-trained Structured World Models (C-SWMs). C-SWMs utilize a contrastive approach for representation learning in environments with compositional structure. We structure each state embedding as a set of object representations and their relations, modeled by a graph neural network. This allows objects to be discovered from raw pixel observations without direct supervision as part of the learning process. We evaluate C-SWMs on compositional environments involving multiple interacting objects that can be manipulated independently by an agent, simple Atari games, and a multi-object physics simulation. Our experiments demonstrate that C-SWMs can overcome limitations of models based on pixel reconstruction and outperform typical representatives of this model class in highly structured environments, while learning interpretable object-based representations.", "review": "Review:###The construction and learning of structured world models is an interesting area of research that could in principle enable better generalisation and interpretability for predictive models. The authors overcome the problem of using pixel-based losses (a common issue being reconstruction of small but potentially important objects) by using a contrastive latent space. The model otherwise makes use of a fixed number of object slots and a GNN transition model, similarly to prior approaches. The authors back up their method with nice results on 3D cubes and 3-body physics domains, and reasonable initial results on two Atari games, with ablations on the different components showing their contributions, so I would give this paper an accept. The comparisons to existing literature and related areas is very extensive, with interesting pointers to potential future work - particularly on the transition model and graph embeddings. As expected, the object-factorized action space appears to work well for generalisation, and could be extended/adapted, but setting a fixed number of objects K is a clearly fundamentally limiting hyperparameter, and so showing how the model performs under misspecification of this hyperparameter is useful to know for settings where this is known (2D shapes, 3D blocks, 3-body physics). The fact that K=1 is the best for Pong but K=5 is the best for Space Invaders raises at least two questions: can scaling K > 5 further improve performance on Space Invaders, and is it possible to make the model more robust to a greater-than-needed number of object slots? On a similar note, the data collection procedure for the Atari games seems to indicate that the model is quite sensitive to domains where actions rarely have an impact on the transition dynamics, or the interaction is more complex (e.g. other agents exist in the world) - coming up with a synthetic dataset where the importance of this can be quantified would again aid understanding of the authors* proposed method."}
{"id": "iclr2020_520", "title": "Decoding As Dynamic Programming For Recurrent Autoregressive Models | OpenReview", "abstract": "Abstract:###Decoding in autoregressive models (ARMs) consists of searching for a high scoring output sequence under the trained model. Standard decoding methods, based on unidirectional greedy algorithm or beam search, are suboptimal due to error propagation and myopic decisions which do not account for future steps in the generation process. In this paper we present a novel decoding approach based on the method of auxiliary coordinates (Carreira-Perpinan & Wang, 2014) to address the aforementioned shortcomings. Our method introduces discrete variables for output tokens, and auxiliary continuous variables representing the states of the underlying ARM. The auxiliary variables lead to a factor graph approximation of the ARM, whose maximum a posteriori (MAP) inference is found exactly using dynamic programming. The MAP inference is then used to recreate an improved factor graph approximation of the ARM via updated auxiliary variables. We then extend our approach to decode in an ensemble of ARMs, possibly with different generation orders, which is out of reach for the standard unidirectional decoding algorithms. Experiments on the text infilling task over SWAG and Daily Dialogue datasets show that our decoding method is superior to strong unidirectional decoding baselines.", "review": "Review:###I think this is a good study, unless I miss something. It proposes a new solution to one of the fundamental problems in the field, which significantly improves previous solutions in terms of accuracy. I will recommend it for acceptance unless I miss something. (I’m not an expert in the field and the problem seems to be so fundamental, making me cautious about judging the novelty of the proposed solution.) There seems to be some unclarity about the optimization algorithm. In short, I suspect that the proposed optimization has some difficulty with its convergence. - In Sec.4, the authors suggest “a two-step block coordinate descent algorithm to alternate between (i) optimizing y’s while g’s are fixed, and (ii) optimizing g’s while y’s are fixed”. They further suggest `a variant of the Viterbi algorithm’ for (i) and gradient-based algorithms for (ii). These seem to make sense to me so far. - Then, in Sec.6, they state that “We use the Nesterov optimizer with a learning rate of 0.1. All mu’s are initialised with 0.5 and are multiplied by 1.2 after 5 epochs”. I think this needs some explanation. - For instance, what is the thought behind the choice of the optimizer? Why was Nesterov’s acceleration necessary instead of plain GD? Is it essential to use the scheduled adjustment of mu? - It states “All mu’s” but there seems only a single mu in Eq.(5). Am I missing something? - Some explanation on what “Nesterov optimizer” and mu would also be necessary for a wide range of readers. - There is a statement on the initialization of the optimization in p.7: “h corresponding to the beam search solution to initialize the g variables”. How sensitive is the optimization to the initial values? For instance, will the results change if g’s are initialized by the solution of the greedy method. - There is a plot without a figure caption in p.8, which shows how the objective cost decreases with parameter updates. Why are their values at ‘epoch’=0 lower than those at the subsequent epochs? Does this mean the initial values give more optimal parameters? Additional comments: - It would be more friendly to the readers to show the definition of the notations like y^n_1. - There is a statement like “Decoding was run for 10 epochs.” I suppose epochs here mean iteration count of the alternated parameter updates in the proposed algorithm. Why do the authors call it `epoch’? - No figure caption for the plot in p.8. It also uses `epochs’ for the horizontal axis title. No axis title for the vertical axis."}
{"id": "iclr2020_521", "title": "The Break-Even Point on the Optimization Trajectories of Deep Neural Networks | OpenReview", "abstract": "Abstract:###Understanding the optimization trajectory is critical to understand training of deep neural networks. We show how the hyperparameters of stochastic gradient descent influence the covariance of the gradients (K) and the Hessian of the training loss (H) along this trajectory. Based on a theoretical model, we predict that using a high learning rate or a small batch size in the early phase of training leads SGD to regions of the parameter space with (1) reduced spectral norm of K, and (2) improved conditioning of K and H. We show that the point on the trajectory after which these effects hold, which we refer to as the break-even point, is reached early during training. We demonstrate these effects empirically for a range of deep neural networks applied to multiple different tasks. Finally, we apply our analysis to networks with batch normalization (BN) layers and find that it is necessary to use a high learning rate to achieve loss smoothing effects attributed previously to BN alone.", "review": "Review:###The authors demonstrate that, during training, there is a point during the early phase of training that leads stochastic gradient descent (SGD) to a point where the covariance of the gradients (K) has a lower spectral norm (smaller first eigenvalue) and improved conditioning in K and the Hessian of the training loss (H). The authors experiments seem to verify that learning rate and batch size do play a part in the spectral norm of K and the conditioning of K. My one issue is that, while effects on K produced by higher learning rates are supposed to be *good*, the authors do not directly relate this back to model performance. From my years of experience training neural networks, I have seen many scenarios in which higher learning rates result in worse performance, even after reducing the learning rate. Can this be related back to the author*s claims? Under what conditions does a higher learning rate lead to these effects on K and H and will it always lead to better model performance? Other comments: In definitions, it says that the eigenvalue of matrix A is lambda_A^i, however, later in the 4th part of the assumptions, the spectral norm of H is referred to as lambda_1^H. Is there a difference here? Typo? The last part of the definitions where Phi(\tau) is introduced should have a formal definition for Phi(\tau) as Phi is initially does not take any parameter \tau. In section 4.1, *further growth of lambda_K^1 K does not translate into an increase of lambda_K^1* lambda_K^1 is repeated. Typo? ** After author response ** Changing from weak accept to accept. The authors have addressed my concerns about the paper."}
{"id": "iclr2020_522", "title": "Unified recurrent network for many feature types | OpenReview", "abstract": "Abstract:###There are time series that are amenable to recurrent neural network (RNN) solutions when treated as sequences, but some series, e.g. asynchronous time series, provide a richer variation of feature types than current RNN cells take into account. In order to address such situations, we introduce a unified RNN that handles five different feature types, each in a different manner. Our RNN framework separates sequential features into two groups dependent on their frequency, which we call sparse and dense features, and which affect cell updates differently. Further, we also incorporate time features at the sequential level that relate to the time between specified events in the sequence and are used to modify the cell*s memory state. We also include two types of static (whole sequence level) features, one related to time and one not, which are combined with the encoder output. The experiments show that the proposed modeling framework does increase performance compared to standard cells.", "review": "Review:###This submitted manuscript is exactly the paper (bearing no difference) that was submitted to ICLR 2019 and also rejected. This submitted manuscript is exactly the paper (bearing no difference) that was submitted to ICLR 2019 and also rejected. This submitted manuscript is exactly the paper (bearing no difference) that was submitted to ICLR 2019 and also rejected. This submitted manuscript is exactly the paper (bearing no difference) that was submitted to ICLR 2019 and also rejected. This submitted manuscript is exactly the paper (bearing no difference) that was submitted to ICLR 2019 and also rejected. This submitted manuscript is exactly the paper (bearing no difference) that was submitted to ICLR 2019 and also rejected."}
{"id": "iclr2020_523", "title": "Training Data Distribution Search with Ensemble Active Learning | OpenReview", "abstract": "Abstract:###Deep Neural Networks (DNNs) often rely on very large datasets for training. Given the large size of such datasets, it is conceivable that they contain certain samples that either do not contribute or negatively impact the DNN*s optimization. Modifying the training distribution in a way that excludes such samples could provide an effective solution to both improve performance and reduce training time. In this paper, we propose to scale up ensemble Active Learning methods to perform acquisition at a large scale (10k to 500k samples at a time). We do this with ensembles of hundreds of models, obtained at a minimal computational cost by reusing intermediate training checkpoints. This allows us to automatically and efficiently perform a training data distribution search for large labeled datasets. We observe that our approach obtains favorable subsets of training data, which can be used to train more accurate DNNs than training with the entire dataset. We perform an extensive experimental study of this phenomenon on three image classification benchmarks (CIFAR-10, CIFAR-100 and ImageNet), analyzing the impact of initialization schemes, acquisition functions and ensemble configurations. We demonstrate that data subsets identified with a lightweight ResNet-18 ensemble remain effective when used to train deep models like ResNet-101 and DenseNet-121. Our results provide strong empirical evidence that optimizing the training data distribution can provide significant benefits on large scale vision tasks.", "review": "Review:###This work makes use of uncertainty estimation methods from active learning to select a subset of training data that produces models with similar (or better) performance compared to models trained on the full training set. It proposes a way to improve the Monte Carlo estimation of model uncertainty by including multiple checkpoints that are generated *for free* during a training run, thereby increasing the number of samples from 5-10 in previous work to 100 in this work. It compares several initialization schemes for the subset model using mutual information as the acquisition function, finds that a *build-up* approach (based on Chitta et. al 2018a) works best, and uses that for the rest of the studies. It then compares several acquisition functions, using the build-up approach, finds that variation ratio performs best, and uses that for the rest of the studies. Next, it compares the Top-1 accuracy on ImageNet obtained by evaluating the ensemble models produced by different ensembling schemes, and finds that ensembling 20 checkpoints from 5 training runs with different random seeds work best. Then, it uses acquisition models that use ensembles from each ensembling scheme to select subsets of the ImageNet data to be used for training the subset model, and then compares the performance of the subset models. Finally, it demonstrates this method of selecting a subset of the training data works even if the subset is used to train a model with a different architecture from the acquisition model. Strengths: - Algorithm is likely to be useful in practice. Training dataset can be *compressed* using a smaller architecture like ResNet-18, then used to train larger architectures like DenseNet-121, thus saving the amount of compute per training epoch. Using training checkpoints in the ensemble is very practical but not obvious (to me), since my first intuition would be that checkpoints from the same training run would not provide enough diversity to improve the acquisition function. I am glad that there were thorough experiments to address this concern and demonstrate that it works. - Experiments answer key questions about the method proposed, and the sequence of experiments have a clear logical flow. Good baselines. Clear notation and problem set-up. Weakness that affected the score: - Missing detail on the build-up initialization scheme. The work referred to Chitta et al., 2018a, but that algorithm requires selecting a growth parameter. This growth parameter determines the number of times the subset model needs to be retrained, which can affect the viability of this method in practice. I would like to see the build-up initialization scheme described in greater detail. Clarifications: - In Table 2 and Table 3, are the results in the *Single (1)*, *Checkpoints (5)*, and *Checkpoints (20)* columns obtained by averaging over the 5 random seeds? - In the last column of Table 2, Top-1 accuracy of ~84% from an ensemble of 100 ResNet-18s (Table 2) seem very high. In comparison, ResNet-50 and AmoebaNet-A (2019) obtained a Top-1 accuracy of 77.2% and 83.9% respectively. What do the authors think about this? - Why would one expect high accuracy of the ensemble of NNs in the acquisition model to indicate good sampling quality of the acquisition model? (caption for table 2) Minor issues: - Algorithm 1: first two steps should be kept un-italicized, like the rest of the steps. - Page 7, first paragraph: *This shows that the checkpoints are obtained with no additional computational cost at train time can be used to generate diverse ensembles.* The first *are* in this sentence is unnecessary. - Build-up was chosen as the initialization scheme for the rest of the studies, as it performed best when the acquisition function was fixed at *mutual information*. However, the acquisition function that was finally chosen for the rest of the studies is *variation ratio*, since it performed best when the initialization scheme was fixed at build-up. It would be more convincing if figure 1 also includes variation ratio."}
{"id": "iclr2020_524", "title": "Nonlinearities in activations substantially shape the loss surfaces of neural networks | OpenReview", "abstract": "Abstract:###Understanding the loss surfaces of neural networks is fundamentally important to understanding deep learning. This paper presents how the nonlinearities in activations substantially shape the loss surfaces of neural networks. We first prove that the loss surface of every neural network has infinite spurious local minima, which are defined as the local minima with higher empirical risks than the global minima. Our result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. This result demonstrates that nonlinear networks possess substantial differences to the well-studied linear neural networks. Essentially, the underlying assumptions for the above result are consistent with most practical circumstances where the output layer is narrower than any hidden layer. We further prove a theorem that draws a big picture for the loss surfaces of nonlinear neural networks from the following respects. (1) Smooth and multilinear partition: the loss surface is partitioned into multiple smooth and multilinear open cells. (2) Local analogous convexity: within every cell, local minima are equally good, and equivalently, they are all global minima in the cell. (3) Local minima valley: some local minima are concentrated into a valley in some cell, sharing the same empirical risk. (4) Linear collapse: when all activations are linear, the partitioned loss surface collapses to one single cell, which includes linear neural networks as a simplified case. The second result holds for one-hidden-layer networks for regression under convex loss, while all others apply to networks of arbitrary depth.", "review": " Summary: This paper studies the landscape of deep neural networks with piecewise-linear activation functions. The paper showed that under very mild assumptions, the loss surface admits infinite spurious local minima. Further, it is shown that the loss surface is partitioned into many multilinear cells. If the network is two-layer with two-piece linear activations, it is proved that within each cell every local minimum is global. Pros: --Constructed spurious local minima for piecewise linear activations, for a broader setting than previous papers. --The paper is well written, with detailed explanation of proof skeleton. Cons: The significance of the results are not clear. Details are given below. 1. This paper only considers piecewise linear activation, which is a special type of non-linear activation. The major examples are ReLU and leaky ReLU. However, related results for ReLU have been studied for a few previous works mentioned in the last two paragraphs of Sec. 3.2. In particular, Yun et al. 2019b already proves a similar result for 1-hidden-layer neural-net with ReLU activation. I think extending the construction to broader settings (any depth, any piecewise linear and more losses) is mathematically nice, but the motivation of this extension is somewhat unclear to me. One motivation is that this is helpful for the purpose of understanding a “big picture” of the landscape, which I will discuss next. 2. The second major result is Theorem 2, on the “big picture” with ReLU-like activations. However, Theorem 2 is somewhat trivial to prove, and the link to Theorem 1 is rather weak. (a) The main message of Thm 2 is the partition of the surface into multiple pieces, and each piece has good property. This partition is somewhat straightforward, and has been studied before, in, e.g., [R1]. For a global “big picture”, partitioning itself is not very interesting. Theorem 2 mainly describes the property of each region separately for 2-layer network, which is weaker than [R1]. (b) Theorem 2 seems easy to prove. The 1st, 3rd and 4th property are all straightforward. The 2nd property “local analogous convexity” was given a 2-page proof in the paper. However, I don’t understand why not use the following simple argument: for each region, the network behaves like a deep linear network, thus directly applying existing result shall imply “every local minimum in the region is the global minimum of the region”, right? If not, what is the difficulty? (c) The 3rd property says “some local minima are concentrated as a valley in some cell”. What are the formal definitions of “concentrated” and “valley” in this sentence? (d) The link to Theorem 1 is weak: the link is the 3rd property of Theorem 2 that “some local minima are in a valley”. It is just about some special local minima and weakly related to the other properties on the “global view”. In addition, the fact that “some of them are in a valley” may be due to the very special construction, thus it is not surprising and does not reveal anything interesting about the “big picture”. 3. Other issues: a) While ReLU-type activations are popular, there are still commonly-used activation functions are not piece-wise linear, e.g., tanh, swish. It is not proper to claim that *this paper presents how nonlinearities in activations substantially shape the loss surface* and *almost every practical neural network ....*. I suggest replacing *nonlinearity* with *piecewise linearity* in both the title and the abstract, and modifying the over-statements. b) In Property 1 of Theorem 2, “smooth and multilinear partition” might be a bit misleading. The loss surface should be fractional in general, where multilinear cells are separated by non-smooth boundaries. “Smooth partition” seems to imply that the boundaries are smooth or the partition method is smooth in some sense. c) The name “analogous convexity” is not appropriate. Analogous convexity is not formally defined in the paper. According to Sec. 4.3 third paragraph, “the property of analogous convexity that the local minima wherein are equally good”. It seems that “analogous convexity” is just “all local minima are good”, which is very different from convexity. It is a weaker property than quasi-convexity, star-convexity, etc, and thus it is better not to call it “analogous convexity”. d) Property 3 of Theorem 2 is very far from “mode connectivity”. The proof of Property 3 relies on a special construction of Theorem 1, and the latter is for two arbitrary global minima. [R1] Soudry and Hoffer. *Exponentially vanishing sub-optimal local minima in multilayer neural networks.* arXiv preprint arXiv:1702.05777 (2017). Conclusion: I think this paper is studying an important and interesting question, and the efforts of constructing local minima and understanding big picture are both interesting to me. However, I’m afraid the current form of the paper does not meet the standard of the conference. That being said, it would be a nice paper if the big picture can be explored deeper, and the link to the spurious local minima can be built stronger."}
{"id": "iclr2020_525", "title": "Multi-Agent Interactions Modeling with Correlated Policies | OpenReview", "abstract": "Abstract:###In multi-agent systems, complex interacting behaviors arise due to heavy correlations among agents. However, prior works on modeling multi-agent interactions from demonstrations have largely been constrained by assuming the independence among policies and their reward structures. In this paper, we cast the multi-agent interactions modeling problem into a multi-agent imitation learning framework with explicit modeling of correlated policies by approximating opponents’ policies. Consequently, we develop a Decentralized Adversarial Imitation Learning algorithm with Correlated policies (CoDAIL), which allows for decentralized training and execution. Various experiments demonstrate that CoDAIL can better fit complex interactions close to the demonstrators and outperforms state-of-the-art multi-agent imitation learning methods.", "review": " The authors propose a decentralized adversarial imitation learning algorithm with correlated policies, which recovers each agent’s policy through approximating opponents action using opponent modeling. Extensive experimental results showed that the proposed framework, CoDAIL, better fits scenarios with correlated multi-agent policies. Generally, the paper follows the idea of GAIL and MAGAIL. Differing from the previous works, the paper introduces epsilon-Nash equilibrium as the solution to multi-agent imitation learning in Markov games. It shows that using the concept of epsilon-Nash equilibrium as constraints is consistent and equivalent to adding the difference of the causal entropy of the expert policy and the causal entropy of a possible policy in RL procedure. It makes sense. Below, I have a few concerns to the current status of the paper. 1. The authors propose epsilon-Nash equilibrium to model the convergent state in multi-agent scenarios, however, in section 3.1 the objective function of MA-RL (Equation 5) is still the discounted causal entropy of policy, the same as that of MA-GAIL paper. It is unclear how the epsilon-NE is considered in modeling MA-RL problem. 2. Rather than assuming conditional independence of actions from different agents, the authors considered that the joint policy as a correlated policy conditioned on state and all opponents’ actions. With the new assumption, the paper re-defines the occupancy measure and introduces an approach to approximate the unobservable opponents’ policies, in order to access opponents’ actions. However, in the section 3.2 when discussing the opponents modeling, the paper did not clearly explain how the joint opponent function sigma^{(i)} is designed. The description sigma^{(i)} is confusing. 3. Typos: in equation 14 “i” or “-i”; appendix algorithm 1 line 3 “pi” or “pi”."}
{"id": "iclr2020_526", "title": "Extracting and Leveraging Feature Interaction Interpretations | OpenReview", "abstract": "Abstract:###Recommendation is a prevalent application of machine learning that affects many users; therefore, it is crucial for recommender models to be accurate and interpretable. In this work, we propose a method to both interpret and augment the predictions of black-box recommender systems. In particular, we propose to extract feature interaction interpretations from a source recommender model and explicitly encode these interactions in a target recommender model, where both source and target models are black-boxes. By not assuming the structure of the recommender system, our approach can be used in general settings. In our experiments, we focus on a prominent use of machine learning recommendation: ad-click prediction. We found that our interaction interpretations are both informative and predictive, i.e., significantly outperforming existing recommender models. What*s more, the same approach to interpreting interactions can provide new insights into domains even beyond recommendation.", "review": "Review:###The paper proposes a method to detect which features in the input of recommender systems are interacted each other, i.e., combining them behaves useful information, and examines to feed extracted interactions directly into the recommender systems to measure effects on actual recommendation. The interaction detector consists of 1. perturbing the input vectors, 2. training NNs to utilize its internal non-linear representation as a signal of interaction, and 3. aggregating detected interactions over training set. 1. and 2. are consisting of known methods so that the proposed method is an application of them. For 3. authors introduced a simple heuristic (Algorithm 1). As long as I heard how the NID work to detect interaction from this paper, it is sensitive not only the true interaction between features but also set of features holding similar signals (e.g., if a hidden unit behaves as a feature A, it may be natural to aggregate all features that implies A by itself, regardless of the meaning of their interaction). This is not a desired case as long as the paper specified the interactions in section 3. Maybe it requires more detailed explanation about how good applying NID for this task is. Experiments are conducted to show the behavior of the interaction detector and actual improvement of utilizing extracted interactions as additional features. Experiments look less informative for comparing the proposed method with other existing methods for similar motivations (e.g., some methods introduced in related work) since there is only a trivial baseline by the original LIME*s method. In the case-study of Figure 4(b), I thought that the proposed method is a bit biased for frequent but meaningless features, because it detected *I* or *a* that intuitively occur with any labels. It is maybe because the Algorithm 1. that simply aggregates all detected interactions regardless of their actual importance. For further improvement, it may be necessary to introduce some weighting strategy to detected interactions."}
{"id": "iclr2020_527", "title": "FR-GAN: Fair and Robust Training | OpenReview", "abstract": "Abstract:###We consider the problem of fair and robust model training in the presence of data poisoning. Ensuring fairness usually involves a tradeoff against accuracy, so if the data poisoning is mistakenly viewed as additional bias to be fixed, the accuracy will be sacrificed even more. We demonstrate that this phenomenon indeed holds for state-of-the-art model fairness techniques. We then propose FR-GAN, which holistically performs fair and robust model training using generative adversarial networks (GANs). We first use a generator that attempts to classify examples as accurately as possible. In addition, we deploy two discriminators: (1) a fairness discriminator that predicts the sensitive attribute from classification results and (2) a robustness discriminator that distinguishes examples and predictions from a clean validation set. Our framework respects all the prominent fairness measures: disparate impact, equalized odds, and equal opportunity. Also, FR-GAN optimizes fairness without requiring the knowledge of prior statistics of the sensitive attributes. In our experiments, FR-GAN shows almost no decrease in fairness and accuracy in the presence of data poisoning unlike other state-of-the-art fairness methods, which are vulnerable. In addition, FR-GAN can be adjusted using parameters to maintain reasonable accuracy and fairness even if the validation set is too small or unavailable.", "review": "Review:###This paper proposes to use a GAN style approach for training a classifier that is robust to data poisoning and can achieve a pre-specified notion of group fairness. The contribution of this paper is incremental in the context of prior Adversarial Debasing (AD) approach using essentially same GAN for group fairness and prior work presenting ideas of utilizing clean validation data to defend against data poisoning. This paper is proposing to add an additional discriminator to the AD approach that distinguishes training data and clean validation data. If the training data is poisoned, such distinguishment may be possible and maximizing loss of this discriminator can help to robustify against poisoned samples. Experimental results are insufficient to argue improvement over the AD. There are no AD results in the equalized odds Adult experiment in the supplement. I recommend more detailed comparison against the AD method (including results showing confusion matrices). Also note that AD, as presented in the original paper, is optimizing for demographic parity, but can also be adjusted to other group fairness metrics. Finally, in the context of Adult dataset, it is important to also report performance metrics such as balanced TPR since the labels are quite imbalanced. Are there any real data examples where poisoning does not need to be introduced artificially and the proposed method helps to improve the fairness properties? I think an interesting direction could be to consider data where labels are subjective. For example, a dataset on loan decisions can be naturally *poisoned* with human biases, i.e. information that someone did not receive the loan may be due to an error or bias of a human in charge of the decision making. Lastly I think that the discussion of the prior related work on fairness is incomplete. This paper exclusively covers group fairness, which indeed has been shown to have some disadvantages. For example, prior work [1] has shown that some group fairness notions can not be satisfied simultaneously in certain cases. In this regard it is also important to report multiple group fairness metrics simultaneously in the experiments. The other fairness definition, that has not been mentioned in this paper, is individual fairness [2]. It has legal and intuitive interpretations. Multiple recent papers have explored the direction of individual fairness [3,4,5,6]. [1] Kleinberg, J., Mullainathan, S., & Raghavan, M. (2016). Inherent trade-offs in the fair determination of risk scores. [2] Dwork, C., Hardt, M., Pitassi, T., Reingold, O., & Zemel, R. (2012, January). Fairness through awareness. [3] Garg, S., Perot, V., Limtiaco, N., Taly, A., Chi, E. H., & Beutel, A. (2019, January). Counterfactual fairness in text classification through robustness. [4] Yurochkin, M., Bower, A., & Sun, Y. (2019). Learning fair predictors with Sensitive Subspace Robustness. [5] Kearns, M., Roth, A., & Sharifi-Malvajerdi, S. (2019). Average Individual Fairness: Algorithms, Generalization and Experiments. [6] Jung, C., Kearns, M., Neel, S., Roth, A., Stapleton, L., & Wu, Z. S. (2019). Eliciting and Enforcing Subjective Individual Fairness."}
{"id": "iclr2020_528", "title": "OPTIMAL TRANSPORT, CYCLEGAN, AND PENALIZED LS FOR UNSUPERVISED LEARNING IN INVERSE PROBLEMS | OpenReview", "abstract": "Abstract:###The penalized least squares (PLS) is a classic approach to inverse problems, where a regularization term is added to stabilize the solution. Optimal transport (OT) is another mathematical framework for computer vision tasks by providing means to transport one measure to another at minimal cost. Cycle-consistent generative adversarial network (cycleGAN) is a recent extension of GAN to learn target distributions with less mode collapsing behavior. Although similar in that no supervised training is required, the algorithms look different, so the mathematical relationship between these approaches is not clear. In this article, we provide an important advance to unveil the missing link. Specifically, we reveal that a cycleGAN architecture can be derived as a dual formulation of the optimal transport problem, if the PLS with a deep learning penalty is used as a transport cost between the two probability measures from measurements and unknown images. This suggests that cycleGAN can be considered as stochastic generalization of classical PLS approaches. Our derivation is so general that various types of cycleGAN architecture can be easily derived by merely changing the transport cost. As proofs of concept, this paper provides novel cycleGAN architecture for unsupervised learning in accelerated MRI and deconvolution microscopy problems, which confirm the efficacy and the flexibility of the theory.", "review": "Review:###In this paper, the authors present two contributions: 1) The primary contribution is to show that CycleGAN can be formulated as a probabilistic version of a particular penalized-least squares problem (theory) 2) As proof of concept, they apply their version of CycleGAN to accelerated MRI and deconvolution microscopy (application) While I find the idea to be potentially interesting, the presentation of the theory is unclear and not well-motivated; after reading, I’m not convinced that the connection to CycleGAN is as significant as the authors claim. The experimental results are preliminary. My decision is to reject. Below are separate critiques on the sections. Section 2-3: Hope the authors could clarify / strengthen these points in revision: - Since the discussion in Section 3 is based on the optimization problem in Equation (7), this problem should be well-motivated. Currently it is presented as a problem that has been explored previously by Zhang et al and Aggarwal et al. However, after taking a look at those papers, I don’t understand where this regularization term comes from. In these papers, the regularization term (i.e. equation 2 or 3 of Zhang et al) appears independent of y. Since this term is key to the paper, it should be well explained here. E.g. at the end of section 2.2: G_\theta(y) is a CNN pretrained on what task? - In the inverse problem, the objective is to estimate x from y. Therefore we care about argmin x in Equation (7). In the probabilistic setting presented in Equation (8), analogously the objective is to estimate pi^*, which is the solution to the primal problem. The theory shows that the primal formulation in Equation (8) is equivalent to the dual formulation in Equation (16), but does not show how the dual solution yields the primal solution, which is lacking as obtaining the primal solution seems to be the point of solving the PLS problem. (Interestingly, in Section 4, the authors are using the dual solution x = G_\theta(y) as if it is the mapping given by pi(x|y)… this needs to be explained.) - The authors claim that Proposition 1 shows that the cyclic loss term in their dual formulation is a more general version of the cycle-consistency loss in CycleGAN. But looking closely at Proposition 1 and its proof, it seems that the equivalence holds only for specific weights, not for arbitrary weights. Additionally, the specific weights are unknown (they depend on the solution pi^* to the primal problem…). I do not understand the claim that this is a generalization of cycle-consistency loss, nor do I see how the authors implement their version of the cyclic loss as it depends on unknown weights. - The connection to CycleGAN seems to hold only when p=q=1? - End of section 3: The authors conclude “our cost formulation using (17) with (18) and (19) is more general compared to the standard CycleGAN, since a general form of measurement data generator Hx can be used”. I don’t see the connection between the theory and this claim. Even with CycleGAN, both generators can be arbitrary or fixed if one of them is known. - The proofs are easy to follow, though perhaps they could be moved to the Appendix in favor of providing more motivation and explanation in the main text. Section 4: - The authors motivate the problem with the PLS setup but then they use the learned regularization term x = G_\theta(y) as if it is the mapping given by pi(x|y). I am confused by this. - Putting aside the connection to the PLS problem, my interpretation of the experimental setup is that the authors use CycleGAN with Wasserstein GAN loss instead of the classic discriminator loss, where one of the generators is known (and hence only one generator/discriminator pair is needed). I might be missing something, but I’m not sure that this approach is different enough from CycleGAN. - Considering that the authors have the ground truth, they could provide quantitative evaluation of their method against other methods, rather than showing a few qualitative results where it is working."}
{"id": "iclr2020_529", "title": "Global Relational Models of Source Code | OpenReview", "abstract": "Abstract:###Models of code can learn distributed representations of a program*s syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, we bridge this divide between global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers (GREAT for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identification, we explore the relative merits of traditional and hybrid model families for code representation. Starting with a graph-based model that already improves upon the prior state-of-the-art for this task by 20%, we show that our proposed hybrid models improve an additional 10-15%, while training both faster and using fewer parameters.", "review": "Review:###In this paper, the authors proposed a new method to model the source code for the bug repairing task. Traditional methods use either a global sequence based model or a local graph based model. The authors proposed a new sandwich model like [RNN GNN RNN]. The experiments show that such simple combination of models significantly improve the localization and repair accuracy. The idea is simple, so the technical contribution is a bit low. But the message is clear. The sandwich model can benefit from GNN model that can achieve a higher accuracy at the beginning of training where transformer did a poor job. At the end of training, the sandwich model outperforms both kinds of models. Here are some detailed comments: 1. It would be interesting to have the complete result from Transformer in Figure 1 and Figure 2 which is missing. 2. The results from GGNN (smaller model) in Figure 1 and Figure 2 seems to be not the same. 3. One major benefit of GNN is its efficient local computation. Some industrial applications have also used GNN for recommendation that can be trained very fast. Why GGNN is so slow in this paper? Is this because of implementation?"}
{"id": "iclr2020_530", "title": "Collaborative Inter-agent Knowledge Distillation for Reinforcement Learning | OpenReview", "abstract": "Abstract:###Reinforcement Learning (RL) has demonstrated promising results across several sequential decision-making tasks. However, reinforcement learning struggles to learn efficiently, thus limiting its pervasive application to several challenging problems. A typical RL agent learns solely from its own trial-and-error experiences, requiring many experiences to learn a successful policy. To alleviate this problem, we propose collaborative inter-agent knowledge distillation (CIKD). CIKD is a learning framework that uses an ensemble of RL agents to execute different policies in the environment while sharing knowledge amongst agents in the ensemble. Our experiments demonstrate that CIKD improves upon state-of-the-art RL methods in sample efficiency and performance on several challenging MuJoCo benchmark tasks. Additionally, we present an in-depth investigation on how CIKD leads to performance improvements.", "review": "Review:###This paper introduces a method for using an ensemble of deep reinforcement learning policies, where members of the ensemble are periodically updated to imitate the most promising member of the ensemble. Thus learning proceeds by performing off policy reinforcement learning updates for each individual policy, as well as some supervised learning for inter-policy imitation learning. I start by what I view as the positive aspects about the paper: 1- The algorithm is quite simple (to understand and to implement). 2- Experimental results are performed on a variety of domains, and more importantly, each experiment is motivated by a question. That said, I have some concerns about this paper which I list below: 1- Perhaps my biggest concern is that the approach is not motivated from a theory stand point. There has been interesting results in Osband*s work [Osband, 2016] (and references therein) for randomized value functions which can serve as a foundation for this work. That said, a) Osband*s results, at least immediately, are related to value-function based methods, as opposed to policy gradient b) the KL update which one could argue is the main and only significant contribution of the paper, is not justified by Osband or any other prior work c) there is not anything that this paper adds to the literature to better justify diversity through randomization and/or imitation learning based on the best member of the ensemble. 2- I have found various claims in the paper which are unclear, scientifically not true, or sometimes even contradicting. In Introduction, for example, the authors mention that the agent sometimes gets into a sub-optimal policy and may require a large number of interactions before escaping the sub optimal policy. How does gathering more data help to improve the policy? Either we are in a local maximum, which if we are doing gradient ascent, there is really not much we could do, or that we are in a saddle point, which we can escape by adding some noise to the gradient. [Jin,2017] 3- In section 4.3 the authors talk about on-policy methods requiring importance sampling (IS) ratios. To the best of my knowledge, IS is only used for off-policy learning. Can the authors provide a link to an on-policy method that does IS? 4- Again in section 4.3 authors claim and I quote *Using off-policy methods, all the policies in the ensemble can easily be updated, since off-policy update methods can perform updates from any \tau*. But later on in Section 5.3 authors claim that *off-policy actor-critic methods (e.g. SAC) cannot fully utilize the other agent*s or past experience.* So which statement is true? 5- Again, the KL update is interesting, but is it even surprising that the KL update is necessary for an ensemble of policies updates using policy gradients? In the absence of this KL update, which the authors characterize as the method that Osband proposed, the policies could generally be arbitrarily far from one another. This means that each policy needs to perform policy evaluation using trajectories that are coming from other policies who in principle can be radically different than the policy we want to update. This means that updates will be quite *off-policy* which we know can really degrade the quality of the estimated gradient. This is perhaps why even choosing a random policy to update towards is providing *some* improvement. I think this is the real insight, but it is not really discussed at all in the paper. 6- On the same note, I do not think that one can say Osband*s method is the same as CIKD but only without the KL update. Most notably, Osband*s work was presented for value-function-based methods like DQN. These methods work fundamentally different than policy gradient methods, which rely on (near) on-policy updates to perform good policy improvements. In that sense, the presented results make sense, but I disagree with the framing of the results and how they are presented here. 7- In section 5.3, when the authors utilize more policy updates to have a fair comparison, are they retuning hyper parameters? Surely they need to do that, at least for hyper-parameters that are known to be super important such as the step size. 8- Overall I liked section 5.5 that is trying to dissect causes for improvement. However, it seems like that the *dominant agent* hypothesis has been rejected hastily, unless I misunderstood the experiment. The authors show that the notion of best is spread across different agents. But of course this will be the case in light of the KL update, since the policies are getting closer to one another. Can you redo the experiment in the absence of the KL update? 9- Have the authors thought about any connection between this and genetic algorithms? In genetic algorithms, the idea is the next set of candidates are chosen based on the most promising candidates in the current iteration. CIKD seems like a soft implementation of this idea. In light of the comments above, I am voting for weak rejection, though as I said before, I do see some interesting things in this paper. I encourage the authors to think about CIKD from a theoretical lens in the future."}
{"id": "iclr2020_531", "title": "Additive Powers-of-Two Quantization: A Non-uniform Discretization for Neural Networks | OpenReview", "abstract": "Abstract:###We proposed Additive Powers-of-Two (APoT) quantization, an ef?cient nonuniform quantization scheme that attends to the bell-shaped and long-tailed distribution of weights in neural networks. By constraining all quantization levels as a sum of several Powers-of-Two terms, APoT quantization enjoys overwhelming ef?ciency of computation and a good match with weights’ distribution. A simple reparameterization on clipping function is applied to generate better-de?ned gradient for updating of optimal clipping threshold. Moreover, weight normalization is presented to re?ne the input distribution of weights to be more stable and consistent. Experimental results show that our proposed method outperforms state-of-the-art methods, and is even competitive with the full-precision models demonstrating the effectiveness of our proposed APoT quantization. For example, our 3-bit quantized ResNet-34 on ImageNet only drops 0.3% Top-1 and 0.2% Top-5 accuracy without bells and whistles, while the computation of our model is approximately 2× less than uniformly quantized neural networks.", "review": "Review:###In this paper, the authors proposed a novel quantization method, additive powers-of-two quantization and show both the uniform quantization and powers-of-two quantization are the special case of this method. To train such a quantized network, new clipping function, and weight normalization are proposed. The numerical results show that the proposed method only introduces a small accuracy loss and sometimes even improve accuracy. Compared to the other methods, it also shows better performance. Overall, I think this work is valuable and may be considered for publication. But the reviewer is completely out of this neural network quantization area, and thus not very familiar with the related works. The following are some more detailed comments: 1. On page 3, *when we increase the bit-width from b to b+1 ... be split into 2^b subintervals.* Based on Equation 3, I think it might be 2^(b-1) +1 subintervals. 2. From Figure 1 (c), the introduced quantization method introduces non-monotonic interval steps, which is a little unintuitive. Can the authors explain if this can be further improved? 3. A very important theme of this paper is on hardware, however, I feel the paper doesn*t have a satisfying discussion on the hardware implementation. In order to make the argument more solid, the authors may want to provide more discussion on the hardware implementation trade-offs. Also, a detailed comparison of the quantization size comparison should also be provided. 4. The authors claim the quantized network can sometimes achieve better performance. This statement needs to be further checked. Since the performance provided in the baseline Resnet models can be slightly improved with further training and training schedule tuning. Without a thorough optimization, such a claim might be misleading."}
{"id": "iclr2020_532", "title": "Masked Based Unsupervised Content Transfer | OpenReview", "abstract": "Abstract:###We consider the problem of translating, in an unsupervised manner, between two domains where one contains some additional information compared to the other. The proposed method disentangles the common and separate parts of these domains and, through the generation of a mask, focuses the attention of the underlying network to the desired augmentation alone, without wastefully reconstructing the entire target. This enables state-of-the-art quality and variety of content translation, as demonstrated through extensive quantitative and qualitative evaluation. Our method is also capable of adding the separate content of different guide images and domains as well as remove existing separate content. Furthermore, our method enables weakly-supervised semantic segmentation of the separate part of each domain, where only class labels are provided. Our code is available anonymously at http://bit.ly/2mXTizX.", "review": "Review:###This paper proposes a method to disentangle the common and separate parts of two domains and to focus the attention of the underlying network to the desired part only, without reconstructing the entire target. The proposed method is also able to add or remove separate contents, and to enable weakly-supervised semantic segmentation of the separate part of each domain. This work relates to the problem of content transfer between images. The proposed method consists of five networks: one encoder for common domain invariant features, one encoder for separate domain specific information, one network for mapping encodings from common features from both domains undistinguishable, a decoder that generates sample in the origin domain and a decoder that generates the image that combines content from the origin image and domain specific content from the target image. This last decoder also outputs a mask that focuses the attention of the model to the specific part. The proposed model is trained using a combination of different losses: domain confusion loss, reconstruction losses, and cycle consistency losses. Ablation studies reported in the paper nicely show the contribution of each loss. The final loss is obtained by a weighted sum of the losses: how are the lambda coefficient chosen/learned? The proposed method is evaluated on guided content transfer, out of domain manipulation, attribute removal, sequential content transfer, sequential attribute removal and content addition, weakly supervised segmentation of the domain specific content. Experimental results are clear, thorough and satisfactory, both quantitative and qualitative results are reported, as well as a user study. Presented results demonstrate the strengths and limitations of the proposed approach, and the analysis of the results helps understanding and emphasizing the contribution of the paper. Information in appendix also enable reproducibility by providing parameters and architecture structure. Other comments: did you observe overfit for some choice of parameter? is your validation/test set large enough for evaluating results? did you observe biases in your method (e.g. to specific features/domain specific information)?"}
{"id": "iclr2020_533", "title": "Hebbian Graph Embeddings | OpenReview", "abstract": "Abstract:###Representation learning has recently been successfully used to create vector representations of entities in language learning, recommender systems and in similarity learning. Graph embeddings exploit the locality structure of a graph and generate embeddings for nodes which could be words in a language, products on a retail website; and the nodes are connected based on a context window. In this paper, we consider graph embeddings with an error-free associative learning update rule, which models the embedding vector of node as a non-convex Gaussian mixture of the embeddings of the nodes in its immediate vicinity with some constant variance that is reduced as iterations progress. It is very easy to parallelize our algorithm without any form of shared memory, which makes it possible to use it on very large graphs with a much higher dimensionality of the embeddings. We study the efficacy of proposed method on several benchmark data sets in Goyal & Ferrara(2018b) and favorably compare with state of the art methods. Further, proposed method is applied to generate relevant recommendations for a large retailer.", "review": "Review:###Thanks to the authors for their response. There are still significant issues with motivation, writing, and baseline comparisons (the latter noted by R3). I would encourage the authors to continue to polish and investigate their method and submit to a future conference. ===== This paper proposes an approach to learning embeddings associated with nodes in a graph. Inspired by Hebbian learning, the representations of a node are iteratively updated to be similar to representations of its neighbors. The update is performed by adding a scaled vector sampled from a Gaussian centered on a neighbor*s representation to the current node*s vector. After learning, the embeddings may be used for tasks such as graph reconstruction, link prediction, or product recommendation. Contributions include: * Proposal of an approach to learn embeddings of nodes in a graph in which representations of a node are updated by scaled and Gaussian-perturbed versions of its neighbors* representations. * Experiments demonstrating reconstruction and link prediction performance of the proposed approach on several datasets, as well as product recommendation perfomance on a large retail dataset. There are several significant concerns with the paper as it currently stands. The three most pressing issues are as follows: 1. The proposed algorithm is not situated relative to related work. 2. The paper does not provide the reader with enough details or precision to be able to replicate the work. 3. Experiments do not compare to any baselines other than random embeddings. The paper suggests that the proposed algorithm is a form of Hebbian learning because the representation of nearby nodes in the graph are encouraged to be similar. However, this idea has long been used for learning node embeddings (for example, LLE encourages representations of a node to be predicted as a linear combination of neighboring nodes). The connection seems loose other than a superficial similarity in the update rule and naming the algorithm after Hebbian learning is somewhat misleading. The algorithm is reminiscent of message-passing inference in a continuous Markov random field with pairwise potentials encouraging nearby nodes to have similar representations. I am not an expert in graph embedding approaches, but I would be surprised if the approach could not be easily related to classical approaches such as MDS or LLE. There are several notational/clarity issues: * j is used for both the node index and the embedding of the node itself (equation 1-2). Replacing the j on the left hand side with w_j would resolve ambiguity and bring the equations in line with Algorithm 1. * Equation 2 is inconsistent with equation 1 because they both specify different distributions over the same embedding. Framing equation 1 as an initialization and equation 2 as providing the conditional distribution p(w_j | w_i) may make the situation clearer. * Equation 3 is a mixture of Gaussian distributions, yet delta_j is a vector added to the current node embedding. Instead, first write \tilde{w}_i as a sample from the Gaussian and then let delta_j be the weighted sum of the samples. * Equation 3 is not consistent with equations 4-5. Equation 3 suggests that a node is updated by summing over neighbors and then applying the update. But Algorithm 1 suggests that nodes are updated based on only a single neighbor at a time. * What does it mean when the negative embedding is propagated with a small transition probability? This should be described mathematically. * It is misleading to call the graph a Gaussian hierarchy, since a hierarchy implies that certain nodes are higher than others. * How are the *transition probabilities* set for an unweighted graph? Specifically, the GrQc dataset doesn*t appear to have edge weights. * What values of the variance and \tau hyperparameters were used? * How are reconstructions and link predictions computed? Experimentally, the proposed approach is not compared to any baselines other than random embeddings. The claim made in the paper that the method compares favorably is thus not backed up by results. The results in section 3.2 should be described in greater detail. If the items are nodes, then how are edges and weights determined? Other specific comments: * What is the connection between the current work and hyperbolic geometry of Nickel & Kiela (2017)? The proposed algorithm does not rely on hyperbolic geometry so this seems like a non sequitur. * Algorithm 1: Rather than describing the algorithm in terms of the intended application (products), it would be useful to describe it in general terms and then use retail products as specific application. * Figures 2 and 3 are not particularly useful. The most important information for the reader or practitioner is how various methods compare on the same dataset, not how a single method performs across different datasets. Questions for the authors: * How is the proposed algorithm similar/different to related approaches for learning node embeddings? * What are baseline results for related algorithms on the datasets experimented upon? * What is the role of the variance scaling? How do the results change if the variance is reduced to 0 immediately after random initialization?"}
{"id": "iclr2020_534", "title": "Anomaly Detection by Deep Direct Density Ratio Estimation | OpenReview", "abstract": "Abstract:###Estimating the ratio of two probability densities without estimating each density separately has been shown to provide useful solutions to various machine learning tasks such as domain adaptation, anomaly detection, feature extraction, and conditional density estimation. However, density ratio estimation in the context of deep learning has not been extensively explored yet. In this paper, we apply a Bregman-divergence minimization method for density ratio estimation to deep neural networks and investigate its properties and practical performance in image anomaly detection. Our numerical experiments on the CIFAR-10, CIFAR-100 and Fashion-MNIST datasets demonstrate that deep direct density ratio estimation greatly improves the anomaly detection ability and reduces the computation time over state-of-the-art methods.", "review": "Review:###The paper studies inlier-based outlier detection via density-ratio estimation in a deep learning context. Specifically, they consider the use of KLIEP in conjunction with a deep neural network. One finding is that batch normalisation is harmful for performance of this method. Experiments show consistent gains over a self-supervised baseline. The paper is generally well-written, and provides a nice overview of the density-ratio approach to anomaly detection. Its aims are clearly spelled out, and the careful derivation in Sec 3 of the Bregman perspective of inlier-based anomaly detection serves as a clean introduction to the topic. At the same time, the technical contribution of the paper is modest. As best I could tell, Sec 3 is entirely devoted to a (nice) review of the existing framework of (Sugiyama et al., 2012b), and contains no new material. Sec 4 seems to be the only novel technical contribution of the paper, namely, the observation that batch normalisation affects the KLIEP method adversely. This is certainly an interesting finding, but it is accompanied by no further critical analysis. In particular, there is no deeper explanation for why batch normalisation might affect KLIEP in this way; whether this finding is consistent for different choices of architecture; whether this finding is consistent for different choices of method (e.g., uLSIF); and so on. By itself, I feel there merely observing that batch norm interacts badly with KLIEP is not sufficiently deep as to carry the paper. The experiments compare KLIEP against an unsupervised baseline based on geometric transforms (GTs). This comparison is somehow unfair, since (as I understand) GTs assume one operates in a different problem setting altogether (where there is only a single sample of inliers, and no background contrast sample). Given this, it is surprising that GTs are able to be competitive with the proposed method in several cases; I feel this deserves more comment. It also raises the question of whether one may combine the proposed method with GTs. More such experiments might have made the paper stronger, but in their current form I don*t find them too illuminating or surprising. Another confusion that arose was that the experiments, as best I can tell, focus entirely on KLIEP. This makes the motivation for Sec 3 unclear -- the section introduces an abstract Bregman divergence view of anomaly detection, but then settles on one specific case of this which can be derived rather directly without this (as was done in the original KLIEP paper, to my knowledge). Had there been some study of different instantiations of the power divergence empirically, e.g., that might better justify the discussion in Sec 3. (I would imagine that for example KLIEP is more susceptible to outliers or label noise, which could motivate using intermediate values of ?.) Overall, while I appreciated the paper as an overview of inlier-based anomaly detection, its increment over the current literature appears modest. Minor comments: - *and it is used as an outlier score* -> *and is used as an outlier score* - *neural network ARCHITECTURES for complex image datasets* - *Deep SVDD approach ARE the distance of a data point* - why do you use p^* in Sec 3 rather than just p? - remove italics from Fig 1 labels. - *let us consider using A CNN to ESTIMATE the density* - it seems fairer to use the GT authors* implementation of their own method? - *cleverly impose a non-negativity constraint* -> omit the word *cleverly*."}
{"id": "iclr2020_535", "title": "Neural Architecture Search in Embedding Space | OpenReview", "abstract": "Abstract:###The neural architecture search (NAS) algorithm with reinforcement learning can be a powerful and novel framework for the automatic discovering process of neural architectures. However, its application is restricted by noncontinuous and high-dimensional search spaces, which result in difficulty in optimization. To resolve these problems, we proposed NAS in embedding space (NASES), which is a novel framework. Unlike other NAS with reinforcement learning approaches that search over a discrete and high-dimensional architecture space, this approach enables reinforcement learning to search in an embedding space by using architecture encoders and decoders. The current experiment demonstrated that the performance of the final architecture network using the NASES procedure is comparable with that of other popular NAS approaches for the image classification task on CIFAR-10. The beneficial-performance and effectiveness of NASES was impressive even when only the architecture-embedding searching and pre-training controller were applied without other NAS tricks such as parameter sharing. Specifically, considerable reduction in searches was achieved by reducing the average number of searching to < 100 architectures to achieve a final architecture for the NASES procedure.", "review": "Review:###The paper proposes an interesting idea to perform Neural Architecture Search: first, an auto-encoder is pre-trained to encode/decode an neural architecture to/from a continuous low-dimensional embedding space; then the decoder is fixed but the encoder is copied as an agent controller for reinforcement learning. The controller is optimized by taking actions in the embedding space. The reward is also different from previous works which usually only considered validation accuracy but this work also considers the generalization gap. The idea is interesting, but there are some problems on both the method*s and the experimental sides: 1. NAO [1] also embeds neural architectures to a continuous space. Different from NAO which applies gradient descent in the embedded space, this paper uses RL. I double that RL can work better than gradient descent in a continuous space. The paper should compare with NAO. Ideally, this paper might work better than NAO if the accuracy predictor in the NAO is not accurate, while this paper uses real accuracy as a reward for search. However, this is not soundly compared. 2. It is unreasonable to discretize continuous actions to a Bernoulli distribution. Many RL methods, such as DDPG, can handle continuous actions; 3. The paper uses Eq. 1 as a reward. It*s interesting, but it*s unclear why the generalization error is needed. Ablation study is required. 4. As the community makes more progresses in AutoML, a better and better (smaller and smaller) search space is used. It doesn*t make much sense to compare the search time under different search spaces. Comparison under the same setting (e.g. NASBench-101) is required. Minors: 1. missing a number in *and T0 = epochs* 2. missing *x* in *32 32 RGB in 100 classes*, and *100* should be *10* [1] Luo, Renqian, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. *Neural architecture optimization.* In Advances in neural information processing systems, pp. 7816-7827. 2018."}
{"id": "iclr2020_536", "title": "Probabilistic modeling the hidden layers of deep neural networks | OpenReview", "abstract": "Abstract:###In this paper, we demonstrate that the parameters of Deep Neural Networks (DNNs) cannot satisfy the i.i.d. prior assumption and activations being i.i.d. is not valid for all the hidden layers of DNNs. Hence, the Gaussian Process cannot correctly explain all the hidden layers of DNNs. Alternatively, we introduce a novel probabilistic representation for the hidden layers of DNNs in two aspects: (i) a hidden layer formulates a Gibbs distribution, in which neurons define the energy function, and (ii) the connection between two adjacent layers can be modeled by a product of experts model. Based on the probabilistic representation, we demonstrate that the entire architecture of DNNs can be explained as a Bayesian hierarchical model. Moreover, the proposed probabilistic representation indicates that DNNs have explicit regularizations defined by the hidden layers serving as prior distributions. Based on the Bayesian explanation for the regularization of DNNs, we propose a novel regularization approach to improve the generalization performance of DNNs. Simulation results validate the proposed theories.", "review": "Review:###Summary of the Paper: The authors claim the that i.i.d hypothesis that is often used in the prior when looking for the equivalence between neural networks and GP is not valid. Then, they propose a new interpretation of neural networks as Gibbs distributions (in the case of fully connected layers) and a MRF in the case of convolutional layers. Some simulations are done to verify this. Detailed comments: Overall I believe that the writing of the paper is very sloppy and difficult to read and follow. It is not clear that the GP interpretation cannot be valid simply because the activations and weights are not i.i.d. I.I.D is a sufficient condition but not required in the central limit theorem. For example, the sum of correlated Gaussian variables also tends to a Gaussian distribution. The same for non-Gaussian variables. Therefore, I do not think that the GP interpretation of the NN is wrong simply for that. This simply shows that the i.i.d. prior may be suboptimal. Summing up, I think that this paper needs more work. Currently, I do not think I can extract anything useful from it. The prior is subjective and can be chosen by the user. So if an i.i.d. prior is actually chosen, the corresponding Bayesian neural network will converge to a GP. An i.i.d. prior may be sub-optimal. However, it can be used to interpret neural networks as GP. There is no problem with that. It is well known that the sum of random variables can also converge to a Gaussian distribution even though they are not independent. This questions the claims of the paper. The witting of the paper needs to be improved. There are several expressions that do not sound well. E.g., *, GP with i.i.d.* Z*y in Eq. (9) should depend on l. It is not clear what is the distribution of a hidden layer (activations weights etc..). *...and f Y is an estimation of the true distribution P (Y |X)* what do you mean by that? Eq. (8) seems to be a prob. distribution for the random variable Fy. However, the authors give an expression for f_yl, which does not make sense."}
{"id": "iclr2020_537", "title": "AutoSlim: Towards One-Shot Architecture Search for Channel Numbers | OpenReview", "abstract": "Abstract:###We study how to set the number of channels in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size). A simple and one-shot approach, named AutoSlim, is presented. Instead of training many network samples and searching with reinforcement learning, we train a single slimmable network to approximate the network accuracy of different channel configurations. We then iteratively evaluate the trained slimmable model and greedily slim the layer with minimal accuracy drop. By this single pass, we can obtain the optimized channel configurations under different resource constraints. We present experiments with MobileNet v1, MobileNet v2, ResNet-50 and RL-searched MNasNet on ImageNet classification. We show significant improvements over their default channel configurations. We also achieve better accuracy than recent channel pruning methods and neural architecture search methods with 100X lower search cost. Notably, by setting optimized channel numbers, our AutoSlim-MobileNet-v2 at 305M FLOPs achieves 74.2% top-1 accuracy, 2.4% better than default MobileNet-v2 (301M FLOPs), and even 0.2% better than RL-searched MNasNet (317M FLOPs). Our AutoSlim-ResNet-50 at 570M FLOPs, without depthwise convolutions, achieves 1.3% better accuracy than MobileNet-v1 (569M FLOPs).", "review": "Review:###This paper proposes a simple and one-shot approach on neural architecture search for the number of channels to achieve better accuracy. Rather than training a lot of network samples, the proposed method trains a single slimmable network to approximate the network accuracy of different channel configurations. The experimental results show that the proposed method achieves better performance than the existing baseline methods. - It would be better to provide the search cost of the proposed method and the other baseline methods because that is the important metric for neural architecture search methods. As this paper points out that NAS methods are computationally expensive, it would be better to make the efficiency of the proposed method clear. - According to this paper, the notable difference between the proposed method and the existing pruning methods is that the pruning methods are grounded on the importance of trained weights, but the proposed method focuses more on the importance of channel numbers. It is unclear to me why such a difference is caused by the proposed method, that is, which part of the proposed method causes the difference? And how does the difference affect the final performance?"}
{"id": "iclr2020_538", "title": "Hierarchical Image-to-image Translation with Nested Distributions Modeling | OpenReview", "abstract": "Abstract:###Unpaired image-to-image translation among category domains has achieved remarkable success in past decades. Recent studies mainly focus on two challenges. For one thing, such translation is inherently multimodal due to variations of domain-specific information (e.g., the domain of house cat has multiple fine-grained subcategories). For another, existing multimodal approaches have limitations in handling more than two domains, i.e. they have to independently build one model for every pair of domains. To address these problems, we propose the Hierarchical Image-to-image Translation (HIT) method which jointly formulates the multimodal and multi-domain problem in a semantic hierarchy structure, and can further control the uncertainty of multimodal. Specifically, we regard the domain-specific variations as the result of the multi-granularity property of domains, and one can control the granularity of the multimodal translation by dividing a domain with large variations into multiple subdomains which capture local and fine-grained variations. With the assumption of Gaussian prior, variations of domains are modeled in a common space such that translations can further be done among multiple domains within one model. To learn such complicated space, we propose to leverage the inclusion relation among domains to constrain distributions of parent and children to be nested. Experiments on several datasets validate the promising results and competitive performance against state-of-the-arts.", "review": "Review:###This paper proposes a way to utilize the domain hierarchical structure information to help the general multimodal I2I (called joint multi-domain and multi-modal in the paper). The proposed method applies the hierarchy-regularized learning to learn the nested distributions which are reasonable for the data with a well-defined hierarchical structure. Pros: 1. The proposed method shows the ability to learn the nested distributions with the help of hierarchical structure information. It is a reasonable way to model the general multimodal I2I. I think the authors work in the correct direction. 2. To model the partial order relation in the hierarchy, the authors borrow the thresholded divergence technique from the natural language processing field (Athiwaratkun & Wilson (2018)) and use the KL divergence considering its simple formulation for Gaussians. It sounds reasonable and may benefit other related CV tasks. Cons: 1. The detailed method description is very confusing to me. In section 3.1, it is mentioned that the proposed “method only contains one pair of encoder and decoder for multi-domain X^l_i”, which sounds like there are many pairs of encoder and decoder. While in the following description “using G to output the target image x^l_(i?j) = G(c_i , s^l_j )”, which sounds like use one shared generator. I wonder if it is the case that there is one shared encoder, one shared decoder, but there are different branches for different sub-domains in the domain distributions modeling module? 2. It is mentioned that the penalty between a negative pair should be greater than a margin m. While this important parameter as mentioned in A.2 is not described in the main paper. 3. It is mentioned that “In the extreme case, every instance is a variation mode of the domain-specific information.” It is the case in the ICLR19 paper “Exemplar guided unsupervised image-to-image translation with semantic consistency” which uses one exemplar to achieve the general multi-modal I2I. I wonder what are the advantages and disadvantages of learning the distribution and using the exemplar directly. Besides, if there is no well-defined hierarchical structure, is there any way to apply the proposed method? 4. As to the evaluation, although the authors provide quantitative results by different metrics, a user study evaluation would be good. 5. As to the visualization results, there are still many noticeable artifacts in the results. Maybe the nested distributions are too complex to learn in this framework. My initial rating is on the boardline."}
{"id": "iclr2020_539", "title": "HOW THE CHOICE OF ACTIVATION AFFECTS TRAINING OF OVERPARAMETRIZED NEURAL NETS | OpenReview", "abstract": "Abstract:###It is well-known that overparametrized neural networks trained using gradient based methods quickly achieve small training error with appropriate hyperparameter settings. Recent papers have proved this statement theoretically for highly overparametrized networks under reasonable assumptions. These results either assume that the activation function is ReLU or they depend on the minimum eigenvalue of a certain Gram matrix. In the latter case, existing works only prove that this minimum eigenvalue is non-zero and do not provide quantitative bounds which require that this eigenvalue be large. Empirically, a number of alternative activation functions have been proposed which tend to perform better than ReLU at least in some settings but no clear understanding has emerged. This state of affairs underscores the importance of theoretically understanding the impact of activation functions on training. In the present paper, we provide theoretical results about the effect of activation function on the training of highly overparametrized 2-layer neural networks. A crucial property that governs the performance of an activation is whether or not it is smooth: • For non-smooth activations such as ReLU, SELU, ELU, which are not smooth because there is a point where either the ?rst order or second order derivative is discontinuous, all eigenvalues of the associated Gram matrix are large under minimal assumptions on the data. • For smooth activations such as tanh, swish, polynomial, which have derivatives of all orders at all points, the situation is more complex: if the subspace spanned by the data has small dimension then the minimum eigenvalue of the Gram matrix can be small leading to slow training. But if the dimension is large and the data satis?es another mild condition, then the eigenvalues are large. If we allow deep networks, then the small data dimension is not a limitation provided that the depth is suf?cient. We discuss a number of extensions and applications of these results.", "review": "Review:###Summary: The authors of the paper examine how different activation functions affect training of overparametrized neural networks. They do their analysis in a general way such that it includes most activation functions such as ReLU, swish, tanh, polynomial, etc. The main point of their analysis is that they examine a matrix called the G-matrix which is described in equation (2), and this (positive semi-definite) G-matrix can determine the rate of convergence to zero of the training error. Namely, the minimum eigenvalue of the G-matrix is inversely proportional to the time required to reach a desired amount of error (Theorem 3.1 (Theorem 4.1 from Du et al. (2019a)). The main results separate into two cases: (1) activation functions with a kink (i.e. if the activation function is NOT in C^{r+1}, the space of r+1 continuously differentiable functions, for some finite r) and (2) smooth activation functions. In the first case, the authors show that the minimum eigenvalue of the G-matrix is large, i.e. bounded away from zero after a few assumptions. In the second case, the authors show that polynomial activations have many zero eigenvalues, and sufficiently smooth activations such as tanh or swish have many small eigenvalues, if the dimension of the span of data is sufficiently small. The author’s initial problem setup works on a one-hidden-layer neural network where only the input layer is trained, but provide some extensions in the appendix. The authors also provide some empirical experiments: one synthetic data, and on CIFAR10. The synthetic data experiments agreed with theory, but the experiment on CIFAR10 did have some gap between theory and experiment, although the CIFAR10 with ReLU experiment agreed with theory. Stengths: I appreciate the author’s effort in providing needed theoretical analysis on how activation affects training error for deep neural networks. The authors also provide an extensive appendix that provide seemingly full proofs of the theorems (although this reviewer did not go into detail for most of the appendix). The authors also provide experiments that confirm the theory and also provide examples highlighting the gap (which this reviewer sees as a strength). Weaknesses: A clear weakness of this paper is that the appendix is too long. The authors do provide a proof sketch of the main results and refer to the appendix, but I would have liked to have seen a more focused paper. Having extensions of the main results is nice, but it’s sometimes unclear what is being extended. Perhaps a list of extension would make clear what’s in the appendix. Other comments: (i) I’d like to an explanation to why it’s called the DZXP setting. (ii) On page 4, when explaining the M matrix, I think it should be grad_W F (instead of L) (iii) On page 4, I think there is more to assumption 1, after looking at Allen-Zhu et al. (2019) (https://arxiv.org/pdf/1811.03962.pdf page 4, footnote 5) (iv) the wording from page 4, “the matrix G^(t) does not evolve from its initial value G^(0)” is a bit awkward. Do you mean G^(t) does not change much from G^(0), and as to goes to infinity then G^(t) goes to G^(0)?"}
{"id": "iclr2020_540", "title": "Towards Stable and Efficient Training of Verifiably Robust Neural Networks | OpenReview", "abstract": "Abstract:###Training neural networks with verifiable robustness guarantees is challenging. Several existing approaches utilize linear relaxation based neural network output bounds under perturbation, but they can slow down training by a factor of hundreds depending on the underlying network architectures. Meanwhile, interval bound propagation (IBP) based training is efficient and significantly outperforms linear relaxation based methods on many tasks, yet it may suffer from stability issues since the bounds are much looser especially at the beginning of training. In this paper, we propose a new certified adversarial training method, CROWN-IBP, by combining the fast IBP bounds in a forward bounding pass and a tight linear relaxation based bound, CROWN, in a backward bounding pass. CROWN-IBP is computationally efficient and consistently outperforms IBP baselines on training verifiably robust neural networks. We conduct large scale experiments on MNIST and CIFAR datasets, and outperform all previous linear relaxation and bound propagation based certified defenses in L_inf robustness. Notably, we achieve 7.02% verified test error on MNIST at epsilon=0.3, and 66.94% on CIFAR-10 with epsilon=8/255.", "review": "Review:###This paper proposes a new variation on certified adversarial training method that builds on two prior works IBP and CROWN. They showed the method outperformed all previous linear relaxation and bound propagation based certified defenses. Pros: 1. The empirical results are strong. The method achieved SOTA. Cons: 1. Novelty seems small. It is a straightforward combination of prior works, by adding two bounds together. 2. Adds a new hyperparameter for tuning. 3. Lack of any theoretical insights/motivation for the proposed method. Why would we want to combine the two lower bounds? The reason given in the paper is not very convincing: *IBP has better learning power at larger epsilon and can achieve much smaller verified error. However, it can be hard to tune due to its very imprecise bound at the beginning of training; on the other hand, linear relaxation based methods give tighter lower bounds which stabilize training, but it over-regularizes the network and forbids us to achieve good accuracy.* My questions with regards to this: (i) Why does loose bound result in unstable training? Tighter bound stabilize training? (ii) If we*re concerned with using a tighter bound could result in over-regularization, then why not just combine the natural loss with the tight bound, as natural loss can be seen as the loosest bound. Is IBP crucial? and why?"}
{"id": "iclr2020_541", "title": "Multi-objective Neural Architecture Search via Predictive Network Performance Optimization | OpenReview", "abstract": "Abstract:###Neural Architecture Search (NAS) has shown great potentials in finding a better neural network design than human design. Sample-based NAS is the most fundamental method aiming at exploring the search space and evaluating the most promising architecture. However, few works have focused on improving the sampling efficiency for a multi-objective NAS. Inspired by the nature of the graph structure of a neural network, we propose BOGCN-NAS, a NAS algorithm using Bayesian Optimization with Graph Convolutional Network (GCN) predictor. Specifically, we apply GCN as a surrogate model to adaptively discover and incorporate nodes structure to approximate the performance of the architecture. For NAS-oriented tasks, we also design a weighted loss focusing on architectures with high performance. Our method further considers an efficient multi-objective search which can be flexibly injected into any sample-based NAS pipelines to efficiently find the best speed/accuracy trade-off. Extensive experiments are conducted to verify the effectiveness of our method over many competing methods, e.g. 128.4x more efficient than Random Search and 7.8x more efficient than previous SOTA LaNAS for finding the best architecture on the largest NAS dataset NasBench-101.", "review": "Review:###--Summary-- The authors present an algorithm BOGCN-NAS which combines bayesian optimization and GCNs for searching over NN architectures. The authors emphasize that this method can be used for multi-objective optimization and run experiments over NAS-Bench, LSTM-12K and ResNet models. --Method-- Methodologically, the contribution is somewhat weak. The main technical contribution is to use a GCN to get a global representation of a graph, which can then be used for downstream regression tasks such as predicting accuracy. It’s not clear how much the GCN generalizes in being able to encode arbitrarily large architectures. The two main examples offered are NAS-Bench and LSTM-12K focus on optimizing cell architectures which contain a handful of nodes e.g. (5 in the case of NAS-101). Graph embeddings: The authors have not considered other graph embeddings to use in their Bayesian regression setup. E.g. see https://arxiv.org/pdf/1903.11835.pdf for a list. Bayes-opt: In Algo1, step 5. The authors randomly sample a number of architectures in order to calculate EI scores on them. For large discrete combinatorial search spaces, this approach will not scale. Multi-objective optimization: It’s not clear why GCNs or BO is required for this. Any predictor that generates multiple metrics could substitute in order to create a pareto-curve. Even multi-objective RL based approaches could suffice. Thus multi-objective opt only seems like a minor/tangential contribution. --Experiments-- The main claim of the paper is that this approach works well for the multi-objective case. However, the results only look at two objectives #params vs accuracy. There’s a pretty strong correlation between the two. It’s unclear how the method generalizes when objectives are not correlated. The authors need to thoroughly demonstrate other objectives/find suitable benchmarks for the same as clearly NAS-101 will not suffice. Other concerns: - Table 1 has correlations using 1000 training architectures. Why 1000? Why not 50 (that’s how sec 4.2 is initialized). Also, the correlation results are less impressive in Figure 9. - Table 1 lists the number of params that the predictor uses. Why is this important? How about comparing with a linear regressor? - The results in Sec 4.3 are using random as the only baseline. This is a pretty weak baseline. - In sec 4.4, the authors pick models M1, M2 and M3 as candidate examples.. How were these chosen ? - Sec 4.5 transfer learning results are pretty weak. Transfer across datasets is much more interesting e.g. between ImageNet and Cifar-10. Overall, this paper has some interesting results, which show that GCNs can be useful models to encode graph structured inputs. However, the methodological and experimental results can definitely be strengthened. The authors may consider the following: Address how GCNs can model and scale to general architecture spaces than a small number of nodes in a cell. Address how to sample better over combinatorial search spaces than random in the inner loop of BO. Strengthen MO-opt results. Use better baselines than random and different objectives than accuracy vs #params."}
{"id": "iclr2020_542", "title": "Fairness with Wasserstein Adversarial Networks | OpenReview", "abstract": "Abstract:###Quantifying, enforcing and implementing fairness emerged as a major topic in machine learning. We investigate these questions in the context of deep learning. Our main algorithmic and theoretical tool is the computational estimation of similarities between probability, ```a la Wasserstein**, using adversarial networks. This idea is flexible enough to investigate different fairness constrained learning tasks, which we model by specifying properties of the underlying data generative process. The first setting considers bias in the generative model which should be filtered out. The second model is related to the presence of nuisance variables in the observations producing an unwanted bias for the learning task. For both models, we devise a learning algorithm based on approximation of Wasserstein distances using adversarial networks. We provide formal arguments describing the fairness enforcing properties of these algorithm in relation with the underlying fairness generative processes. Finally we perform experiments, both on synthetic and real world data, to demonstrate empirically the superiority of our approach compared to state of the art fairness algorithms as well as concurrent GAN type adversarial architectures based on Jensen divergence.", "review": "Review:###In this paper, the authors proposed a fairness-aware learning method. In particular, the authors considered two kinds of fairness problem and designed two regularizers accordingly. Essentially, both of these two strategies learn classifiers and calibrate the distributions conditioned on protected variables jointly. The calibration of the distribution is achieved in the framework of optimal transport. This work is a natural extension of the optimal transport-based method shown in (Barrio et al, 2019a,b). The main differences include 1) instead of calibrating distributions after learning classifiers, the proposed method achieves calibration and learning jointly, replacing the primal Wasserstein barycenter problem with the dual form of Wasserstein distance (Arjovsky et al. 2017); 2) the proposed method considers two types of fairness problem. Compared with vanilla GAN, the potential advantage of WGAN on distribution matching is well-known. It seems unfair that the authors compared the vanilla GAN-based regularizer with the proposed WGAN-based regularizer just on EMD because EMD corresponds to the proposed regularizer directly. In Table 1, although the DI of vanilla GAN is higher than that of WGAN, its ACC is also higher than that of WGAN as well. In Figure 5 (a, b), if we set lambda=0.6 for WGAN and lambda=1 for vanilla GAN, both of them can achieve ~0.838 ACC and ~0.100 DI. In Figure 5(c), what do the points represent? Why not use DI as the x-axis? Because of the issues in experiments, it is hard to evaluate the improvements of the proposed method. Additionally, the proposed method always causes the degradation of ACC when improving DI. However, the method in (Barrio et al, 2019a) just applies a Wasserstein barycenter-based post-processing but can suppress the degradation on ACC greatly. Could the authors discuss the differences and the advantages of the proposed method in detail? Could the authors consider more recent work as their baselines? In summary, the method makes sense, but its novelty is limited and the improvements are incremental. Minors: Page 6, Line 3: Figure 3 —> Figure 2. I suggest swapping Figure 2 and Figure 3."}
{"id": "iclr2020_543", "title": "Evolutionary Reinforcement Learning for Sample-Efficient Multiagent Coordination | OpenReview", "abstract": "Abstract:###Many cooperative multiagent reinforcement learning environments provide agents with a sparse team-based reward as well as a dense agent-specific reward that incentivizes learning basic skills. Training policies solely on the team-based reward is often difficult due to its sparsity. Also, relying solely on the agent-specific reward is sub-optimal because it usually does not capture the team coordination objective. A common approach is to use reward shaping to construct a proxy reward by combining the individual rewards. However, this requires manual tuning for each environment. We introduce Multiagent Evolutionary Reinforcement Learning (MERL), a split-level training platform that handles the two objectives separately through two optimization processes. An evolutionary algorithm maximizes the sparse team-based objective through neuroevolution on a population of teams. Concurrently, a gradient-based optimizer trains policies to only maximize the dense agent-specific rewards. The gradient-based policies are periodically added to the evolutionary population as a way of information transfer between the two optimization processes. This enables the evolutionary algorithm to use skills learned via the agent-specific rewards toward optimizing the global objective. Results demonstrate that MERL significantly outperforms state-of-the-art methods such as MADDPG on a number of difficult coordination benchmarks.", "review": " This paper proposes an integration of neuroevolution and gradient-based learning for reinforcement learning applications. The evolutionary algorithm focuses on sparse reward and multiagent/team optimization, while the gradient-based learning is used to inject selectively improved genotypes in the population. This work addresses a very hot topic, i.e. the integration of NE and DRL, and the proposed method offers the positive side of both without introducing major downsides. The presented results come from a relatively simple but useful multiagent benchmark which has broad adoption. The paper is well written, presents several contributions that can be extended and ported to other work, and the results are statistically significant. There is one notable piece missing which forces me to bridle my enthusiasm: a discussion of the genotype and of its interpretation into the network phenotype. The form taken by the actual agent is not explicitly stated; following the adoption of TD3 I would expect a policy and two critics, for a grand total of three neural networks, but this remains unverified. And if each agent is composed of three neural networks, and each individual represents a team, does this mean that each genotype is a concatenation of three (flattened) weight matrices per each agent in the team? What is the actual genotype size? It sounds huge, I would expect to be at least several hundred weights; but then this would clash with the proposed minuscule population size of 10 (recent deep neuroevolution work from Uber uses populations THREE orders of magnitude larger). Has the population size been proportionated to the genotype dimensionality? Would it be possible to reference the widely adopted defaults of industry standard CMA-ES? Speaking of algorithms, where is the chosen EA implementation discussed? The overview seems to describe a textbook genetic algorithm, but that has been overtaken as state-of-the-art since decades, constituting a poor match for TD3. Omitting such a chapter severely limits not only the reproducibility of the work but its full understanding. For example, does the EA have sufficient population size to contribute significantly to the process, or is it just performing as a fancy version of Random Weight Guessing? Could you actually quickly run RWG with direct policy search (rather than random action selection) to establish the effective complexity of the task? My final rating after rebuttal will vary wildly depending on the ability to cover such an important piece of information. A few minor points, because I think that the paper appearance deserves to match the quality of the content: - The images are consistently too small and hard to read. I understand the need to fit in the page limit by the deadline, but for the camera ready version it will be necessary to trim the text and rescale all images. - The text is well written but often slowing down the pace for no added value, such as by dedicating a whole page to discussing a series of previously published environments. - The hyperparameters of the evolutionary algorithm look completely unoptimized. I would expect a definite improvement in performance with minimal tuning. - The *standard neuroevolutionary algorithm* from 2006 presented as baseline has not been state-of-the-art for over a decade. I would understand its usage as a baseline if that is indeed the underlying evolutionary setup, but otherwise I see no use for such a baseline. ----------------------------------------------------------------------------------------------- # Update following the rebuttal phase ----------------------------------------------------------------------------------------------- Thank you for your work and for the extended experimentation. I am confident the quality of the work is overall increased. The core research question behind my original doubt however remains unaddressed: does the EC part of the algorithm sensibly support the gradient-descent part, or is the algorithm basically behaving as a (noisy) multi-agent TD3? Such a contribution by itself would be undoubtedly important. Submitting it as a principled unification of EC and DL however would be more than a simple misnomer: it could mislead further research in what is an extremely promising area. The scientific approach to clarify this point would be to design an experiment showcasing the performance of MARL using a range of sensible population sizes. To understand what *sensible* means in this context, I refer to a classic: http://www.cmap.polytechnique.fr/~nikolaus.hansen/cec2005ipopcmaes.pdf A lower bound for the population size with simple / unimodal fitness functions would be . With such a complex, multimodal fitness though, no contribution from the EA can be expected (based on common practice in the EC field) without at least doubling or tripling that number. The upper bound does not need to be as high as with the recent Uber AI work (10k), but certainly showing the performance with a population of a few hundreds would be the minimum necessary to support your claim. A population size of 10 represents a proper lower bound for a genotype of up to 10 parameters; it is by no means within a reasonable range with your dimensionality of 10*000 parameters, and no researcher with experience in EC would expect anything but noise from such results -- with non-decreasing performance uniquely due to elitism. The new runs in Appendice C only vary the population size for the ES algorithm, proposed as a baseline. No performance of MARL using a sensible population size is presented. The fundamental claim is thereby unsustainable by current results. The idea is extremely intriguing and very promising, easily leading to supportive enthusiasm; it is my personal belief however that accepting this work in such a premature stage (and with an incorrect claim) could stunt further research in this direction. [By the way, the reference Python CMA-ES implementation runs with tens of thousands of parameters and a population size of 60 in a few seconds per generation on a recent laptop: the claim of performance limitations as an excuse for not investigating a core claim suggests that more work would be better invested prior to acceptance.]"}
{"id": "iclr2020_544", "title": "On the Linguistic Capacity of Real-time Counter Automata | OpenReview", "abstract": "Abstract:###While counter machines have received little attention in theoretical computer science since the 1960s, they have recently achieved a newfound relevance to the field of natural language processing (NLP). Recent work has suggested that some strong-performing recurrent neural networks utilize their memory as counters. Thus, one potential way to understand the sucess of these networks is to revisit the theory of counter computation. Therefore, we choose to study the abilities of real-time counter machines as formal grammars. We first show that several variants of the counter machine converge to express the same class of formal languages. We also prove that counter languages are closed under complement, union, intersection, and many other common set operations. Next, we show that counter machines cannot evaluate boolean expressions, even though they can weakly validate their syntax. This has implications for the interpretability and evaluation of neural network systems: successfully matching syntactic patterns does not guarantee that a counter-like model accurately represents underlying semantic structures. Finally, we consider the question of whether counter languages are semilinear. This work makes general contributions to the theory of formal languages that are of particular interest for the interpretability of recurrent neural networks.", "review": "Review:###Motivated by a link between LSTMs and counter machines (suggested by recent work, e.g. Merrill, 2019 et al.), this paper studies the formal properties of counter machines (and LSTMs by extension) as grammars, in hopes of discovering why LSTMs perform particularly well in language tasks despite having no obvious hierarchical structure. It makes the following contributions. It shows that: (1) many variants of counter machine converge to the same formal language, (2) the counter languages are closed under common set operations (e.g. intersection, union, and complement), (3) counter machines are incapable of evaluating boolean expressions, and (4) only a weak subclass of CLs are sublinear (and most are not). While this paper gives thorough proofs, I would have liked to see more connection to practical NLP with some experiments. Also, I would have liked to see more concrete takeaways from this paper: if correctly detecting surface patterns doesn*t mean that LSTMs build correct semantic representations, what can ensure that LSTMS do have a correct semantic representation? As this paper is far from my area of expertise, I*m willing to change my score based on my co-reviewers."}
{"id": "iclr2020_545", "title": "Robust And Interpretable Blind Image Denoising Via Bias-Free Convolutional Neural Networks | OpenReview", "abstract": "Abstract:###We study the generalization properties of deep convolutional neural networks for image denoising in the presence of varying noise levels. We provide extensive empirical evidence that current state-of-the-art architectures systematically overfit to the noise levels in the training set, performing very poorly at new noise levels. We show that strong generalization can be achieved through a simple architectural modification: removing all additive constants. The resulting *bias-free* networks attain state-of-the-art performance over a broad range of noise levels, even when trained over a limited range. They are also locally linear, which enables direct analysis with linear-algebraic tools. We show that the denoising map can be visualized locally as a filter that adapts to both image structure and noise level. In addition, our analysis reveals that deep networks implicitly perform a projection onto an adaptively-selected low-dimensional subspace, with dimensionality inversely proportional to noise level, that captures features of natural images.", "review": "Review:###This paper looks at how deep convolutional neural networks for image denoising can generalize across various noise levels. First, they argue that state-of-the-art denoising networks perform poorly outside of the training noise range. The authors empirically show that as denoising performance degrades on unseen noise levels, the network residual for a specific input is being increasingly dominated by the network bias (as opposed to the purely linear Jacobian term). Therefore, they propose using bias-free convolutional neural networks for better generalization performance in image denoising. Their experimental results show that bias-free denoisers significantly outperform their original counter-parts on unseen noise levels across various popular architectures. Then, they perform a local analysis of the bias-free network around an input image that is now a strictly linear function of the input. They empirically demonstrate that the Jacobian is approximately low-rank and symmetric, therefore the effect of the denoiser can be interpreted as a nonlinear adaptive filter that projects the noisy image onto a low-dimensional signal subspace. The authors show that most of the energy of the clean image falls into the signal subspace and the effective dimensionality of this subspace is inversely proportional to the noise level. Even though it is theoretically not too well-motivated in the paper why the bias term degrades generalization performance, the experimental results seem to clearly demonstrate the merit of bias-free denoisers. Moreover, the analysis of the network Jacobian and its interpretation as a nonlinear adaptive filter provides some interesting insight in the local properties of bias-free denoisers. Therefore, I would recommend accepting this paper, if the authors provide a theoretical discussion on why the bias term might degrade generalization performance. Some smaller comments: -It is not clear if d should be multiplied by sigma^2 in its definition on page 7. The definition mentions dependence on noise variance, but the formula does not have it. -In Section 3 in the expression of the mean squared error it is not defined what g(y) means. -Axis labels are missing on Fig. 3."}
{"id": "iclr2020_546", "title": "Keyframing the Future: Discovering Temporal Hierarchy with Keyframe-Inpainter Prediction | OpenReview", "abstract": "Abstract:###To flexibly and efficiently reason about temporal sequences, abstract representations that compactly represent the important information in the sequence are needed. One way of constructing such representations is by focusing on the important events in a sequence. In this paper, we propose a model that learns both to discover such key events (or keyframes) as well as to represent the sequence in terms of them. We do so using a hierarchical Keyframe-Inpainter (KeyIn) model that first generates keyframes and their temporal placement and then inpaints the sequences between keyframes. We propose a fully differentiable formulation for efficiently learning the keyframe placement. We show that KeyIn finds informative keyframes in several datasets with diverse dynamics. When evaluated on a planning task, KeyIn outperforms other recent proposals for learning hierarchical representations.", "review": "Review:###The paper introduces a model trained for video prediction hierarchically: a series of significant frames called “keyframes” in the paper are first predicted and then intermediate frames between keyframes couples are generated. The training criterion is maximum likelihood with a variational approximation. Experiments are performed on 3 different video datasets and the evaluation is performed for 3 tasks: keyframe detection, frame prediction and planning in robot videos. The idea of generating an abstraction or a summary of a video via a sequence of important frames is attractive and could probably be used in different contexts. The proposed model is new and the authors introduce some clever ideas in order to train it. The evaluation work is important and the authors propose different settings for this evaluation. The paper also present weaknesses. First the motivation for keyframes generation should be better developed: the model does not perform better than baselines for video frames prediction so that keyframes generation should be motivated by other applications. Planning as proposed by the authors could be one, but in this case it should be more developed. The main weakness is however the technical presentation which is painful to follow. When it is possible to get a general picture of what is done, it is quite difficult to figure out exactly how the model works. A global rewriting and maybe a better focus are required for publication. The probabilistic model (section 3.1) is relatively clear, even if it could be improved. It seems that the generation of a keyframe and the prediction of the corresponding time (tau^n) are independent (eq. 3). This could be commented. Also it seems that in eq. 3 the log(K|z..) term should be inside an expectation. Section 4 was difficult to decipher for me. My understanding is that instead of sampling from a multinomial during training, you bypass this non differentiable operation by using what you call “soft targets” thus obtaining a differentiable objective (eq. 4). Is that true? In any case, the procedure should be made a lot clearer. The “intermediate frame” passage also remained confuse for me. Considering the experiments, the authors make an important effort in order to evaluate different aspects of their model. In a fisrt step, they evaluate the ability of the model to generate significant keyframes using a detection setting. It is not clear how they define ground truth frames for this evaluation. Those ground truth frames are defined as the frames where the movement in the image changes, which is easy on the Brownian movement dataset but what about the others? Also the baselines used in this comparison are weak. In the paper of Denton, they suggest some way to detect surprise and apparently this is not what you used. This should be justified/ commented. For keyframe modeling the proposed model behaves similarly to the baselines and even performs worse than the simpler “jumpy” model. Concerni g the paragraph about the selection of the number of predicted keyframes, it is not clear what is the reference (ground truth) number of target keyframes. The planning experiments are interesting, but difficult to follow at least from the main text. Overall, I think that there are several interesting ideas and realizations. They should be better put in perspective and explained. ----- post rebuttal ----------- Thanks for the detailed answer. The paper is largely improved both for the style and the comparisons. But still requires further improvements. I will keep my score."}
{"id": "iclr2020_547", "title": "Sparse Transformer: Concentrated Attention Through Explicit Selection | OpenReview", "abstract": "Abstract:###Self-attention-based Transformer has demonstrated the state-of-the-art performances in a number of natural language processing tasks. Self attention is able to model long-term dependencies, but it may suffer from the extraction of irrelevant information in the context. To tackle the problem, we propose a novel model called Sparse Transformer. Sparse Transformer is able to improve the concentration of attention on the global context through an explicit selection of the most relevant segments. Extensive experimental results on a series of natural language processing tasks, including neural machine translation, image captioning, and language modeling, all demonstrate the advantages of Sparse Transformer in model performance. Sparse Transformer reaches the state-of-the-art performances in the IWSLT 2015 English-to-Vietnamese translation and IWSLT 2014 German-to-English translation. In addition, we conduct qualitative analysis to account for Sparse Transformer*s superior performance.", "review": " CONTRIBUTIONS: C1. Sparse Transformer: A modification of the Transformer, limiting attention to the top-k locations. (That is a complete statement of the proposed model.) C2. Experiments showing that, quantitatively, the Sparse Transformer out-performs the standard Transformer on translation, language modeling, and image captioning. C3. Experiments showing that, qualitatively, in translation, when generating a target word, the Sparse Transformer better focusses attention on the aligned source word RATING: Reject REASONS FOR RATING (SUMMARY). The innovativeness seems low given the several previous proposals for sparse attention, the results are not dramatic enough to compensate for the lack of originality, and the comparison to other models is wanting. REVIEW Strengths: The paper is clearly written. The question of whether the Transformer’s attention is too diffuse is of interest. The proposal is admirably simple. The quantitative metrics include comparison against many alternative models. Weaknesses: A primary area of deficiency concerns the relation of the proposed model to other proposals for sparse attention: the authors cite 5 of them (and 2 more are cited in the comment by Cui). The paper should clearly identify the differences between the proposed model and earlier models: it does not discuss this at all. The deficiencies in these previous models should be clearly stated and demonstrated: they are only described as “either restricted range of attention or training difficulty” (Sec 6). A rationale for why the proposal can be expected to remedy these deficiencies should be stated clearly: it is not stated at all. Experimental demonstration that the proposed innovation actually remedies the identified deficiencies should be provided, but is not. A proposal to use a top-k filter immediately raises the question of the value of k. This is not discussed at all. In particular, no empirical results are given concerning the sensitivity of the reported successes to choosing the correct value for k. We are only told that “k is usually a small number such as 5 or 10” (Sec 3). The experimental details in the appendix do not even state the value of k used in the models reported. It is an interesting discovery that in the translation task, attention at the top layer of the standard Transformer is strongly focused on the end of the input. This is described as an “obvious problem” (Sec 7). But it can’t obviously be a problem because the performance of the standard Transformer is only very slightly lower than that of the Sparse Transformer: if anything is obvious, it is that processing in the standard Transformer packs a lot of information into its final encoding of the end of the input string, which functions rather like an encoding of the entire sentence. Presumably, the experimental results reported are those from a single model, since we are not told otherwise. There should be multiple tests of the models with different random initializations, with the means and variances of measures reported. It is possible, however, that limitations of computational resources made that infeasible, although the Appendix seems to indicate that no hyperparameter tuning was done, which greatly reduces computational cost. COMMENTS FOR IMPROVEMENT, NOT RELEVANT TO RATING DECISION Although the tiny sample of visualized attention weights provided is useful, a large-scale quantitative assessment of a main claim concerning translation might well be possible: that attention is in fact concentrated on the aligned word might be testable using an aligned bilingual corpus or perhaps an existing forced aligner could be used. Much space could be saved: it is not necessary to review the standard Transformer, and the modification proposed is so simple that it can be precisely stated in one sentence (see C1 above): the entire page taken up by Sec. 3 is unnecessary, as it adds only implementation details. Errors that took more than a moment to mentally correct, all on p. 12: The definition of the BPC should be E[log P(x(t+1) | h(t))]: all parentheses are missing “regrad” should be “regard” “derivative” should be “differentiable” in the final sentence"}
{"id": "iclr2020_548", "title": "Bridging ELBO objective and MMD | OpenReview", "abstract": "Abstract:###One of the challenges in training generative models such as the variational auto encoder (VAE) is avoiding posterior collapse. When the generator has too much capacity, it is prone to ignoring latent code. This problem is exacerbated when the dataset is small, and the latent dimension is high. The root of the problem is the ELBO objective, specifically the Kullback–Leibler (KL) divergence term in objective function. This paper proposes a new objective function to replace the KL term with one that emulates the maximum mean discrepancy (MMD) objective. It also introduces a new technique, named latent clipping, that is used to control distance between samples in latent space. A probabilistic autoencoder model, named -VAE, is designed and trained on MNIST and MNIST Fashion datasets, using the new objective function and is shown to outperform models trained with ELBO and -VAE objective. The -VAE is less prone to posterior collapse, and can generate reconstructions and new samples in good quality. Latent representations learned by -VAE are shown to be good and can be used for downstream tasks such as classification.", "review": "Review:###Summary: The authors propose changing the objective function used to train VAEs (the ELBO) to an alternative objective which utilizes an L1-norm penalty on the aggregated variational posterior means. The authors motivate this modification through the maximum mean discrepancy but I was unable to follow this argument --- adding additional details to the paper for this argument would greatly help. The empirical evaluation shows that the proposed technique is able to improve utility on downstream classification tasks but ultimately has significant issues with the experimental setup. Overall: 1) One general issue I found with this work is the claim that the KL divergence is the overarching cause of posterior collapse. This remains a popular story within the community but existing work has provided evidence that the actual cause is not so simple. For example, [1] showed that with a sufficiently powerful decoder posterior collapse may occur even when training with marginal log-likelihood. For the authors reference, [2] is a concurrent ICLR submission which breaks down various categories and causes of posterior collapse. 2) In several places the authors write that a smaller KL term leads to less informative latent variables. While I understand the intention of the authors I believe this argument is too imprecise. For example, one can encode an arbitrary amount of information into a single real-valued scalar variable (even doing so when that value is << 1). In which case, all dimensions but one can be collapsed completely (significantly reducing the KL) and the final dimension can feed all information into a decoder powerful enough to decode it. Furthermore, the breakdown of equation (3) is a bit troubling. The same analysis could lead one to assume that L2 regularization forces parameters to be zero leading to problematic predictions. It is important that the balance between the KL term and the conditional log-likelihood is considered together. 3) Page 1, footnote 3 refers to the KL divergence as *one such term* for regularizing a VAE. The KL divergence is necessary for the VAE objective and any other regularization term would imply a difference model. In fact, in this work the model is referred to as the -VAE; while relatively minor I would argue that this is inaccurate as we are no longer optimizing a VAE objective or even recovering a tractable lower bound on log marginal likelihood. This leads into my primary concern, which is that it is not clear what the objective function to be optimized represents or indeed what the advantages in doing so are compared to ELBO. Replacing the L2 norm of the mean with the L1 norm of the aggregated means (note that this is not merely replacing the L2 norm in the per-datapoint KL with the L1 norm) changes the objective significantly but is not discussed in enough detail in the paper. The authors claim that this is emulating MMD but I am not sure exactly what they mean. MMD depends on a feature map lying in a reproducing Hilbert space and measures the distance in expectation between the features of two distributions. What is the feature map being used here? How is this objective related to MMD exactly? Moreover, the remaining terms in the KL divergence are retained in the objective which adds further confusion. Overall, I felt that the justification for the change to the objective function was lacking and am ultimately unconvinced that this is a sensible alternative to training with ELBO. 4) How exactly is the latent clipping enforced? There are several possible approaches which may impose different training dynamics. 5) I feel that Section 4.1 is a good inclusion as it allows us to better understand the effects of each of the changes to the training regime. Unfortunately, I felt that the conclusions I drew from these experiments mostly suggested that latent clipping is likely to be most responsible for the performance gains observed in the VAE. I suspect that the reason we don*t see the VAE without latent clipping is that it experiences unstable learning dynamics and fails to converge. Latent clipping is not applied to the standard ELBO objectives in later empirical experiments --- I believe the authors should also compare downstream utility under the ELBO objective with latent clipping to convince the reader that the VAE objective really does provide benefits. 6) The empirical evaluation is very limited. The authors compare four different VAE-like objectives on MNIST and FashionMNIST. From the appendix, it seems that the optimizers for all models share the same hyperparameters --- in my experience VAEs are susprisingly sensitive to settings of learning rate and batch size (even when using Adam) and so I would expect a grid search over at least the learning rate. Additionally, it seems that the reconstruction loss is simply the L2 distance between the decoder outputs and data and does not include rescaling by the observation noise. This is essentially the same as keeping the observation noise fixed to 1 and has serious ramifications on the quality of the learned latent space [3]. 7) The experiments focus on downstream utility which I agree is a valuable metric. However, it would also be valuable to compare the ELBO of these trained models. Even if the VAE is not trained to maximize likelihood of the same probabilistic model, one might hope that if it reduces posterior collapse it could lead to a better model. Minor: - In the abstract you claim *this problem is exacerbated when the dataset is small and the latent dimension is high*. Could you please provide a reference for this claim. - *encoder learns to map the data distribution to a simple distribution such as Gaussian*. This is not quite correct, each data point is mapped to a simple distribution but the data distribution is mapped to a more complex distribution (i.e. the aggregated posterior ). - What is meant by an *approximately diagonal co-variance* at the top of page 5. References: [1] Fixing a Broken ELBO, Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, and Kevin Murphy [2] The usual suspects? Reassessing Blame for VAE Posterior Collapse, Anonymous, https://openreview.net/forum?id=r1lIKlSYvH [3] Understanding posterior collapse in generative latent variable models, James Lucas, George Tucker, Roger Grosse, and Mohammad Norouzi Note: This review was edited after release to remove a duplicate paragraph. Paragraph indices were updated accordingly."}
{"id": "iclr2020_549", "title": "AdamT: A Stochastic Optimization with Trend Correction Scheme | OpenReview", "abstract": "Abstract:###Adam-typed optimizers, as a class of adaptive moment estimation methods with the exponential moving average scheme, have been successfully used in many applications of deep learning. Such methods are appealing for capability on large-scale sparse datasets. On top of that, they are computationally efficient and insensitive to the hyper-parameter settings. In this paper, we present a new framework for adapting Adam-typed methods, namely AdamT. Instead of applying a simple exponential weighted average, AdamT also includes the trend information when updating the parameters with the adaptive step size and gradients. The newly added term is expected to efficiently capture the non-horizontal moving patterns on the cost surface, and thus converge more rapidly. We show empirically the importance of the trend component, where AdamT outperforms the conventional Adam method constantly in both convex and non-convex settings.", "review": "Review:###Overall I think there is a room for this paper to improve. I expect to see more comprehensive experiments for comparing Adam and AdamT. The authors applied AdamT to four different tasks and compared training and testing loss difference. Are the models used in these tasks state-of-the-art? I did not see a discussion about whether parameters (including learning rate) are tuned for the best results for each optimization method. Also it is hard to visualize the difference in a figure (e.g. Figure 3) to claim whether it is significant. I would suggest using more quantitative measures. More details: - label all equations - second paragraph on page 3: *are denoted as alpha and _x0008_eta* --> gamma and _x0008_eta? - *Equation 3.1*: which equation? - FIg. 1: Has the author tried different learning rates for Adam? I wonder whether the faster convergence speed by the proposed trend estimation can be achieved by a larger learning rate - Fig. 3: The testing loss for Adam+Dropout and AdamT+Dropout does not seem to be significant. Is the loss smoothed or averaged across different runs to remove randomness? Can the authors apply AdamT to the state-of-the-art model for CIFAR-10 and can still show advantages?"}
{"id": "iclr2020_550", "title": "Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions | OpenReview", "abstract": "Abstract:###Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, we first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate. Among all these attacks, we find that CapsNets always perform better than convolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack was proportional to the visual similarity between the source and target class. Additionally, the resulting perturbations can cause the input image to appear visually more like the target class and hence become non-adversarial. These suggest that CapsNets use features that are more aligned with human perception and address the central issue raised by adversarial examples.", "review": "Review:###This paper proposed a new defense method for capsule networks. For both white-box and black-box settings, the proposed CapNets has shown superior performance than two variants of CNNs. The visualizations of adversarial examples generated by the CapNets are more aligned with the human perception which is very insightful. On the corrupted MNIST dataset, the results show the proposed defense method can also be used well as an out-of-distribution detector. Overall the paper is clearly written and easy to follow. Here I have some concerns: The major one is the limited comparisons. For the defense of CNNs, the author only implemented two types of the strategy, which is derived from the characteristics of the capsule network. However, there are also other a lot of defense methods for CNNs which are designed according to the characteristics of themselves. I think the author should also compare the defense performance with those methods to hold the strong claims that CpasNets always perform better than convolutional networks. Also, the experiments performed on Cifar-10 is very limited. The adversarial examples generated by the CapsNets shown in Figure 3 and Figure 11 are indeed changing the number shape which is aligned with human perception. However, some studies for CNNs has also found similar results on MNIST(Towards Deep Learning Models Resistant to Adversarial Attacks) and CIFAR-10(RANDOM MASK: Towards Robust Convolutional Neural Networks). The author should provide more visualizations on other datasets such as CIFAR-10 to support the contribution that the features captured by CapsNets are more aligned with human perception than CNNs. ========================================================= After Rebuttal: I thank the author for the response. I still think the argument that CpasNets always perform better than convolutional networks is an overstatement since you only performed a few defense methods. A milder one is more suitable. Also, the high variance of the Capsule Network in Figure 10 can tell something but it is not enough. Could you find similar visible semantic changes in CIFAR-10 dataset as Figure 5? If yes, you should also list some results. The mentioned CNN works could find similar phenomena on both MNIST and CIFAR-10 dataset. I do not see the strong evidence for the argument that features captured by CapsNets are more aligned with human perception than CNNs. To sum up, I think some arguments are overstatements. But this is a good work to analyze the robustness of Capsule Net, I would like to rate 6."}
{"id": "iclr2020_551", "title": "A New Multi-input Model with the Attention Mechanism for Text Classification | OpenReview", "abstract": "Abstract:###Recently, deep learning has made extraordinary achievements in text classification. However, most of present models, especially convolutional neural network (CNN), do not extract long-range associations, global representations, and hierarchical features well due to their relatively shallow and simple structures. This causes a negative effect on text classification. Moreover, we find that there are many express methods of texts. It is appropriate to design the multi-input model to improve the classification effect. But most of models of text classification only use words or characters and do not use the multi-input model. Inspired by the above points and Densenet (Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700–4708, 2017.), we propose a new text classification model, which uses words, characters, and labels as input. The model, which is a deep CNN with a novel attention mechanism, can effectively leverage the input information and solve the above issues of the shallow model. We conduct experiments on six large text classification datasets. Our model achieves the state of the art results on all datasets compared to multiple baseline models.", "review": " This paper presents a multi-input model for text classification. The presentation of this paper is very poor. The novelty is very limited. The justification of the proposed architecture is not persuasive. The experiment design has many flaws. All the compared baselines are extremely weak. The authors need to follow the experiment setup in more recent text classification papers. Authors need at least to compare their methods to BERT. The current version of this paper is definitely not an ICLR publication."}
{"id": "iclr2020_552", "title": "Ridge Regression: Structure, Cross-Validation, and Sketching | OpenReview", "abstract": "Abstract:###We study the following three fundamental problems about ridge regression: (1) what is the structure of the estimator? (2) how to correctly use cross-validation to choose the regularization parameter? and (3) how to accelerate computation without losing too much accuracy? We consider the three problems in a unified large-data linear model. We give a precise representation of ridge regression as a covariance matrix-dependent linear combination of the true parameter and the noise. We study the bias of -fold cross-validation for choosing the regularization parameter, and propose a simple bias-correction. We analyze the accuracy of primal and dual sketching for ridge regression, showing they are surprisingly accurate. Our results are illustrated by simulations and by analyzing empirical data.", "review": "Review:###This paper presents a theoretical study of ridge regression, focusing on the practical problems of correcting for the bias of the cross-validation based estimate of the optimal regularisation parameter, and quantification of the asymptotic risk of sketching algorithms for ridge regression, both in the p / n -> gamma in (0, 1) regime (n = # data points, p = # dimensions). The authors derive most of their results exploiting their (AFAICT) new asymptotic characterisation of the ridge regression estimator which may be of independent interest. The whole study is complemented by a series of numerical experiments. I am recommending this paper to be accepted for publication at ICLR. The paper is clearly written, makes several solid theoretical contributions, and recommends a simple and practical bias correction for CV-based estimates of the optimal ridge regulariser. While ICLR is biased towards deep learning focused publications, the work of Belkin, Hsu, Ma, Bartlett, Hastie, Montanari, Rakhlin, Liang and others (apologies to everyone whose name was omitted---it is for the sole reason of brevity) has recently shown that we can learn non-negligible amount about neural networks from study of linear models. Comments: - Most of the examples in the paper focus on the regime gamma < 1. Would you expect the observed behaviours to be significantly different when gamma > 1? I am asking specifically because of [1] which has found that the bias of the risk estimate obtained via cross-validation is often most extreme when p >> n, which makes me wonder about what would an experiment like those in fig.2 look like in the p >> n regime? - I was somewhat confused when I first read the statement of thm.2.1. In particular, the definition of asymptotic equivalence requires (roughly speaking) that any series of projections of the difference between the two random vectors converges to zero a.s. However, within the theorem, you introduce Z without much explanation which confused me because not every Z ~ standard normal would be asymptotically equivalent. I needed to look at the proof to understand how Z is “coupled” with hat(beta), which I think should not be necessary. If possible, I would either say that there exists a (series of) Z (all standard normal) s.t. the asymptotic equivalence holds, or add some other clarification (possibly in the form of a footnote). - When you are citing a book, please consider citing exact pages or at least chapters/sections (e.g., when citing the exact shortcut from Hastie et al. (2019)). - In the definition of asymptotic equivalence (starting at the bottom of p.2), did you mean to assume limsup ||w|| < infty a.s. (or is limsup not needed here)? References: [1] Tibshirani, R. J., & Tibshirani, R. (2009). A bias correction for the minimum error rate in cross-validation. The Annals of Applied Statistics, 822-829."}
{"id": "iclr2020_553", "title": "Compressed Sensing with Deep Image Prior and Learned Regularization | OpenReview", "abstract": "Abstract:###We propose a novel method for compressed sensing recovery using untrained deep generative models. Our method is based on the recently proposed Deep Image Prior (DIP), wherein the convolutional weights of the network are optimized to match the observed measurements. We show that this approach can be applied to solve any differentiable linear inverse problem, outperforming previous unlearned methods. Unlike various learned approaches based on generative models, our method does not require pre-training over large datasets. We further introduce a novel learned regularization technique, which incorporates prior information on the network weights. This reduces reconstruction error, especially for noisy measurements. Finally we prove that, using the DIP optimization approach, moderately overparameterized single-layer networks trained can perfectly fit any signal despite the nonconvex nature of the fitting problem. This theoretical result provides justification for early stopping.", "review": "Review:###This paper proposes use of the deep image prior (DIP) in compressed sensing. The proposed method, termed CS-DIP, solves the nonlinear regularized least square regression (equation (3)). It is especially beneficial in that it does not require training using a large-scale dataset if the learned regularization is not used. Results of numerical experiments demonstrate empirical superiority of the proposed method on the reconstruction of chest x-ray images as well as on that of the MNIST handwritten digit images. The demonstrated empirical efficiency of the proposal is itself interesting. At the same time, the proposal can be regarded as a straightforward combination of compressed sensing and the DIP, so that the main contribution of this paper should be considered rather marginal. I would thus like to recommend *weak accept* of this paper. In my view the concept of DIP provides a very stimulating working hypothesis which claims that what is important in image reconstruction is not really representation learning but rather an appropriate network architecture. The results of this paper can be regarded as providing an additional empirical support for this working hypothesis. On the other hand, what we have understood in this regard seem quite limited; for example, on the basis of the contents of this paper one cannot determine what network architecture should be used in the proposed CS-DIP framework applied to a specific task. I think that the fact that DIP is still a working hypothesis should be stressed more in this paper. It does not reduce the value of this paper as one providing an empirical support for it. I think that the theoretical result of this paper, summarized as Theorem 4.1, does not tell us much about CS-DIP. The theorem shows that overfitting occurs even if one uses a single hidden-layer ReLU network. As the authors argue, it would suggest necessity of early stopping in the proposal. On the other hand, I could not find any discussion on early stopping in the experiments: On page 14, lines 22-23, it seems that the theoretical result is not taken into account in deciding the stopping criterion, which would make the significance of the theoretical contribution quite obscure. Page 4, line 10: a phenomen(a -> on) Page 13: The paper by Ulyanov, Vedaldi, and Lempitsky on DIP has been published as a conference paper in the proceedings of CVPR 2018, so that appropriate bibliographic information should be provided. Page 16, line 26: On the right-hand side of the equation, the outermost phi should not be there. Page 17, line 3: 3828 should perhaps read 3528."}
{"id": "iclr2020_554", "title": "Few-shot Learning by Focusing on Differences | OpenReview", "abstract": "Abstract:###Few-shot classification may involve differentiating data that belongs to a different level of labels granularity. Compounded by the fact that the number of available labeled examples are scarce in the novel classification set, relying solely on the loss function to implicitly guide the classifier to separate data based on its label might not be enough; few-shot classifier needs to be very biased to perform well. In this paper, we propose a model that incorporates a simple prior: focusing on differences by building a dissimilar set of class representations. The model treats a class representation as a vector and removes its component that is shared among closely related class representatives. It does so through the combination of learned attention and vector orthogonalization. Our model works well on our newly introduced dataset, Hierarchical-CIFAR, that contains different level of labels granularity. It also substantially improved the performance on fine-grained classification dataset, CUB; whereas staying competitive on standard benchmarks such as mini-Imagenet, Omniglot, and few-shot dataset derived from CIFAR.", "review": "Review:###In this paper the authors propose a metric based model for few-shot learning. The goal of the proposed technique is to incorporate a prior that highlight better the dissimilarity between closely related class prototype. Thus, the proposed paper is related to prototypical neural network (use of prototype to represent a class) but differ from it by using inner product scoring as a similarity measure instead of the use of euclidean distance. There is also close similarity between the proposed method and matching network overall, the paper does not highlight the novelty of their proposed method especially prototypical network and matching network. Thus, the related work session is so general and does not tackle the close models in details. The experiments do not provide convincing evidence of the correctness of the proposed approach. Several parts are unclear/incomprehensible: (1) The Introduction is confusing and does not demonstrate the problem that the paper is trying to solve. Specifically, the described intuition (Mill’s method of difference) is not convincing (2) the first sentence of the section “Our work.” (page 1) is long and unclear … “In this paper, we propose a model that focuses on the differences in the support set of closely related classes in assigning the class label to a new instance in the novel task.” (3) the use of the two level of embedding is confusing and not clear. Figure 1 is also confusion and not clear. the correctness of the proposed approach is not proved by the conducted experiment and does not provide convincing and fair comparison with SoA techniques: (1) The experiments do not provide the details of the used architecture compared to your baseline. (how many layers are used in both embedding systems) (2) In Table 1 you are using the results reported by Chen et al. (2019) did you use his framework (Resnet or 4 layers CNN) **Minor comments** The definition of the embedding function f = (f_g o f_f) (in line 1 page 5) is not consistent with the domain of each function f_g is defined on R^H x R^H. K is not defined (last line page 3)"}
{"id": "iclr2020_555", "title": "Is Deep Reinforcement Learning Really Superhuman on Atari? Leveling the playing field | OpenReview", "abstract": "Abstract:###Consistent and reproducible evaluation of Deep Reinforcement Learning (DRL) is not straightforward. In the Arcade Learning Environment (ALE), small changes in environment parameters such as stochasticity or the maximum allowed play time can lead to very different performance. In this work, we discuss the difficulties of comparing different agents trained on ALE. In order to take a step further towards reproducible and comparable DRL, we introduce SABER, a Standardized Atari BEnchmark for general Reinforcement learning algorithms. Our methodology extends previous recommendations and contains a complete set of environment parameters as well as train and test procedures. We then use SABER to evaluate the current state of the art, Rainbow. Furthermore, we introduce a human world records baseline, and argue that previous claims of expert or superhuman performance of DRL might not be accurate. Finally, we propose Rainbow-IQN by extending Rainbow with Implicit Quantile Networks (IQN) leading to new state-of-the-art performance. Source code is available for reproducibility.", "review": "Review:###This paper proposes a new way to benchmark DRL algorithms using the Atari environment which is twofold, one part is a set of emulator recommendations, the other part is what quantity we should consider as a *human reference*. The paper also compares Rainbow and Rainbow-IQN, where the IQN improvement matches the proposed human normalized score improvment. I*m not quite sure how to rate this paper, I have put weak-reject for now, as I don*t strongly disagree with anything in the paper, but at the same time: - the difference to Machados et al. is marginal, but is a bit surprising - the Rainbow-IQN improvement is too incremental to be considered a significant contribution - there are some interesting remarks on why Atari is _not_ necessarily a good environment, e.g. most of Section 6, but this clashes with the paper*s premise that we should be using Atari. In a way, this paper reads like an interesting technical review of Atari, but I don*t think it provides enough new knowledge to be a conference paper. Detailed comments: - I find it a bit weird that the many weaknesses of Atari as a platform are presented at DRL being bad at Atari. The line between environment design and algorithm design can be blurry, but in Atari*s case, the weird peculiarities of each game are known to make it an inconvenient benchmark. - In the same vein, why is Atari+SABER better than other RL environments? This is rather crucial. We should only work on improving a benchmark if it is a useful benchmark, yet, we have many clues that Atari is not. - The link to TwinGalaxies should be a proper reference with the time of visit, especially if humans break new records in the future. - Why only compare Rainbow and a variant of Rainbow? I understand compute resources being a limitation, but at the same time, the reasoning behind having standardized testing is to be able to compare a wide variety of algorithms. This paper would be much stronger if it focused on a few representative games (e.g. one reflex game, one hard exploration game, etc.) and tested these games with a bunch of DRL algorithms. That ranking might reveal something very interesting. - The paper is easy to read, but there are a few grammar mistakes here and there."}
{"id": "iclr2020_556", "title": "SSE-PT: Sequential Recommendation Via Personalized Transformer | OpenReview", "abstract": "Abstract:###Temporal information is crucial for recommendation problems because user preferences are naturally dynamic in the real world. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed for better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-the-art results. However, SASRec, just like the original Transformer model, is inherently an un-personalized model and does not include personalized user embeddings. To overcome this limitation, we propose a Personalized Transformer (SSE-PT) model, outperforming SASRec by almost 5% in terms of NDCG@10 on 5 real-world datasets. Furthermore, after examining some random users* engagement history, we find our model not only more interpretable but also able to focus on recent engagement patterns for each user. Moreover, our SSE-PT model with a slight modification, which we call SSE-PT++, can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed, striking a balance between performance and speed requirements. Our novel application of the Stochastic Shared Embeddings (SSE) regularization is essential to the success of personalization. Code and data are open-sourced at https://github.com/SSE-PT/SSE-PT.", "review": "Review:###The manuscript proposes SSE-PT, a sequential recommendation model based on transformer and stochastic shared embedding (SST). Experiments on several datasets show that SSE-PT outperforms a number of baseline methods. Some analytical results are also provided. Overall, I think this work is not suitable for ICLR due to following reasons. The novelty of this work is limited. This work is based on SASREC [W Kang, ICDM2018] and uses transformer to encode user-item interactions in sequential manner. The difference is that this work adds user embedding in bottom layer and utilizes SSE for regularization as well as designs SSE-PT++ by sampling. To me, there is little extension or novelty. The experiment results are not convincing. Most of results are copied from [W Kang, ICDM2018] except HGN in Table 1. Table 1 shows SASREC is much better than HGN [C Ma, KDD2019]. However, I checked the results in HGN paper and found HGN is much better than SASREC. Even though datasets are different, most of them are from Amazon data. I was not convinced by this result due to the large difference. In addition, I did not understand why the authors change evaluation metrics in Table 3, i.e., from NDCG/Recall@10 to NDCG/Recall@5. I found SSE-PT without regularization and with different regularizations are much worse than the best result, which makes me concern about the effectiveness of personalized transformer. I did not see ablation study or discussion about this. Update: I have considered author rebuttal. I appreciate the extensive hyper-parameter sensitivity and ablation study in the paper, while these cannot be a key factor in evaluating paper as most of them can be done easily. I main concerns still lie in the novelty and experimental results. I still think this work is not suitable for ICLR and I keep my score."}
{"id": "iclr2020_557", "title": "FairFace: A Novel Face Attribute Dataset for Bias Measurement and Mitigation | OpenReview", "abstract": "Abstract:###Existing public face image datasets are strongly biased toward Caucasian faces, and other races (e.g., Latino) are significantly underrepresented. The models trained from such datasets suffer from inconsistent classification accuracy, which limits the applicability of face analytic systems to non-White race groups. To mitigate the race bias problem in these datasets, we constructed a novel face image dataset containing 108,501 images which is balanced on race. We define 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups. Evaluations were performed on existing face attribute datasets as well as novel image datasets to measure the generalization performance. We find that the model trained from our dataset is substantially more accurate on novel datasets and the accuracy is consistent across race and gender groups. We also compare several commercial computer vision APIs and report their balanced accuracy across gender, race, and age groups.", "review": "Review:###This manuscript introduces a new face dataset, FairFace, that is balanced in terms of race composition. The authors also performed extensive empirical validations on the new dataset as well as several existing face datasets to show that models trained on this new dataset do have better balance in terms of recognition error across different demographic subgroups. First of all, there is no doubt that, as a community, we all should work together to mitigate existing bias (unbalanced ratio in terms of race) in the dataset used to train automated recognition system. From this perspective, the manuscript does a good job in building and introducing the FairFace dataset and conducted a thorough comparison with existing face datasets. I appreciate the efforts the authors put in building it. My main concern is that the contributions of this paper are not well-aligned with the Call-For-Paper of ICLR. Specifically, although ICLR does have sub-field that aims at applications in vision, audio, speech, natural language processing and robotics, it more or less focuses on novel applications in these areas with techniques related to representation learning. On the other hand, the main contribution of this paper is not about any specific representation learning techniques or applications, but rather a novel dataset. Hence I believe it may find a better fit at other conferences/journals that have specific focus on this respect, e.g., CVPR or FAT*."}
{"id": "iclr2020_558", "title": "YaoGAN: Learning Worst-case Competitive Algorithms from Self-generated Inputs | OpenReview", "abstract": "Abstract:###We tackle the challenge of using machine learning to find algorithms with strong worst-case guarantees for online combinatorial optimization problems. Whereas the previous approach along this direction (Kong et al., 2018) relies on significant domain expertise to provide hard distributions over input instances at training, we ask whether this can be accomplished from first principles, i.e., without any human-provided data beyond specifying the objective of the optimization problem. To answer this question, we draw insights from classic results in game theory, analysis of algorithms, and online learning to introduce a novel framework. At the high level, similar to a generative adversarial network (GAN), our framework has two components whose respective goals are to learn the optimal algorithm as well as a set of input instances that captures the essential difficulty of the given optimization problem. The two components are trained against each other and evolved simultaneously. We test our ideas on the ski rental problem and the fractional AdWords problem. For these well-studied problems, our preliminary results demonstrate that the framework is capable of finding algorithms as well as difficult input instances that are consistent with known optimal results. We believe our new framework points to a promising direction which can facilitate the research of algorithm design by leveraging ML to improve the state of the art both in theory and in practice.", "review": "Review:###Update to the Review after the rebuttal from the Authors: After carefully reviewing the responses by the authors especially on my concerns about the significance of solving an instance of a given problem and the improvement in the exposition of the ideas I would like to amend my earlier decision and recommend to accept. For completeness below is the original review. This paper introduces a framework to learn to generate solutions to online combinatorial optimization problems with worst case guarantees. The framework as the authors claim eliminates the need for manual hard to solve instance/data creation, which is necessary to teach the model to provide the aforementioned worst case guarantees. Therefore the main contribution of the paper can be said that this framework shows that it is possible to train a machine learning model, which can learn an algorithm to solve hard online combinatorial optimization problems and this training can be done without knowing much about the actual optimization problem domain. The only input required is the way to calculate the objective function of the actual problem. This contribution is demonstrated on two classes of problems: Ski-Rental and Fractional AdWords. The framework requires two neural networks one for solution generation agent and one for problem instance generation. These two networks are trained jointly from scratch and the underlying algorithm for the training is provided. Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems. Is it because the ML solution would be faster to compute with big instances? Is it because with the proposed approach one can curate sophisticated heuristic solutions when provable optimality is out of reach? This paper should be rejected because proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem. Although they show that the Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, ie. problem instances are not generated by use of a machine learning model, which is one of main claims the authors are making. Therefore I do not find being able to solve this problem as a supporting evidence for the contributions claimed. In particular there is not any theoretical not experimental evidence that the approach would scale to any instances where a pure optimization approach would be slow to provide any meaningful solutions. I find this important because for combinatorial optimization usually scale matters a lot. While a small instance of a problem can be solve by a general purpose solver quickly a small increase in the problem size can turn out to be intractable. When proposing a machine learning approach to such problems I would expect the model to scale better than pure optimization approach so that there would be demonstrable benefit. Although the paper proposes an interesting framework I would argue that it is a “green apple” in the sense that authors need to motivate the approach better and expand the contribution beyond solving a particular instance. Authors acknowledge the fact that their experimental setup is rather limited in Appendix C.1, which I agree with and they also claim that there is a representation for a uniform algorithm for any number of advertisers for the AdWords problem, however they leave this as a future work, which I find unfortunate. I would recommend taking this direction rigorously and expand the contribution, which would prove to be a very sound contribution. In order to clarify the exposition the following are some questions: 1. Authors call the approach YaoGAN due to its structural similarity to GANs. I understand the fact that they are training two neural networks in an alternating scheme, which is similar to the GAN training. How can one evaluate the solutions generated by this framework similar to how GAN generators are evaluated? Can one walk the latent distribution of the algorithm agent and draw insights, which might lead into tailoring some algorithms that would be appropriate for some input distribution although in general inferior in terms of worst case guarantees? 2. The main technical contribution claim needs to be elaborated. I understand how the game theoretic framework is established but how does this manifest itself in the algorithm described in Section 3.1 needs more explanation. 3. Authors claim there are two shortcomings of the previous method proposed in Kong et. al 2018. They need to elaborate how their method overcomes these issues better. 4. Authors state that fractional relaxation of combinatorial mainly integer optimization problems, which is accurate. Yet their approach is only able to solve the fractional version of the AdWords problem. In addition I agree with the fact that although continuous relaxations to integer optimization problems might provide insightful directions they usually employed to to prove bounds on the heuristic approaches. Yet the authors stop at only solving this version with a machine learning approach, which does not hit the bar for me. I would have expected the authors to at least elaborate on why the current framework is not suitable for the non-relaxed problem. What are the shortcomings? 5.In Appendix A authors talk about no-regret dynamics, which are relevant. However, they state they loosely follow this approach. What does that entail? What kind of theoretical guarantees are given up due to not following this, a better exposition on this topic would help to support the claims. 6. In appendix C.2 authors provide additional plots for the Fractional AdWords problem. However, they retain from providing any intuition about them. In particular what is the conclusion to be drawn from Figure 5. This needs more elaboration. Is this way of training results expected? What is the lesson learned? 7.In Figure 8 they provide example data from experience array. What are the significance of these examples? How they help us understand the problem instance generation was actually able to find interesting instances? What kind of dynamics are under covered? These are not directly revealed by only looking at the pictures one needs more explanation to support the claims."}
{"id": "iclr2020_559", "title": "Recurrent Layer Attention Network | OpenReview", "abstract": "Abstract:###Capturing long-range feature relations has been a central issue on convolutional neural networks(CNNs). To tackle this, attempts to integrate end-to-end trainable attention module on CNNs are widespread. Main goal of these works is to adjust feature maps considering spatial-channel correlation inside a convolution layer. In this paper, we focus on modeling relationships among layers and propose a novel structure, *Recurrent Layer Attention network,* which stores the hierarchy of features into recurrent neural networks(RNNs) that concurrently propagating with CNN and adaptively scales feature volumes of all layers. We further introduce several structural derivatives for demonstrating the compatibility on recent attention modules and the expandability of proposed network. For semantic understanding on learned features, we also visualize intermediate layers and plot the curve of layer scaling coefficients(i.e., layer attention). Recurrent Layer Attention network achieves significant performance enhancement requiring a slight increase on parameters in an image classification task with CIFAR and ImageNet-1K 2012 dataset and an object detection task with Microsoft COCO 2014 dataset.", "review": "Review:###This paper proposes an attention mechanism to improve the performance of convolutional networks. Rather than the more common purely layer-local attention from Transformer-style models, the authors here use a separate parallel stream to carry attention information across the layers. This is an interesting an novel idea, but unfortunately the paper is let down by being extremely unreadable: Due to a combination of many grammatical errors, and what appears to be plain sloppy writing, it*s virtually impossible to follow the main ideas. E.g. phrasing the attention stream as a a recurrent network is either extremely confusing or plainly incorrect: There is no time series input, so is the implied recurrence over processing steps, similar to Neural Turing Machines or the more recent Universal Transformer model? Are the weights even shared across layers? Or is the only thing that makes it *recurrent* that the structure of an LSTM cell is used? Overall, this seems like a nice extension of the ideas behind Squeeze-and-Excite networks, i.e. extracting global features from convolutional feature maps and using them for an attention-style reweighing. The results are not earth-shattering, but outperforming S&E indicates to me that the authors are onto something with the proposed attention mechanism. While the manuscript is certainly far from publishable in its current form, I would welcome to see a revised version submitted to a different forum. A couple of examples just from the abstract (I am not commenting on the rest of the paper): - *attention module* -> *attention modules* - *Main goal of* -> *The Main goal of* - ‘Recurrent Layer Attention network,’ -> ‘Recurrent Layer Attention network’, - *concurrently propagating* -> *concurrently propagate* - *of proposed* -> *of the proposed* - *scaling coefficients(i.e., layer attention)* -> *scaling coefficients (i.e., layer attention)* - *Recurrent Layer Attention network* -> *The Recurrent Layer Attention network* Note that these issues are just cosmetic (minor grammar issues and sloppiness), it gets far worse in the main text and many sentences are just not understandable at all to me."}
{"id": "iclr2020_560", "title": "FLUID FLOW MASS TRANSPORT FOR GENERATIVE NETWORKS | OpenReview", "abstract": "Abstract:###Generative Adversarial Networks have been shown to be powerful tools for generating content resulting in them being intensively studied in recent years. Training these networks requires maximizing a generator loss and minimizing a discriminator loss, leading to a difficult saddle point problem that is slow and difficult to converge. Motivated by techniques in the registration of point clouds and the fluid flow formulation of mass transport, we investigate a new formulation that is based on strict minimization, without the need for the maximization. This formulation views the problem as a matching problem rather than an adversarial one, and thus allows us to quickly converge and obtain meaningful metrics in the optimization path.", "review": "Review:###The paper addresses the task of constructing a generative model for data using a novel optimal transport-based method. The paper proposes an alternative view of obtaining generative models by viewing the generation process as a transport problem (specifically, fluid flow mass transport) between two point clouds living in high-dimensional space. To solve the transport problem, a discretization scheme is proposed, which gives rise to a variant of point cloud registration problem, which is solved using numerical optimization. The results are provided on synthetic data and a real MNIST data. With generative models, and particularly generative adversarial networks (GANs) being notoriously hard to train, alternative ways of constructing generative models are needed. The paper does a good job displaying the potential optimization issues arising when training GANs, and the basic theoretical foundations used in the paper have been validated in prior work. However, with the introduction of a transport-based formulation, its respective issues may arise, that are not described in the paper. Will the optimization always converge, and if yes, to which kind of optimum? What are the requirements for the point clouds R and T? While the conceptual contribution is that the , the technical novelty is limited and mostly amounts to the appropriate choice of distributions in R and T and applying the discretization schemes to be able to compute the experiments. Unfortunately, the model is not studied theoretically, i.e. no description is given regarding the class of generative tasks that could be solved using the method, or the class of functions that could be learn in such a way. The experimental evaluation demonstrates on only two simple examples the results of the work. More experiments are needed to fully understand the possibilities of the framework. The convergence speed is not indicated, and the efficiency of the optimization is not described. To summarize, I believe the paper should not be accepted in its present (early) form, as (1) more detailed theoretical insight are needed, and (2) much more computational experiments are needed to fully validate the method."}
{"id": "iclr2020_561", "title": "Transformer-XH: Multi-hop question answering with eXtra Hop attention | OpenReview", "abstract": "Abstract:###Transformers have obtained significant success modeling natural language as a sequence of text tokens. However, in many real world scenarios, textual data inherently exhibits structures beyond a linear sequence such as tree and graph; an important one being multi-hop question answering, where evidence required to answer questions are scattered across multiple related documents. This paper presents Transformer-XH, which uses eXtra Hop attention to enable the intrinsic modeling of structured texts in a fully data-driven way. Its new attention mechanism naturally “hops” across the connected text sequences in addition to attending over tokens within each sequence. Thus, Transformer-XH better answers multi-hop questions by propagating information between multiple documents, constructing global contextualized representations, and jointly reasoning over multiple pieces of evidence. This leads to a simpler multi-hop QA system which outperforms previous state-of-the-art on the HotpotQA FullWiki setting by large margins.", "review": "Review:###This paper introduces the Transformer XH model architecture, a transformer architecture that scales better to long sequences / multi-paragraph text compared to the standard transformer architecture. As I understand, Transformer XH is different from the standard transformer architecture in two main ways: 1) adding attention across paragraphs (or sub-sequences) via the CLS token and 2) defining the structure of that attention based on the entities in the paragraphs (or sub-sequences). The paper tackles an important problem (learning from long sequences) and achieves good empirical results on HotpotQA. Modification #1 has already been explored by previous works which should be discussed in the paper. *Generating Wikipedia by Summarizing Long Sequences* by Liu, Saleh, et al. propose the T-DMCA model, with a similar motivation: enabling the transformer to work on/scale to longer sequences. T-DMCA and Transformer XH have some difference (T-DMCA seems to have more capacity while Transformer XH is simpler); I think it is necessary to compare against Transformer XH against T-DMCA on HotpotQA to know whether a new architecture is really necessary for HotpotQA. *Generating Long Sequences with Sparse Transformers* from Child et al. also proposes another general transformer architecture that can handle long sequences, and it would be ideal to compare against this architecture as well. Sparse Transformers reduce the time complexity of attention to reduce O(n? n), which seems similar to the reduction that Transformer XH gets. For modification #2 (defining the attention structure beforehand using e.g. entity linking), it does not seem too difficult to learn the attention structure directly instead, as confirmed by the ablation in Table 3 which uses a fully connected graph structure. A model that learned the attention pattern or used a fully connected graph would be more general (but more similar to T-DMCA and sparse transformers). The empirical results are good. It*s nice that a simple/straightforward architecture like Transformer XH works quite well (compared to some previous approaches which were not as elegant). However, I do not feel that prior work has explored the best transformer architectures for HotpotQA (such as Sparse Transformers or T-DMCA), and since this work is specifically proposing a new transformer architecture, I think it is important to compare directly against other transformer architectures. Other (previously) SOTA models like SR-MRS are quite simple (conceptually), so it*s likely that such models will also be outperformed by transformer architectures that are better adapted to long sequences. In general, I think that the most relevant baseline already present is SR-MRS rather than CogQA; the fact that such a simple approach like SR-MRS works well is an important fact about HotpotQA to take into account (even if SR-MRS is concurrent). In other words, SR-MRS shows that previous models like CogQA are likely weaker baselines. Because of the missing baselines and limited novelty compared to prior work, I overall lean towards rejecting the paper (despite the good empirical results). I do have a few specific questions for the authors: - Would you mind providing further details about the following sentence? *For better retrieval quality, we use a BERT ranker (Nogueira & Cho, 2019) on the set Dir ? Del and keep top K ranked ones.* - Is only answer-level supervision used? Or is supporting-fact level supervision used to train any of the rankers or pipeline?"}
{"id": "iclr2020_562", "title": "I love your chain mail! Making knights smile in a fantasy game world | OpenReview", "abstract": "Abstract:###Dialogue research tends to distinguish between chit-chat and goal-oriented tasks. While the former is arguably more naturalistic and has a wider use of language, the latter has clearer metrics and a more straightforward learning signal. Humans effortlessly combine the two, and engage in chit-chat for example with the goal of exchanging information or eliciting a specific response. Here, we bridge the divide between these two domains in the setting of a rich multi-player text-based fantasy environment where agents and humans engage in both actions and dialogue. Specifically, we train a goal-oriented model with reinforcement learning via self-play against an imitation-learned chit-chat model with two new approaches: the policy either learns to pick a topic or learns to pick an utterance given the top-k utterances. We show that both models outperform a strong inverse model baseline and can converse naturally with their dialogue partner in order to achieve goals.", "review": "Review:###In this paper, the authors studied goal-oriented dialogue agents in the setting of a multi-player text-based fantasy environment. This problem is very interesting and also popular, which is a main direction for game company. But for the writing, I have a feeling that they translate to English by machine, e.g., *We define the general task of,...* in the second paragraph in introduction. They trained a goal-oriented model with reinforcement learning. Numerically, they compare their method with a strong inverse model baseline."}
{"id": "iclr2020_563", "title": "AUGMENTED POLICY GRADIENT METHODS FOR EFFICIENT REINFORCEMENT LEARNING | OpenReview", "abstract": "Abstract:###We propose a new mixture of model-based and model-free reinforcement learning (RL) algorithms that combines the strengths of both RL methods. Our goal is to reduce the sample complexity of model-free approaches utilizing fictitious trajectory rollouts performed on a learned dynamics model to improve the data efficiency of policy gradient methods while maintaining the same asymptotic behaviour. We suggest to use a special type of uncertainty quantification by a stochastic dynamics model in which the next state prediction is randomly drawn from the distribution predicted by the dynamics model. As a result, the negative effect of exploiting erroneously optimistic regions in the dynamics model is addressed by next state predictions based on an uncertainty aware ensemble of dynamics models. The influence of the ensemble of dynamics models on the policy update is controlled by adjusting the number of virtually performed rollouts in the next iteration according to the ratio of the real and virtual total reward. Our approach, which we call Model-Based Policy Gradient Enrichment (MBPGE), is tested on a collection of benchmark tests including simulated robotic locomotion. We compare our approach to plain model-free algorithms and a model-based one. Our evaluation shows that MBPGE leads to higher learning rates in an early training stage and an improved asymptotic behaviour.", "review": " ### Summary ### This paper focuses on model based reinforcement learning (RL). Specifically, the authors consider the setting of combining model based and model free RL algorithms by using the learned dynamics model to generate new data for training the model free algorithm. In order to capture the uncertainty of the environment and the model, the author applied Baysian neural network to learn the dynamics of the environment. The authors approximated the true Bayesian inference process by keeping an anchored ensemble of neural networks, where the prior and posterior of network weights are approximately Gaussian. The ensemble of dynamics model is then used to generate data to train a PPO[1] based agent. In order to prevent the agent from exploiting the learned dynamics model, the authors propose a heuristic way of balancing the amount of real data and model generated data by comparing the rewards. The authors evaluate the proposed algorithm on simulated robotic locomotion environments in MuJoCo, and the results show that the proposed method has better same efficiency compared to baseline methods in some environments. ### Review ### Overall I think this paper presents an interesting idea in combing model based and model free RL algorithms. The idea is very well presented and authors include empirical evidence to support the proposed method. However I do find a number of shortcomings that need to be addressed. Pro: 1. The idea for this paper is really well presented. The structure of the paper is well organized and the experiment results are easy to interpret. 2. The authors provide a detailed description of the configurations and the hyperparameters for each experiments. Such description would be very helpful if the results in this paper are to be reproduced. Con: 1. I’m not convinced about the magnitude of novelty in this paper. The proposed method seems very similar to ME-TRPO[2] and it seems to me that the novelty comes from the application of Bayesian ensemble techniques and the generated data ratio tuning heuristics. While these variations might be important for the final performance of the proposed method, the paper does not include any ablation study to further justify the importance of these variations. 2. I’m not convinced about some of the performance of some of the baseline methods presented in this paper. In this paper, MB-MPO[3] does not improve at all during training on Half-Cheetah environment. However, in the original MB-MPO paper, the algorithm does improve and the performance seems to be comparable to that of the proposed method in this paper. 3. The experiment results are not very strong for the proposed method. In 3 of 4 environments, the proposed algorithm does not show much advantage over the baseline algorithms. The only environment in which the proposed method shows significant improvement is Half-Cheetah, and I believe that the baseline algorithms might not be properly tuned in this environment. 4. The paper lacks certain baseline comparisons. There are many other model based RL algorithms developed recently, and it would be important to compare to these methods. Some examples would be ME-TRPO[2], SLBO[4] and MBPO[5]. The idea in the paper is well presented and carefully investigated. However, I am still not convinced about the novelty of the proposed idea and the magnitude of performance improvement. Therefore, I would not recommend acceptance before these problems are addressed. References [1] Schulman, John, et al. *Proximal policy optimization algorithms.* arXiv preprint arXiv:1707.06347 (2017). [2] Kurutach, Thanard, et al. *Model-ensemble trust-region policy optimization.* arXiv preprint arXiv:1802.10592 (2018). [3] Clavera, Ignasi, et al. *Model-based reinforcement learning via meta-policy optimization.* arXiv preprint arXiv:1809.05214 (2018). [4] Luo, Yuping, et al. *Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees.* arXiv preprint arXiv:1807.03858 (2018). [5] Janner, Michael, et al. *When to Trust Your Model: Model-Based Policy Optimization.* arXiv preprint arXiv:1906.08253 (2019)."}
{"id": "iclr2020_564", "title": "Variational Autoencoders for Highly Multivariate Spatial Point Processes Intensities | OpenReview", "abstract": "Abstract:###Multivariate spatial point process models can describe heterotopic data over space. However, highly multivariate intensities are computationally challenging due to the curse of dimensionality. To bridge this gap, we introduce a declustering based hidden variable model that leads to an efficient inference procedure via a variational autoencoder (VAE). We also prove that this model is a generalization of the VAE-based model for collaborative filtering. This leads to an interesting application of spatial point process models to recommender systems. Experimental results show the method*s utility on both synthetic data and real-world data sets.", "review": "Review:###The work describes an application of a spatial point process for solving problems with missing data. The authors introduce a novel method based on a non-parametric definition of point intensities for the multivariate case. The method incorporates VAE framework to effectively handle missing points via smooth intensity estimation and enjoys amortized inference for efficient computations and quick prediction generation. Using a sequence of mild assumptions, the authors show connection to a popular VAE-based collaborative filtering model, which turns out to be a special case of their approach. This is a rigorous study providing theoretically justified evidence on the effectiveness of the proposed approach. Apart from the issues in the last part of experiments with classical collaborative filtering task (which will be detailed below), the work presents a solid research. I would therefore vote for accepting it. The text is well structured, and all key points are clearly explained. The problem solved by the authors is well described, and the motivation for this work is convincing. The way point process theory is applied constitutes a rigorous probabilistic approach. The authors convincingly justify the need for all approximations and simplifications made in the model. One of key results making the entire model feasible is supported by the corresponding theorem proved by the authors. I haven’t carefully verified all the derivations, though. My major concern is related to the last part with experiments on the Movielens data. As the authors state, “applications without explicit spatial information, we embed each event into a latent space as a vector.” “No spatial information” is exactly the case with the standard collaborative filtering task, which the authors attempt to solve. This leads to an introduction of an additional model like GNN, which is unrelated to the main approach. As GNN is involved it’s not immediately obvious that the improvement over standard VAE architecture, observed in the experiments on ML-100K and ML-1M, is due to a better point process modelling. No evidence is provided to argue that this is not simply due to a good compression or a good data preprocessing achieved by a GNN architecture itself. Therefore, the results on a pure recommender systems part are not convincing. What would happen if GNN was trained and fed into another (simpler) algorithm? Maybe a simple KNN based algorithm would produce comparable or even better results? As indicated by the work of [Dacrema, Cremonesi, Jannach 2019] on “A Worrying Analysis of Recent Neural Recommendation Approaches”, VAE-CF (along with several other recently proposed neural network-based methods) is inferior to even properly tuned kNN-based models. I would not be surprised, if a kNN model trained on GNN output would produce even better results than the proposed VAE-SPP. Another related question is how incorporating GNN affects the training time? Is it comparable to that of VAE-CF or is it much worse? Computational performance is an important part in making practical decisions and should be also considered. Furthermore, both ML datasets used for tests are too small and not very representative to make any generalized conclusions. Even on a larger ML-20M dataset an optimal SVD-based model can be trained within several minutes on a standard CPU on a laptop (according to my experiments, VAE-CF would take at least twice longer on Tesla K80). Therefore, it can hardly be considered a realistic example. In practice, there could be millions and hundreds of millions of items. The authors even mention it in the in the introduction, using it as a vehicle to motivate their approach. However, computing similarities between that many entities can be a laborious task on its own, which adds an extra layer of complexity and again is not directly related to the main approach. It can easily become a bottleneck or make further computations inefficient. More efficient similarity computations may in contrast reduce the resulting accuracy. The issue can get even worse, because, unlike classical MF methods, there’s still no proper support for sparse operations in NN frameworks. In the VAE framework it means that, during the training, user batches will be converted into dense arrays and may become inefficient to work with in terms of memory and CPU utilization (a few non-zero entries vs. hundreds of millions of explicitly stored zeroes). In spite of all this, I’d also suggest rephrasing “We validate these bene?ts through extensive experiments” as it sounds a bit exaggerated (if we are considering real recommender systems applications). I agree that the proposed approach is potentially applicable in real cases for recommender systems, however, there’s still not enough evidence for this. In fact, I don’t even think that completely removing the part with ML-100K and ML-1M datasets would make the whole work any worse. Clearly stating the region of applicability of the proposed approach would be enough. Right now some statements in this section in contrast are raising concerns rather than convincing the reader. The wording should be at least changed, so that readers do not get an impression that the case with classical CF task is solved purely by the proposed VAE-SPP approach. Other remarks to help improve the text: 1) “… points are more likely to … form clusters than the simple Poisson process …” the sentence seems to be inconsistent. 2) “The generative process of our model can be described as follow:” -> … as follows: 3) Page 4, last paragraph, line 6 – shouldn’t the upper bound for summation be N_u instead of just N? References: Dacrema, Maurizio Ferrari, Paolo Cremonesi, and Dietmar Jannach. *Are we really making much progress? A worrying analysis of recent neural recommendation approaches.* In Proceedings of the 13th ACM Conference on Recommender Systems, pp. 101-109. ACM, 2019."}
{"id": "iclr2020_565", "title": "SPECTRA: Sparse Entity-centric Transitions | OpenReview", "abstract": "Abstract:###Learning an agent that interacts with objects is ubiquituous in many RL tasks. In most of them the agent*s actions have sparse effects : only a small subset of objects in the visual scene will be affected by the action taken. We introduce SPECTRA, a model for learning slot-structured transitions from raw visual observations that embodies this sparsity assumption. Our model is composed of a perception module that decomposes the visual scene into a set of latent objects representations (i.e. slot-structured) and a transition module that predicts the next latent set slot-wise and in a sparse way. We show that learning a perception module jointly with a sparse slot-structured transition model not only biases the model towards more entity-centric perceptual groupings but also enables intrinsic exploration strategy that aims at maximizing the number of objects changed in the agent’s trajectory.", "review": " This paper introduces a model that learns a slot-based representation, along with a transition model to predict the evolution of these representations in a sparse fashion, all in a fully unsupervised way. This is done by leveraging a self-attention mechanism to decide which slots should be updated in a given transition, leaving the others untouched. The model learns to encode in a slot-wise and is trained on single step transitions. This work tackles an important problem, and is very well motivated and presented in a very clear fashion. It reuses some known ideas and components, but combines them in a nice way. I especially liked the use of attention to select what to update, which is a good prior to have. However, the results presented unfortunately seem to fall a bit short in this current version, and some decisions might have had too much of an effect on some of these shortcomings. Given some improvements, this work might become quite promising, but for the time being I am leaning against publication. 1. Most modeling decisions are clear and well-motivated, however the choice to make the transition model f_trans always be applied only “slot-wise” might be too restrictive. Indeed, for a given action, this means that 2 slots have to independently learn the effect of that action (e.g. as shown in the example in Figure 2. right), and that some interactions are ~impossible to learn (e.g. in Sokoban, pushing a box requires knowing about the location of both the agent and the box). This could have been alleviated if the transition had access to “interactions outcomes” (e.g. if using a GraphNet, or in your model, if \tilde{s} was provided to the transition function f_theta). Other works (including Zambaldi et al 2018, which is cited several times), handle this appropriately.?Did you try to provide \tilde{s} to the transformation operator (e.g. in Figure 7 left)? 2. Adding a direct comparison to pure non-slotted versions of the model/baselines would have been quite useful, as currently it is unclear why certain things are failing. 3. Similarly, finding what was the output of the CNN encoder for pixel inputs was a bit too difficult. It is explained in the Appendix that one maps into 4x4 feature maps, but that might be too large for the current environments? Indeed for Sokoban, this means that any grid is partially supported by several “slots”, which may hurt the results more than they should (especially combined with the slot-wise transition constraints expressed above). 4. The early state of the current results are quite visible in all examples of the “Separate training” model predictions (Figure 3, 5, 8, 9, 10 and 11). None of these actually show this model performing a “correct” prediction for t+1? They only predict no changes, or nonsensical interpolations… This is not sufficient to try to make an argument about the “joint training” helping, and most discussions about “what information they contain” is strenuous at best. 5. The paper keeps mentioning that it “implicitly imposes transitions to be sparse”, however it is never explained how that would come about? I understand that the softmax in the self-attention may tend to become “peaky” and hence only affect a few slots (and the results do seem to confirm this observation), but I was expecting an explicit loss to enforce this fact. The current emphasis seems a bit ill-funded, so I would present more evidence to it or downplay it. 6. Some of the results shown seem hard to interpret or provide only weak evidence for the proposed model: a. Figure 2. Left does seem to indicate a benefit in using the self-attention module, but it is hard to know how much of an effect the gap between the orange and red curves actually imply. This figure is overall a bit too small to interpret, and it might be better to split the 2 conditions into sub-plots. The names of the curves in the legend do not correspond to anything described in the text/caption (but I could understand them…).?I was expecting more discussion of the results in Figure 2, for example at the end of Section 4.1. b. As explained above, Figure 3 only shows that the Joint Training can perform a 1-step prediction, which is ok but is the bare minimum.?Does it handle multi-step rollouts? c. Figure 4 does not seem to provide any significant results or insights. I would interpret them as showing no significant difference between the curves, and they are too small to extract any information out of them. I would remove this figure fully. d. Figure 5 is unclear about what f_k=0 really is, and once again just shows that Separate training does nothing. I expected it to be exactly reconstructing s_t (given that the others are trying to predict s_t+1)? But this is only the case for the joint training, and without knowing what x_t actually was, it is hard to trust. The fact that f_k changes what it does as it is being increased makes the whole point hard to interpret. 7. Figure 6 was quite interesting, and I feel like this could be pushed forward in a quite interesting manner. The choice of “maximizing the number of entities selected” was fair as a first try, but I could imagine it failing to generalize to more complex environments, or to be easily exploited if one ever decides to pass gradients back into the representation from the policy.?It was unfortunate that the legends do not correspond to anything expressed in the main text or caption (e.g. why do you not reuse the “valid_move”, …, ”blocked_push” names that you thoroughly introduce?). What is “random_XX”??Could you comment on why the curves seem to have a large increase in variance along the 80000-100000 updates region? Details: 8. Figure 2 (right) was good to explain how the model worked, but I would actually change its location and try to move it into Figure 1. Similarly, Figure 7 in the Appendix seemed rather necessary to understand the model, and belongs in the main text in my opinion. 9. When presenting the self-attention block, it would be good to directly state that these are MLPs receiving [s_t, a_t] (as done in the Appendix). 10. Is sigma^2 in the decoder fixed? To which value? 11. When presenting the “sparse” and “full” settings, having access to Figure 7 and the rest of the Appendix might be beneficial, it would be good to point forward to it."}
{"id": "iclr2020_566", "title": "A Latent Morphology Model for Open-Vocabulary Neural Machine Translation | OpenReview", "abstract": "Abstract:###Translation into morphologically-rich languages challenges neural machine translation (NMT) models with extremely sparse vocabularies where atomic treatment of surface forms is unrealistic. This problem is typically addressed by either pre-processing words into subword units or performing translation directly at the level of characters. The former is based on word segmentation algorithms optimized using corpus-level statistics with no regard to the translation task. The latter learns directly from translation data but requires rather deep architectures. In this paper, we propose to translate words by modeling word formation through a hierarchical latent variable model which mimics the process of morphological inflection. Our model generates words one character at a time by composing two latent representations: a continuous one, aimed at capturing the lexical semantics, and a set of (approximately) discrete features, aimed at capturing the morphosyntactic function, which are shared among different surface forms. Our model achieves better accuracy in translation into three morphologically-rich languages than conventional open-vocabulary NMT methods, while also demonstrating a better generalization capacity under low to mid-resource settings.", "review": "Review:###This paper proposes a method, Latent Morphology Model (LMM), for producing word representations for a (hierarchical) character-level decoder used in neural machine translation (NMT). The main motivation is to overcome vocabulary sparsity or highly inflectional languages such as Arabic, Czech, and Turkish (experimented in the paper). To model the morphological inflection, they decouple lemmas and inflection types into 2 latent variables (z and f) where f is enforced to be sparse (arguably mimic the process of the human). The literature review of NMT and the discussion on the potential advantage of morphology are concise. The proposed model is a variation of Luong & Manning (2016) and Schulz et al (2018) models, thus, their main contribution is an introduction of the latent morphological features to the decoder. The proposed LMM is trained by sampling z and f from prior directly, and the sparsity of the morphological features is encouraged by L0 of the feature vector (parameterized as independent Kumaraswamy variables). They perform the main empirical study using 3 languages by translating from English to justify the proposed LMM. Lastly, they provide a quantitative analysis of the perplexities of unseen words and a qualitative on words generated of a lemma with different feature vectors. Overall, this paper could provide a novel insight into the role of modeling morphology as latent variables. However, the experiments and analyses do not sufficiently support the claim of mimicking the process of the inflection (besides the gain in performance). Some clarification would be appreciated. In section 3, the model, to my understanding, is agnostic to the morphological inflection, except the sparsity regularization (i.e., it could model any transformation including changing the lemma itself). In addition, there is nothing in the training particularly specific about the morphology. For example, z and f are generated from only the prior whereas we could get more accurate posteriors from observing the word itself. Some background on the language might help motivate the choice of model. Are morphological inflections ambiguous? Are the morphology labels hard to obtain? I think more discussion on previous attempts to model morphology (e.g., Vylomova et al 2017, Passban et al 2018) will be very helpful to the readers. For the experiments (section 4), the model with LMM is shown to consistently outperform character and hierarchical models. The multi-domain performance also shows the model*s ability to tackle a larger vocabulary set. However, we cannot certainly conclude that the gain comes from LMM successfully modeling the inflections. A contrast of performance with less morphological languages might reveal some insight (unfortunately I do not have enough knowledge to recommend languages). Finally, the feature variation analysis is interesting, but we only see one lemma from one language. Further discussion such as the consistency of features and the types of morphology, or the similarity of the lemma vector across context, will be helpful. Minor Questions: 1. Given a word, how ambiguous it is to determine the stem and the morphological type in the subject languages? 2. How do you compute char-level PPL of the subword model? 3. In 4.4.4, did you obtain z of `go` from just translating `go` without any context?"}
{"id": "iclr2020_567", "title": "Risk Averse Value Expansion for Sample Efficient and Robust Policy Learning | OpenReview", "abstract": "Abstract:###Model-based Reinforcement Learning(RL) has shown great advantage in sample-efficiency, but suffers from poor asymptotic performance and high inference cost. A promising direction is to combine model-based reinforcement learning with model-free reinforcement learning, such as model-based value expansion(MVE). However, the previous methods do not take into account the stochastic character of the environment, thus still suffers from higher function approximation errors. As a result, they tend to fall behind the best model-free algorithms in some challenging scenarios. We propose a novel Hybrid-RL method, which is developed from MVE, namely the Risk Averse Value Expansion(RAVE). In the proposed method, we use an ensemble of probabilistic models for environment modeling to generate imaginative rollouts, based on which we further introduce the aversion of risks by seeking the lower confidence bound of the estimation. Experiments on different environments including MuJoCo and robo-school show that RAVE yields state-of-the-art performance. Also we found that it greatly prevented some catastrophic consequences such as falling down and thus reduced the variance of the rewards.", "review": "Review:###This paper proposes a novel deep reinforcement learning algorithm at the intersection of model-based and model-free reinforcement learning: Risk Averse Value Expansion (RAVE). Overall, this work represents a significant but incremental step forwards for this *hybrid*-RL class of algorithms. However, the paper itself has significant weaknesses in its writing, analysis, and presentation of ideas. The main strength of this work is its empirical section. The proposed algorithm is fairly compared against many relevant baselines on a variety of continuous control tasks, on both the Mujoco and Roboschool simulators, and demonstrates significant performance increases across all of them. The visualization in Figure 4 is interesting, and provides good insight into the issues with DDPG and STEVE, and the reasons for the success of RAME. Based on these results, the authors have convinced that RAME is a state-of-the-art algorithm. However, in spite of its strong performance on benchmarks, I believe that this paper needs a significant overhaul/rewrite before publication. One major criticism I have is on the authors* treatment of the different types of uncertainty. The introductory sections spend a large amount of time on the differences between aleatoric and epistemic uncertainty, and various other related concepts. But when it comes to the actual new algorithm, that the authors only barely touched upon the (very important) point that V[Q^DVE] is actually a *mix* of epistemic and aleatoric uncertainties. When minimizing this term, it*s not clear whether epistemic or aleatoric uncertainty is actually being reduced. (Based on the experiments, which report only expected values, not CVaR or anything of the sort, it seems the authors care only about reducing epistemic uncertainty; if so, it*s unclear why they choose to minimize a term which includes aleatoric uncertainty too.) A more principled understanding of this quantity seems essential to this line of work. Similarly, *risk* typically refers to aleatoric uncertainty, but the risk-senstive component of RAME computes the confidence lower bound w.r.t. a mix of aleatoric and epistemic uncertainty. Calling this algorithm *risk sensitive* is likely to generate confusion, in my opinion. A few other points on presentation of ideas: - Switching from a deterministic to a stochastic model is a trivial extension of MVE/STEVE, and way too much time is spent on this point (even going so far as to name a new algorithm!). Section 4 contains no new insight, beyond *deterministic models can be bad in stochastic environments*, which is obvious. Consider re-thinking your experiments for this section to help readers better understand *what* goes wrong. - In my opinion, RAME is as much a successor to TD3 as it is to STEVE. Taking the lower outcome of an ensemble of size 2 is equivalent to applying a penalty based on the stdev of the outcome distribution, with ?=1. The introduction of the paper should be re-written to clearly point out that that RAME ~= TD3 + STEVE. - I*d also like to see a lot more ablations, on at least a few environments. There are at least four factors that need to be teased apart: 1) deterministic vs stochastic model 2) lower confidence bound penalty 3) adaptive ? for LCBP 4) STEVE reweighting. Does RAME require all of these elements to perform well? How do these elements interact? Some feedback on the writing: - There are various small factual errors. For example, in the very first paragraph, the Dyna algorithm is attributed to Kuturach et al, instead of Sutton (1990). (The algorithm from Kuturach is ME-TRPO.) - The notation is not very good; there are lots of symbols flying around everywhere, and it makes the ideas (which are fundamentally very simple) a bit difficult to parse. For example, naming each parameter-set individually, everywhere, is unnecessary. - There are some strange non-sequiturs and overall lack of flow. I think that this work has a lot of potential, and am especially impressed by the empirical results. I recommend rejection in its current form, but hope to see a revised version of this work appear at a conference in the near future."}
{"id": "iclr2020_568", "title": "Pipelined Training with Stale Weights of Deep Convolutional Neural Networks | OpenReview", "abstract": "Abstract:###The growth in the complexity of Convolutional Neural Networks (CNNs) is increasing interest in partitioning a network across multiple accelerators during training and pipelining the backpropagation computations over the accelerators. Existing approaches avoid or limit the use of stale weights through techniques such as micro-batching or weight stashing. These techniques either underutilize of accelerators or increase memory footprint. We explore the impact of stale weights on the statistical efficiency and performance in a pipelined backpropagation scheme that maximizes accelerator utilization and keeps memory overhead modest. We use 4 CNNs (LeNet-5, AlexNet, VGG and ResNet) and show that when pipelining is limited to early layers in a network, training with stale weights converges and results in models with comparable inference accuracies to those resulting from non-pipelined training on MNIST and CIFAR-10 datasets; a drop in accuracy of 0.4%, 4%, 0.83% and 1.45% for the 4 networks, respectively. However, when pipelining is deeper in the network, inference accuracies drop significantly. We propose combining pipelined and non-pipelined training in a hybrid scheme to address this drop. We demonstrate the implementation and performance of our pipelined backpropagation in PyTorch on 2 GPUs using ResNet, achieving speedups of up to 1.8X over a 1-GPU baseline, with a small drop in inference accuracy.", "review": "Review:###The paper proposed a new pipelined training strategy to fully utilize the memory and computational power to speed up the training process. In order to overcome the generalization degradation of the proposed method, the authors further introduced the so-called hybrid method to combine their proposed pipelined method and normal training. The pipelined method is interesting. For the pipelined process itself, it is similar to model parallelization. For the method proposed by the paper, it is like the async-SGD method. The paper merged these two ideas together but did not solve the problem from async-SGD, i.e. with a large number of processes, the generalization performance degrades (in the paper, it is so-called *stages*). Even with the hybrid method, the accuracy still drops. Also, the sentence, *We demonstrate the implementation and performance of our pipelined backpropagation in PyTorch on 2 GPUs using ResNet, achieving speedups of up to 1.8X over a 1-GPU baseline, with a small drop in inference accuracy.*, is confusing. If I use data parallelization, the gain should be also around 2. The ResNet on Cifar-10 results are not convincing. The normal accuracy of ResNet20 on Cifar-10 is around 92 but the paper reported 91.1%. Based on this, I think the paper has some room for improvement."}
{"id": "iclr2020_569", "title": "Span Recovery for Deep Neural Networks with Applications to Input Obfuscation | OpenReview", "abstract": "Abstract:###The tremendous success of deep neural networks has motivated the need to better understand the fundamental properties of these networks, but many of the theoretical results proposed have only been for shallow networks. In this paper, we study an important primitive for understanding the meaningful input space of a deep network: \textit{span recovery}. For , let be the innermost weight matrix of an arbitrary feed forward neural network , so can be written as , for some network . The goal is then to recover the row span of given only oracle access to the value of . We show that if is a multi-layered network with ReLU activation functions, then partial recovery is possible: namely, we can provably recover linearly independent vectors in the row span of using non-adaptive queries to . Furthermore, if has differentiable activation functions, we demonstrate that \textit{full} span recovery is possible even when the output is first passed through a sign or thresholding function; in this case our algorithm is adaptive. Empirically, we confirm that full span recovery is not always possible, but only for unrealistically thin layers. For reasonably wide networks, we obtain full span recovery on both random networks and networks trained on MNIST data. Furthermore, we demonstrate the utility of span recovery as an attack by inducing neural networks to misclassify data obfuscated by controlled random noise as sensical inputs.", "review": "Review:###** Summary The paper studies how to recover the span of a NN from a limited number of queries. The problem belongs to the general question of how to reconstruct functions from black-box interaction and it may find application in obfuscation attacks where very large perturbations of the input do not affect the output. The main contribution of the paper is on the theoretical analysis of a simple non-adaptive and a more sophisticated adaptive algorithm. The main finding is that under mild conditions on the structure of the NN partial recovery is possible. The empirical validation show that in practice, it is often the case the full span recover is actually possible, as the structure and weights of common NN are *friendly* enough. ** Evaluation While the content of the paper lies a bit off my expertise, my impression is that this is a solid technical and theoretical contribution. Detailed comments: 1- Theoretical results: The properties proved in Thm.3.4 and 4.3 are quite powerful, showing that (partial/approximate) span recovery is possible with a relatively small amount of samples (of order of n*k, where n is the original input size and k is the size of the span), in a computationally efficient way (in particular for the non-adaptive algorithm), and for Relu NN or NN with differentiable layers and final threshold function. The main question is of course the validity of the assumptions needed to prove the theorems. Asm.1 and 2 are overall reasonable and they are very well supported by Lemma 3.1. The first two assumptions in page 6 are straightforward, while I*ll less convinced of 3 and 4. In fact, they need to hold for any subspace V of any dimension smaller than k. I wonder whether the assumptions may become less and less likely as the size of the subspace decrease. 2- The theorems and the paper are mostly well written but some parts may be clearer. 3- Alg.1: the computation of the gradient is never really explained apart from the high-level lemma 3.2. While an actual algorithm is reported in the appendix, it would be better to have it explained already in the main text. 4- Right after Lemma 3.2 it is said *which demonstrates the claim*. I am not sure which claim it refers to. 5- In alg.1 there is a parameter r which defines the number of queries of the algorithm. Thm 3.4 provides an upper bound on the number of queries needed and it does depend on k. Since k is initially unknown, how do you actually parameterize the algorithm? is there a stopping condition that can be tested? 6- In Thm 3.4 it is said that the algorithm returns the subspace V in time that is polynomial in the main parameters of the problem. Yet, I*m not sure where such complexity comes from. In Alg.1 it seems like the subspace is the direct output of the algorithm, so the complexity is r times the cost of computing the gradient, which according to Lem3.2. is poly(n). Is this the way you finally obtain the complexity? 7- One thing I*m doubtful about is the fact that the result in Thm 3.4 seems to be independent from the depth d and width k_i of the different layers. Some conditions may be implicit in the Asm.1 and 2, though. Furthermore, in the experiments it is clearly showed that thin NNs may make the support not recoverable. Could you please make such limit more explicit in the theory? 8- In alg.4 I think lines 5-7 are just the way to execute line 4. Is that correct? If not, how do you execute line 4? 9- In alg.4 line 8 and 9 are not easy to follow and they are not really discussed in the main text. Could you please clarify? 10- The empirical validation is relatively simple but it illustrates quite well the theory. Still I wish the authors could report results that dig more in detail in the theoretical results showing how tight they are (e.g., in the dependency on n, k, and other factors). The current results provide just a hint on how accurate/informative the theory is. 11- In the empirical result, it would be great to have a much more thorough validation of the difference between the non-adaptive and the adaptive algorithms. In the current results it seems like there is very limited difference."}
{"id": "iclr2020_570", "title": "Understanding and Stabilizing GANs* Training Dynamics with Control Theory | OpenReview", "abstract": "Abstract:###Generative adversarial networks~(GANs) have made significant progress on realistic image generation but often suffer from instability during the training process. Most previous analyses mainly focus on the equilibrium that GANs achieve, whereas a gap exists between such theoretical analyses and practical implementations, where it is the training dynamics that plays a vital role in the convergence and stability of GANs. In this paper, we directly model the dynamics of GANs and adopt the control theory to understand and stabilize it. Specifically, we interpret the training process of various GANs as certain types of dynamics in a unified perspective of control theory which enables us to model the stability and convergence easily. Borrowed from control theory, we adopt the widely-used negative feedback control to stabilize the training dynamics, which can be considered as an regularization on the output of the discriminator. We empirically verify our method on both synthetic data and natural image datasets. The results demonstrate that our method can stabilize the training dynamics as well as converge better than baselines.", "review": "Review:###This paper proposes a novel view for stabilising GANs from the perspective of control theory. This view provides new insights into GAN training and may inspire future research along this direction. This paper is overall well written, with a smooth introduction of background material that might be less familiar for machine learning researchers. There are places that need further clarification, but I think the proposed direction is promising. Questions about the method: - Since the proposed method starts from Laplace transform, it would be helpful to further discuss the connection between other methods that regularises the eigenvalues of the Jacobian (such as spectral-normalisation), which work in the frequency domain from a different perspective. For example, could the proposed regulariser be interpreted as imposing certain constraint on the spectrum of Jacobian? - Does section 2.2 depend on the assumption of linear dynamics? - Does the E in eq.7 come from eq. 4? - Could you give some intuition for the paragraph above section 3.4, about the different form of inputs when treating D and G as dynamics? For consistency, it is perhaps better to keep the dependency of p_D and p_G on x explicit (same for eq. 10), unless this is intended? - My main concern about the analysis is that it shows why several methods (e.g., momentum, multiple update steps) are *not* helpful for stabilising GANs, but does not tell why training with these methods, as well as others such as gradient penalty, *do converge* in practice with properly chosen hyper-parameters? Experiments: - About setup: the paper reports using ResNets for natural images as in Mescheder et al. (2018). However, Mescheder et al. (2018) uses DCGAN for CIFAR10, which raises further questions about the scores on this dataset: - The baseline scores of Reg-SGAN and Reg-WGAN seem to be worse than those reported in Mescheder et al. (2018), which have inception scores above 6 according to Figure 6 of their paper. In Figure 5 of this paper, they are clearly below 6. What’s the reason for this discrepancy?"}
{"id": "iclr2020_571", "title": "Neural Network Out-of-Distribution Detection for Regression Tasks | OpenReview", "abstract": "Abstract:###Neural network out-of-distribution (OOD) detection aims to identify when a model is unable to generalize to new inputs, either due to covariate shift or anomalous data. Most existing OOD methods only apply to classification tasks, as they assume a discrete set of possible predictions. In this paper, we propose a method for neural network OOD detection that can be applied to regression problems. We demonstrate that the hidden features for in-distribution data can be described by a highly concentrated, low dimensional distribution. Therefore, we can model these in-distribution features with an extremely simple generative model, such as a Gaussian mixture model (GMM) with 4 or fewer components. We demonstrate on several real-world benchmark data sets that GMM-based feature detection achieves state-of-the-art OOD detection results on several regression tasks. Moreover, this approach is simple to implement and computationally efficient.", "review": "Review:###===Summary=== The authors propose to perform out of distribution detection for regression models by fitting a generative model in the feature space of the regression model. An input example is deemed to be out-of-distribution if it has low likelihood under this generative model. ===Overall Assessment=== I recommend that the paper is rejected. There are a number of aspects that need to be improved. You should fix these and resubmit to a future conference. The paper focuses on the difference between regression and classification tasks and claims that the paper*s method addresses an unmet need for OOD for regression. However, both the proposed method and the analysis justifying it are generic enough to be applied to both regression and classification. The paper handles technical claims far too casually in sec 4 and does not provide sufficient justification that the claims are true. There are natural baselines, such as using a generative model on the raw input space, that are ignored. ===Comments=== Remark 1 feels to me like it was added for the sake of having more math in the paper, not because it is crucial to the paper*s argument. You remark at various places that existing methods don*t naturally generalize from classification to regression. However, you never fully explain why. Also, your proposed method can be applied out-of-the box to classification problems. Your analysis in sec 4 trivially applies to binary classification tasks, and could be naturally extended to multi-class classification where w is not a vector but a num_classes x num_features matrix. The parallel should be between classification and heteroskedastic regression, since there you have a distribution per example. The logic in *In-distribution features are intrinsically low dimensional* is insufficient The connection between section 4 and your proposed method is not particularly precise. You also have lots of technical claims in 4 that are unsubstantiated. For example, you write *this new network will likely have less discarded information than the shallower network*. What does *likely* mean? In what sense are you making an actual technical statement? Each of the subsections in sec 4 has similar issues. *The CNNs are pre-trained on ImageNet (Denget al., 2009) and the last layer is replaced with a linear layer that produces a single output.* Why did you do this? Did you fine tune or just retrain the top layer? *For these two baselines, the variance of the forward passes is used as a metric for detecting OOD inputs* Can you explain why these are reasonable baselines for OOD? Why no baseline that fits a generative model in input space? You should cite Ren et al. *Likelihood Ratios for Out-of-Distribution Detection*"}
{"id": "iclr2020_572", "title": "Towards Understanding the Spectral Bias of Deep Learning | OpenReview", "abstract": "Abstract:###An intriguing phenomenon observed during training neural networks is the spectral bias, where neural networks are biased towards learning less complex functions. The priority of learning functions with low complexity might be at the core of explaining generalization ability of neural network, and certain efforts have been made to provide theoretical explanation for spectral bias. However, there is still no satisfying theoretical results justifying the existence of spectral bias. In this work, we give a comprehensive and rigorous explanation for spectral bias and relate it with the neural tangent kernel function proposed in recent work. We prove that the training process of neural networks can be decomposed along different directions defined by the eigenfunctions of the neural tangent kernel, where each direction has its own convergence rate and the rate is determined by the corresponding eigenvalue. We then provide a case study when the input data is uniformly distributed over the unit shpere, and show that lower degree spherical harmonics are easier to be learned by over-parameterized neural networks.", "review": "Review:###I must qualify my review by stating that I am not an expert in kernel methods, and the mathematics in the proof is more advanced than I typically use. So it is possible that there are technical flaws to this work that I did not notice. That being said, I found this to be quite an interesting paper. It provides a concise explanation for the types of features learned by ANNs: those that correspond to the largest eigenvalues of the kernel function. Because these typically correspond to the lowest-frequency components, this means that the ANNs tend to first learn the low frequency components of their target functions. This provides a nice explanation for how ANNs can both: a) have enough capacity to memorize random data; yet b) generalize fairly well in many tasks with structured input data. In the case of structured data, there are low frequency components that correspond to successfully generalized solutions. I have a few questions about the generality of this result, and its application to make better machine learning systems: 1) As far as I can tell, the proof applies strictly vanilla SGD (algorithm 1). Would it be possible to extend this proof to other optimizers (say, ADAM)? That extension would help to connect this theory to the practical side of the field. 2) Given that the kernel depends on the loss function, and it*s the eigenspectrum of the kernel*s integrator operator that determines the convergence properties, can this work be applied to engineering better loss functions for practical applications?"}
{"id": "iclr2020_573", "title": "Bayesian Variational Autoencoders for Unsupervised Out-of-Distribution Detection | OpenReview", "abstract": "Abstract:###Despite their successes, deep neural networks still make unreliable predictions when faced with test data drawn from a distribution different to that of the training data, constituting a major problem for AI safety. While this motivated a recent surge in interest in developing methods to detect such out-of-distribution (OoD) inputs, a robust solution is still lacking. We propose a new probabilistic, unsupervised approach to this problem based on a Bayesian variational autoencoder model, which estimates a full posterior distribution over the decoder parameters using stochastic gradient Markov chain Monte Carlo, instead of fitting a point estimate. We describe how information-theoretic measures based on this posterior can then be used to detect OoD data both in input space as well as in the model’s latent space. The effectiveness of our approach is empirically demonstrated.", "review": "Review:###After reading all the reviews, the comments, and the additional work done by the Authors, I have decided to confirm my rating. ================== This paper leverage probabilistic inference techniques to maintain a posterior distribution over the parameters of a variational autoencoder (VAE). This results in a Bayesian VAE (BVAE) model, where instead of fitting a point estimate of the decoder parameters via maximum likelihood, they estimate their posterior distribution using samples generated via stochastic gradient Markov chain Monte Carlo (MCMC). The informativeness of an unobserved input x* / latent z* is then quantified by measuring the (expected) change in the posterior over model parameters after having observed x* / z*. The motivation is clear, when considered inputs which are uninformative about the model parameters, they are likely similar to the data points already in the training set. In contrast, inputs which are very informative about the model parameters are likely different from everything in the training data. The contributions are: - A Bayesian VAE model which uses state-of-the-art Bayesian inference techniques to estimate a posterior distribution over the decoder parameters. - A description of how this model can be used to detect outliers both in input space and in the model’s latent space. - Results showing that this approach outperforms state-of-the-art outlier detection methods. The paper is well written, and the proposed ideas are well motivated. However, the experiment section is too limited. The authors should at least use one more dataset such as CIFAR10. They just use FashionMNIST vs MNIST FashionMNIST (held-out). In addition, it would strengthen the paper if the authors could show at least initial result about how the model performs to detect out of distribution in the latent space, given that it is considered as part of the contribution. The paper lacks some references such as: - Predictive uncertainty estimation via prior networks, NEURIPS 2018."}
{"id": "iclr2020_574", "title": "DeepV2D: Video to Depth with Differentiable Structure from Motion | OpenReview", "abstract": "Abstract:###We propose DeepV2D, an end-to-end deep learning architecture for predicting depth from video. DeepV2D combines the representation ability of neural networks with the geometric principles governing image formation. We compose a collection of classical geometric algorithms, which are converted into trainable modules and combined into an end-to-end differentiable architecture. DeepV2D interleaves two stages: motion estimation and depth estimation. During inference, motion and depth estimation are alternated and converge to accurate depth.", "review": "Review:###This work proposes a neural network architecture for joint depth and camera motion estimation on video sequences. The authors propose an architecture that incorporates classic principles from SfM, namely depth computation based on cost volumes and motion estimation based on the reprojection error of features. Extensive experiments on a variety of datasets are shown and support that this approach provides strong results. + Principled approach that marries the best aspects of deep learning with classic principles from multi-view geometry. + Well-written paper + Generalizes well + Significantly outperforms the state of the art + Clearly shows that more views help + Seems to be robust to initialization Question: What happens if more Gauss-Newton steps are made? Could one trade computation for quality here? Minor issues: - Equation (1), right: f_z should probably be f_x - Paragraph between Eq. (1) and (2): There seems to be something wrong with the typesetting of x^i=... (e.g. the equals sign) - Same for the paragraph before Eq. (1) and x=(u,v) Summary: This paper presents a well-engineered and non-trivial system that leverages principles from different fields in a very reasonable way. The results look great both qualitatively and quantitatively. The experiments are extensive and show a clear improvement over the state-of-the-art."}
{"id": "iclr2020_575", "title": "Lyceum: An efficient and scalable ecosystem for robot learning | OpenReview", "abstract": "Abstract:###We introduce Lyceum, a high-performance computational ecosystem for robotlearning. Lyceum is built on top of the Julia programming language and theMuJoCo physics simulator, combining the ease-of-use of a high-level program-ming language with the performance of native C. Lyceum is up to 10-20Xfaster compared to other popular abstractions like OpenAI’sGymand Deep-Mind’sdm-control. This substantially reduces training time for various re-inforcement learning algorithms; and is also fast enough to support real-timemodel predictive control with physics simulators. Lyceum has a straightfor-ward API and supports parallel computation across multiple cores or machines.The code base, tutorials, and demonstration videos can be found at: https://sites.google.com/view/lyceum-anon.", "review": "Review:###This paper introduces a new software system for robot learning. The emphasis of the paper is on scalability. The authors rightly identify that many of today*s advances in robot learning are out of reach of many groups due to the sheer scale of the computing infrastructure needed to generate results. The key novelty of the new approach is in switching to the Julia programming language which offers the easy of a high level programming language with the speed of a native language. The authors offer bindings to influential robotic libraries and demo some existing RL algo*s in their library. I have a few concerns with accepting this paper in a conference: - The authors don*t go into a lot of depth on why existing robotics results require so much resources. The examples they give (lots of cores, lots of GPU*s) suggest that not the *glue* code is the main bottleneck but rather the float-compute heavy parts. I think the paper would be a lot stronger if the authors could do a detailed analysis of existing RL experiments and highlight which bits of the computation they are focussed on speeding up. - Related, the experiment the authors show don*t address the problems which they quote in their motivation. A strong paper would motivate the need for a new ecosystem from a set of problems and then show that their new software solves those problems. - In the experiment section, I believe the authors should also think more carefully about comparing two pieces of software which are leading to a different result: figure 1 clearly shows that the results of OpenAI baseline and their approach lead to very different outcomes. In the description the authors hypothesize this is likely due to very different implementation (i.e. OpenAI baseline doing gradient clipping and many other things). If so, I don*t trust the experiment performance results as such. There is an interesting discussion to be had when a paper introducing a new piece of software warrants publication in the conference. I can think of a few reasons - A library has been validated through significant adoption (e.g. Tensorflow/PyTorch). - A library introduces a whole new paradigm/algorithm to solve an existing ML problem (e.g. low precision backprop). - A piece of software solves a practical problem (academic/industry) which democratizes an area (e.g. Dactyl on a moderately sized cluster). In this particular submission, I don*t see a key breakthrough of the sort above. The paper is well written and a good description of the new library. At this point, this paper feels more like a whitepaper about the new library than a scientific paper which claims to solve a problem in a new way. Minor: Typo in first sentence of conclusion *intoducing*"}
{"id": "iclr2020_576", "title": "Frontal low-rank random tensors for high-order feature representation | OpenReview", "abstract": "Abstract:###Representing high-order (second-order or higher) information in deep neural nets is essential in many tasks such as fine-grained visual understanding and multi-modal information fusion. Bilinear models are often used to extract second-order information. As a basis, extracting higher-order information requires extra computation. In this paper, we propose an approach to representing high-order information via a simple yet effective bilinear form. Specifically, our contribution is two-fold: (1) From the multilinear perspective, we derive a bilinear form of low complexity, assuming that the three-way tensor has low-rank frontal slices. (2) Rather than learning the tensor entries from data, we sample the entries from different underlying distributions, and prove that the underlying distribution influences the information order. We perform temporal action segmentation experiments to evaluate our method. The results demonstrate that our bilinear form, employed as intermediate layers in deep neural nets, is computationally efficient; meanwhile it is effective as it achieves new state-of-the-art results on public benchmarks.", "review": "Review:###This paper proposes a computationally efficient bilinear form to extract high-order information for visual information. The key idea is to assume the frontal slices of the projection matrix T to be low-rank. Also, it performs random sampling from a distribution to fill the entries of the project matrices E and F, making them random instead of learning from data. The effectiveness of this random projection is theoretically analyzed based on different distributions. Experimental study is conducted on action segmentation to compare the proposed scheme with the existing ones. This paper is well-organised and carefully presented. Nevertheless, with respect to the literature, the technical novelty (low-rank assumption and random matrix projection) seems to be limited. Moreover, the experimental study is not as strong as the expected. The improvement over the existing methods is not sufficiently signficant or consistent. For action recognition, only the results of GTEA in Table 2 are promising. Experimental study on other vision tasks and benchmark datasets shall to be conducted to provide more validation, for example, by using Fine-grained image recognition datasets which have been widely used to assess bilinear based methods."}
{"id": "iclr2020_577", "title": "Deflecting Adversarial Attacks | OpenReview", "abstract": "Abstract:###There has been an ongoing cycle where stronger detection mechanisms and defenses against adversarial attacks are subsequently broken by a more advanced defense-aware attack. We present a new approach, which we argue is a step towards ending this cycle by deflecting adversarial attacks, i.e., by forcing the attacker to produce an input which semantically resembles the attack*s target class. To this end, we first propose a stronger defense mechanism based on capsule networks which combines three detection mechanisms to achieve state-of-the-art detection performance on both standard and defense-aware attacks. We then show that undetected attacks against our defense are often classified as the adversarial target class by performing a human study where participants are asked to label the class of images produced by the attack. These attack images thus can no longer be called adversarial, as our network classifies them the same way as humans do.", "review": " Authors combine different adv detection schemes to detect adv examples. In cases that their approach has failed they perform a human study and conclude that those examples are misclassified by humans as well thus deflecting adv attacks. Although combining existing adversarial detection methods are interesting but I think the paper lacks novelty. Also the work is motivated by stating that stronger attacks can break the defenses that are not certified. But isn*t it the approach taken in this paper? How do we know other attacks would lead to the same empirical results as reported in the paper?"}
{"id": "iclr2020_578", "title": "EMS: End-to-End Model Search for Network Architecture, Pruning and Quantization | OpenReview", "abstract": "Abstract:###We present an end-to-end design methodology for efficient deep learning deployment. Unlike previous methods that separately optimize the neural network architecture, pruning policy, and quantization policy, we jointly optimize them in an end-to-end manner. To deal with the larger design space it brings, we train a quantization-aware accuracy predictor that fed to the evolutionary search to select the best fit. We first generate a large dataset of <NN architecture, ImageNet accuracy> pairs without training each architecture, but by sampling a unified supernet. Then we use these data to train an accuracy predictor without quantization, further using predictor-transfer technique to get the quantization-aware predictor, which reduces the amount of post-quantization fine-tuning time. Extensive experiments on ImageNet show the benefits of the end-to-end methodology: it maintains the same accuracy (75.1%) as ResNet34 float model while saving 2.2× BitOps comparing with the 8-bit model; we obtain the same level accuracy as MobileNetV2+HAQ while achieving 2×/1.3× latency/energy saving; the end-to-end optimization outperforms separate optimizations using ProxylessNAS+AMC+HAQ by 2.3% accuracy while reducing orders of magnitude GPU hours and CO2 emission.", "review": "Review:###This paper proposes to perform both neural architecture search and pruning+quantization under the same evolutionary search process. However, as it is computationally expensive to quantize and fine-tune a model and then measure the resulting accuracy, the authors propose to use a machine-learned predictor as a surrogate. I have two main concerns with this paper: First, I think the method is not really am end-to-end joint optimization approach as motivated in Eq 4. By using a machine-learned predictor to approximate the accuracy, there are no guarantees in this method. It is misleading to call it end-to-end joint optimization. Second, I think more details are needed to convince me to trust the results; currently I do not know if all the comparisons are fair. In particular: (a) It is difficult to discern how much computation is needed for this machine-learned predictor. The use of transfer as illustrated in Fig. 2 is a reasonable idea, but in practice how do we know if this predictor is good enough? How many samples do you actually used in the experiments, and how do you kow that number is sufficient? (b) The the paper makes claims that need to be backed up, e.g. in pg. 6: *In fact, we find that 80k data pairs is a suitable size to train a good full precision accuracy predictor.* Is this referring to this dataset, or in general? (c) Table 2 compares many methods, but it is not clear if they are apples-to-apples comparison, esp. in terms of search space and resource constraints. It is also not clear if all the computation is reported in the Search Cost and CO2 columns. (d) Sec 5.1 says *Our small model (Ours-B) can have 2.2% accuracy boost than mixed-precision MobileNetV2 search by HAQ (from 71.9% to 74.1%); our large model (Ours-C) attains better accuracy (from 74.6% to 75.1%) while only requires half of BitOps. How do you define the small model and the large model? Were they just two models that handpicked from the final population of evolutionary search? What criteria is used to pick them? (e) Figure 6 measure predictor accuracy, which is extremely pertinent. But it is not clear how the data (train and test) were created. Was this based on generating the quantized dataset, which is expensive?"}
{"id": "iclr2020_579", "title": "PNEN: Pyramid Non-Local Enhanced Networks | OpenReview", "abstract": "Abstract:###Existing neural networks proposed for low-level image processing tasks are usually implemented by stacking convolution layers with limited kernel size. Every convolution layer merely involves in context information from a small local neighborhood. More contextual features can be explored as more convolution layers are adopted. However it is difficult and costly to take full advantage of long-range dependencies. We employ non-local operation to build up connection between every pixel and all remain pixels. Moreover a novel emph{Pyramid Non-local Block} is devised to robustly estimate pairwise similarity coefficients between different scales of content patterns. Considering computation burden and memory consumption, we exploit embedding feature maps with coarser resolution to represent content patterns with larger spatial scale. Through elaborately combining the pyramid non-local blocks and dilated residual blocks, we set up a emph{Pyramid Non-local Enhanced Network} for edge-preserving image smoothing. It achieves state-of-the-art performance in imitating three classical image smoothing algorithms. Additionally, the pyramid non-local block can be directly incorporated into existing convolution neural networks for other image processing tasks. We integrate it into two state-of-the-art methods for image denoising and single image super-resolution respectively, achieving consistently improved performance.", "review": "Review:###In this work the authors propose the Pyramid Non-local Block (PNB) as a neural network module to estimate similarity coefficients between parts of an image. The PNB, unlike a convolution, can efficiently compute correlations between spatially distant parts of the input. In the pyramid non-local enhanced networks, the PNB is combined with standard dilated residual blocks. Applications of this new network are shown and evaluated on edge-preserving smoothing, image denoising and image restoration, where in all cases the evaluations show a small but consistent gain in PSRN compared to baseline models. The PNB is based on prior work proposing such non-local blocks, and the main innovation is the introduction of multi-scale processing in the form of a scale pyramid for the reference and embedding feature maps (obtained with strided convolutions with larger kernels). This makes it possible to compute correlations over larger spatial scales without paying the exponential computational cost. The paper cites prior work appropriately and lists the relevant hyperparameters used for training. The examples in the appendices show interesting visual improvements. As is, I believe this paper would be a better match for a more specialized venue (e.g. a computer vision conference). The main shortcomings of the paper are its limited novelty (multiscale pyramids are an old *trick* in computer vision), and the use of baselines that are behind SOTA for the discussed problems -- e.g. Dai el al, *Second-order Attention Network for Single Image Super-Resolution* could be a more current baseline for SISR (instead of MemNet) and Liu et al, *Non-Local Recurrent Network for Image Restoration* for denoising (which the authors cite and briefly discuss in the introduction). Also, in the RDN paper (which the authors used as a baseline for denoising) applications to superresolution are presented, with reported results better than the ones achieved with PNEN in Table 5. I found it surprising that a less well performing method (MemNet) was used as the baseline for the SISR task in the present work. For edge preserving smoothing, where the largest relative gains in evaluation scores are seen in the present work, the baselines and evaluation protocols seem to be less well established than for the remaining two tasks (the original paper using ResNets for this doesn*t use PSNR/SSIM for evaluation). It should also be noted that Zhu et al (2019) formed the ground truth set by asking for human preferences between results of existing algorithms, which makes the interpretation of the score gains less clear than in SISR and denoising where the actual target image is completely known. Suggestions for improvements: * The visual examples in the appendices do a good job of showing the improvements from using PNB. Are there any new failure modes that the baseline networks do not exhibit? * For superresolution evaluation, it would be more informative to list (at least in appendices) results for more downsampling factors than 3x (related work seems to examine 2x, 3x, 4x, and 8x). Similarly, for denoising, data for various noise levels would provide more insight into the behavior of the proposed model. * In Table 4, and 5, what are the averages computed over? * In Zhu et al (2019) which was used a ground truth for the edge-preserving smoothing experiments, each undistorted image is associated with multiple filtered results selected as preferred by human evaluators. How is this information incorporated into the PSNR/SSIM scores in Table 1?"}
{"id": "iclr2020_580", "title": "Deep Interaction Processes for Time-Evolving Graphs | OpenReview", "abstract": "Abstract:###Time-evolving graphs are ubiquitous such as online transactions on an e-commerce platform and user interactions on social networks. While neural approaches have been proposed for graph modeling, most of them focus on static graphs. In this paper we present a principled deep neural approach that models continuous time-evolving graphs at multiple time resolutions based on a temporal point process framework. To model the dependency between latent dynamic representations of each node, we define a mixture of temporal cascades in which a node*s neural representation depends on not only this node*s previous representations but also the previous representations of related nodes that have interacted with this node. We generalize LSTM on this temporal cascade mixture and introduce novel time gates to model time intervals between interactions. Furthermore, we introduce a selection mechanism that gives important nodes large influence in both hop subgraphs of nodes in an interaction. To capture temporal dependency at multiple time-resolutions, we stack our neural representations in several layers and fuse them based on attention. Based on the temporal point process framework, our approach can naturally handle growth (and shrinkage) of graph nodes and interactions, making it inductive. Experimental results on interaction prediction and classification tasks -- including a real-world financial application -- illustrate the effectiveness of the time gate, the selection and attention mechanisms of our approach, as well as its superior performance over the alternative approaches.", "review": "Review:###The paper focuses on the problem of modeling interaction processes over dynamically evolving graphs and perform inference tasks such future interaction prediction and interaction classification. Specifically, the paper proposes a temporal point process based formulation to model the interaction dynamics where the conditional intensity function is parameterized by a recurrent network. With an occurrence of any event, the recurrent architecture updates the embeddings of the nodes involved in that event which then affects the intensity function and hence the likelihood of future events. The paper uses intensity based likelihood to train for future interaction prediction task while cross-entropy based loss for classification task. The paper demonstrates the efficacy of the method through experiments across multiple datasets and compare against representative baselines and further provides ablation analysis for the proposed architecture. The paper demonstrates markedly improved empirical performance on multiple datasets and also performs the task of interaction classification which is not seen in recent works on evolving graphs, which are plus points. However, there are several concerns with the overall work that makes this paper weaker: (1) The main concern is with the novelty and more importantly the justification/analysis of the contributions proposed approach. (2) Further, while the ablation study provides some insights into architecture, it is not adequate (3) The paper misses comparison with a very important and recently proposed baseline, JODIE [1]. Main Comments: -------------- - The paper leverages existing techniques built for learning over evolving graphs and augments it with three modifications: explicit use LSTM with time gate, stacked LSTM approach with fusion (Aggregation) and attention mechanism to select important neighbors to contribute to embedding update. The use of LSTM with time gate and fusion mechanism is very incremental contribution. The attention mechanism proposed here is novel compared to existing works. However, there is very little justification or analysis provide or either of the contributions. This is big drawback of this contribution. - For instance, the authors mention that stacked LSTM is used to capture multiple resolution. Can they provide some analysis or empirical demonstration that this actually happens? Also, the authors mention they use K in range of {1,2,3,4} but do not provide details what was useful for each dataset and how is it useful. How does the scaling parameter and alpha affect the performance and what are their roles? Also, what does superscript *task* signify? Similarly, they propose coattention mechanism with adaptive gate functions but does not provide any analysis of why they are useful and what characteristics they capture in the data that allows it to select most relevant neighbors. Is the attention mechanism temporally dependent? - The authors perform ablation studies by switching off each component as a whole but considering the way this architecture is built, this is not a very useful exercise except knowing that each component contributes to the performance. A more detailed analysis and ablation is required. For instance, can the authors show performance with different K and how it deteriorates/improves with it? Also, for stacked LSTM case, the authors show what happens when you use last layer, but what happens if the authors use only one layer (I guess this is K=1?) or don*t use residual connections? When the time gate is switched off, does the authors also remove deltas from intensity function? what happens in this scenario? How does subgraph depth affect the quality of performance? What happens if authors don*t sue adaptive gate functions? - Figure 2 shows an example of bipartite graph, however, it seems datasets in experiments does have non-bipartite case? Is this true or the method only works for bipartite case? - The use of proposed Algorithm 2 is not well justified. Why does the author need coloring and hashing mechanism instead of simpler BFS/randomwalk routine to collect previous interactions? Also, is this subgraph created for each event or it is computed offline during training? Further, the subgraph used for selection mechanism same as subgraph used for backtracking in LSTM? - Further, is it true that the training is done in order of ColorGraphSeq or is it done in order of dataset? How does the authors capture dependencies across dataset in later case? - Do you also update cell states with selection mechanism? The DIP-UNIT equation in selection section does not show that update. Also, are the embeddings updated only during train or also during validation/evaluation? - The authors only present the results as-is without any insights on the performance of DIP model vs others and why they are able to demonstrate good performance. It is highly desired that authors add discussion section for each set of results to provide such information - The authors include support for new nodes for interaction classification task but remove them for interaction prediction task which is strange. Is there a specific reason for this? What is the effect on the performance if new nodes are allowed in test? Further, why is interaction classification not compared with temporal baselines? All baselines produce embeddings and the authors mention that classification for this paper is independent of marker history. While the temporal baselines do not train for the task, the authors can train a second stage classifier with learned embeddings to perform classification - The authors do not compare with recently proposed JODIE [1] which is a big miss. The comparison is required as it also models interaction processes in a novel way by actually predicting the next embedding directly instead of modeling the intensity. An empirical comparison and discussion of this method is required to compare with various state-of-art methods. Minor: ------- - The authors need to use better and consistent notations. Also, as the overall approach uses similar flow as previous papers such as DeepCoevolve, it is recommended that the authors make the presentation simpler to position it clearly with existing works. On page 3, section 3.2 both bold-face and normal letters are used as vectors. Is a vector? - Please provide numbers to equations for better referencing [1] Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks, Kumar et. al. KDD 2019"}
{"id": "iclr2020_581", "title": "Stable Rank Normalization for Improved Generalization in Neural Networks and GANs | OpenReview", "abstract": "Abstract:###Exciting new work on generalization bounds for neural networks (NN) given by Bartlett et al. (2017); Neyshabur et al. (2018) closely depend on two parameter- dependant quantities: the Lipschitz constant upper bound and the stable rank (a softer version of rank). Even though these bounds typically have minimal practical utility, they facilitate questions on whether controlling such quantities together could improve the generalization behaviour of NNs in practice. To this end, we propose stable rank normalization (SRN), a novel, provably optimal, and computationally efficient weight-normalization scheme which minimizes the stable rank of a linear operator. Surprisingly we find that SRN, despite being non-convex, can be shown to have a unique optimal solution. We provide extensive analyses across a wide variety of NNs (DenseNet, WideResNet, ResNet, Alexnet, VGG), where applying SRN to their linear layers leads to improved classification accuracy, while simultaneously showing improvements in genealization, evaluated empirically using—(a) shattering experiments (Zhang et al., 2016); and (b) three measures of sample complexity by Bartlett et al. (2017), Neyshabur et al. (2018), & Wei & Ma. Additionally, we show that, when applied to the discriminator of GANs, it improves Inception, FID, and Neural divergence scores, while learning mappings with low empirical Lipschitz constant.", "review": "Review:###While spectral normalization is often used to improve generalization by directly bounding the Lipschitz constant of linear layers, recent works have highlighted alternate methods that aim to reduce generalization error. This paper shows how to implement these *stable rank* normalizations with little computational overhead. The authors then apply the method to a wide variety of classification and GAN problems to show the benefits of stable rank normalization. This is a good paper and can be accepted. The added value comes from their Thm. 1, where they detail precisely how to project a real matrix onto one of lower srank while preserving the largest k eigenvalues. The spectral preservation k seems to be a new feature of their method. Full proofs and additional results are provided in appendices. There seems to be enough information to implement the described methods. The paper is carefully written and introductory sections do a great job of putting the problem in perspective. Very few typos (*calssification*, run a spell check). --------- Fun to think about ------ here are some extra comments Some related older introductory approaches could also be quickly mentioned: - linear layers represented as *bottlenecks* to enforce low rank explicitly - or solving in manifold of reduced-rank matrices directly For simplicity, they target the same srank r=c*min(m,n) for all layers, even though only the sum of sranks is important. For CNNs with only a few linear layers is there any observable difference by lightly deviating from this? Does the first linear layer typically contribute the lion*s share to the sum of sranks? It is interesting that by only addressing the linear layers of deep CNNs they are able to see consistent improvements. [i.e. 3 linear layers after 101 CNN layers]. This makes me wonder whether future work will also address how *stable rank* concepts might be extended to the convolutional layers. As a starting point, spectral values of the block-circulant matrices corresponding to convolutions have been described [ Sedghi et al. *Singular Values of Convolutional Layers* ]."}
{"id": "iclr2020_582", "title": "Intensity-Free Learning of Temporal Point Processes | OpenReview", "abstract": "Abstract:###Temporal point processes are the dominant paradigm for modeling sequences of events happening at irregular intervals. The standard way of learning in such models is by estimating the conditional intensity function. However, parameterizing the intensity function usually incurs several trade-offs. We show how to overcome the limitations of intensity-based approaches by directly modeling the conditional distribution of inter-event times. We draw on the literature on normalizing flows to design models that are flexible and efficient. We additionally propose a simple mixture model that matches the flexibility of flow-based models, but also permits sampling and computing moments in closed form. The proposed models achieve state-of-the-art performance in standard prediction tasks and are suitable for novel applications, such as learning sequence embeddings and imputing missing data.", "review": "Review:###The paper proposes to directly model the (conditional) inter-event intervals in a temporal point process, and demonstrates two different ways of parametrizing this distribution, one via normalizing flows and another via a log-normal mixture model. To increase the expressiveness of the resulting model, the parameters of these conditional distributions are made to be dependent on histories and additional input features through a RNN network, updating at discrete event occurrences. The paper is very well written and easy to follow. I also like the fact that it is probably among the first of those trying to integrate neural networks into TPPs to look at directly modeling inter-event intervals, which offers a different perspective and potentially also opens doors for many new methods to come. I have just three comments/questions. 1. The log-normal mixture model has a hyper-parameter K. Similarly, DSFlow also has K, and SOSFlow has K and R. How are these hyper-parameters selected? I don*t seem to find any explanation in the paper (not even in appendix F.1)? 2. To better demonstrate that a more complicated (e.g. multi-modal) inter-event interval distribution is necessary and can really help with data modeling, I*d be interested to see e.g. those different interval distributions (learnt from different models) being plotted against each other (sth. similar to Figure 8, but with actual learnt distributions), and preferably with some meaningful explanations as to e.g. how the additional modes capture or reflect what we know about the data. 3. Even though the current paper mainly focuses on inter-event interval prediction, I think it*s still helpful to also report the model*s prediction accuracy on marks in a MTPP. The *Total NLL* in Table 5 is one step towards that, but a separate performance metric on mark prediction alone would have been even clearer."}
{"id": "iclr2020_583", "title": "XLDA: Cross-Lingual Data Augmentation for Natural Language Inference and Question Answering | OpenReview", "abstract": "Abstract:###While natural language processing systems often focus on a single language, multilingual transfer learning has the potential to improve performance, especially for low-resource languages. We introduce XLDA, cross-lingual data augmentation, a method that replaces a segment of the input text with its translation in another language. XLDA enhances performance of all 14 tested languages of the cross-lingual natural language inference (XNLI) benchmark. With improvements of up to 4.8, training with XLDA achieves state-of-the-art performance for Greek, Turkish, and Urdu. XLDA is in contrast to, and performs markedly better than, a more naive approach that aggregates examples in various languages in a way that each example is solely in one language. On the SQuAD question answering task, we see that XLDA provides a 1.0 performance increase on the English evaluation set. Comprehensive experiments suggest that most languages are effective as cross-lingual augmentors, that XLDA is robust to a wide range of translation quality, and that XLDA is even more effective for randomly initialized models than for pretrained models.", "review": " The paper provides an analysis of a cross-lingual data augmentation technique dubbed XLDA, which consists of replacing parts of an input text with its translation in another language. Building on the mBERT approach, the authors show that at fine-tuning time it is beneficial to augment the training set of XNLI with cross-lingual hypotheses and premises instead of in-language pairs. For each language in XNLI, they show results by augmenting with each of the 14 other languages in the dataset, and show significant improvements over per-language performance. The paper explores an interesting idea and shows that cross-lingual data augmentation works well. However, their analysis is limited to the XNLI and the Squad dataset, which do not cover a suitable range of tasks to fully conclude on the importance of XLDA for generally improving NLU tasks. It would have been interesting to show the effect of cross-lingual data augmentation for other GLUE tasks by augmenting the datasets with machine translation for instance. And also compare this model on these tasks to the monolingual back-translation approach, similar to https://arxiv.org/abs/1904.12848 . Applying XLDA with the latest open-source XLM models from the cross-lingual language model pretraining paper which obtain higher performance than the multilingual BERT would also make the results more convincing. While I share the excitement of using cross-lingual models to improve monolingual performance, I also feel like this paper lacks novelty and further evaluation to be accepted at ICLR, and would be more suited in a more NLP-focused venue."}
{"id": "iclr2020_584", "title": "Neural Architecture Search by Learning Action Space for Monte Carlo Tree Search | OpenReview", "abstract": "Abstract:###Neural Architecture Search (NAS) has emerged as a promising technique for automatic neural network design. However, existing NAS approaches often utilize manually designed action space, which is not directly related to the performance metric to be optimized (e.g., accuracy). As a result, using manually designed action space to perform NAS often leads to sample-inefficient explorations of architectures and thus can be sub-optimal. In order to improve sample efficiency, this paper proposes Latent Action Neural Architecture Search (LaNAS) that learns actions to recursively partition the search space into good or bad regions that contain networks with concentrated performance metrics, i.e., low variance. During the search phase, as different architecture search action sequences lead to regions of different performance, the search efficiency can be significantly improved by biasing towards the good regions. On the largest NAS dataset NASBench-101, our experimental results demonstrated that LaNAS is 22x, 14.6x, 12.4x, 6.8x, 16.5x more sample-efficient than Random Search, Regularized Evolution, Monte Carlo Tree Search, Neural Architecture Optimization, and Bayesian Optimization, respectively. When applied to the open domain, LaNAS achieves 98.0% accuracy on CIFAR-10 and 75.0% top1 accuracy on ImageNet in only 803 samples, outperforming SOTA AmoebaNet with 33x fewer samples.", "review": "Review:###This paper introduces a Neural Architecture Search algorithm that attempts to solve the problems of the existing NAS only utilizing manually designed action space (not related to the performance). The paper proposes LaNAS which is based on an MCTS algorithm to partition the search space into tree nodes by the performance in the tree structure. The performance of the method is shown in the NASBench-101 dataset and Cifar-10 open domain search. I lean to reject this paper because (1) the motivation is not well justified by the experiments, (2) the comparison on NASBench-101 is not convincing, (3) some important explanation of the method is missing. Main arguments The main contribution of the paper is using a learned action space in MCTS rather than a manually designed MCTS algorithm for NAS. However, as far as I know, the MCTS approach for the NAS problem is not a standard solution for NAS (which is not proved to be practically useful in other people’s papers) which diminishes the contribution of the improvements of MCTS in NAS. Lack of the main comparison. For the motivation of the proposed method, the authors mention the drawbacks of other NAS methods used fixed action space in their RL or MCTS module. However, the authors only show that using a learned action space in MCTS is better than a fixed MCTS algorithm in the experiments. What about using a learned action space in the RL module such as PPO in NAS comparing to the fixed one? The comparison of NASBench-101 is not convincing. The authors compared with the BO method and claimed that the method is 16.5x more efficient than BO. However, the recently released paper “BANANAS: Bayesian Optimization with Neural Networks for Neural Architecture Search” said that their method is 3.8x more efficient than the one proposed here, which is quite confusing. Some important explanation of the methods is missing. Throughout the paper, the method to sample from a leaf node is only mentioned in 3.3. However, the corresponding sampling method is unclear. The paper only mentions that MCMC has been adopted to sample from the target subspace. In my opinion, it is not so trivial to use MCMC here and should be elaborated in more detail. Otherwise, people cannot use it. As given in figure 3, if c is set to be a very small number, the search is similar to simply using a series of predictors and always samples the models with better-predicted accuracies. Is MCTS really useful here? To show the effectiveness of MCTS, it is recommended to experiment on different values of c. Results given in the upper row of Fig.5 is not useful. In practice, it is already painful to sample about 1000 models and train them for the Cifar-10 dataset. It is more useful to see how this method behaves in the range of (0,1000). However, in Fig.5, different methods are all overlapping in this range and hard to tell whether this method is better than other methods"}
{"id": "iclr2020_585", "title": "GraphNVP: an Invertible Flow-based Model for Generating Molecular Graphs | OpenReview", "abstract": "Abstract:###We propose GraphNVP, an invertible flow-based molecular graph generation model. Existing flow-based models only handle node attributes of a graph with invertible maps. In contrast, our model is the first invertible model for the whole graph components: both of dequantized node attributes and adjacency tensor are converted into latent vectors through two novel invertible flows. This decomposition yields the exact likelihood maximization on graph-structured data. We decompose the generation of a graph into two steps: generation of (i) an adjacency tensor and(ii) node attributes. We empirically demonstrate that our model and the two-step generation efficiently generates valid molecular graphs with almost no duplicated molecules, although there are no domain-specific heuristics ingrained in the model. We also confirm that the sampling (generation) of graphs is faster in magnitude than other models in our implementation. In addition, we observe that the learned latent space can be used to generate molecules with desired chemical properties", "review": "Review:###Contributions: 1. This paper proposes an invertible flow-based method for the one-shot graph generation. 2. The paper demonstrates their method on a molecular graph generation task. 3. Empirical results show the effectiveness of the proposed method. The merit of the proposed invertible flow method is two folds. First, it can guarantee a one hundred percent reconstruction accuracy. Second, it can be adapted to generate graphs with various types (such as molecules) without incorporating much domain knowledge. Below are my concerns regarding this paper. [Page 7, Table 2] My first concern is: does the reconstruction performance matters in the graph generation case? Typically a lower reconstruction error does not mean a worse model to generate reasonable new graphs. So the reconstruction error should accompany with other criterions. In Table 2, I can see CD-VAE and JT-VAE does better in generating valid, novel and unique graphs. So I wonder whether it worth sacrificing novelty to pursue a perfect reconstruction. [Page 7, Sec 4.2] The authors mention they cannot reproduce the decoder of CG-VAE and JT-VAE. So I expect they mention somewhere in this paper that they will release their code once published. [Page 5, Sec 3.3.1] The authors should be explicit by saying we replace sliced matrices (z_X[l^-,:,] in Eq. 2 and z_A[l^-,:,:] in Eq. 4) with masked matrices rather than just saying *Eqs. (2,4) are implemented with masking patterns*. [Page 5, Sec 3.3.1] Can you explain the gain of masking? To my understanding, even with masking you still need a sequence of N coupling layers to update each node once. [Page 5, Sec 3.3.1] The second paragraph in Sec 3.3.1 is confusing to me. The masking scheme indeed makes the whole process, not permutation invariant. But I*m confusing about the way you fix it. Can you explain your *permutation invariant coupling*? E.g., why you need to change the indexing on the non-node axis? Overall, I think the method proposed in this paper sacrifices some more import aspects in graph generation such as novelty and uniqueness by introducing an invertible flow architecture. And some parts in the paper may require a significant re-writing, such as Sec 3.3.1."}
{"id": "iclr2020_586", "title": "Spike-based causal inference for weight alignment | OpenReview", "abstract": "Abstract:###In artificial neural networks trained with gradient descent, the weights used for processing stimuli are also used during backward passes to calculate gradients. For the real brain to approximate gradients, gradient information would have to be propagated separately, such that one set of synaptic weights is used for processing and another set is used for backward passes. This produces the so-called *weight transport problem* for biological models of learning, where the backward weights used to calculate gradients need to mirror the forward weights used to process stimuli. This weight transport problem has been considered so hard that popular proposals for biological learning assume that the backward weights are simply random, as in the feedback alignment algorithm. However, such random weights do not appear to work well for large networks. Here we show how the discontinuity introduced in a spiking system can lead to a solution to this problem. The resulting algorithm is a special case of an estimator used for causal inference in econometrics, regression discontinuity design. We show empirically that this algorithm rapidly makes the backward weights approximate the forward weights. As the backward weights become correct, this improves learning performance over feedback alignment on tasks such as Fashion-MNIST and CIFAR-10. Our results demonstrate that a simple learning rule in a spiking network can allow neurons to produce the right backward connections and thus solve the weight transport problem.", "review": "Review:###summary This paper considers the *weight transport problem* which is the problem of ensuring that the feedforward weights is the same as the feedback weights in the spiking NN model of computation. This paper proposes a novel learning method for the feedback weights which depends on accurately estimating the causal effect of any spiking neuron on the other neurons deeper in the network. Additionally, they show that this method also minimizes a natural cost function. They run many experiments on FashionMNIST and CIFAR-10 to validate this and also show that for deeper networks this approaches the accuracy levels of GD-based algorithms. comments Overall I find this paper to be well-written and _accessible_ to someone who is not familiar with the biologically plausible learning algorithms. To overcome the massive computational burden, they employ a novel experimental setup. In particular, they use a separate non-spiking neural network to train the feedforward weights and use the spiking neurons only for alignment of weights. They have experimental evidence to show that this method is a legitimate workaround. I find their experimental setup and the results convincing to the best of my knowledge. The experimental results indeed show the claim that the proposed algorithm has the properties stated earlier (i.e., learns the feedback weights correctly and that using this to train deep neural nets provide better performance than weight alignment procedure). I must warn that I am not an expert in this area and thus, might miss some subtleties. Given this, it is also unclear to me why this problem is important and thus, would leave the judgement of this to other reviewers. Here I will score only based on the technical merit of the method used to solve the problem. I had one minor comment on the arrangement of the writing of the paper. Section 4 starts off with *Results* but the earlier sub-sections are not really about the results. I would split section 4 as methodology/algorithm and include the everything until section 4.4. From sub section 4.5 onwards are the actual results. overall decision Without commenting on the importance of this problem, I think this paper merits an acceptance based on the technical content. The paper provides convincing experiments to test the properties the author claim the new algorithm has."}
{"id": "iclr2020_587", "title": "Task Level Data Augmentation for Meta-Learning | OpenReview", "abstract": "Abstract:###Data augmentation is one of the most effective approaches for improving the accuracy of modern machine learning models, and it is also indispensable to train a deep model for meta-learning. However, most current data augmentation implementations applied in meta-learning are the same as those used in the conventional image classification. In this paper, we introduce a new data augmentation method for meta-learning, which is named as ``Task Level Data Augmentation** (referred to Task Aug). The basic idea of Task Aug is to increase the number of image classes rather than the number of images in each class. In contrast, with a larger amount of classes, we can sample more diverse task instances during training. This allows us to train a deep network by meta-learning methods with little over-fitting. Experimental results show that our approach achieves state-of-the-art performance on miniImageNet, CIFAR-FS, and FC100 few-shot learning benchmarks. Once paper is accepted, we will provide the link to code.", "review": "Review:###This paper introduces a new data augmentation method for meta-learning, named as *Task Level Data Augmentation (Task Aug).* The general data augmentation methods add various translations to the original data to amplify the original data. However, in the meta-learning problems, especially few-shot learning, which is experimented in the paper, the smallest unit is rather a task than data. Therefore, the number of data instances per class is not as critical as other learning problems. Therefore, TaskAug increases the number of classes instead of the number of instances per class by generating images that are clearly recognized as different classes from the original data. With an increased number of classes, the number of task instances that can be sampled also increases. TaskAug rotates the natural images by 90, 180, 270 degrees making three new classes. Furthermore, TaskAug put smaller weights on the new classes letting the model prioritize the original classes. The smaller weights are implemented using two arguments, p_max and T. To build a task instance, classes of augmented classes are selected with the probability p and classes of original data are selected with the probability 1-p. The probability p increases from 0 to p_max linearly for T task instances. Overall, this paper proposes a novel task augmentation method for meta-learning problems and shows that it improves the learning performance from the experiments. However, the class generating method is limited to rotating, which is questionable in terms of novelty and efficacy, and the writing is unclear and unpolished. As it is already mentioned in the paper, rotating images do not generate images that are clearly separated from the original class, which the algorithm intended. For example, balls of the natural image dataset, or the alphabet *O* and the number *0* of character dataset can not be clearly separated by rotating and rotating *6* by 180 degrees generated *9* whose class already exists. The authors only claim that the features from the new classes can provide useful information and do not propose any remedy. Moreover, rotating images is one of the most popular data augmentation methods which are used to amplify the images per class. It is hard to argue that the same translation method does different work. Also, Table 1 and Table 4 show only the contributions of *+ens+val* which is not the main contribution of the paper. The ensemble method is from the Huang et al. *s work, and *+val* additionally uses validation data. To improve this paper, the authors can contemplate other method(s) to increase the number of classes which can be clearly separated from the existing classes as the algorithm intended in the first place. Minor comments: There are some unclear parts in the paper. 1) In Algorithm 1, p<-p_max*min{1,t/T} means that p increases from 0 to p_max for the first T tasks, but the paper says that p increase from 0 to o_max after T tasks in Section 3.2. 2) The writing in 4.1.3 is unclear and hard to get what *+ens*,*+val*, and *+ens+val* means. It seems like *+ens* takes ensemble of 60 models acquired during 60 training epochs using training classes, *+val* adds validation classes for the last epoch and chooses the last model, and *+ens+val* adds validation classes for the last epoch and takes ensemble of 60 models. But I*m not sure if I understood correctly. 3) Why were the validation classes used for the training? 4) It is hard to understand Figure 3. How about using the same color or same mark for TaskAug 1-shot & 5-shot, Baseline 1shot-5shot, and so on? Because 1-shot and 5-shot are clearly separated in the graph. Some typos/errors: Section 4, line 4, probably -> probability"}
{"id": "iclr2020_588", "title": "Distilling the Knowledge of BERT for Text Generation | OpenReview", "abstract": "Abstract:###Large-scale pre-trained language model, such as BERT, has recently achieved great success in a wide range of language understanding tasks. However, it remains an open question how to utilize BERT for text generation tasks. In this paper, we present a novel approach to addressing this challenge in a generic sequence-to-sequence (Seq2Seq) setting. We first propose a new task, Conditional Masked Language Modeling (C-MLM), to enable fine-tuning of BERT on target text-generation dataset. The fine-tuned BERT (i.e., teacher) is then exploited as extra supervision to improve conventional Seq2Seq models (i.e., student) for text generation. By leveraging BERT*s idiosyncratic bidirectional nature, distilling the knowledge learned from BERT can encourage auto-regressive Seq2Seq models to plan ahead, imposing global sequence-level supervision for coherent text generation. Experiments show that the proposed approach significantly outperforms strong baselines of Transformer on multiple text generation tasks, including machine translation (MT) and text summarization. Our proposed model also achieves new state-of-the-art results on the IWSLT German-English and English-Vietnamese MT datasets.", "review": "Review:###This paper uses BERT in text generation via distillation. Specifically, on top of the common MLE training loss for generating tokens from left to right, this paper adds a distillation regularization term in the cross entropy form, in the hope that the likelihood of the masked token predicted by BERT (bidirectional) is close to the likelihood predicted by the autoregressive generation model. The experiments are performed on text generation tasks like machine translation, text summarization, and image captioning. Specifically, it got SOTA on IWSLT14 German-English and IWSLT15 English-Vietnamese datasets. Overall, this paper presents a neat idea for using BERT in text generation. I would recommend a weak accept due to the following issues: 1. The so-called SOTA results are not very surprising because this method uses BERT, which is pre-trained on huge extra datasets. Thus, one may argue that the comparison may be unfair. 2. En-De (and En-Fr) are more important benchmark datasets in machine translation. However, the results in Table 3 are much worse than Ott et al. or Wu et al. The experiments can be more complete if WMT16 is used in training and checkpoint averaging is also employed. Minor suggestions: the text in Figure 2 are hard to discern. Please improve the quality of the figures."}
{"id": "iclr2020_589", "title": "Causally Correct Partial Models for Reinforcement Learning | OpenReview", "abstract": "Abstract:###In reinforcement learning, we can learn a model of future observations and rewards, and use it to plan the agent*s next actions. However, jointly modeling future observations can be computationally expensive or even intractable if the observations are high-dimensional (e.g. images). For this reason, previous works have considered partial models, which model only part of the observation. In this paper, we show that partial models can be causally incorrect: they are confounded by the observations they don*t model, and can therefore lead to incorrect planning. To address this, we introduce a general family of partial models that are provably causally correct, but avoid the need to fully model future observations.", "review": "Review:###This paper tackles the issue of identifying the causal reasoning behind why partial models in MBRL settings fail to make correct predictions under a new policy. The novel contribution was a framework for learning better partial models based on models learning an interventional conditional, rather than an observational conditional. The paper tried to provide both theoretical and experimental reasoning for this framework. I vote to (weak) reject the paper due to the major issues with section 2. Furthermore, the paper hard or at times almost impossible to understand as too many assumptions are made and too little is explained. Recommendations Because your graphs are not MDPs, you are not framing your example as an RL problem. This is causing a number of issues with notation and lack of clarity in the argument you*re making. 1. It is unclear to me that the FuzzyBear example is correctly constructed as a RL example, reasons being that: - Figure 1 (a) & (b) do not correspond to an MDP as two different states, teddy vs grizzly, are both designated as s_1 and similarly, the two possible actions, hug or run, are both designated as a_1 and thus are not distinct. - Note your terminal state for the episodes - Have a reward for (s0, a0) as every s-a pair should have a reward - It would be helpful to note that the environments in Figure 1 are stochastic 2. Clarify notation. There are a number of assumptions about what background knowledg the reader should have. Given the bridging of disciplines in the paper, it would be useful to provide more detail on notation in Section 3. 3. Add a section on reinforcement learning in Section 3. If it*s the last subsection in section 3, you could describe the relationship between the various causal reasoning and RL principles. This would further clarify how you*re bridging these subtopics. 4. For sentence, *Fundamentally, the problem is due to causally incorrect reasoning: the model learns the observational condi- tional p(r|s0, a0, a1) instead of the interventional conditional given by p(r|s0, do(a0), do(a1)) = s1 p(s1|s0, a0)p(r|s1, a1).* As you don*t cover the meaning of the do() operator until a later paragraph, provide a quick description of it as it is not common knowledge to a general AI audience, e.g., where do() indicates that the action was taken. 5. Correct the following sentences, *Mathematically, the model with learn the following conditional probability:* *In Section 3, we review relevant concepts from causal reasoning based on which we propose solutions that address the problem.* 6. I recommend putting the interventional conditional equation, p(r|s0, do(a0), do(a1)) = ??s1 p(s1|s0, a0)p(r|s1, a1), on its own line as the reader is doing a comparison of it with the previous equation, p(r|s0, a0, a1), given on page 2. 7. Strengthen your abstract by aligning more with claims you make in your conclusion. 8. The experiments in Figure 5 are averaged over 5 seeds. This is not enough to be statistically significant - furthermore, there are no error bars in the Figure. Question(s): 1. You*ve indicated two policies for Figure 1 (a): - pi1: the agent knows it is encountering a teddy bear, so it will hug - pi2: the agent knows it is encountering a grizzly bear, so it will run Is this the *change in the behavior policy* that you*re referring to? If so, make this clearer, this currently requires a lot of work by the reader to make sense of it. 2. What are the partially observable parts of the environments in Figure 1 (a) & (b)? Make this clear."}
{"id": "iclr2020_590", "title": "Optimistic Adaptive Acceleration for Optimization | OpenReview", "abstract": "Abstract:###This paper considers a new variant of AMSGrad called Optimistic-AMSGrad. AMSGrad is a popular adaptive gradient based optimization algorithm that is widely used in training deep neural networks. The new variant assumes that mini-batch gradients in consecutive iterations have some underlying structure, which makes the gradients sequentially predictable. By exploiting the predictability and some ideas from Optimistic Online learning, the proposed algorithm can accelerate the convergence and also enjoys a tighter regret bound. We evaluate Optimistic-AMSGrad and AMSGrad in terms of various performance measures (i.e., training loss, testing loss, and classification accuracy on training/testing data), which demonstrate that Optimistic-AMSGrad improves AMSGrad.", "review": "Review:###This paper proposes an online optimization method called Optimistic-AMSGrad, which combines two existing methods: (i) AMSGrad (Reddi et al 2018) and (ii) optimistic online learning where the prediction step is done with the extrapolation algorithm by Scieur et al 2016. The authors do a good job of presenting the method (by introducing the background in proper order), the paper seems self-contained and cites the relevant literature. The regret analysis of the proposed algorithm is provided, where the obtained regret can be smaller than AMSGrad depending on whether or not the guess of the gradient and the gradient are close. In my opinion the boundedness assumption (footnote 2) is quite important here, and should be mentioned in the main text. It is not clear how the different ways of accelerations combined in this method interact when the guess is not good. In other words, if the guess is not good this method could be slower then AMSGrad. Moreover, AMSGrad has stability property allowed from the ratio between 1st and 2nd moment estimate. In Optimistic-AMSGrad if m_t is bad, obtaining next w_{t+1} (line 9) would include ratio between outdated/bad 1st mom. estimate and new 2nd-moment estimate. In short, the method’s stability and outperformance might rely on the selection of the algorithm for gradient prediction. In my understanding, extragradient has clear advantages in games, as if considering simple bilinear examples it is the only method that converges. However, for a single objective, its advantages are not clear to me (after reading the paper). Thus, I think it would be useful if the authors could provide comparison over *wall clock time* as well as long-run comparisons when the compared methods converge (it would be interesting to see if Optimistic-AMSGrad obtains better final train/test accuracy?). In many of the experiments where Opt-AMSGrad outperforms, the accuracy of the baseline still goes up--whereas the latter is computationally cheaper, so it is not clear from the provided results why a practitioner should use this method. Moreover, the experimental results would be much more convincing if the authors do multiple runs using different seeds and present mean and standard deviation of the methods. In the context of games, using more computationally demanding optimizers makes sense as training is unstable. In this case, after reading the paper, it is not clear to me what is the problem that the proposed method solves (or its advantages). Indeed, its advantage depends on how good the guess of g_{t+1} is. However, the extra-computation cost to obtain a good guess needs to be justified, or proven empirically that gets better performances faster (wall-clock time), or final ones. In summary: (i) the paper is well-presented and provides hyperparameter sensitivity results; (ii) the paper is very interesting, but (imo) it should leave clearer message why one should use this method; (iii) the proposed method has tighter regret, but only in some (data-dependent) cases and combines existing methods, limiting novelty. Hence, given the pros and cons, I am not confident recommending acceptance, and I think improved experimental results (error bars & wall-clock, see above) would make the results more significant. --- Minor --- - Alg.2: maybe add input hyperparameter r to optimistic-AMSGrad and a line between lines 8-9 that calls function which obtains the guess m_{t+1} (r should be passed to it). It would make it more clear that your algorithm has parameter r (sec. D.2) - I could be wrong, but in my opinion, using \theta as first moment est. is slightly confusing, as it is normally used to denote parameters; similarly, the authors could use hat/prime on top of a variable to denote the ‘guess’ of that same variable, making it easier to follow. - maybe add init of \theta_0 in alg1&2 - if I am correct amsgrad also does bias correction of the initial values of 1st and 2nd moment estimates; if that’s the case it could be useful adding a note that this is omitted for clarity if a reader implements it - pg2: we just would like -> we would like"}
{"id": "iclr2020_591", "title": "Deep Lifetime Clustering | OpenReview", "abstract": "Abstract:###The goal of lifetime clustering is to develop an inductive model that maps subjects into clusters according to their underlying (unobserved) lifetime distribution. We introduce a neural-network based lifetime clustering model that can find cluster assignments by directly maximizing the divergence between the empirical lifetime distributions of the clusters. Accordingly, we define a novel clustering loss function over the lifetime distributions (of entire clusters) based on a tight upper bound of the two-sample Kuiper test p-value. The resultant model is robust to the modeling issues associated with the unobservability of termination signals, and does not assume proportional hazards. Our results in real and synthetic datasets show significantly better lifetime clusters (as evaluated by C-index, Brier Score, Logrank score and adjusted Rand index) as compared to competing approaches.", "review": "Review:###This paper proposes a method to cluster subjects based on the latent lifetime distribution. The proposed model clusters by maximizing the empirical divergence between different clusters. The problem setting of this paper is not clear to me. I am not sure which variables are observed and which are not. For example, in the Friendster experiment, the data of 5 months from joining is used for clustering. However, the termination windows are chosen to be 10 months. Therefore, it is clear that the observed data will not contain the termination signals, and I do not believe the training of the model is possible, without observing any termination signals. In the paper, do we consider only one type or multiple types of events? Is a vector that represents the attributes or properties of an event? Some details of the model are not clear to me. In Equation (2), the input of the neural network differs in length across different subject , because the number of observed events for each subject is different. How does the proposed neural network take inputs of different lengths? How the non-decreasing function is defined in Section 3.2? Is it a function of the observed data for each subject? How the empirical distribution in Equation (4) is computed is also not clear to me. How is a vector? Is it constructed by concatenating with different ? How to normalize such that it is a valid probabilistic distribution? Since is high dimensional, it looks very challenging to estimate the joint distribution. The overall objective function is given by Equation (4), is it correct? In Equation (4), why should we compute the minimum values across all possible pairs of clusters rather than the summation of all pairs? If Equation (4) is the overall objective function, then it looks like the model does not contain a component that maximizes a likelihood function. How is it guaranteed that the model will fit the data? It looks like the model will converge to a trivial solution that is a constant such that for one cluster and for another cluster, if the likelihood function is not involved. This will give a maximum divergence between distributions. In summary, it seems that numerous technical details are missing and the paper might contain technical flaws. I do not suggest the acceptance of this paper."}
{"id": "iclr2020_592", "title": "Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue | OpenReview", "abstract": "Abstract:###Knowledge-grounded dialogue is a task of generating an informative response based on both discourse context and external knowledge. As we focus on better modeling the knowledge selection in the multi-turn knowledge-grounded dialogue, we propose a sequential latent variable model as the first approach to this matter. The model named sequential knowledge transformer (SKT) can keep track of the prior and posterior distribution over knowledge; as a result, it can not only reduce the ambiguity caused from the diversity in knowledge selection of conversation but also better leverage the response information for proper choice of knowledge. Our experimental results show that the proposed model improves the knowledge selection accuracy and subsequently the performance of utterance generation. We achieve the new state-of-the-art performance on Wizard of Wikipedia (Dinan et al., 2019) as one of the most large-scale and challenging benchmarks. We further validate the effectiveness of our model over existing conversation methods in another knowledge-based dialogue Holl-E dataset (Moghe et al., 2018).", "review": "Review:###The paper looks at the problem of knowledge selection for open-domain dialogue. The motivation is that selecting relevant knowledge is critical for downstream response generation. The paper highlights the one-to-many relations when selecting knowledge which makes the problem even more challenging. It tries to address this by taking into account the history of knowledge selected at previous turns. The paper proposes a Sequential Latent Model which represents the knowledge history as some latent representation. From this methodology they select a piece of knowledge at the current turn and use it to decode an utterance. The model is trained in a joint fashion to learn which knowledge to select and on generating the response. As the two are strongly correlated. Additionally there is an auxiliary loss to help correctly identify if the knowledge was correctly selected. Additionally a copy mechanism is introduced to try to copy words from the knowledge during decoding. The experiments are run on the Wizard of Wikipedia dataset where there are annotations for which knowledge sentence is selected and on Holl-E, where they transform the dataset to have a single sentence tied to a response. For automatic metrics there is significant improvement over baselines for correctly selecting a piece of knowledge and generating a response. Additionally there is human evaluation that also shows significant improvement. Their model also seems to generalize well to domains that were not seen during training time over baselines models. The contribution of the paper is the novel approach to selecting knowledge for open-domain dialogue. This work is significant in that by improving knowledge selection we see a subsequent improvement in response generation quality which is the overall downstream task within this problem space. I believe this paper should be accepted because of the significant and novel approach of modeling previous knowledge sentences selected. The linking of this knowledge selection model to topic tracking as stated in the paper is of clear importance, as ensuring topical depth and topical transition are two key aspects for open-domain dialog. Feedback on the paper In Figure 3, please provide the knowledge sentence that was selected. Please provide the inter-annotator agreement for human evaluation. I think it would be interesting to see what is the copy mechanism actually adding in terms of integration of knowledge vs the WoW MemNet approach. Are those two truely comparable because one does not have copy? For Related Work, also cite Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations Small grammatical errors *Recently, Dinan et al. (2019) propose to tackle* -> *Recently, Dinan et al. (2019) proposed to tackle* *which subsequently improves the knowledge-grounded chit-chat.* -> *which subsequently improves knowledge-grounded chit-chat.* Some questions for the authors in terms of future direction How is the performance of the model impacted with longer dialog context vs shorter? The Holl-E dataset was transformed from spans of knowledge to a single knowledge sentence. It would be interesting to see what happens when the knowledge selected is over multiple sentences. The knowledge pool currently consists of 67.57 sentences on average. How will this method scale as the amount of knowledge sentences grows?"}
{"id": "iclr2020_593", "title": "Mesh-Free Unsupervised Learning-Based PDE Solver of Forward and Inverse problems | OpenReview", "abstract": "Abstract:###We introduce a novel neural network-based partial differential equations solver for forward and inverse problems. The solver is grid free, mesh free and shape free, and the solution is approximated by a neural network. We employ an unsupervised approach such that the input to the network is a points set in an arbitrary domain, and the output is the set of the corresponding function values. The network is trained to minimize deviations of the learned function from the PDE solution and satisfy the boundary conditions. The resulting solution in turn is an explicit smooth differentiable function with a known analytical form. Unlike other numerical methods such as finite differences and finite elements, the derivatives of the desired function can be analytically calculated to any order. This framework therefore, enables the solution of high order non-linear PDEs. The proposed algorithm is a unified formulation of both forward and inverse problems where the optimized loss function consists of few elements: fidelity terms of L2 and L infinity norms, boundary conditions constraints and additional regularizers. This setting is flexible in the sense that regularizers can be tailored to specific problems. We demonstrate our method on a free shape 2D second order elliptical system with application to Electrical Impedance Tomography (EIT).", "review": "Review:###Summary: In the paper, the authors purpose to use neural networks to model both the function and the parameters and in a sense, unify the forward and inverse problems. The authors demonstrate the work on Electrical Impedance Tomography problem. The authors also purpose a new regularizer, the sum of L2 and L_inf norm of the differential operator. Concerns: there have been plenty of works that use neural networks to model the function for forward problems and another bunch of works that use neural networks to model parameters to do inverse problems. It is not clear to me if combining the two will really give us benefits, since we are still doing these two problems separately. If we are doing some alternating training, unifying them could be useful. The mesh free part is less interesting in my opinion. Works using feed forward neural networks are mesh free in general. And when you try to use the solution, or to compare with some ground truth generated by traditional methods, usually we still need to make the solution discrete to use it. The experiments in the paper is limited. It compares with only one work in the forward task, but no comparison in the inverse problem. It is hard to evaluate its performance. The theory is also needed. It is not very clear why L2 + L_inf regulation terms will help us. After all, in computer science, conference papers are considered as final publications. So a more extensive studied is expected. I would suggest submit this work for a workshop. Decision: This work need further experiments and theoretical analyze. I suggest weakly reject this paper."}
{"id": "iclr2020_594", "title": "Learning Classifier Synthesis for Generalized Few-Shot Learning | OpenReview", "abstract": "Abstract:###Object recognition in real-world requires handling long-tailed or even open-ended data. An ideal visual system needs to reliably recognize the populated visual concepts and meanwhile efficiently learn about emerging new categories with a few training instances. Class-balanced many-shot learning and few-shot learning tackle one side of this problem, via either learning strong classifiers for populated categories or learning to learn few-shot classifiers for the tail classes. In this paper, we investigate the problem of generalized few-shot learning (GFSL) -- a model during the deployment is required to not only learn about *tail* categories with few shots, but simultaneously classify the *head* and *tail* categories. We propose the Classifier Synthesis Learning (CASTLE), a learning framework that learns how to synthesize calibrated few-shot classifiers in addition to the multi-class classifiers of ``head** classes, leveraging a shared neural dictionary. CASTLE sheds light upon the inductive GFSL through optimizing one clean and effective GFSL learning objective. It demonstrates superior performances than existing GFSL algorithms and strong baselines on MiniImageNet and TieredImageNet data sets. More interestingly, it outperforms previous state-of-the-art methods when evaluated on standard few-shot learning.", "review": "Review:###Summary: This paper tackles the generalized few-shot learning (GFSL) problem [1-2], that learns (few-shot) novel classes while not forgetting original classes. Built upon [1], the authors propose a different form of the weight generator for novel classes. Pros: - The proposed method outperforms the prior work, in both GFSL and the standard few-shot setting. - The authors suggest the harmonic mean as an evaluation metric for GFSL. Cons: 1. Limited novelty & Prior works are not properly cited. The problem of generalized few-shot learning (GFSL) is proposed in [1-2]. Hence, the authors should cite [1-2] in the last paragraph on page 1, where they introduce the GFSL problem. Also, many parts of the method are from [1], that [1] proposes an attention-based weight generator and this work only differs from the detailed attention mechanism. However, the authors do not cite [1] in the method section but only shortly mentions it on the last page. I strongly believe that the authors should clarify their contributions, as newcomers may misunderstand contributions and give wrong credits. 2. The source of the improvements? The authors should explain why the proposed method is better than [1]. One possible reason is that [1] uses a dictionary of size |S| (the size of the seen dataset S), but the proposed method uses 2-3 times of them, as stated in Appendix C.1 and Figure A8. Other reasons, e.g., the attention coefficients (Eq. (7) of [1] vs Eq. (4) of this paper), the combination weights (Eq. (8) of [1] vs Eq. (5) of this paper), or the classifier form (cosine similarity of [1] and linear of this paper) could also be a candidate. The authors should identify the source of improvements. 3. Results in the standard few-shot setting. The authors should clarify if the results are reproduced or copied from prior work. It seems that they are copied since the numbers are identical, e.g., see Table 1 of [3] and Table 1 of [4]. If so, the authors should specify where the numbers are from. Also, the caption of Table 4 and Table 5 seems to be wrong, as LEO [3] uses WRN-28-10 backbone instead of ResNet-12. Finally, the source of the gain also should be investigated. If the joint learning of many-shot and few-shot is the reason, DFSL [1] also should outperform other methods. Minor comments: - In Table 1, the authors state the reference for IFSL but not for L2ML* and DFSL*. For consistency, the authors should state all references or none. - Why DFSL* has a quotation mark? L2ML* is an inductive version of L2ML, but DFSL is already designed for the inductive setting. - OptNet in Table 4 and Table 5 should be changed to MetaOptNet [4]. [1] Gidaris and Komodakis. Dynamic Few-Shot Visual Learning without Forgetting. CVPR 2018. [2] Ren et al. Incremental Few-Shot Learning with Attention Attractor Networks. NeurIPS 2019. [3] Rusu et al. Meta-Learning with Latent Embedding Optimization. ICLR 2019. [4] Lee et al. Meta-Learning with Differentiable Convex Optimization. CVPR 2019."}
{"id": "iclr2020_595", "title": "Omnibus Dropout for Improving The Probabilistic Classification Outputs of ConvNets | OpenReview", "abstract": "Abstract:###While neural network models achieve impressive classification accuracy across different tasks, they can suffer from poor calibration of their probabilistic predictions. A Bayesian perspective has recently suggested that dropout, a regularization strategy popularly used during training, can be employed to obtain better probabilistic predictions at test time (Gal & Ghahramani, 2016a). However, empirical results so far have not been encouraging, particularly with convolutional networks. In this paper, through the lens of ensemble learning, we associate this unsatisfactory performance with the correlation between the models sampled with dropout. Motivated by this, we explore the use of various structured dropout techniques to promote model diversity and improve the quality of probabilistic predictions. We also propose an omnibus dropout strategy that combines various structured dropout methods. Using the SVHN, CIFAR-10 and CIFAR-100 datasets, we empirically demonstrate the superior performance of omnibus dropout relative to several widely used strong baselines in addition to regular dropout. Lastly, we show the merit of omnibus dropout in a Bayesian active learning application.", "review": " PAPER SUMMARY: The paper argues that (ensembles of models with different types of dropout applied to each model) perform better than (ensembles in which the same type of dropout is applied to each model). They attribute this to increasing model diversity in the former case, and experimentally validate their claims. MAJOR COMMENTS: 1. Motivation Unclear: - Notational issues in (2): MSE(h_t | x) involves y, but y is not specified in the definition. As a result, later E_{x}[…] is evaluated disregarding the dependence on y, which is ambiguous. - Derivation of the second equality in (2) is not obvious, and needs a detailed proof. Notational issues exist in switching between H and h. - The above is used to argue that “the more diverse the models, the better performance achieved”. It is unclear how this follows from (2). 2. Unclear / Imprecise Writing - Before Sec. 3.4: “This is because higher Dropout rates lead to smaller effective network capacities” Needs reference. - “Dropout uncertainty can be obtained sequentially”. Unclear what this means. 3. The entire proposed method is described in one line, “we propose a novel omnibus dropout strategy, which merely combines all the aforementioned methods”. It is very unclear as to what the authors mean by combination. 4. Experiments - It seems that Omnibus dropout leads to a moderate “diversity”, and improves ensemble performance only for SVHN, out of the three datasets tested in Figure 4. For SVHN, the improvement is 0.1% accuracy, which is within the standard error (0.1). Hence, it seems that the proposed method provides no performance improvement. - A Similar statement is true for the other metrics (NLL, Brier Score, ECE). Score [Scale of 1-10]: 3: Reject The paper needs either more convincing experiments demonstrating the claims or theoretical analysis explaining the behavior for small networks. The paper needs to be rewritten to improve presentation, and state motivation, problem statement, and contributions clearly."}
{"id": "iclr2020_596", "title": "Cross-Dimensional Self-Attention for Multivariate, Geo-tagged Time Series Imputation | OpenReview", "abstract": "Abstract:###Many real-world applications involve multivariate, geo-tagged time series data: at each location, multiple sensors record corresponding measurements. For example, air quality monitoring system records PM2.5, CO, etc. The resulting time-series data often has missing values due to device outages or communication errors. In order to impute the missing values, state-of-the-art methods are built on Recurrent Neural Networks (RNN), which process each time stamp sequentially, prohibiting the direct modeling of the relationship between distant time stamps. Recently, the self-attention mechanism has been proposed for sequence modeling tasks such as machine translation, significantly outperforming RNN because the relationship between each two time stamps can be modeled explicitly. In this paper, we are the first to adapt the self-attention mechanism for multivariate, geo-tagged time series data. In order to jointly capture the self-attention across different dimensions (i.e. time, location and sensor measurements) while keep the size of attention maps reasonable, we propose a novel approach called Cross-Dimensional Self-Attention (CDSA) to process each dimension sequentially, yet in an order-independent manner. On three real-world datasets, including one our newly collected NYC-traffic dataset, extensive experiments demonstrate the superiority of our approach compared to state-of-the-art methods for both imputation and forecasting tasks.", "review": "Review:###This paper empirically studies the effectiveness of transformer models for time series data imputation. In particular, the paper studies the effect of generalized forms of self-attention that can attend across dimensions of the input. Generalizing self attention to work across dimensions of a multi dimensional time series is a good idea, and the experiments in the paper seem to support its effectiveness. The paper does provide some ablation results to compare their three forms of modified self attention, which is good. I believe the primary contribution of the paper is as an empirical study into the effectiveness of generalized self attention in time series datasets. However, I have to vote to reject the paper. My primary issue with the paper is its tone. While generalized forms of self attention are a good idea, the paper strongly emphasizes that it is a novel idea. In particular, image models that use self attention regularly attend too multiple dimensions. Consider for instance arXiv:1802.05751, arXiv:1904.09793, arXiv:1712.09763, arXiv:1805.08318. There is also several existing generalized self attention discussions: arXiv:1812.01243 or arXiv:1805.00912 The idea of extending self attention to look at multiple dimensions is fairly obvious. If the paper changed its tone from purporting to construct a novel method of self-attention (which I do not believe it does), to being an empirical study of the utility of self attention models for doing time series imputation I would be much more willing to accept it, though as a purely empirical study the bar would be high on the standard of the experiments, the reported experiments are on rather small datasets. Also, the paper could use an edit for grammar."}
{"id": "iclr2020_597", "title": "To Relieve Your Headache of Training an MRF, Take AdVIL | OpenReview", "abstract": "Abstract:###We propose a black-box algorithm called {it Adversarial Variational Inference and Learning} (AdVIL) to perform inference and learning on a general Markov random field (MRF). AdVIL employs two variational distributions to approximately infer the latent variables and estimate the partition function of an MRF, respectively. The two variational distributions provide an estimate of the negative log-likelihood of the MRF as a minimax optimization problem, which is solved by stochastic gradient descent. AdVIL is proven convergent under certain conditions. On one hand, compared with contrastive divergence, AdVIL requires a minimal assumption about the model structure and can deal with a broader family of MRFs. On the other hand, compared with existing black-box methods, AdVIL provides a tighter estimate of the log partition function and achieves much better empirical results.", "review": " This paper presents a black-box style learning algorithm for Markov Random Fields (MRF). The approach doubles down on the variational approach with variational approximations for both the positive phase and negative phase of the log likelihood objective function. For the negative phase, the authors use two separate variational approximations, one of which involves the modeling of the latent variable prior under the approximating distribution, The approach is novel, as far as I know, though not particularly so, and I view this as one of the weak point of the paper. That said, it does seems like a fairly creative combination of existing approaches. As others have found in the past, a variational approximation to the partition function contribution to the loss function (i.e. the negative phase) results in the loss of the variational lower bound on log likelihood and the connection between the resulting approximation and the log likelihood becomes unclear. To deal with this issue, the authors argue (in Lemma 1) that the gradient of their approximate objective is at least in the same direction as the ELBO (lower bound) objective. The result is fairly obvious, but the conditions for validity have interesting consequences for the training algorithm, as it relates the approximation error to the norm of the gradient of the ELBO loss. I have a minor issue with the discussion (in the last paragraph of sec. 3.2) stating that the theoretical statement of the proposed objective relies on a much weaker assumption than the nonparametric assumption made in the theoretical justification of GANs. While I agree with the statement as such, the GAN development makes a stronger statement about the nature of the learning trajectory. Specifically, it states that the generator is minimizing a Jenson-Shannon divergence which has a fixed point at the true data density. In the current development, Theorem 1 only states that the optimization process will converge to the stationary points of the approximate ELBO objective (L1 in the paper*s notation). Clarity: I found the paper to be very well written with a clear exposition of the material and sound development of the technical details. Relevance and Significance: This paper is highly relevant to the ICLR community and -- to the extent that one believes that training and inference in MRFs is important -- also significant. One this last point, it seems ironic to me that the proposed strategy for training the MRF is through the use of three separate directed graphical models (an encoder q(h | x), a decoder and a VAE to model the approximate prior over the latents h). In most modeling situations, one would simply impose the directed graphical model directly and skip the formalization in terms of an MRF. I would appreciate a more forceful motivation of the relevance of MRFs rather than just stating it as a important model with applications. What is unique about the MRF formalism that -- for practical applications -- could not be effectively captured in a directed graphical model? I note that I am aware of the theoretical representation differences between directed and undirected models, I am wondering how these differences actually matter in practical applications at scale. Experiments: The authors show the empirical advantages offered by the proposed method over the existing literature. I was surprised not to see how this model performs on the binarized MNIST dataset, and would like to see that result as well as CIFAR likelihood. MNIST, in particular, is a well studied dataset that many readers will be able to easily interpret. Its absence seems like a serious omission. What is meant by *RBM loss* in Fig. 2(d), I do not see this defined? I am somewhat alarmed at the use of 100 updates of the joint model q(v,h) (K1 = 100) for every update of the other parameters. For larger scale domains, I fear this could become an important obstacle to effective model training. The comparison to PCD-1 in Fig. 3 seems a bit unfair in that the learning curve ends at 8000 iterations, while PCD-1 continues to improve NLL. I would like to see this curve extended until we start to see signs of overfitting. Perhaps PCD-1 results in performance that is far better than AdVIL. I would also like to see a comparison to CD-k, which often outperforms PCD-k. While I understand the stance taken by the authors that these methods leverage the tractability of the conditional distributions, these strategies are sufficiently general to be considered widely applicable and a true competitor for AdVIL. With respect to Deep Boltzmann Machine (DBM), I would prefer to see quantitative comparisons against published results. Here again, MNIST would be a useful dataset. It seems as though, in the application of AdVIL to the DBM, the authors are exploiting the structure of the model in how they define their sampling procedure. Is that the case? More detail for this application of AdVIL would be nice. Also, I would like to see the test estimated NLL (via AIS) learning curves for VCD and AdVIL. Given the comparison to PCD in the RBM setting, I am somewhat surprised that AdVIL is so competitive with VCD in the case of the DBM."}
{"id": "iclr2020_598", "title": "Hierarchical Summary-to-Article Generation | OpenReview", "abstract": "Abstract:###In this paper, we explore \textit{summary-to-article generation}: the task of generating long articles given a short summary, which provides finer-grained content control for the generated text. To prevent sequence-to-sequence (seq2seq) models from degenerating into language models and better controlling the long text to be generated, we propose a hierarchical generation approach which first generates a sketch of intermediate length based on the summary and then completes the article by enriching the generated sketch. To mitigate the discrepancy between the ``oracle** sketch used during training and the noisy sketch generated during inference, we propose an end-to-end joint training framework based on multi-agent reinforcement learning. For evaluation, we use text summarization corpora by reversing their inputs and outputs, and introduce a novel evaluation method that employs a summarization system to summarize the generated article and test its match with the original input summary. Experiments show that our proposed hierarchical generation approach can generate a coherent and relevant article based on the given summary, yielding significant improvements upon conventional seq2seq models.", "review": " This paper presents a new task on generating articles from their corresponding summaries. They propose a two-step generation process, where a sketch is first generated from the summary, and then the articles is generated from the sketch. Reinforcement learning is used for model training. I think the task is interesting. My major concern lies in the evaluation. Since the task is about article-level generation, i.e. long text generation, it would be necessary to evaluate the structure or discours flow of the text in addition to relevance. It is not extremely hard for neural models to generate content that is relevant to the prompt (e.g. summaries), from my opinion, the difficulty comes from how to maintain a coherent and connective discourse flow rather than drifting away to various topics or producing repetitive or generic content. However, none of these aspects is evaluated in the submission. I would suggest the authors consider these generation quality aspects. Other comments: - The lengths of the generated articles should be reported, since different lengths of articles would significantly affect the perception of their relevance and quality. - The author should commment on ethical concerns of the task. Since the summary only contains a subset of the information from the article, the model tends to generate fabricated content by filling in the rest of the narrative. This issue should be addressed in the discussion section. - Fig 8&9 captions are incorrect."}
{"id": "iclr2020_599", "title": "Single Deep Counterfactual Regret Minimization | OpenReview", "abstract": "Abstract:###Counterfactual Regret Minimization (CFR) is the most successful algorithm for finding approximate Nash equilibria in imperfect information games. However, CFR*s reliance on full game-tree traversals limits its scalability and generality. Therefore, the game*s state- and action-space is often abstracted (i.e. simplified) for CFR, and the resulting strategy is then mapped back to the full game. This requires extensive expert-knowledge, is not practical in many games outside of poker, and often converges to highly exploitable policies. A recently proposed method, Deep CFR, applies deep learning directly to CFR, allowing the agent to intrinsically abstract and generalize over the state-space from samples, without requiring expert knowledge. In this paper, we introduce Single Deep CFR (SD-CFR), a variant of Deep CFR that has a lower overall approximation error by avoiding the training of an average strategy network. We show that SD-CFR is more attractive from a theoretical perspective and empirically outperforms Deep CFR with respect to exploitability and one-on-one play in poker.", "review": "Review:###The paper studies * SD-CFR* neural network to approximate the tabular in CFR. It builds on DCFR but computes average strategy from value network buffer and the results show that SD-CFR is better than DCFR in practice. This paper is similar to DCFR in terms of , and the written should be improved. For example, the notion of Leduc poker in section B is not consistent with section 6.1. My major concern is that the experiment is done a small action space poker game, which limits the contribution. How about the performance of SD-CFR in the unlimited version which has more actions and the game size is controllable that can compute the exploitability? Furthermore, the experiment should be conducted more thoroughly. For example, to test the robustness??, apply another network architectures instead of using the DCFR version; verify the network has generalization across info sets instead of just remember the value of each Infoset; evaluate the influence of one-hot encodings of cards rather than just the embeddings chosen in the experiment."}
{"id": "iclr2020_600", "title": "Evaluating and Calibrating Uncertainty Prediction in Regression Tasks | OpenReview", "abstract": "Abstract:###Predicting not only the target but also an accurate measure of uncertainty is important for many applications and in particular safety-critical ones. In this work we study the calibration of uncertainty prediction for regression tasks which often arise in real-world systems. We show that the existing definition for calibration of a regression uncertainty [Kuleshov et al. 2018] has severe limitations in distinguishing informative from non-informative uncertainty predictions. We propose a new definition that escapes this caveat and an evaluation method using a simple histogram-based approach inspired by reliability diagrams used in classification tasks. Our method clusters examples with similar uncertainty prediction and compares the prediction with the empirical uncertainty on these examples. We also propose a simple scaling-based calibration that preforms well in our experimental tests. We show results on both a synthetic, controlled problem and on the object detection bounding-box regression task using the COCO and KITTI datasets.", "review": "Review:###This paper focuses on the calibration for the regression problem. First, it investigates the shortcomings of a recently proposed calibration metric [1] for regression, and show theoretically where this metric can be fooled. Later, it introduces new metrics for measuring the calibration in regression problem which are instantiated from similar idea as the ECE calibration metric [2] used for classification problem. The paper defines the uncertainty as the mean square error and like the ECE idea, it divides the samples into different uncertainty bins and for each bin, it calculates RMSE of network output uncertainty. The RMSE versus variance of estimated uncertainty is depicted as the reliability diagram. To summarize the calibration error as the numbers, the paper proposes two other metrics as ENCE and CV that explain the overall calibration error for the network and calibration error for each sample, respectively. It also shows the effectiveness of the metrics, on the synthetic data where the calibration metric proposed in [1] can be fooled VS. more informative metrics like proposed reliability diagram with RMSE and mVAR. The other experiment is conducted on object detection problem where the uncertainty of bounding box positional outputs is considered as the calibration for the regression problem. Pros: 1- Investigating theoretically the shortcoming of the calibration metric for regression is interesting 2- Proposing the new metrics specifically designed for regression in novel, however it needs more accurate investigation. Cons: The paper is not well-written and the experiments and discussions are not support the ideas, more specifically I can mention several concerns: 1- Motivation: The flaw of the previously proposed calibration metric is not explained clearly. The paper only discusses in which scenario, the metric cannot work properly considering. However, the considered assumption (for instance the output of uncertainty estimation from the network has uniform distribution) can never happen in real scenario. Then the importance of redefining of calibration method is not clear. 2- Accuracy of Proposed Method: The measure proposed for calibration (Eq. 8) should be shown it will converge to the definition of calibration proposed in Eq.5, which is missing in the paper. 3- Lack of Clarity in some Parts: The paper introduces the new architecture for adding the uncertainty output to the regression network. But this new architecture is not explained clearly. I suggest the authors add more details about this part. 4- Justification: The paper trains the uncertainty part of the network with NLL loss function and later fine-tunes the output with temperature scaling method. But this calibration method is not related to the proposed metric which is claimed it has better calibration clarity. Then the importance of using the new metric to define the calibration is not clear. 5- Experiments: The experiment setup needs more accuracy. The paper should investigate the shortcomings of the previous metric in the same setting as the new proposed metric. In experiment Sec. 4.1, the settings for obtaining the output of the network uncertainty is different then comparing the results are not fare. The experiments for real scenario is not wide enough. It just shows that the parameters get calibrated. However we expect to see more results about the CV metric and its importance to define. Overall, I think this paper should be rejected as 1) the novelty of proposed metrics are not enough. 2) the motivation and justification of why the proposed metrics is better than previous metrics is not clear. 3) the experiments are not supporting the idea. References: [1] Kuleshov, Volodymyr, Nathan Fenner, and Stefano Ermon. *Accurate Uncertainties for Deep Learning Using Calibrated Regression.* International Conference on Machine Learning. 2018. [2] Naeini, Mahdi Pakdaman, Gregory Cooper, and Milos Hauskrecht. *Obtaining well calibrated probabilities using bayesian binning.* Twenty-Ninth AAAI Conference on Artificial Intelligence. 2015."}
{"id": "iclr2020_601", "title": "Directional Message Passing for Molecular Graphs | OpenReview", "abstract": "Abstract:###Graph neural networks have recently achieved great successes in predicting quantum mechanical properties of molecules. These models represent a molecule as a graph using only the distance between atoms (nodes) and not the spatial direction from one atom to another. However, directional information plays a central role in empirical potentials for molecules, e.g. in angular potentials. To alleviate this limitation we propose directional message passing, in which we embed the messages passed between atoms instead of the atoms themselves. Each message is associated with a direction in coordinate space. These directional message embeddings are rotationally equivariant since the associated directions rotate with the molecule. We propose a message passing scheme analogous to belief propagation, which uses the directional information by transforming messages based on the angle between them. Additionally, we use spherical Bessel functions to construct a theoretically well-founded, orthogonal radial basis that achieves better performance than the currently prevalent Gaussian radial basis functions while using more than 4x fewer parameters. We leverage these innovations to construct the directional message passing neural network (DimeNet). DimeNet outperforms previous GNNs on average by 77% on MD17 and by 41% on QM9.", "review": "Review:###This paper beneficially incorporates directional information into graph neural networks for molecular modeling while preserving equivariance. This paper is a tour de force of architecture engineering. Continuous equivariance, potential field representation, and bandwidth limited basis functions are synthesized in a compelling manner. I found the exposition extremely intelligible despite my lack of familiarity with molecular modeling. The contribution is clear, although applicability beyond the specific domain of molecular modeling is possibly limited. Being continuously equivariant with respect to rotations is interesting, but seems to require problems where the input is encoded as a point cloud in a vector space; I*m not familiar with such problems. Nonetheless, the domain of molecular modeling is sufficiently important in isolation. I recommend acceptance, because the contribution is strong, the writing is excellent, the ideas are well-motivated, and the experiments support the empirical claims."}
{"id": "iclr2020_602", "title": "Doubly Normalized Attention | OpenReview", "abstract": "Abstract:###Models based on the Transformer architecture have achieved better accuracy than models based on competing architectures. A unique feature of the Transformer is its universal application of a self-attention mechanism, which allows for free information flow at arbitrary distances. In this paper, we provide two alternative views of the attention mechanism: one from the probabilistic view via the Gaussian mixture model, the other from the optimization view via optimal transport. Following these insights, we propose a new attention scheme that requires normalization on both the upper and lower layers, called the doubly-normalized attention scheme. We analyze the properties of both the original and the new attention schemes, and find that the doubly-normalized attention mechanism directly mitigates two unwanted effects: it resolves the explaining-away effect and alleviates mode collapse. We conduct empirical studies that quantify numerical advantages for the doubly-normalized attention model, as well as for a hybrid model that dynamically combines both attention schemes to achieve improved performance on several well-known benchmarks.", "review": "Review:###This paper proposes a novel (self)-attention mechanism dubbed *doubly normalized attention*. When taking the attention between keys and query, standard attention proceeds by computing o_i = 1/Z sum_j exp(q_i * k_j) * v_j. This corresponds to normalizing the logits exp(q_i * k_j) over the keys (j). The authors point out that this can lead to an *explaining away* phenomenon: basically, the score for a particular j, exp(q_i * k_j), could be 0 for all i, which means that information coming from position *j* is basically discarded, and the authors argue this is bad. Moreover, they argue that standard attention suffers from *mode collapse* (i didn*t quite get / found clear what this effect corresponds to in standard architectures). To palliate the two problems, they propose to re-normalize the attention weights along the columns. Basically, first computing *responsibilities* (in a EM flavor) p_j = 1/Z sum_i exp(q_i * k_j) , and then computing the attention by renormalizing p_j, o_i = (1 / sum p) sum_j pi_j * v_j. The author show that this corresponds to one step of EM (or basically kmeans if q_i are cluster centers and k_i = v_i are the data points). The authors test their model on headline generation, a visual qa setting and some natural language understanding task, reporting modest gains (0.2-0.4 on SQUAD/MNLI) w.r.t. standard attention. This work has some quite interesting aspects due to the fact that it tries to interpret and generalize attention computations under a more general framework. I appreciated the theoretical work done by the authors, even if sometimes it appeared to me more complex than needed to be. However, this paper also has some weaknesses which prevent me for putting it above the acceptance bar, namely (i) lack novelty of proposed method and (ii) somewhat weak motivation. I*d be happy to increase my score if the author shed light on the problems below. 1) About the lack of novelty: 1.1) The doubly normalized attention appears to me identical to EM routing proposed in the capsule network (ICLR *18, https://openreview.net/pdf?id=HJWLfGWRb). I think the author mention this in the appendix. If it is, what*s the main contribution of the first part of the paper ? I agree that the analogue to Gaussian Mixture Models appears novel but solving GMMs are just a special applications of EM-type algorithms. 2) About weak motivation: 2.1) The authors mention the “explaining away effect” as a problem to solve. It is however unclear as to whether/how the performance of the architecture tested suffered in virtue of this phenomena. Can you justify or quantify this ? Can you attempt at actually computing how much information is discarded by looking at the attention distribution ? You could maybe link this to works analyzing *pruning heads* https://arxiv.org/abs/1905.09418 ? This could give more depth to the paper. 2.2) I didn*t quite get the section on *mode collapse* and why adding residual layers would help in solving mode collapse in standard transformers architectures (as written in a note in the paper). Could you clarify this point ? 2.3) The analogue to optimal transport felt less well-motivated and it was unclear what new understanding was gained from the reframing of self-attention as constrained optimization. I don*t know what to get out of it."}
{"id": "iclr2020_603", "title": "Towards Certified Defense for Unrestricted Adversarial Attacks | OpenReview", "abstract": "Abstract:###Certified defenses against adversarial examples are very important in safety-critical applications of machine learning. However, existing certified defense strategies only safeguard against perturbation-based adversarial attacks, where the attacker is only allowed to modify normal data points by adding small perturbations. In this paper, we provide certified defenses under the more general threat model of unrestricted adversarial attacks. We allow the attacker to generate arbitrary inputs to fool the classifier, and assume the attacker knows everything except the classifiers* parameters and the training dataset used to learn it. Lack of knowledge about the classifiers parameters prevents an attacker from generating adversarial examples successfully. Our defense draws inspiration from differential privacy, and is based on intentionally adding noise to the classifier*s outputs to limit the attacker*s knowledge about the parameters. We prove concrete bounds on the minimum number of queries required for any attacker to generate a successful adversarial attack. For a simple linear classifiers we prove that the bound is asymptotically optimal up to a constant by exhibiting an attack algorithm that achieves this lower bound. We empirically show the success of our defense strategy against strong black box attack algorithms.", "review": " Although this paper*s title contains *certified defense* and *unrestricted adversarial attack*, what I believe this paper is doing is analyzing the query complexity of query-based black-box attacks under simple linear models such as logistic regressions (or kernelized versions). The authors considered a binary classifier with the additional capability of giving *no response* when the confidence is low. In addition, the output of the classifier has to be perturbed by a random Gaussian vector. The authors then define several metrics including defensibility and query privacy to develop the query complexity on the considered model. The authors tested the query performance on two attacks: (1) the sign attack proposed by the authors and (2) the simba attack proposed by Guo et al. I have several concerns regarding this paper: 1. In my perspective, the title is very misleading and does not properly justify the claims made in this paper. *Certified defense* usually refers to consistent top-1 prediction of a perturbed data sample under a defined threat model. The paper reads like the authors are actually certifying the defined defensibility metric but without a threat model to certify. In addition, the attack setting is limited to black-box attacks (i.e. zero-order adversary), whereas in certified defense the attack assumption is white-box. 2. It is also very unclear how unrestricted attack plays a role in the studied problem. In the introduction, the authors* definition of adversarial examples is *any input is considered a valid adversarial example as long as it induces the classifier to predict a different label than an oracle classifier.* But what is the oracle classifier? How do we justify the credibility of the *adversarial examples* in the experiments? 3. Only two black-box attacks were compared in this paper, one is the sign attack proposed by the authors, the other is the simba attack proposed by Guo et al. To my knowledge, simba attack paper has not been published at any peer-reviewed venue. In other words, both attacks are not widely recognized attacks or methods from published papers. Therefore, the performance evaluation is not fully justified. Since there are many black-box attacks from published papers, why not do performance analysis on those attacks? 4. Similar to 3, the classifier setting is also uncommon. Although I am happy to see classifiers have the ability to give no-response, admittedly this type of classifier is rarely used in practice, not to mention the analysis is tied with Gaussian perturbation on the output. The technical contributions can be limited if the main contribution of this paper is characterizing the query complexity (or defensibility) of an uncommon classifier with Gaussian perturbation on the output. I believe providing more insights on how the analysis can be useful to mainstream classifiers are critical and necessary. ***Post-rebuttal comments I thank the authors for the response. I hope the comments areuseful for preparing a future version of this work. ***"}
{"id": "iclr2020_604", "title": "Deep Interaction Processes for Time-Evolving Graphs | OpenReview", "abstract": "Abstract:###Time-evolving graphs are ubiquitous such as online transactions on an e-commerce platform and user interactions on social networks. While neural approaches have been proposed for graph modeling, most of them focus on static graphs. In this paper we present a principled deep neural approach that models continuous time-evolving graphs at multiple time resolutions based on a temporal point process framework. To model the dependency between latent dynamic representations of each node, we define a mixture of temporal cascades in which a node*s neural representation depends on not only this node*s previous representations but also the previous representations of related nodes that have interacted with this node. We generalize LSTM on this temporal cascade mixture and introduce novel time gates to model time intervals between interactions. Furthermore, we introduce a selection mechanism that gives important nodes large influence in both hop subgraphs of nodes in an interaction. To capture temporal dependency at multiple time-resolutions, we stack our neural representations in several layers and fuse them based on attention. Based on the temporal point process framework, our approach can naturally handle growth (and shrinkage) of graph nodes and interactions, making it inductive. Experimental results on interaction prediction and classification tasks -- including a real-world financial application -- illustrate the effectiveness of the time gate, the selection and attention mechanisms of our approach, as well as its superior performance over the alternative approaches.", "review": "Review:###This paper considers modeling continuous time-evolving graphs using a temporal point process framework. It introduces a time gate in the LSTM to handle the temporal dependency and uses an attention mechanism to select relevant nodes to learn the underlying dynamics. Overall, this paper is not easy to understand in detail. Firstly, it is unclear how and why the temporal point process can deal with growing/shrinking graph nodes and changing interactions. Secondly, how does the DIP-UNIT handle the continuous graph changing? What if the graph changes with an uneven speed? Thirdly, how do all the small pieces work together to achieve the goal of the paper? An overview diagram or a toy example would greatly improve the readability of the paper. Besides, what is the computational cost of the proposed network? How large a graph could be and how fast its changes could be captured?"}
{"id": "iclr2020_605", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators | OpenReview", "abstract": "Abstract:###While masked language modeling (MLM) pre-training methods such as BERT produce excellent results on downstream NLP tasks, they require large amounts of compute to be effective. These approaches corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some input tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the model learns from all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by methods such as BERT and XLNet given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where we match the performance of RoBERTa, the current state-of-the-art pre-trained transformer, while using less than 1/4 of the compute.", "review": "Review:###Summary: Authors offer an alternative for masked LM pretraining that*s more sample-efficient called replaced token detection. Their method basically replaces certain input tokens with alternatives which are sampled from a generator and train a discriminative model to determine whether its generated or real. The work shows empirical success getting better results than GPT with a fraction of the compute on GLUE and others. Positives: Idea is simple and makes sense intuitively, but not something one would think immediately would work better with a such a small fraction of the compute. I think the formulations of the experiments and ideas to develop this are adequate. Concerns & Questions: I*d like to see a little more investigation into Table 3. I don*t have intuition over why these results are the way that they are and the text nor the experimentation really gives me an indication. How well does this model work with very very little compute; lets say you have only a couple of gpu hours. Whats the degradation in performance? Overall I*d like to see more clarity in the overall analysis because I*m still unsure how to interpret your results on the why certain choices/experimental groups get the performance numbers they get. ------------------------------------------------------------------------------------------------------------------------ After the author response, I have changed my score to a 6. I think the paper merits acceptance."}
{"id": "iclr2020_606", "title": "Generative Cleaning Networks with Quantized Nonlinear Transform for Deep Neural Network Defense | OpenReview", "abstract": "Abstract:###Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under white-box attacks. In this paper, we develop a new generative cleaning network with quantized nonlinear transform for effective defense of deep neural networks. The generative cleaning network, equipped with a trainable quantized nonlinear transform block, is able to destroy the sophisticated noise pattern of adversarial attacks and recover the original image content. The generative cleaning network and attack detector network are jointly trained using adversarial learning to minimize both perceptual loss and adversarial loss. Our extensive experimental results demonstrate that our approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. For example, it improves the classification accuracy for white-box attacks upon the second best method by more than 40\\% on the SVHN dataset and more than 20\\% on the challenging CIFAR-10 dataset.", "review": "Review:###This paper developed a method for defending deep neural networks against adversarial attacks based on generative cleaning networks with quantized nonlinear transform. The network is claimed to recover the original image while cleaning up the residual attack noise. The authors developed a detector network, which serves as the dual network of the target classifier network to be defended, to detect if the image is clean or being attacked. This detector network and the generative cleaning network are jointly trained with adversarial learning so that the detector network cannot find any attack noise in the output image of generative cleaning network. The experimental results demonstrated that the proposed approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. A few comments: 1. It does not provide theoretical reasons why the prosed method can defend against those attacks. 2. The experiments are a bit messy and the attacks* setup need to improve. 3. The proposed defense showed only empirical results against the target attack. It seems to provide no theoretical / provable guarantees."}
{"id": "iclr2020_607", "title": "Improving Exploration of Deep Reinforcement Learning using Planning for Policy Search | OpenReview", "abstract": "Abstract:###Most Deep Reinforcement Learning methods perform local search and therefore are prone to get stuck on non-optimal solutions. Furthermore, in simulation based training, such as domain-randomized simulation training, the availability of a simulation model is not exploited, which potentially decreases efficiency. To overcome issues of local search and exploit access to simulation models, we propose the use of kino-dynamic planning methods as part of a model-based reinforcement learning method and to learn in an off-policy fashion from solved planning instances. We show that, even on a simple toy domain, D-RL methods (DDPG, PPO, SAC) are not immune to local optima and require additional exploration mechanisms. We show that our planning method exhibits a better state space coverage, collects data that allows for better policies than D-RL methods without additional exploration mechanisms and that starting from the planner data and performing additional training results in as good as or better policies than vanilla D-RL methods, while also creating data that is more fit for re-use in modified tasks.", "review": "Review:###The paper aims to improve exploration in DRL through the use of planning. This is claimed to increase state space coverage in exploration and yield better final policies than methods not augmented with planner derived data. The current landscape of DRL research is very broad, but RRT can only directly be applied in certain continuous domains with continuous action spaces. With learned embedding functions, RRT can be applied more broadly (see *Taking the Scenic Route: Automatic Exploration for Videogames* Zhan 2019). The leap from RRT-like motion planning to the general topic of *planning* for policy search is not well motivated explained with respect to the literature. Uses of Monte Carlo Tree Search (as in AlphaGo) seem obviously related here. This reviewer moves to reject the paper primarily on the grounds of overinterpreting experimental results from a single, extremely simple example RL task. In a domain so small, we can*t tease out the role of exploration, we aren*t engaging with the *deep* of DRL, and we are only considering one specific kind of planning. The implicit claims of general improvement to exploration and improved downstream policies are not supported by the experimental results. At the same time, no theoretical argument is attempted that would make up for the very narrow nature of the experiments. Questions for the authors: - If HalfCheetah is used to motivate the work, and it is so easily available in the open source offerings from OpenAI, why isn*t one (or many more) tasks of *at least* this complexity considered? MountainCar is one of the gym environments with a 2D phasespace compatible with the kinds of plots used in this paper. - Could the authors taxonomize the landscape of planning and provide a specific argument for focusing on RRT? (RRT is a fun algorithm, but how will you draw the attention of other researchers who are currently focused on Atari games?)"}
{"id": "iclr2020_608", "title": "Model-based Saliency for the Detection of Adversarial Examples | OpenReview", "abstract": "Abstract:###Adversarial perturbations cause a shift in the salient features of an image, which may result in a misclassification. We demonstrate that gradient-based saliency approaches are unable to capture this shift, and develop a new defense which detects adversarial examples based on learnt saliency models instead. We study two approaches: a CNN trained to distinguish between natural and adversarial images using the saliency masks produced by our learnt saliency model, and a CNN trained on the salient pixels themselves as its input. On MNIST, CIFAR-10 and ASSIRA, our defenses are able to detect various adversarial attacks, including strong attacks such as C&W and DeepFool, contrary to gradient-based saliency and detectors which rely on the input image. The latter are unable to detect adversarial images when the L_2- and L_infinity- norms of the perturbations are too small. Lastly, we find that the salient pixel based detector improves on saliency map based detectors as it is more robust to white-box attacks.", "review": "Review:###This paper presents a method for training networks to detect adversarial examples and by virtue of doing so, providing defense against adversarial attacks. Two different approaches are examined, in which a saliency map is used in combination with the input as a mask. In one instance the saliency mask is based on a classifier used to distinguish *normal* from adversarial examples. In the other instance, the salient pixels themselves form the basis for defense. In both cases, the saliency map is combined with the image for training a CNN by way of an element-wise product. Overall, this presents a relatively simplistic way of deriving representations of saliency and combining these with inputs for training that builds robustness against white and black box attacks. At the same time, the empirical results presented reveal a considerable degree of success in providing a defense against such attacks. I find that this presents an interesting contribution to the literature addressing both adversarial attacks, and new notions on ways of characterizing saliency."}
{"id": "iclr2020_609", "title": "Discriminative Particle Filter Reinforcement Learning for Complex Partial observations | OpenReview", "abstract": "Abstract:###Deep reinforcement learning has succeeded in sophisticated games such as Atari, Go, etc. Real-world decision making, however, often requires reasoning with partial information extracted from complex visual observations. This paper presents Discriminative Particle Filter Reinforcement Learning (DPFRL), a new reinforcement learning framework for partial and complex observations. DPFRL encodes a differentiable particle filter with learned transition and observation models in a neural network, which allows for reasoning with partial observations over multiple time steps. While a standard particle filter relies on a generative observation model, DPFRL learns a discriminatively parameterized model that is training directly for decision making. We show that the discriminative parameterization results in significantly improved performance, especially for tasks with complex visual observations, because it circumvents the difficulty of modelling observations explicitly. In most cases, DPFRL outperforms state-of-the-art POMDP RL models in Flickering Atari Games, an existing POMDP RL benchmark, and in Natural Flickering Atari Games, a new, more challenging POMDP RL benchmark that we introduce. We further show that DPFRL performs well for visual navigation with real-world data.", "review": "Review:###What is the specific question/problem tackled by the paper? Representation learning in POMDPs in order to ignore spurious information in observations. Is the approach well motivated, including being well-placed in the literature? Some comparisons to related work are missing; while the comparisons would enrich the paper, their absence is not fundamentally limiting to the conclusions. There*s an additional PSR-related work that can be seen as learning representations for POMDPs (Guo et al., Neural predictive belief representations, arXiv:1811.06407). This work is in line with the work of Gregor et al., 2019, and both provide suitable representation learning techniques for POMDPs. These representation learning in the paper is based on action-conditional predictions of future quantities, which is complementary to the approach proposed in the paper. That is, one could conceive adding action-conditional predictions of the future with the particles as the RNN states. Does the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous. I think the support is somewhat adequate. The claim that the proposed method handles spurious information is well supported by the experiment in mountain hike, but not quite so by the Atari experiments. The performance (upon introduction of the *natural* on top of flickering) takes a big hit for both DPFRL and DVRL. Still, the performance improvement of DPFRL over DVRL is still an encouraging result. Summarize what the paper claims to do/contribute. Be positive and generous. The paper proposes a neural implementation of particle filters, by treating samples of RNN states as particles. The particles are used to estimate moment-generating functions evaluated at trained vectors, which in turn are supposed to provide more information for the policy*s decision making. The paper uses a discriminator to shape the representation. The ablation study suggests that all three components (particles, MGFs & discrimination) are necessary. However, the third component has been shown not to be exclusively helpful for representation learning (Gregor et al., Guo et al.) I would suggest a study in comparison to Gregor et al.*s method (DRAW) instead. Clearly state your decision (accept or reject) with one or two key reasons for this choice. I vote for acceptance. Provide supporting arguments for the reasons for the decision. I think the algorithmic idea in this paper is a step in the right direction and can be of interest for the community. I would hope for the benchmarks to be more like the Habitat, and less like Atari with background videos. The conclusions in the latter benchmark seem less likely to apply to tasks in physically structured environments. Provide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment. I think it is important for the paper to qualify the kind of POMDPs being considered. The defining features of most of the environments being used is that the state is observed through a noisy channel. Many POMDPs are of interest because the observations are really providing partial information about the state, even if it is noiseless. This is the case for the Habitat setting. Because the paper*s claims about the adequacy of the method for POMDPs rests on the choice of environments, I think it*s important to quality what kind of POMDPs are being considered here. I would also caution against stating that the environment is closer to the real world. It would perhaps be better to say that the natural flickering is more interesting than the natural and the flickering because it benchmarks robustness to irrelevant information in observations, provided almost in tandem with state information; with intermittently missing observations. Please add some explanation about how the negative examples are sampled for the contrastive estimation."}
{"id": "iclr2020_610", "title": "Tensor Graph Convolutional Networks for Prediction on Dynamic Graphs | OpenReview", "abstract": "Abstract:###Many irregular domains such as social networks, financial transactions, neuron connections, and natural language structures are represented as graphs. In recent years, a variety of graph neural networks (GNNs) have been successfully applied for representation learning and prediction on such graphs. However, in many of the applications, the underlying graph changes over time and existing GNNs are inadequate for handling such dynamic graphs. In this paper we propose a novel technique for learning embeddings of dynamic graphs based on a tensor algebra framework. Our method extends the popular graph convolutional network (GCN) for learning representations of dynamic graphs using the recently proposed tensor M-product technique. Theoretical results that establish the connection between the proposed tensor approach and spectral convolution of tensors are developed. Numerical experiments on real datasets demonstrate the usefulness of the proposed method for an edge classification task on dynamic graphs.", "review": " This paper presents a M-product based temporal GCNs to handle dynamic graphs. Experiments on four real datasets are performed to verify the effectiveness of the proposed model. Overall, I think this paper make a few contributions to advocate tensor M-product. However, there are several big issues as listed below. Given the current status, I could not accept the paper. Pros: 1, The generalization brought by M-product seems to be general as it includes quite a few graph convolution elements for 3D tensors in a natural way. 2, The experimental setup is reasonable. Datasets are collected from practical problems and of moderately large scale. 3, The paper is clearly written and easy to follow. Cons & Questions: 1, My first concern is that M-product formulation does not bring any new insights as people have already used some of the key elements in practice for a long time. For example, the M-transform is just applying 1x1 convolution to multi-channel image. Slice-wise matrix multiplication is also common in practice. 2, Moreover, I think there are several challenges in the M-product formulation which prevent the technique from being practical. (1) Sharing M such that frontal slices of the transformed signal are the same, i.e., each row of M share the same vector, limits the model capacity significantly. If there is no sharing mechanism, then the model learned on one sequence of graphs could not be applied to another sequence of graphs given two sequences have different lengths. (2) If you learn M from data, how could you ensure that M is invertible? In the paragraph before section 4.1, an edge classification formulation is proposed where the inverse M-transform is abandoned. However, if in practice, you do not need the inverse transform, then do those theoretical properties still hold and what is the meaning of introducing such M-product formulation? 3, A few temporal GCN baselines are neither compared or discussed, e.g., [1]. 4, Could you explain why all the other GCN variants performs significantly worse with a symmetrized adjacency matrix compared to using the asymmetric one? [1] Li, Y., Yu, R., Shahabi, C. and Liu, Y., 2017. Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. arXiv preprint arXiv:1707.01926. ====================================================================================================== After I read authors* reply and other reviewers* comments, I would like to keep my original rating as the issues have not been properly addressed. I agree with the Reviewer #4 that the theoretical results are a bit artificial and trivial."}
{"id": "iclr2020_611", "title": "Are Few-shot Learning Benchmarks Too Simple ? | OpenReview", "abstract": "Abstract:###We argue that the widely used Omniglot and miniImageNet benchmarks are too simple because their class semantics do not vary across episodes, which defeats their intended purpose of evaluating few-shot classification methods. The class semantics of Omniglot is invariably “characters” and the class semantics of miniImageNet, “object category”. Because the class semantics are so similar, we propose a new method called Centroid Networks which can achieve surprisingly high accuracies on Omniglot and miniImageNet without using any labels at metaevaluation time. Our results suggest that those benchmarks are not adapted for supervised few-shot classification since the supervision itself is not necessary during meta-evaluation. The Meta-Dataset, a collection of 10 datasets, was recently proposed as a harder few-shot classification benchmark. Using our method, we derive a new metric, the Class Semantics Consistency Criterion, and use it to quantify the difficulty of Meta-Dataset. Finally, under some restrictive assumptions, we show that Centroid Networks is faster and more accurate than a state-of-the-art learning-to-cluster method (Hsu et al., 2018).", "review": "Review:###The paper is concerned with few-shot classification, both its benchmarks and method used to tackle it. The scope of the few-shot classification problem can be set relatively widely, depending on what data is available at what stage. In general few-shot classification is an important ability of intelligent systems and arguably an area in which biological systems outperform current AI systems the most. The paper makes a number of contributions. (1) It suggests an approach to do a specific type of clustering and compares it favorably to the existing literature. In a specific sense the approach does not use supervised labels (“without labels at meta-evaluation time”). (2) It applies that approach to currently existing datasets and achieves “surprisingly high accuracies” in that setting, with the implication that this shows a weakness in these datasets when used for benchmarking (“too easy”). (3) It further suggests a metric, dubbed “class semantics consistency criterion”, that aims to quantify this shortcoming of current benchmarks on these datasets. (4) It assesses a specific meta-dataset using that metric, confirming it is harder in this sense, at least in specific settings. My assessment of the paper is mildly negative; however this is an assessment with low confidence given that I am no expert on few-shot classification or related areas. While the authors first example (the “Mongolian” alphabet of the Omniglot dataset and geometric shapes falling into different categories) illustrates the problem space well and is indeed quite intuitive, the same cannot be said about either the specific setting they consider nor the metric they propose. It’s not immediately clear that the other approaches from the literature they compare their method to were conceived for the setting considered here, or indeed optimized for it. The authors do show good accuracy on clustering Omniglot characters without using labels and thus indeed demonstrate a high amount of class semantics consistency for that dataset. The results on miniImageNet are less clear-cut, and the results of the evaluation of the meta-dataset appear to depend on the specific setting considered. This makes it unclear to what extent the proposed metric is general and predictive. To their credit, the authors state that in future work they are looking to make their metric “more interpretable and less dependent on the backbone architectures”. I believe the paper might benefit from being given additional attention. A streamlined and more accessible version might well be an important contribution in the future."}
{"id": "iclr2020_612", "title": "Deep Audio Priors Emerge From Harmonic Convolutional Networks | OpenReview", "abstract": "Abstract:###Convolutional neural networks (CNNs) excel in image recognition and generation. Among many efforts to explain their effectiveness, experiments show that CNNs carry strong inductive biases that capture natural image priors. Do deep networks also have inductive biases for audio signals? In this paper, we empirically show that current network architectures for audio processing do not show strong evidence in capturing such priors. We propose Harmonic Convolution, an operation that helps deep networks distill priors in audio signals by explicitly utilizing the harmonic structure within. This is done by engineering the kernel to be supported by sets of harmonic series, instead of local neighborhoods for convolutional kernels. We show that networks using Harmonic Convolution can reliably model audio priors and achieve high performance in unsupervised audio restoration tasks. With Harmonic Convolution, they also achieve better generalization performance for sound source separation.", "review": "Review:###The paper considers the effectiveness of standard convolutional blocks for modelling learning tasks with audio signals. The effectiveness of a neural network architecture is assessed by evaluating its ability to map a random vector to a signal corrupted with an additive noise. Figure 1 illustrates this process with a network taking a single standard normal vector as input and having a single target output consisting of some signal corrupted with additive noise. The paper is not well written and it is rather difficult to follow. It is also not well structured with a number of relevant concepts properly described only sections after they appear for the first time. The first issue I had with the paper was the notion of audio prior. It was only after reading the whole paper that I have realized what this means. Having said this, it is unclear why the employed notion would work in general. I see why it could work when the distribution of the input vector and additive noise are correlated. This has not been clarified nor discussed and I believe it merits a couple of sentences. In the introduction, the paper states *... unlike CNNs for image modelling, the design of deep neural networks for auditory signals has not yer converged*. First, it is not clear what it means for the architecture to converge. If we assume that it refers to standard convolutions with a couple of widely accepted filter size and max pooling, the I would say that in speech recognition the structures that work are quite similar for mel-frequency coefficients or fbank features as inputs (which are again convolutional feature extraction layers). Shortly after this, there is a question on justification of network designs. I disagree with a potential implication that this is well understood for image processing. For some insights relevant to speech, the work by Mallat (*Group invariant scattering*, 2012) might be useful. Figure 1 and the paragraph just below its caption are not clear. It is not explained what is the input/output of the network and this is of great importance for the understanding of the illustration in Figure 1. The introduction does not explicitly define the notion of audio prior and the whole paper is about this. In my opinion, it is wrong to assume that a reader has seen the paper by Lempitsky et al (2018). Section 2.1, the optimization objective as formulated implies that z and x_0 are completely independent. I do not see how any meaningful conclusion can be derived by fitting a map between independent input and output vectors. Some assumption is required for the proper notion of *audio prior* (if not, then a discussion arguing for the opposite). Section 3, opening paragraph concludes that standard CNNs are not the best blocks to model learning tasks with audio signals. For this implication, one needs the exact structure of CNN network and more details with regard to the experiment itself. In particular, there are deep CNNs (with mel-frequency coefficients as inputs) that work very well in speech recognition (e.g., on noisy datasets such as aurora4). This illustration does not say anything about the influence of the depth and number of convolutional blocks on a learning task. The language should be more moderate here and, in general, some additional work is required on the motivation of harmonic convolutions. In my understanding, harmonic convolutions are a special case of deformable convolutions (Dai et al., 2017). In essence, standard convolution is applied over time and deformable over the frequency axis of a spectrogram. The main contribution seems to be in that the work provides a structure to the offsets in Dai et al. (Section 2.1, 2017). If I am correct, then this should be discussed in details and the harmonic convolution needs to be placed in the context of prior work. It might help by starting with a review of that work and then introducing the imposed structure on the offset vectors. I am having problems understanding the illustration in Figure 3. In the experiments, the work is evaluated on signal de-noising (audio restoration) and sound separation. The first task is carried out under the assumption that the signal has been corrupted with Gaussian noise and shows advantages of the approach over baselines which include standard convolutional networks. It would be interesting here to see how the depth of a convolutional network affects the performance. Also, as the approach is (in my understanding) a special case of deformable convolutions it would be insightful to show an experiment with that baseline. While additive noise is difficult on its own, many signals are corrupted by channel noise. It would be interesting to add an experiment with different types of channel noise and which network design is more likely to de-convolve the noise from the signal. The second experiment deals with separation of sounds of different musical instruments and the results again show advantages of harmonic convolutions over the baselines."}
{"id": "iclr2020_613", "title": "Continual Learning using the SHDL Framework with Skewed Replay Distributions | OpenReview", "abstract": "Abstract:###Human and animals continuously acquire, adapt as well as transfer knowledge throughout their lifespan. The ability to learn continuously is crucial for the effective functioning of agents interacting with the real world and processing continuous streams of information. Continuous learning has been a long-standing challenge for neural networks as the repeated acquisition of information from non-uniform data distributions generally lead to catastrophic forgetting or interference. This work proposes a modular architecture capable of continuous acquisition of tasks while averting catastrophic forgetting. Specifically, our contributions are: (i) Efficient Architecture: a modular architecture emulating the visual cortex that can learn meaningful representations with limited labelled examples, (ii) Knowledge Retention: retention of learned knowledge via limited replay of past experiences, (iii) Forward Transfer: efficient and relatively faster learning on new tasks, and (iv) Naturally Skewed Distributions: The learning in the above-mentioned claims is performed on non-uniform data distributions which better represent the natural statistics of our ongoing experience. Several experiments that substantiate the above-mentioned claims are demonstrated on the CIFAR-100 dataset.", "review": " I think there might be some interesting ideas in the work, but I think the authors somehow did not manage to position themselves well within the *recent* works on the topic or even with respect to what continual learning (CL) is understood to be in these recent works. E.g. CL is a generic learning problem, and most algorithms are generic (with a few caveats in terms of what information is available) in the sense that they can be applied regardless of task (be it RL, be it sequence modelling etc.). This work seems limited to image classification. The SHDL wavelets pre-processing, if I understood it, is specific for images and probably even there under some assumption (e.g natural images). The autoencoder (middle bit) is trained on all tasks the CL needs to face, if I understood the work correctly (phase 0 + phase 1). This potentially makes the CL problem much simpler because you are limiting yourself to the top layer only when dealing with CL, not the rest. Not to mention that I don*t understand the motivation of the autoencoder. I think ample results show that unsupervised learning fails in many instances to provide the right features and underperforms compared to learning discriminative features by just backproping from the cross entropy (discriminative loss) all the way down. The only instance I know of for doing this is in low data regime where there is no alternative. I think the modularity used needs to be better introduced. Why the autoencoder, why the first layer of wavelets? Is it for the benefit for CL? I can understand the wavelets, since they are not learnt. But the autoencoder? The autoencoder being trained on all data feels like a cheat. I think the citation of the perceptron a bit strange. Do you really use the original perceptrion from 58? Why? We have much better tools now !? I think the different metrics introduced are interesting and useful. Though you should somehow find common ground to existing works as well to ensure a point of comparison. In the results section I almost got lost. What is the final performance on Cifar. How does this compare to a model that is not trained in a CL regime? What loss do you get from the proposed parametrizaton? In the comparison with EWC and iCarl, there the whole model was dealing with the CL problem, right? (all intermediary layers). I*m actually surprised iCarl is not doing better (I expect it can do better than EWC). Maybe provide a few more information of hyperparam used for this comparison. Overall I think the paper is not ready for being published. Not without addressing these points: * role of modularity (if not CL -- then why? ; is the modular structure original or part of the previous works cited, e.g. where the wavelets are introduced and so forth) * better integration with recent literature; provide answers and settings that allow apple to apple comparison so one can easily understand where this approach falls; if the method is not meant for this *traditional settings and metrics* please still provide them, and then motivate why this regime is not interesting and explain better the regime the method is meant for * as it stands the work is light on the low level details; Hyper-params and other details are not carefully provided (maybe consider adding an appendix with all of these). I have doubts that the work is reproducible without these details."}
{"id": "iclr2020_614", "title": "A FRAMEWORK FOR ROBUSTNESS CERTIFICATION OF SMOOTHED CLASSIFIERS USING F-DIVERGENCES | OpenReview", "abstract": "Abstract:###Formal verification techniques that compute provable guarantees on properties of machine learning models, like robustness to norm-bounded adversarial perturbations, have yielded impressive results. Although most techniques developed so far requires knowledge of the architecture of the machine learning model and remains hard to scale to complex prediction pipelines, the method of randomized smoothing has been shown to overcome many of these obstacles. By requiring only black-box access to the underlying model, randomized smoothing scales to large architectures and is agnostic to the internals of the network. However, past work on randomized smoothing has focused on restricted classes of smoothing measures or perturbations (like Gaussian or discrete) and has only been able to prove robustness with respect to simple norm bounds. In this paper we introduce a general framework for proving robustness properties of smoothed machine learning models in the black-box setting. Specifically, we extend randomized smoothing procedures to handle arbitrary smoothing measures and prove robustness of the smoothed classifier by using -divergences. Our methodology achieves state-of-the-art}certified robustness on MNIST, CIFAR-10 and ImageNet and also audio classification task, Librispeech, with respect to several classes of adversarial perturbations.", "review": "Review:###This paper extends existing work on certified robustness using smoothed classifier. The fundamental contribution is a framework that allows for arbitrary smoothing measure, in which certificates can be obtained by convex optimization. A good number of technical contributions Framework for certificate under arbitrary smoothing measure -> Theorem 1, and proof in A.4 (good use of duality) Full calculation/result of certificates under different divergneces in Table 1. Reasonable set of empirical evidence. Overall a lot of good things to be said, below are some questions/comments that could improve the paper: *Technical* Personally, I cannot get a lot of value out of the distinction between full-information and information-limited certification. It’d be great if I can get some clarification on this. *Experiments* Generally, more details of the experiments should be included. How are the convex optimization problems actually solved (e.g., what methods/tools)? How much more computational overhead is there? Oddly, seems like we’re missing CIFAR10 results completely. Sec 5.1., What does each dot in Figure 3a represent? Sec 5.2, Somewhat strangely, Figure 3b is results on l0 perturbation, but not l2. What happens for l2 when we use M>1? How does this compare to other extensions (likely the SOTA) like [1]? Sec 5.3, It is unclear from the writing whether the comparisons to previous works were done on the same ResNet architecutre with the same clean accuracy. Please clarify. I’m not sure why we need the Librispeech results. It’s not motivated clearly. Also, from writing it seems the adversary zeros out a segment. It’s unclear if this is a reasonable kind of attack to expect on speech. If I block out a segment of the speaker, we probably don’t expect any system to do well on speaker recognition. I suggest removing this result, or somehow make it a lot more better motivated/conducted. Clarify if I missed reasons why simply showing your method works on speech is impressive. Here are suggestions on writing: Contribution --- in both the abstract and introduction, the experimental results should be stated clearer. Be more specific, e.g., “Show SOTA certified l2 robustness on X,Y,Z, establish first certified robustness on Librispeech, and first results on certified l0, l1 robustness on A,B,C.” More broadly in the introduction, please motivate why “adversarial attacks as measured by other smoothing measure is important”. Past studies focus on l2-norm not just because they are do-able, but also white noise (which is naturally measured by l2-norm) is something to expect in practice. Justify why l0, l1 would also be important. I recommend accepting this paper, but would do so with more passion if some of the comments/questions can be addressed. Best, Reference: [1] https://arxiv.org/abs/1906.04584"}
{"id": "iclr2020_615", "title": "GumbelClip: Off-Policy Actor-Critic Using Experience Replay | OpenReview", "abstract": "Abstract:###This paper presents GumbelClip, a set of modifications to the actor-critic algorithm, for off-policy reinforcement learning. GumbelClip uses the concepts of truncated importance sampling along with additive noise to produce a loss function enabling the use of off-policy samples. The modified algorithm achieves an increase in convergence speed and sample efficiency compared to on-policy algorithms and is competitive with existing off-policy policy gradient methods while being significantly simpler to implement. The effectiveness of GumbelClip is demonstrated against existing on-policy and off-policy actor-critic algorithms on a subset of the Atari domain.", "review": "Review:###This paper proposed a new off-policy actor-critic algorithm. It uses a noisy version of the current policy, instead of the true policy probability, in the importance sampling part and the policy gradient form. The noisy version is achieved by Gumble-softmax. Intuitively, the noisy policy probability will make the importance sampling ratio less variance, thus improve the off-policy actor-critic algorithm. I think the contribution of this paper is not enough to meet the standard of ICLR in general. The main reason is that this work seems to be a quiet incremental work on top of off-policy actor-critic [Degris et al 2012] and ACER [Wang et al 2016], at the same time, lack of proper empirical and theoretical justification of the algorithm. 1) The proposed algorithm simply replace pi(a|s) with F(s, a) in the off-policy actor-critic algorithm where F is a Gumble-softmax over pi(a|s). All other parts (replay buffer, clipped IS) are from ACER [Wang et al 2016]. So the main contribution is actually using Gumble-softmax instead of using experience replay. 2) It needs to be explained more why adding noise to the policy scores can make the off-policy learning better. It*s probably (?) understandable that adding noise could make importance sampling stable, but for the gradient of log pi part it*s less clear. The paper need to provide more insight on why we want to do this. 3) If we want to add noise to the probability score pi(a|s), a simple baseline is directly adding a uniform noise over the score. This paper compares several ways of adding noise to the logit of policy, but directly adding different noise on the probability are also natural baselines. An even more baseline could be GumbelClip without the noise at all, which can end up with a simpler version of ACER. It*s useful to see these baselines be compared in all domains instead of just 1. 4) The significance of the experimental results is also unclear. In 3/8 domains ACER eventually beat GumbelClip with a pretty large margin, and only in 2/8 GumbelClip ended up with a slightly higher performance than ACER. I acknowledge that ACER ensembles many other techniques to off-policy actor-critic, but I did not see why GumbelClip could not work with these techniques."}
{"id": "iclr2020_616", "title": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection | OpenReview", "abstract": "Abstract:###We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a ``manifold* close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall.", "review": "Review:###This paper adapts the concept of Robust Subspace Recovery (RSR) as a layer in an auto-encoder model for anomaly detection. A loss function is proposed that combines reconstruction error and a regularizer that enforces robustness against outliers. The reconstruction error expresses the accuracy of the nonlinear dimensionality reduction imposed by the autoencoder. The regularizer is the sum of absolute deviations from the latent subspace that represents a linear structure robust against outliers. An alternative procedure is applied where the loss terms are applied iteratively during training. Once trained, the reconstruction error is used directly for anomaly detection with a threshold. The AUC is used as a performance measure. The method is compared against 6 other methods (LOF, OCSVM, IF, DESBM, GT, DAGMM). The setting is fully unsupervised, meaning that the training data contains various amounts of anomalies, and the results are parametrized with the amount of corruption. The results show that the proposed approach outperforms the other methods in most cases, especially for larger amounts of corruption. An ablation study compares the approach with auto-encoder-only and a non-alternating gradient descent (fixed factors for each part of the loss function) and shows that the alternating method outperfroms all by a wide margin. PROS: * A novel approach to fully unsupervised anomaly detection that beats the state of the art. * The RSR layer is a simple fully connected layer and the loss function is simple to calculate, making the approach computationally efficient. * A pseudo-code algorithm is provided in the appendix, which should help reproducibility. * The paper is well written and the math is clearly laid out. * The result benchmarks are sufficiently exhaustive in both the methods that are compared and the datasets used. * The ablation study is informative and shows the effect of the regularization term of the loss function as well as the effect of alternating the gradient descent with the separate losses. CONS: * There is a serious problem in the results (Figure 1) as the AP curves show better scores for larger corruption factors. Are the AP-score graphs flipped ? Please explain. * The AUC and AP scores need to be defined. * The results should include the case where the training data is not contaminated with outliers (c=0). This would correspond to the semi-supervised scenario and it would be very interesting to see how the method compares to DAGMM and GT which are build for that scenario. * It would be interesting to see the effect of varying the subspace dimension. The authors chose 10 for all experiments, why is this number chosen, what would be the effect of choosing a smaller one ? This is a key parameter as it defines the structure of the projection subspace. Should this parameter be systematically tuned for each dataset ? Overall this is a good paper proposing a novel approach to fully unsupervised anomaly detection with state-of-the art results."}
{"id": "iclr2020_617", "title": "Learning To Explore Using Active Neural Mapping | OpenReview", "abstract": "Abstract:###This work presents a modular and hierarchical approach to learn policies for exploring 3D environments. Our approach leverages the strengths of both classical and learning-based methods, by using analytical path planners with learned mappers, and global and local policies. Use of learning provides flexibility with respect to input modalities (in mapper), leverages structural regularities of the world (in global policies), and provides robustness to errors in state estimation (in local policies). Such use of learning within each module retains its benefits, while at the same time, hierarchical decomposition and modular training allow us to sidestep the high sample complexities associated with training end-to-end policies. Our experiments in visually and physically realistic simulated 3D environments demonstrate the effectiveness of our proposed approach over past learning and geometry-based approaches.", "review": "Review:###This paper proposes a new architecture and policy for coverage maximization (which the authors call exploration). Overall the paper is well written, but I have some major concerns. However I am not an expert in navigation / robotics so i have given myself the lowest confidence for this paper. My highest level concern is that this approach seems extremely complicated (eg Figs 1 and 2), as well as employing several sub-algorithms as part of the procedure (eg Fast Marching Method). It*s not clear to me why any of the components are necessary, though I do appreciate the ablation study. But even within that ablation not all components are ablated (e.g., why GRU units?). My experience suggests that extremely complicated architectures such as this one are brittle and don*t generalize (and it goes against Sutton*s *bitter lesson*). The fact that the experiments are so small does not help. Perhaps more challenging domains would yield negative results. Further, how tuned are the baselines? And it seems that the baselines are general RL agents and not optimized for coverage maximization like this architecture. The authors say * We will also open-source the code*, has this been done? Open-sourcing would help others reproduce the results since as it stands I think this is too complicated to be reproduced. The level of intricacy makes me think that perhaps this paper is more suited to a robotics conference. Secondly, the paper mentions exploration a lot, but it*s not clear to me how this is a principled exploration strategy. Exploration is not in fact defined as *visit as much area as possible* or *maximize the coverage in a fixed time budget*, as the authors suggest. In fact the sentences *We follow the exploration task setup proposed by Chen et al. 2019 where the objective is to maximize the coverage in a fixed time budget. [The] coverage is defined as the total area in the map known to be traversable* appears twice in this manuscript. Exploration is better defined within the context of the explore-exploit tradeoff, whereby an agent must sometimes take sub-optimal actions in order to learn more about the environment in the hope of possibly increasing it*s long-term return. Conflating *coverage-maximization* and exploration is confusing. I think the paper should be rewritten to de-emphasize exploration and instead talk about coverage-maximization, which is more accurate. *Exploration has also been studied more generally in RL for faster training (Schmidhuber, 1991).* I certainly would *not* cite Schmidhuber 91 as the canonical reference of exploration in RL. Far, far, more appropriate would be either the Sutton+Barto RL book (which doesn*t do a great job covering exploration but is at least a decent overall reference) or the works of Auer 2002 and Jaksch et al 2010, and related papers. The Schmidhuber citation should be removed and replaced with a few that actually make sense in this context. I don*t understand how the goals (especially long-term) are generated and trained. Is the long-term goal trained using the reward signal? This is not properly explained. *and summarize major these below* typo, probably should be themes or theses? *agnet pose* typo."}
{"id": "iclr2020_618", "title": "SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference | OpenReview", "abstract": "Abstract:###We present a modern scalable reinforcement learning agent called SEED (Scalable, Efficient Deep-RL). By effectively utilizing modern accelerators, we show that it is not only possible to train on millions of frames per second but also to lower the cost. of experiments compared to current methods. We achieve this with a simple architecture that features centralized inference and an optimized communication layer. SEED adopts two state-of-the-art distributed algorithms, IMPALA/V-trace (policy gradients) and R2D2 (Q-learning), and is evaluated on Atari-57, DeepMind Lab and Google Research Football. We improve the state of the art on Football and are able to reach state of the art on Atari-57 twice as fast in wall-time. For the scenarios we consider, a 40% to 80% cost reduction for running experiments is achieved. The implementation along with experiments is open-sourced so results can be reproduced and novel ideas tried out.", "review": "Review:###The paper proposes a new reinforcement learning agent architecture which is significantly faster and way less costly than previously distributed architectures. To this end, the paper proposes a new architecture that utilizes modern accelerators more efficiently. The paper reads very well and the experimental results indeed demonstrate improvement. Nevertheless, even though working in deep learning for years and have also some experience with Reinforcement learning I am not in the position to provide an expert judgment on the novelty of the work. I do not know if ICLR is the right place of the paper (I would probably suggest a system architectures conference for better assessment of the work)."}
{"id": "iclr2020_619", "title": "Stabilizing Off-Policy Reinforcement Learning with Conservative Policy Gradients | OpenReview", "abstract": "Abstract:###In recent years, advances in deep learning have enabled the application of reinforcement learning algorithms in complex domains. However, they lack the theoretical guarantees which are present in the tabular setting and suffer from many stability and reproducibility problems citep{henderson2018deep}. In this work, we suggest a simple approach for improving stability and providing probabilistic performance guarantees in off-policy actor-critic deep reinforcement learning regimes. Experiments on continuous action spaces, in the MuJoCo control suite, show that our proposed method reduces the variance of the process and improves the overall performance.", "review": "Review:###This paper proposed to use target network policy as a conservative policy for performance evaluation. Instead of performing Polyak averaging on the target network, the authors proposed to utilize statistical hypothesis testing to check whether the performance of the online policy is better than the target network policy and update the target network policy according to the results of the hypothesis testing. The paper is clear written and easy to follow the core idea. As for the experiments, the authors evaluate the proposed method on a variant of TD3 (Conservative-TD3) and the experimental results indicate the proposed method indeed reduces the variance of the expected return. Ablation studies has been provided in the appendix to show the effectiveness of the proposed method. Besides the promising results, I believe there are several concerns that should be clarified before we can conclude that the proposed method can improve the stability. - Stability Measurement: While the experimental results show that the proposed method can reduce the variance of expected return, it is not a direct measurement of the stability or the robustness of the learned policy. It is better to show whether the proposed method can satisfy stability or robustness definition of RL algorithms. (For example, whether the proposed method can improve the robustness: Given a policy, by adding a epsilon perturbation to the input, the output is still epsilon-robustness) Otherwise, the author should add some discussion to clarify the difference. - Conservative Updates on Q functions. In Actor critic frameworks such as DDPG or TD3, the performance of the actor is usually determined by the critic. The proposed method is more likely to “select” stable actors rather than directly improving the stable of the training process or the stability of the policy. I wonder whether it is possible to improve the stability of Q updates such that the “selection” process of policy can be easier, which may accelerate the current training process (or making the hypothesis easier to satisfy). - More related work should be compared. The authors only compare the original version of TD3 and the modified proposed method. Other recent proposed methods to improve stability should be compared in the experiments, such as Constrained Policy Optimization (Achiam et. al 2017), Lypunov-based Safe Policy methods (such as Chow et. al 2019). - Noisy Environment. The authors demonstrate the stability of the proposed method in the ordinary mujoco benchmarks. How does the proposed method perform in the noisy MDP settings? Since the original Mujoco implementation is deterministic, the experiments that the author conducted are not enough to show the proposed method can generalize to more realistic settings such as noisy MDPs. It would be more convincing if the method can still perform well in such settings to support the claim. Overall I think the authors proposed a simple but yet effective method to improve the stability of policy, while the current paper requires more comparison with other methods and more challenging settings to show the effectiveness of the proposed method. -------------------------------- I will update my score if the author clarify above questions. ------------------------------- The author clarified one of my main concern, but the other reviewers point out that the comparison is not fair (using only 5 seeds and discarding the failure seeds). Related Papers: Achiam, Joshua, et al. *Constrained policy optimization.* Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017. Chow, Yinlam, et al. *Lyapunov-based Safe Policy Optimization for Continuous Control.* arXiv preprint arXiv:1901.10031 (2019)."}
{"id": "iclr2020_620", "title": "Actor-Critic Approach for Temporal Predictive Clustering | OpenReview", "abstract": "Abstract:###Due to the wider availability of modern electronic health records (EHR), patient care data is often being stored in the form of time-series. Clustering such time-series data is crucial for patient phenotyping, anticipating patients’ prognoses by identifying “similar” patients, and designing treatment guidelines that are tailored to homogeneous patient subgroups. In this paper, we develop a deep learning approach for clustering time-series data, where each cluster comprises patients who share similar future outcomes of interest (e.g., adverse events, the onset of comorbidities, etc.). The clustering is carried out by using our novel loss functions that encourage each cluster to have homogeneous future outcomes. We adopt actor-critic models to allow “back-propagation” through the sampling process that is required for assigning clusters to time-series inputs. Experiments on two real-world datasets show that our model achieves superior clustering performance over state-of-the-art benchmarks and identifies meaningful clusters that can be translated into actionable information for clinical decision-making.", "review": "Review:###The authors propose a clustering approach for time series that encourages instances with similar time profiles to be clustered together. The approach consists of three modules: an encoder, a cluster assigner and a (future outcome) predictor, all specified as neural networks. The objective of the model is to produce cluster embeddings that are as informative of the outcomes as possible, while not being a direct function of covariates. Note that (2) may look misleading because it indicates that the outcome is a function of the cluster assignment, however, it does not show that the assignment is indeed a function of the covariates. It is not entirely clear how a patient is assigned to a cluster provided that cluster assignments are a function of time. It is desirable that performance metrics do not seem affected by the unknown number of clusters, however, this makes for difficult to interpret clusters. More so in practice when the number of identified clusters is a function of the model architecture and hyperparameters (alpha and _x0008_eta). Is the number of clusters selected by cross-validation and if so, what performance metric is used to select the best choice? In Table 3 for UKCF with 3 comorbidities, how are AUROC and AUPRC evaluated provided these are binary predictions?"}
{"id": "iclr2020_621", "title": "A Simple Technique to Enable Saliency Methods to Pass the Sanity Checks | OpenReview", "abstract": "Abstract:###{em Saliency methods} attempt to explain a deep net*s decision by assigning a {em score} to each feature/pixel in the input, often doing this credit-assignment via the gradient of the output with respect to input. Recently citet{adebayosan} questioned the validity of many of these methods since they do not pass simple {em sanity checks}, which test whether the scores shift/vanish when layers of the trained net are randomized, or when the net is retrained using random labels for inputs. % for the inputs. %Surprisingly, the tested methods did not pass these checks: the explanations were relatively unchanged. We propose a simple fix to existing saliency methods that helps them pass sanity checks, which we call {em competition for pixels}. This involves computing saliency maps for all possible labels in the classification task, and using a simple competition among them to identify and remove less relevant pixels from the map. Some theoretical justification is provided for it and its performance is empirically demonstrated on several popular methods.", "review": "Review:###I Summary The paper directly answers two sanity checks for saliency maps proposed by Adebayo et al (2018): 1. randomizing the weights of a model to prove that the input*s resulting saliency map is different from a trained model* saliency map. 2. randomizing the inputs* labels to make the same proof. The authors propose a *competitive version of saliency method* which uses the saliency scores of every pixel for each labels and zero out: positive scored pixels that would not be maximal for the predicted class and negative scored pixels that are not minimal for the worst predicted label. Overall the method solves the aforementioned sanity checks, the authors claim it also generates more refined saliency maps. II Comments 1. Content The paper can be hard to read, due to multiple writing mistakes, abrupt phrasing, not well-articulated sentences. However, the idea is easy to understand and interesting but the contribution does not seem strong enough in its actual state. My main concern is that the method seems to be designed only to answer the sanity checks: the resulting saliency maps can hardly be seen as more informative as other existing methods (eg figure 1). Quantitative measures (ROAR & KAR, Hooker et al. 2018) or surveys to show that the newly obtained saliency maps are more refined or help to best localize regions of interest would be a big bonus. 2. Writing The paper comports numerous typos, those do not impact the score of the review except if the sentence is not understandable. Please see the following points as support to improve the clarity of the paper. - Abstract last sentence: *Some theoretical justification is provided* -> *Some* is vague and makes your claim less credible -> *theoretical justifications are given in the last paragraph to support our method...* - Intro paragraph 2 first sentence lack some words, l 2 product -> a product *See paper XX et al* -> *As in XX et al, we can see that* or *As stated in XX et al*, *See* is too familiar, formalizing the phrasing gives more credibility to your work - Related work *To give an example, does the map change a lot if we blank out or modify a portion of the image that humans find insignificant Kim et al. (2019)? * This is not very well articulated, *a lot* is vague and a little familiar, *significantly* could be used here. Moreover, the citation is a little abrupt *as we can see in XX* would work better Little typo on etc.. *fare best* -> far better? The wording is still vague, it would help to add a quantitative measure that*s -> that is - Section 3 *This figure highlights* -> which figure? (I think you just missed citing the fig here) - Section 4 First sentence: Why is it a good idea? The claim is a little abrupt and could be detailed a little more *destroy the saliency map* -> destroy is a very strong word *These random variables are complicated.* -> This statement seems a little out of place and abrupt *some constants* -> *constants* (too vague otherwise as before) - Subsection 4.1 *randomly sampling a few such methods* I believe there is a typo? *See figure 3* is abrupt as a sentence itself *as you can see in figure 3 bla bla* Figure 4 The image is small and hard to see on printed paper (same for the images in the appendix, they could be stacked over multiple lines instead of just one horizontal row) Definition 2 punctuation at the end - Section 5 *The available code for these maps is slow, and computing even gradient for all 1000 ImageNet labels can be rather slow.* What is the aim of this sentence? - subsection 5.3 lables -> labels III Conclusion The method itself is interesting, it would be interesting to see more qualitative results on the obtained saliency map itself: Does it produce more information? Is it more meaningful etc. Because as of now, it only seems to answer the two aforementioned sanity checks. As for the writing, it is not always clear and can impede the understanding of the paper. I would be glad to change my review if those points are addressed."}
{"id": "iclr2020_622", "title": "City Metro Network Expansion with Reinforcement Learning | OpenReview", "abstract": "Abstract:###This paper presents a method to solve the city metro network expansion problem using reinforcement learning (RL). In this method, we formulate the metro expansion as a process of sequential station selection, and design feasibility rules based on the selected station sequence to ensure the reasonable connection patterns of metro line. Following this formulation, we train an actor critic model to design the next metro line. The actor is a seq2seq network with attention mechanism to generate the parameterized policy which is the probability distribution over feasible stations. The critic is used to estimate the expected reward, which is determined by the output station sequences generated by the actor during training, in order to reduce the training variance. The learning procedure only requires the reward calculation, thus our general method can be extended to multi-factor cases easily. Considering origin-destination (OD) trips and social equity, we expand the current metro network in Xi*an, China, based on the real mobility information of 24,770,715 mobile phone users in the whole city. The results demonstrate the effectiveness of our method.", "review": "Review:###In this paper the authors train a seq2seq model through reinforcement learning to iteratively expand a city metro network. The authors show that different objectives can be satisfied with this approach, such as the accessibility to different areas (something the authors call social equity indicator) or maximising origin-destination trips. The paper is interesting but could use a more extensive comparison to alternative approaches or ablated version of the same approach. For example, what if the approach would only take into account the last metro station instead of the complete previous sequence? Would it work less well? Additionally, the baseline method the approach is compared against is not explained in enough detail. In addition to RL methods, method such as genetic algorithm have shown great promise in producing layouts, such as for wind turbines (e.g. Grady et al. “Placement of wind turbines using genetic algorithms”). I wonder if such an approach would work equally well for designing metro lines and if RL is really the best technique here (which it might be but I’m not convinced yet). Because of the mentioned shortcomings, I believe the paper should be improved before publication. Additionally, the paper would benefit from a careful spell and grammar check. I found multiple typos, especially in the introduction."}
{"id": "iclr2020_623", "title": "P-BN: Towards Effective Batch Normalization in the Path Space | OpenReview", "abstract": "Abstract:###Neural networks with ReLU activation functions have demonstrated their success in many applications. Recently, researchers noticed a potential issue with the optimization of ReLU networks: the ReLU activation functions are positively scale-invariant (PSI), while the weights are not. This mismatch may lead to undesirable behaviors in the optimization process. Hence, some new algorithms that conduct optimizations directly in the path space (the path space is proven to be PSI) were developed, such as Stochastic Gradient Descent (SGD) in the path space, and it was shown that SGD in the path space is superior to that in the weight space. However, it is still unknown whether other deep learning techniques beyond SGD, such as batch normalization (BN), could also have their counterparts in the path space. In this paper, we conduct a formal study on the design of BN in the path space. According to our study, the key challenge is how to ensure the forward propagation in the path space, because BN is utilized during the forward process. To tackle such challenge, we propose a novel re-parameterization of ReLU networks, with which we replace each weight in the original neural network, with a new value calculated from one or several paths, while keeping the outputs of the network unchanged for any input. Then we show that BN in the path space, namely P-BN, is just a slightly modified conventional BN on the re-parameterized ReLU networks. Our experiments on two benchmark datasets, CIFAR and ImageNet, show that the proposed P-BN can signi?cantly outperform the conventional BN in the weight space.", "review": "Review:###The proposal is an adapted batch normalization method for path regularization methods used in the optimization of neural networks. For neural networks with Relu activations, there exits a particular singularity structure, called positively scale-invariant, which may slow optimization. In that regard, it is natural to remove these singularities by optimizing along invariant input-output paths. Yet, the paper does not motivate this type of regularization for batchnormalized nets. In fact, batch normalization naturally remedies this type of singularity since lengths of weights are trained separately from the direction of weights. Then, the authors motivate their novel batch-normalization to gradient exploding (/vanishing) which is a completely different issue. I am not sure whether I understood the established theoretical results in this paper. Let start with Theorem 3.1: I am not sure about the statement of the theorem. Is this result for a linear net? I think for a Relu net, outputs need an additional scaling parameter that depends on all past hidden states (outputs). Theorem 3.2 and 4.1 do not seem informative to me. Authors are saying that if some terms in the established bound in Theorem 4.1 is small, then exploding gradient does not occur for their novel method. The same argument can be applied to the plain batchnorm result in Theorem 3.2. For me, it is not clear to see the reason why the proposed method remedies the gradient exploding (/vanishing)."}
{"id": "iclr2020_624", "title": "How many weights are enough : can tensor factorization learn efficient policies ? | OpenReview", "abstract": "Abstract:###Deep reinforcement learning requires a heavy price in terms of sample efficiency and overparameterization in the neural networks used for function approximation. In this work, we employ tensor factorization in order to learn more compact representations for reinforcement learning policies. We show empirically that in the low-data regime, it is possible to learn online policies with 2 to 10 times less total coefficients, with little to no loss of performance. We also leverage progress in second order optimization, and use the theory of wavelet scattering to further reduce the number of learned coefficients, by foregoing learning the topmost convolutional layer filters altogether. We evaluate our results on the Atari suite against recent baseline algorithms that represent the state-of-the-art in data efficiency, and get comparable results with an order of magnitude gain in weight parsimony.", "review": " This paper suggests three different disconnected ideas for improving the number of parameters of deep vision models for playing ATARI and to improve the training speed. - Tensor-regression layer to replace fully connected layers - Wavelet-scattering layer to replace the first convolutional layer - Second order optimization (K-FAC) All the ideas mentioned in this paper are existing ones (although properly attributed), so the novelty of this work is relatively low. The paper mentions that this particular combination is *novel*, but it is not clear is there is any significant synergy between these methods and why it should be considered interesting in this particular setup. Also the paper conflates sample-efficiency with parameter-efficiency. However, there is no indication that any of these methods address sample-efficiency which would be an interesting and useful contribution. Also the experiments are neither very conclusive nor are they easy to interpret. For example in the pong case, there is no discernable effect of the compression ratio as the highest and lowest compression give the best (and comparable) results. Also the results come without confidence intervals. So, in general, I would consider this paper to be an uninspired combination of pre-existing ideas with weak and inconclusive experimental results: a clear reject."}
{"id": "iclr2020_625", "title": "A multi-task U-net for segmentation with lazy labels | OpenReview", "abstract": "Abstract:###The need for labour intensive pixel-wise annotation is a major limitation of many fully supervised learning methods for image segmentation. In this paper, we propose a deep convolutional neural network for multi-class segmentation that circumvents this problem by being trainable on coarse data labels combined with only a very small number of images with pixel-wise annotations. We call this new labelling strategy ‘lazy’ labels. Image segmentation is then stratified into three connected tasks: rough detection of class instances, separation of wrongly connected objects without a clear boundary, and pixel-wise segmentation to find the accurate boundaries of each object. These problems are integrated into a multi-task learning framework and the model is trained end-to-end in a semi-supervised fashion. The method is demonstrated on two segmentation datasets, including food microscopy images and histology images of tissues respectively. We show that the model gives accurate segmentation results even if exact boundary labels are missing for a majority of the annotated data. This allows more flexibility and efficiency for training deep neural networks that are data hungry in a practical setting where manual annotation is expensive, by collecting more lazy (rough) annotations than precisely segmented images.", "review": " The submission presents a neural network for multi-task learning on sets of labeled data that are largely weakly supervised (in this case, partially segmented instances), augmented by comparatively fewer fully supervised annotations. The multiple tasks are designed to make use of both the weak as well as as full (‘strong’) labels, such that performance on fully annotated machine-generated output is improved. As noted in the related work section (Section 2), multi-task methods aim to use benefits from underlying common information that may be ignored in a single-task setting. The network presented here is quite similar to most of these multi-task approaches: a common feature encoder, and partially distinct feature decoding and classification parts. The (minor) novelty mainly comes from the distinct types of weak/strong annotation data fed here: instance scribbles, boundary scribbles, and (some or few) full segmentations. The submission is overall well written and provides sufficient clarity and a good overview of the approach. Section 3 presents a probabilistic decomposition of the proposed architecture. With some fairly standard assumptions and simplifications, the loss in Eq. 3 becomes rather straightforward (weighted cross entropy) The actual network architecture described in Section 3.2 takes a standard U-Net as a starting point and modifies it in a fairly targeted way for the different expected types of annotations. These annotations (Section 3.3) are cheaper than full labels on a same-size dataset; it is not completely clear, however, if the mentioned scribbles need to capture each instance in the training set, or if some can also be left out. Without this being explicitly mentioned, I will assume the former. The experimental evaluation is done reasonably well, although I am not familiar with any of the presented data sets. The SES set seems to be specific to the submission, while the H&E data set has been used at least one other relevant publication (Zhang et al.). My main issue here is that at least on the SES set, which does not seem to be that large, the score difference is not that big, so dataset bias could play some part (which is unproven, but so is the opposite). Experimental evaluation does not leave the low-number-of-classes regime, and I’m left wondering how the method might compare on a semantically much richer data set, e.g. Cityscapes. Finally, unmodified U-Net is by now a rather venerable baseline, so I’m also wondering how the proposed multi-task learning could be used in other (more recent) architectures, i.e. whether the idea can be generalized sufficiently. While I think the ideas per se have relatively minor novelty, the combination seems novel to me, and that might warrant publication."}
{"id": "iclr2020_626", "title": "Super-AND: A Holistic Approach to Unsupervised Embedding Learning | OpenReview", "abstract": "Abstract:###Unsupervised embedding learning aims to extract good representations from data without the use of human-annotated labels. Such techniques are apparently in the limelight because of the challenges in collecting massive-scale labels required for supervised learning. This paper proposes a comprehensive approach, called Super-AND, which is based on the Anchor Neighbourhood Discovery model. Multiple losses defined in Super-AND make similar samples gather even within a low-density space and keep features invariant against augmentation. As a result, our model outperforms existing approaches in various benchmark datasets and achieves an accuracy of 89.2% in CIFAR-10 with the Resnet18 backbone network, a 2.9% gain over the state-of-the-art.", "review": "Review:###This paper proposed a so-called super-AND algorithm to learn useful representations in an unsupervised manner, which hopefully reduces the demand for training deep networks with a large amount of labeled samples. The main idea follows the anchor neighborhood discovery approach proposed by Huang et al., by first finding neighborhood for each sample, performing neighborhood selection (curriculum learning) and finally optimizing a unification entropy loss (and data augmentation loss) that distinguished one neighborhood pairs from another. The novelty of the paper looks very limited considering its similarity with Huang et al’s paper. Besides, the writing and organization of the paper have a large space of improvement. In its current version, the discussion of basic concept and ideas look fragmented and incoherent. It is not easy to read through the paper to clearly capture the main theme of the paper. Many notations are hard to follow. For example, in defining the problem, authors mentioned that the bold p_i in equation (1) is defined as the probability of image being in its own class- what is the meaning of this? Then why p_i is a vector and the th entry corresponds to one memory m_j? Does each memory slot correspond to one class? If so, how do you update the memory? And if not, why the probability vector p has a dimension that is the same as the number of memory slots? This would look very confusing to the readers. Another example is that there are many different versions of the loss functions listed in Section~3, like unification entropy loss and augmentation loss; are they both optimized and can authors clearly state their global loss function? The motivation of the paper is to separate samples from different classes far away from each other and local anchor neighbors should have similar labels; however, from the embedding visualization presented in figure~(3), I feel that it is very similar to (or even a bit worse than) the original AND algorithm in terms of class separability. I hope that the authors can spend more efforts clarifying their ideas and make their writing coherent so that readers can have a better experience reading it. Also avoid using vague terms like “learning representations that are visually meaningful” without clearly elaborating on its meaning."}
{"id": "iclr2020_627", "title": "Self-Educated Language Agent with Hindsight Experience Replay for Instruction Following | OpenReview", "abstract": "Abstract:###Language creates a compact representation of the world and allows the description of unlimited situations and objectives through compositionality. These properties make it a natural fit to guide the training of interactive agents as it could ease recurrent challenges in Reinforcement Learning such as sample complexity, generalization, or multi-tasking. Yet, it remains an open-problem to relate language and RL in even simple instruction following scenarios. Current methods rely on expert demonstrations, auxiliary losses, or inductive biases in neural architectures. In this paper, we propose an orthogonal approach called Textual Hindsight Experience Replay (THER) that extends the Hindsight Experience Replay approach to the language setting. Whenever the agent does not fulfill its instruction, THER learn to output a new directive that matches the agent trajectory, and it relabels the episode with a positive reward. To do so, THER learns to map a state into an instruction by using past successful trajectories, which removes the need to have external expert interventions to relabel episodes as in vanilla HER. We observe that this simple idea also initiates a learning synergy between language acquisition and policy learning on instruction following tasks in the BabyAI environment.", "review": "Review:###This paper proposes THER (textual hindsight experience replay), which extends the HER algorithm to the case when goals are represented as language. Whereas in HER the mapping from states to goals is the identity function, THER trains a separate modeling network which performs a mapping from states to goals represented by language. The policy (represented as a Q-function trained via DQN) takes in the goal (a command) as an additional argument as done in HER, which allows the agent to be commanded different tasks. The authors evaluate THER on the MiniGrid environment, where they demonstrate that THER greatly outperforms vanilla goal-conditioned DQN, even in the presence of significant label noise. Overall, combining HER with language-based goals is an interesting and novel problem, and potentially a promising approach to solving language-conditioned reinforcement learning where sparse rewards are common. The authors show fairly convincingly that THER heavily outperforms DQN, which fails to improve from the random initial policy. However, I have several conceptual concerns with the proposed algorithm: 1) There seems to be a bootstrapping problem in the algorithm, with regards to the instruction generator and the policy. If the algorithm does not succeed in reaching goals, then the instruction generator m_w has little training data. However, if m_w is not good, then the algorithm will not be able to give good reward signal to the policy. HER does not have this problem as it has an oracle goal mapping function, so m_w is always good. Evidently, the algorithm worked on the domain that it was tested in, but do the authors have any intuition on when this bootstrapping behavior could be harmful, or some justification on why it would not happen? If the language was more complex (and not limited to a small set of template instructions), would the THER approach still be reasonable? 2) How does the algorithm detect if a given goal state corresponds to successful execution of a command? Or in the notation of the paper, how is f(s,g) implemented? In general, this does not seem like a trivial question to answer if one were to implement this algorithm in a real-world scenario. My overall decision is borderline (learning towards accept), as the experiments were well done and serve as a good proof-of-concept, but I am unsure if this approach will scale well outside of the particular tested domain."}
{"id": "iclr2020_628", "title": "On Stochastic Sign Descent Methods | OpenReview", "abstract": "Abstract:###Various gradient compression schemes have been proposed to mitigate the communication cost in distributed training of large scale machine learning models. Sign-based methods, such as signSGD (Bernstein et al., 2018), have recently been gaining popularity because of their simple compression rule and connection to adaptive gradient methods, like ADAM. In this paper, we perform a general analysis of sign-based methods for non-convex optimization. Our analysis is built on intuitive bounds on success probabilities and does not rely on special noise distributions nor on the boundedness of the variance of stochastic gradients. Extending the theory to distributed setting within a parameter server framework, we assure exponentially fast variance reduction with respect to number of nodes, maintaining 1-bit compression in both directions and using small mini-batch sizes. We validate our theoretical findings experimentally.", "review": "Review:###The paper presents an improved analysis of the signSGD gradient estimator. The authors propose to relax the requirements on the gradient estimator in Bernstein (2019). The only requirement imposed on the gradient is that it should have the correct sign with probability greater than 1/2. In particular this approach allows the gradient estimate to be biased as opposed to Bernstein (2019) which requires unbiased gradients. The authors also show this condition to be necessary by a small counterexample. In my view the paper presents a relatively minor but still interesting extension of the work in Bernstein (2019). The main problem is that the relaxation is not well motivated in terms of scenarios where this might be applicable. Experimental validation is also very weak. It is claimed in the experiment section that the stochastic gradient of the Rosenbrock function g(x) = del f_i(x) + eps, where eps is a 0-mean Gaussian and i is uniform random is biased. This seems incorrect to me and the gradient estimate should be unbiased when the expectation is taken over the randomness in i and eps. A key claim of the paper is the ability to use biased gradient estimates. Experimental validation of this (in light of the above) is completely missing. The experiments that are presented on MNIST are very general and not very closely connected to the specific claims of the paper. The only real conclusion drawn is that larger batch sizes improve convergence. I think the paper needs better targeted experiments. They need to show covergence in a case where the conditions in Berstein (2019) do not hold. How are the properties of the \rho norm related to the observations on l_1 norm for high and l_2 norm for low SNR components in Bernstein (2019)? If they are related this should be referenced."}
{"id": "iclr2020_629", "title": "Learning to Remember from a Multi-Task Teacher | OpenReview", "abstract": "Abstract:###Recent studies on catastrophic forgetting during sequential learning typically focus on fixing the accuracy of the predictions for a previously learned task. In this paper we argue that the outputs of neural networks are subject to rapid changes when learning a new data distribution, and networks that appear to *forget* everything still contain useful representation towards previous tasks. We thus propose to enforce the output accuracy to stay the same, we should aim to reduce the effect of catastrophic forgetting on the representation level, as the output layer can be quickly recovered later with a small number of examples. Towards this goal, we propose an experimental setup that measures the amount of representational forgetting, and develop a novel meta-learning algorithm to overcome this issue. The proposed meta-learner produces weight updates of a sequential learning network, mimicking a multi-task teacher network*s representation. We show that our meta-learner can improve its learned representations on new tasks, while maintaining a good representation for old tasks.", "review": "Review:###Summary: This paper explores learning without forgetting / the online learning setting. They employ a novel meta-learned learning algorithm to this end. Writing: For the most part the writing was clear and easy to follow. There where a couple typos on the top of page 2 that should be fixed. Motivation: The motivation for wanting meta-learning as well as various algorithmic choices are clear. The one piece of motivation I did not fully understand is why not forgetting on the feature space is so important. My understanding of the method is that it should be applicable in both settings (with and without relearning the last layer). Infact, I would expect the difference between the meta-learned method and the baselines to only increase in this setting. I find the distillation based learning to be a clever alternative to the computationally heavy optimizing over past performance. Experiments: This work provides a nice build up of experiments. Experiment 1 demonstrates the principles. In my opinion you should caution the reader given the meta-train, meta-test split. D_{B_1} and D_{B_2} are the same distribution and thus it will be easy for the learned update rule to memorize features. Given your learned update rule architecture I doubt this will be the case though. I believe the authors are aware of this though as this issue is addressed in experiments 2 and 3. Please include what the error bars are over in the captions. Experiments 2 and 3 are interesting and demonstrate the method on a more realistic setting. From the details it seems like this was difficult to get to work -- needing a complex schedule for example. Further elaboration or study of these details (e.g. ablations) would help the field. Also please include what the +- is for the experiments in table 1. Figure 6 is not referenced in the text. It was also difficult for me to understand though I finally got it. Overall, I believe the baselines could be made considerably stronger. Meta-learning expends considerable compute to find a good learned update rule. Spending similar amounts of compute tuning the baselines would be appreciated. Second, the meta-learned update rule presented here is essentially a learned optimizer and thus considerably more powerful than SGD. What optimizers did you use for LwF and EWC? Where the hyper parameters tuned here in an attempt to use similar compute? Where there learning rate schedules also tuned? Questions / concerns: Cost of running this not discussed. I would expect that both meta-training, and training are considerably more expensive. I am curious in particular One motivation for meta-learning update rules in this way is that this cost can be amortized ahead of time and the learned update rule can transfer to new very different tasks. Without transfer like this, however, it*s unclear if a method such as this is useful in general. Some discussion to this end I think would be helpful. I am not docking this work for not doing this type of generalization work though as we must start someplace and meta-training on similar data distributions is a logical place to do so. I am unclear as to your exact meta-training setup from algorithm 1. Does your meta-gradient (DL/dtheta) get computed every inner iteration (iteration of t)? If so how many steps do you back prop through? As of now it looks like your only backpropping a single iteration / application of f. Second, when computing this meta-gradient do you compute the true derivative or a first order approximation common in other work? Overall: I would recommend this paper for acceptance as it presents an interesting approach to solving the catastrophic forgetting issue with a compelling set of diverse experiments."}
{"id": "iclr2020_630", "title": "HOW IMPORTANT ARE NETWORK WEIGHTS? TO WHAT EXTENT DO THEY NEED AN UPDATE? | OpenReview", "abstract": "Abstract:###In the context of optimization, a gradient of a neural network indicates the amount a specific weight should change with respect to the loss. Therefore, small gradients indicate a good value of the weight that requires no change and can be kept frozen during training. This paper provides an experimental study on the importance of a neural network weights, and to which extent do they need to be updated. We wish to show that starting from the third epoch, freezing weights which have no informative gradient and are less likely to be changed during training, results in a very slight drop in the overall accuracy (and in sometimes better). We experiment on the MNIST, CIFAR10 and Flickr8k datasets using several architectures (VGG19, ResNet-110 and DenseNet-121). On CIFAR10, we show that freezing 80% of the VGG19 network parameters from the third epoch onwards results in 0.24% drop in accuracy, while freezing 50% of Resnet-110 parameters results in 0.9% drop in accuracy and finally freezing 70% of Densnet-121 parameters results in 0.57% drop in accuracy. Furthermore, to experiemnt with real-life applications, we train an image captioning model with attention mechanism on the Flickr8k dataset using LSTM networks, freezing 60% of the parameters from the third epoch onwards, resulting in a better BLEU-4 score than the fully trained model. Our source code can be found in the appendix.", "review": " In this paper, the authors performed an empirical study on the importance of neural network weights and to which extent they need to be updated. Some observations are obtained such as from the third epoch on, a large proportion of weights do not need to be updated and the performance of the network is not significantly affected. Overall speaking, the qualitative result in the paper has already been discovered in many previous work, although the quantitative results seem to be new. However, there is large room to improve regarding the experimental design and the comprehensiveness of the experiments. Just name a few as follows: 1) For different models and different tasks, the quantitative results are different. There is no deep discussion on the intrinsic reason for this, and what is the most important factor that influences the redundancy of weight updates. The authors came to the conclusion that from the third epoch on, no need to update most of the weights. “3” seems to be a magic number to me. Why is it? No solid experiments were done regarding this, and no convincing analysis was made. 2) The datasets used in the experiments are not diverse enough and are not of large scale. For example, the CIFA-10 and MNIST datasets are relatively of small scale. What if the datasets are much larger like ImageNet. In such more complicated case, will the weight updates still be unnecessary? Will the ratio and the epoch number change? What is the underlying factor determining these? For another example, there are many NLP datasets for language understanding and machine translation, which are of large scale. Why choosing an image captioning dataset (which I do not agree to be real-life experiments when compared with language understanding and machine translation)? Can the observations generalizable to more complicated tasks and datasets? 3) The models studied in the paper are also a little simple, especially for the text task. Why just using a single-layer LSTM? Why not popularly used Transformer? As a summary, for an empirical study to be convincing, the tasks, datasets, scales, model structures, detailed settings, and discussions are the critical aspects. However, as explained above, this paper has not done a good job on these aspects. Significantly more work needs to be done in order to make it an impactful work. *I read the author rebuttal, but would like to keep my rating unchanged."}
{"id": "iclr2020_631", "title": "Target-directed Atomic Importance Estimation via Reverse Self-attention | OpenReview", "abstract": "Abstract:###Estimating the importance of each atom in a molecule is one of the most appealing and challenging problems in chemistry, physics, and material engineering. The most common way to estimate the atomic importance is to compute the electronic structure using density-functional theory (DFT), and then to interpret it using domain knowledge of human experts. However, this conventional approach is impractical to the large molecular database because DFT calculation requires huge computation, specifically, O(n^4) time complexity w.r.t. the number of electrons in a molecule. Furthermore, the calculation results should be interpreted by the human experts to estimate the atomic importance in terms of the target molecular property. To tackle this problem, we first exploit machine learning-based approach for the atomic importance estimation. To this end, we propose reverse self-attention on graph neural networks and integrate it with graph-based molecular description. Our method provides an efficiently-automated and target-directed way to estimate the atomic importance without any domain knowledge on chemistry and physics.", "review": "Review:###This paper proposes a reverse self-attention on graph neural networks and integrate it with graph-based molecular description. It provides an efficient-automated and target-directed way to estimate the atomic importance without any domain knowledge on chemistry and physics. In particular, it trains graph attention network to predict the molecular property using the molecular network and calculate the atomic importance scores based on the sum of the incoming attention weights. Pros: This paper presents an interesting application of the graph attention network to other scientific areas. And the developed reverse self-attention score, although simple, is useful in estimating the importance of atoms in a molecular network. Cons: However, the paper also needs some improvement in its evaluation (see detailed comments below). Detailed comments: • In addition to the evaluation metric (7), it is also important to hire human experts to evaluate the performance. For example, just sample some test samples and annotate them by human experts so that they can be compared with the predicted results. • In order to further justify the metric (7), it is also important to report how consistent (7) is with the human evaluation. • The method is not compared with any of the previous methods/baselines. At least, it should evaluate how accurate of the proposed automatic ML-based method is when compared to previous (more expensive) human-based method. • It would be better to mention earlier what are the molecular property to be predicted by GAT during training. Just a few simple examples would be sufficient."}
{"id": "iclr2020_632", "title": "Unsupervised Few Shot Learning via Self-supervised Training | OpenReview", "abstract": "Abstract:###Learning from limited exemplars (few-shot learning) is a fundamental, unsolved problem that has been laboriously explored in the machine learning community. However, current few-shot learners are mostly supervised and rely heavily on a large amount of labeled examples. Unsupervised learning is a more natural procedure for cognitive mammals and has produced promising results in many machine learning tasks. In the current study, we develop a method to learn an unsupervised few-shot learner via self-supervised training (UFLST), which can effectively generalize to novel but related classes. The proposed model consists of two alternate processes, progressive clustering and episodic training. The former generates pseudo-labeled training examples for constructing episodic tasks; and the later trains the few-shot learner using the generated episodic tasks which further optimizes the feature representations of data. The two processes facilitate with each other, and eventually produce a high quality few-shot learner. Using the benchmark dataset Omniglot, we show that our model outperforms other unsupervised few-shot learning methods to a large extend and approaches to the performances of supervised methods. Using the benchmark dataset Market1501, we further demonstrate the feasibility of our model to a real-world application on person re-identification.", "review": "Review:###This paper considers the problem of learning an image representation for few-shot learning without using image labels during training. This is a well-motivated problem since (as the paper points out) learning such a representation using *episodes* of low-shot learning problems as examples may require a large amount of annotated data. The paper proposes an iterative algorithm which alternates between clustering the images using the current model and updating the model using the clusters as *pseudo-labels*. This approach is not particularly elegant as there is no clear objective being optimized, but it may nevertheless be effective. One main claim of the paper is that the iterative nature of this process is key to the success of the algorithm. The choice of model is a multi-layer conv-net which is trained using SGD (Adam). The paper investigates both triplet (with and without hard negatives) and *prototype* losses for learning the model parameters. To find clusters, the paper adopts the DBSCAN algorithm using the Jaccard similarity of the k-reciprocal neighbour sets. I am not aware of other papers that use self-supervised learning to obtain a representation which is specifically suitable for few-shot learning via nearest-neighbour classification. As is noted in the paper, the absence of supervision during training more closely resembles the scenario of few-shot learning in biological systems. The design decisions are well motivated throughout the paper. The appendices are high quality and make the paper much more complete. In particular: the method for choosing epsilon, the empirical study of the effect of epsilon and the discussion of the behaviour of the cluster sizes as training proceeds. Principal concerns: (1.1) The proposed approach is particularly similar to DeepCluster (Caron et al.). Besides the use of a different clustering algorithm, it seems that the main high-level difference is the use of *episodic training*, in which the algorithm is trained to compare examples to a query example, rather than to classify single examples. I would have preferred to see a comparison to non-episodic training. (It might be necessary to train a linear classifier on top of the final feature representation rather than simply build a nearest-neighbour classifier, but still this is convex, cheap and even closed-form in the case of least-squares regression.) (1.2) While the algorithm has been demonstrated on real images in the Market1501 dataset, it would have been much more convincing to see it demonstrated on a more widely-used dataset for few-shot learning such as Mini-ImageNet. (1.3) There are several recent papers on unsupervised feature learning using self supervision, especially as an alternative to ImageNet-classification pre-training. There is no discussion of these approaches, yet they might perform better than the proposed algorithm, especially for tasks with real images. Some examples of such papers are: - *Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks* - *Representation Learning with Contrastive Predictive Coding* - *Unsupervised representation learning by predicting image rotations* The literature on using auto-encoders to learn feature representations is also relevant. Issues with details in the paper: (2.1) The DeepCluster paper observed that non-negligible accuracy could be achieved using a randomly-initialized conv-net (12% when chance is 0.1%). This enabled the use of clusters as pseudo-labels. Is a similar effect observed with your datasets and random initialization? (2.2) For large datasets, the clustering algorithm might be prohibitively expensive? It would be useful to discuss the complexity of this algorithm. (2.3) It would be interesting to see the effect of varying the frequency of the clustering during training (i.e. how many gradient steps are taken before updating the clustering). (2.4) It is concerning that the accuracy drops sharply as \rho increases [6, 7, 8]*10^-5 in Table 4. (2.5) It is stated that batch-normalization at the network output helps prevent over-fitting. Why? My intuition is that it would be more helpful for avoiding regions of the loss function which have a small gradient magnitude. What happens if you remove it? (2.6) What is the test time procedure? Do you use the mean feature when there are k > 1 shots (even for the network trained with the hinge loss)? Do you L2-normalize the representation vectors? (2.7) It seems potentially brittle to use hard negatives in the triplet loss with pseudo-labels? If the labels were wrong, then the hard negatives might not really be negatives. Nevertheless, this does not seem to be an issue, at least with these datasets. (2.8) There are no error-bars anywhere in the paper. Minor comments: (3.1) Tables 3 and 4 would be easier to interpret if plotted on an axis. (3.2) The word *concurrently* suggests that the clustering and the training are performed simultaneously. I would prefer *alternating*. (3.3) A grammatical review of the paper is required. For example in the abstract: *to a large extent (extend) and approaches the performance of (to the performances of) supervised methods*. (3.4) Remember to use log and max in latex to improve the appearance."}
{"id": "iclr2020_633", "title": "FINBERT: FINANCIAL SENTIMENT ANALYSIS WITH PRE-TRAINED LANGUAGE MODELS | OpenReview", "abstract": "Abstract:###While many sentiment classification solutions report high accuracy scores in product or movie review datasets, the performance of the methods in niche domains such as finance still largely falls behind. The reason of this gap is the domain-specific language, which decreases the applicability of existing models, and lack of quality labeled data to learn the new context of positive and negative in the specific domain. Transfer learning has been shown to be successful in adapting to new domains without large training data sets. In this paper, we explore the effectiveness of NLP transfer learning in financial sentiment classification. We introduce FinBERT, a language model based on BERT, which improved the state-of-the-art performance by 14 percentage points for a financial sentiment classification task in FinancialPhrasebank dataset.", "review": "Review:###This paper described the application of BERT in the field of financial sentiment analysis. Authors find that when fine-tuned with in-domain data, BERT outperforms the state-of-the-art, demonstrating that language model pre-training can transfer knowledge learned from unsupervised large corpus to new domain with minimum effort. Experiments are conducted to explore 1) the utility of different in-domain dataset for further pre-training; 2) strategies to avoid catastrophic forgetting, and 3) effectiveness of fine-tuning a subset of the full model. I am in favor of rejecting this paper and my reasons are as follows: First, this paper may lack deeper innovation, although it demonstrates a good application of the BERT models in financial domain. For example, the framework of general-domain LM pretraining, to in-domain LM pretraining and finally in-domain classifier fine-tuning, as well as techniques of catastrophic forgetting were already proposed in Howard & Ruder 2018. Therefore, I think this paper may be more suitable for other (finance) application-oriented venues. Second, the dataset used in evaluation is of small size (for example, Financial PhraseBank test set has one 1K). Thus, even though the paper is about transfer learning to domains without large data, I find it might be more convincing to draw a solid conclusion with a larger test set. This paper is well organized and easy to follow. It may be beneficial to clarify in a few places (if space permits): 1) Some description or statistics of the data may be helpful (e.g., average sentence length or some examples); 2) Citations to Elmo and ULMFit can be made more explicit. Authors did cite Peters 2018 and Howard 2018 at the beginning of the paper, but may want to explicitly associate them with ‘Elmo’ and ‘ULMFit’ when these two terms first occur respectively; 3) For table 2, does the ‘all data’ or ‘data with 100% agreement’ include training data (80%) or just the test data (20%)? The difference between FinBERT(-domain) and ULMFit can be explicitly contrasted in the paper. Is the former initialized with BERT while latter with ULMFit?"}
{"id": "iclr2020_634", "title": "Statistical Adaptive Stochastic Optimization | OpenReview", "abstract": "Abstract:###We investigate statistical methods for automatically scheduling the learning rate (step size) in stochastic optimization. First, we consider a broad family of stochastic optimization methods with constant hyperparameters (including the learning rate and various forms of momentum) and derive a general necessary condition for the resulting dynamics to be stationary. Based on this condition, we develop a simple online statistical test to detect (non-)stationarity and use it to automatically drop the learning rate by a constant factor whenever stationarity is detected. Unlike in prior work, our stationarity condition and our statistical test applies to different algorithms without modification. Finally, we propose a smoothed stochastic line-search method that can be used to warm up the optimization process before the statistical test can be applied effectively. This removes the expensive trial and error for setting a good initial learning rate. The combined method is highly autonomous and it attains state-of-the-art training and testing performance in our experiments on several deep learning tasks.", "review": "Review:###The authors explore how stationarity tests can be leveraged to automatically tune the learning rate during training. Their algorithm also add a robust line search algorithm, to reduce the need to tune the initial learning rate. The paper is clear and the literature review is honest and thorough. However, it is unclear to me if the contribution of the authors is enough, as the method used and its presentation are very close to Lang and al. In particular: - First bullet point on page 2: Because of its conceptual and analytical simplicity, it greatly simplifies implementation and deployment in software packages. It is unclear why the approach proposed by Lang is more complicated to use - It is a recurrent theme in the paper that the proposed method is a simple interval test compared to a more complicated equivalence test in Lang et al. It is unclear to me what the authors mean by that, as pages 5 and 6 of Lang clearly details a confidence interval test too. - If SASA+ is indeed just a new presentation of the algorithm detailed by Lang, the line search contribution does not justify a paper in my opinion - In page 5, *Another major difference is that they set non-stationarity as the null hypothesis and stationarity as the alternative hypothesis (opposite to ours).* I am not sure how the authors arrived to the conclusion that non-stationarity was the null hypothesis in Lang and would appreciate some clarifications on this point. The test used in SASA is a simple t test with a variance corrected to account for the auto-correlation of the gradients. About the empirical work: - The experiments seems plausible. Hyper parameters were search for fairly for the competing methods. Adam could have benefited from a finer learning rate schedule, as it is only decreased once compared to several time for the SGD. I would indeed expect a performance gap between Adam and SGD, but I think most of it in this case comes from the one step schedule. - Line search is performed but no metrics were shown to discuss the computational overhead of evaluating the model and its gradients for different parameters during the search. SALSA appears to be an already existing algorithm on which a line search was plugged in. The line search part, which appears to be the only contribution, is not discussed enough in my opinion (in terms of computation cost for instance) To conclude, I think the presented work is too close to the existing literature and that the progress made is very incremental. EDIT after rebuttal: My concerns have been addressed, I revise my rating from weak reject to weak accept."}
{"id": "iclr2020_635", "title": "Unsupervised Clustering using Pseudo-semi-supervised Learning | OpenReview", "abstract": "Abstract:###In this paper, we propose a framework that leverages semi-supervised models to improve unsupervised clustering performance. To leverage semi-supervised models, we first need to automatically generate labels, called pseudo-labels. We find that prior approaches for generating pseudo-labels hurt clustering performance because of their low accuracy. Instead, we use an ensemble of deep networks to construct a similarity graph, from which we extract high accuracy pseudo-labels. The approach of finding high quality pseudo-labels using ensembles and training the semi-supervised model is iterated, yielding continued improvement. We show that our approach outperforms state of the art clustering results for multiple image and text datasets. For example, we achieve 54.6% accuracy for CIFAR-10 and 43.9% for 20news, outperforming state of the art by 8-12% in absolute terms.", "review": "Review:###This paper proposes a method for unsupervised clustering. Similarly to others unsupervised learning (UL) papers like *Deep Clustering for Unsupervised Learning of Visual Features* by Caron et al., they propose an algorithm alternating between a labelling phase and a training phase. Though, it has interesting differences. For example, unlike the Caron et al. paper, not all the samples get assigned a labels but only the most confident ones. These samples are determined by the pruning of a graph whose edges are determined by the votes of an ensemble of clustering models. Then, these pseudo labels are used within a supervised loss which act as a regularizer for the retraining of the clustering models. Novelties /contributions/good points: * Votes from the clustering models to create a graph * Using a graph to identify the most important samples for pseudo labelling * Modification of the ladder network to be used as clustering algorithm * Good amount of experiments and good results Weaknesses: * The whole experiment leading to Table 1 in page 2 is unclear for me. I have trouble understanding the experiment settings. Could you please rephrase it. About initial/ final clustering for example and the rest as well. The whole thing puzzles me whereas the experiments section at the end is much more clear. * Lack of motivation about why using the Ladder method rather than another one. Other recent methods have better results in semi-supervised learning. * Algorithm 1 seems quite ad-hoc. Do more principled algos exist to solve this problem ? You could write about it and at least explain why it would not be feasible here. The sentence *The intuition is that most of the neighbours of that node will also be connected with each other* is unmotivated: no empirical proof for this ? * Related work section is too light. It is an important section and should really not be hidden or neglected. * In the experiments, you could add the *Deep Clustering for Unsupervised Learning of Visual Features* as baseline as well even if they use it for unsupervised learning as they do clustering as well. * In the experiments, you use the features extracted from ResNet-50 but what about finetuning this network rather than adding something on top or even better starting from scratch. Because here CIFAR-10 benefits greatly from the ImageNet features. I know that you should reproduce the settings from other papers but it might be good to go a bit beyond. Especially, if the settings of previous papers are a bit faulty. * Regarding, the impact of number of models in section D of the appendix, there is no saturation at 10 models. So how many models are necessary for saturation of the performance ? * Minor point: several times, you write *psuedo*. Conclusion: the algorithm is novel and represents a nice contribution. Though, there are a lot of weaknesses that could be solved. So, I am putting *Weak accept* for the moment but it could change towards a negative rating depending on the rebuttal."}
{"id": "iclr2020_636", "title": "Wide Neural Networks are Interpolating Kernel Methods: Impact of Initialization on Generalization | OpenReview", "abstract": "Abstract:###The recently developed link between strongly overparametrized neural networks (NNs) and kernel methods has opened a new way to understand puzzling features of NNs, such as their convergence and generalization behaviors. In this paper, we make the bias of initialization on strongly overparametrized NNs under gradient descent explicit. We prove that fully-connected wide ReLU-NNs trained with squared loss are essentially a sum of two parts: The first is the minimum complexity solution of an interpolating kernel method, while the second contributes to the test error only and depends heavily on the initialization. This decomposition has two consequences: (a) the second part becomes negligible in the regime of small initialization variance, which allows us to transfer generalization bounds from minimum complexity interpolating kernel methods to NNs; (b) in the opposite regime, the test error of wide NNs increases significantly with the initialization variance, while still interpolating the training data perfectly. Our work shows that -- contrary to common belief -- the initialization scheme has a strong effect on generalization performance, providing a novel criterion to identify good initialization strategies.", "review": "Review:###[Summary] This paper studies the impact of initialization noise on the theories of wide neural networks in the Neural Tangent Kernels (NTK) regime. The paper proves that the difference between the trained neural net and the kernel interpolator (with the NTK) can be bounded by O(sigma^L + 1/sqrt{m}), where sigma^2 is the initializing variance of each individual weight entry. Relationships between the generalization error of these two functions are derived from the above bound. [Pros] The general message that this paper conveys is interesting -- the initial network f_{\theta_0}(x), which is typically omitted (or made small by making sigma small) in NTK analyses, can deviate the converged NN from the kernel interpolator in terms of generalization error. [Cons] There are fundamental mistakes in the statements/proofs of Theorem 2, 3, 4: -- Theorem 2: the statement is “whp over W, the bound … holds uniformly for x”. The proof relies on Lemma 3, whose statement is also uniform over x, but the proof applies the Markov inequality *for a single x* and is thus valid only for a single x. (As it’s Markov, it seems not sensible to apply the union bound upon it.) -- Theorem 3: the difference between L^NN_test and L^int_test should be on the order of (sigma^L + 1/sqrt{m}) rather than it squared. To bound the difference in squared loss we have a^2 - b^2 <= O(1) * |a-b| (if a, b are bounded by O(1)). We don’t have a^2 - b^2 <= |a - b|^2. -- Theorem 4: J(X_test) as defined is a vector whose dimension grows with the number of test data points, where the theorem requires it to be a scalar. Indeed the treatment of test data as a fixed matrix (rather than samples from a distribution) is already a bit atypical. *** I have read the authors* rebuttal and the other reviews, and I*m glad to see the issues with Theorem 3 and 4 pointed out above are fixed in the revision. However, I also agree with the other reviewers that the paper in the present stage has not yet demonstrated sufficient technical contributions, and thus I am keeping my original evaluation."}
{"id": "iclr2020_637", "title": "Semi-Supervised Named Entity Recognition with CRF-VAEs | OpenReview", "abstract": "Abstract:###We investigate methods for semi-supervised learning (SSL) of a neural linear-chain conditional random field (CRF) for Named Entity Recognition (NER) by treating the tagger as the amortized variational posterior in a generative model of text given tags. We first illustrate how to incorporate a CRF in a VAE, enabling end-to-end training on semi-supervised data. We then investigate a series of increasingly complex deep generative models of tokens given tags enabled by end-to-end optimization, comparing the proposed models against supervised and strong CRF SSL baselines on the Ontonotes5 NER dataset. We find that our best proposed model consistently improves performance by F1 in low- and moderate-resource regimes and easily addresses degenerate model behavior in a more difficult, partially supervised setting.", "review": "Review:###The paper considers the task of Named Entity Recognition and formulates it as a the task of segmenting a sequence of text tokens by a corresponding sequence of tags. This requires to use a special tag symbol which is labelling text parts that are not in any entity span. Consequently this special tag will never occur in partially annotated training examples. Moreover, the authors consider and explore a multitude of generative models whose decoders (conditional probability of the text sequence given the tag sequence) are beyond simple conditional independent models (as in standard HMMs). To learn such models from semi-supervised and partially labelled training data, the authors propose to use the VAE approach, assuming the encoder to be a linear CRF model (i.e. a conditional HMM). Implementing this program requires (i) to formulate the tasks for unlabelled and partially labelled training data, (2) to use an approximate but differentiable sampler for the encoder model and (3) to compute the KL-divergence for Markov chain models. The authors consider known options for each of these problems and then analyse the resulting approach for a multitude of generative models experimentally. In my opinion the task formulation, the related models and considered learning tasks are highly interesting and conceptually relevant. Nevertheless, I would not recommend to publish the paper in its present state for the following reasons. The paper is in my view to much application oriented and cluttered with various application/implementation details, which are rather obscuring several interesting and relevant conceptual questions. The same holds in my view for the unnecessary large number of considered model variants. The paper can be improved by moving the conceptual and technical explanations from appendices to the main body and delegating application details, details of some model variants etc. to the appendices. Further questions/comments: - Why are you using a fully factorising prior for the tag sequences? Would it be possible to use a Markov chain model here? - Why and where from comes the log probability of the encoder (q) in the supervised loss? - The score function estimation and other similar approaches indeed suffer from high variance gradients. On the other hand, the proposed relaxed *perturb and MAP* approach is clearly an approximation. It is therefore not clear to me why the latter is to be preferred to the former."}
{"id": "iclr2020_638", "title": "A Gradient-Based Approach to Neural Networks Structure Learning | OpenReview", "abstract": "Abstract:###Designing the architecture of deep neural networks (DNNs) requires human expertise and is a cumbersome task. One approach to automatize this task has been considering DNN architecture parameters such as the number of layers, the number of neurons per layer, or the activation function of each layer as hyper-parameters, and using an external method for optimizing it. Here we propose a novel neural network model, called Farfalle Neural Network, in which important architecture features such as the number of neurons in each layer and the wiring among the neurons are automatically learned during the training process. We show that the proposed model can replace a stack of dense layers, which is used as a part of many DNN architectures. It can achieve higher accuracy using significantly fewer parameters.", "review": "Review:###This paper introduces a new neural network architecture, in which all neurons (called *floating neurons*) are essentially endowed with *input* and *output* embedding vectors, the product of which defines the weight of the connection between any two neurons. The authors discuss two network architectures employing floating neurons: (a) multi-layer floating neural networks and (b) farfalle neural network (FNN), in which there is one hidden layer, but additional recurrent connections are introduced between the hidden neurons. As mentioned by the authors, the proposed architecture is similar to architectures employing low-rank weight matrix factorization. In my opinion, the main novelty lies in: (a) *floating neuron* interpretation, (b) additional weight matrix normalization, and (c) FNN architecture similar to that of a *floating neuron* RNN network with additional restrictions. I find the proposed idea to be promising and quite intriguing, but I think that the paper has some room for improvement and provided empirical evidence might be insufficient (including for understanding the importance of individual model components), which in turn makes the claims of potential practical attractiveness less justified. I will be happy to update the final score provided with more compelling arguments or empirical analysis of the proposed architecture. Addressing the following issues might greatly improve the quality of the paper: 1. In Section 4.1, the authors compare FNN and DNN on MNIST and CIFAR10 datasets. My concern is that the authors pick a seemingly arbitrary DNN architecture (just a single one) and restrict comparison to it. One issue is that ~50% accuracy on CIFAR10 can be easily demonstrated by a variety of 5-layer DNN architectures including those much smaller, with just ~600k parameters (!) and possibly even lower. This makes the 90% parameter reduction claim not particularly meaningful. And why were models matched based on the total number of neurons, but not, say, the total number of parameters, or other measures? I believe that these questions require additional discussion and empirical evidence. Just as an example, if it was possible to sample (potentially randomly) different DNN architectures (with a reasonable parameter prior) and compare them with FNNs on a 2D accuracy-parameters plot (or using other important metrics), it would provide much more information to the reader. 2. Another important point that I would like to make is that there is much more that can be done to explore the hyper-parameter space of FNN to isolate which particular factors play a decisive role in its superior accuracy. The authors present us with a specific choice of the normalization function, and values of k and d, but it would be very informative to study how results change when different choices are considered. FNNs differ from DNNs in at least three aspects: usage of low-rank factorization, weight normalization and recurrent structure. How important are these individual aspects? Are some of them redundant, or almost redundant, or do FNNs require all of these components to achieve their peak performance? In other words, I believe that a careful ablation study would greatly improve this publication. 3. As a minor note, I think that the statement that FNNs *are more general* than floating neural networks is only partially correct. If I am not mistaken, FNN can also be *unrolled* and represented as a multi-layer floating neural network with additional parameter sharing. Also, the computational complexity of the constructed FNN (in Theorem 1) appears to be significantly higher than that of the floating neural network (especially for high l). This would imply that FNNs do not necessarily supersede multi-layer floating neural networks, at least when the computational complexity is of importance. 4. There are a few minor misprints throughout the text. For example, in *0<j<=j* in the proof of Theorem 1, or in *R output floating neurons for the final deduction from hidden neuron* (output should be S). Also, I could not find information about the value of d used in the described experiments (which I estimated to be 256; is this correct?). 5. In Section 4.3, the authors propose to use FNNs for the final layers of conventional CNN architectures. The issue is that the VGG16 network chosen for experiments was probably picked because it uses several large fully-connected (FC) layers in its tail whereas all more recent and efficient CNN architectures actually gravitate towards a smaller single FC layer. It is possible that FNNs could still be used in FC layers of these modern networks as well (especially with a large number of classes). But additional empirical results for these architectures would, in my opinion, be much more convincing. Updated: The authors updated the text and addressed many of my questions. In my opinion, this improved the paper and made some of its claims much better justified. I change the rating to *Weak Accept*."}
{"id": "iclr2020_639", "title": "A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions | OpenReview", "abstract": "Abstract:###One of the distinguishing characteristics of modern deep learning systems is that they typically employ neural network architectures that utilize enormous numbers of parameters, often in the millions and sometimes even in the billions. While this paradigm has inspired significant research on the properties of large networks, relatively little work has been devoted to the fact that these networks are often used to model large complex datasets, which may themselves contain millions or even billions of constraints. In this work, we focus on this high-dimensional regime in which both the dataset size and the number of features tend to infinity. We analyze the performance of a simple regression model trained on the random features for a random weight matrix and random bias vector , obtaining an exact formula for the asymptotic training error on a noisy autoencoding task. The role of the bias can be understood as parameterizing a distribution over activation functions, and our analysis actually extends to general such distributions, even those not expressible with a traditional additive bias. Intruigingly, we find that a mixture of nonlinearities can outperform the best single nonlinearity on the noisy autoecndoing task, suggesting that mixtures of nonlinearities might be useful for approximate kernel methods or neural network architecture design.", "review": "Review:###This paper analyzed the asymptotic training error of a simple regression model trained on the random features for a noisy autoencoding task and proved that a mixture of nonlinearities can outperform the best single nonlinearity on such tasks. Comments: 1.The paper is well written and provides sound derivation for the theories. 2. Since this area is out of my expertise, I’m not sure whether merely extending the work of Pennington & Worah (2017) to non-Gaussian data distributions is significant enough or not. 3. Except for Fig 4, the other figures seem out of the context. There is no explanation for the purpose of those figures in the main contents. It is a bit hard for the audience to figure out what to look at in the figures or what the figures try to prove. 4. In “..., and our analysis actually extends to general such distributions, ... ”, “general” should be “generalize”. 5. In “And whether these products generate a medical diagnosis or a navigation decision or some other important output, ..”, “whether” should be “no matter”. 6. “..., they may not be large in comparison to the number of constraints they are designed asked satisfy.” should be “... they are designed to satisfy”."}
{"id": "iclr2020_640", "title": "Smart Ternary Quantization | OpenReview", "abstract": "Abstract:###Neural network models are resource hungry. Low bit quantization such as binary and ternary quantization is a common approach to alleviate this resource requirements. Ternary quantization provides a more flexible model and often beats binary quantization in terms of accuracy, but doubles memory and increases computation cost. Mixed quantization depth models, on another hand, allows a trade-off between accuracy and memory footprint. In such models, quantization depth is often chosen manually (which is a tiring task), or is tuned using a separate optimization routine (which requires training a quantized network multiple times). Here, we propose Smart Ternary Quantization (STQ) in which we modify the quantization depth directly through an adaptive regularization function, so that we train a model only once. This method jumps between binary and ternary quantization while training. We show its application on image classification.", "review": "Review:###This paper studies mixed-precision quantization in deep networks where each layer can be either binarized or ternarized. The authors propose an adaptive regularization function that can be pushed to either 2-bit or 3-bit through different parameterization, in order to automatically determine the precision of each layer. Experiments are performed on small-scale image classification data sets MNIST and CIFAR-10. The proposed regularization method is simple and straightforward. However, many details are not stated clearly enough for reproduction. E.g, since the proposed regularization already promotes binary or ternary weights, whey is there still a thresholding operation at the end of Section 3? Is it because the proposed regularization can not provide strict binary or ternary weights? Does the method require one more hard binarization/ternarization step after _x0008_eta is learned. Indeed, tan(x) is not well-defined when x=pi/2, and the derivative tan*(x)= 1+tan^2(x) can be large when x is near pi/2, and does gradient descent work well in this case? The experiments are only performed on small-scale data sets. Thus it is hard to tell if the proposed method also works for larger networks or data sets? Moreover, it is not fair to use *best validation accuracy* for comparison with other methods since the validation set is seen during training and it is not clear if the hyper-parameters of the proposed methods are tuned for best performance on the seen validation set. It would be more fair to compare the test accuracy like in the BinaryConnect (BC) paper. Yet another concern is that many recent methods that can train mixed-precision networks are not compared. For instance, the HAQ method [1] searches for precision for each layer using the reinforcement learning method, how does the proposed method perform when compared with it? [1]. Wang, Kuan, et al. *HAQ: Hardware-Aware Automated Quantization with Mixed Precision.* Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019."}
{"id": "iclr2020_641", "title": "DropEdge: Towards Deep Graph Convolutional Networks on Node Classification | OpenReview", "abstract": "Abstract:###Over-fitting and over-smoothing are two main obstacles of developing deep Graph Convolutional Networks (GCNs) for node classification. In particular, over-fitting weakens the generalization ability on small dataset, while over-smoothing impedes model training by isolating output representations from the input features with the increase in network depth. This paper proposes DropEdge, a novel and flexible technique to alleviate both issues. At its core, DropEdge randomly removes a certain number of edges from the input graph at each training epoch, acting like a data augmenter and also a message passing reducer. Furthermore, we theoretically demonstrate that DropEdge either retards the convergence speed of over-smoothing or relieves the information loss caused by it. More importantly, our DropEdge is a general skill that can be equipped with many other backbone models (e.g. GCN, ResGCN, GraphSAGE, and JKNet) for enhanced performance. Extensive experiments on several benchmarks verify that DropEdge consistently improves the performance on a variety of both shallow and deep GCNs. The effect of DropEdge on preventing over-smoothing is empirically visualized and validated as well. Codes will be made public upon the publication.", "review": "Review:###The authors propose a simple but effective strategy that aims to alleviate not only overfitting, but also feature degradation (oversmoothing) in deep graph convolutional networks (GCNs). Inspired by dropout in traditional MLPs and convnets, the authors clearly motivate their contribution in terms of alleviating both overfitting and oversmoothing, which are problems established both in previous literature as well as validated empirically by the authors. Ultimately, the authors provide solid empirical evidence that, while a bit heuristic, their method is effective at alleviating at least partially the issues of overfitting and oversmoothing. I vote weak-accept in light of convincing empirical results, some theoretical exploration of the method*s properties, but limited novelty. Pros: Simple, intuitive method Draws from existing literature relating to dropout-like methods Little computational overhead Solid experimental justification Some theoretical support for the method Cons: Method is somewhat heuristic Mitigates, rather than solves, the issue of oversmoothing Limited novelty (straightforward extension of dropout to graphs edges) Unclear why dropping edges is *valid* augmentation Followup-questions/areas for improving score: It would be nice to have a principled way of choosing the dropout proportion; 0.8 is chosen somewhat arbitrarily by the authors (presumably because it generally performed well). There is at least a nice interpretation of choosing 0.5 for the dropout proportion in regular dropout (maximum regularization). As brought up in the comments, edges to drop out to the graph*s properties is an interesting direction to explore. While the authors state that they would like to keep the method simple and general, the method is ultimately devised as an adaptation of dropout to graphs, so exploiting graph-specific properties seems reasonable and a potential avenue to further improving performance. p2: *First, DropEdge can be considered as a data augmentation technique* Why are these augmentations valid; why should the output of the network be invariant to these augmentations? I would like to see some justification for why the proposed random modification of the graph structure is valid; intuitively, it seems like it might make the learning problem impossible in some cases. Deeper analysis of the (more interesting, I think) layer-independent regime would be nice. (As a side-note, the name *layer-independent* for this regime is a bit confusing, as the edges dropped out *do* depend on the layer here, whereas in the *layer dependent* regime, edges dropped out do *not* depend on the layer). Comments: Figure 1 could probably be re-organized to better highlight the comparison between GCNs with and without DropEdge; consolidating the content into 2 figures instead of 4 might be more easily parsable. Adding figure-specific captions and defining the x axis would also be nice. Use *reduce* in place of *retard* p2 * With contending the scalability* improve phrasing p2 *By recent,* -> *Recently,* p2 *difficulty on* -> *difficulty in* p2 * deep networks lying* -> *deep networks lies* p3 *which is a generation of the conclusion* improve phrasing p3 * disconnected between* -> *disconnected from* p4 *adjacent matrix* -> *adjacency matrix* p4 *severer * -> *more severe* p5 *but has no help* -> *but is no help* p5 *no touch to the adjacency matrix* -> improve phrasing"}
{"id": "iclr2020_642", "title": "Localized Meta-Learning: A PAC-Bayes Analysis for Meta-Leanring Beyond Global Prior | OpenReview", "abstract": "Abstract:###Meta-learning methods learn the meta-knowledge among various training tasks and aim to promote the learning of new tasks under the task similarity assumption. However, such meta-knowledge is often represented as a fixed distribution, which is too restrictive to capture various specific task information. In this work, we present a localized meta-learning framework based on PAC-Bayes theory. In particular, we propose a LCC-based prior predictor that allows the meta learner adaptively generate local meta-knowledge for specific task. We further develop a pratical algorithm with deep neural network based on the bound. Empirical results on real-world datasets demonstrate the efficacy of the proposed method.", "review": "Review:###### Summary This paper proposes a tight bound in generalization to new tasks in meta-learning framework, by controlling the task prior with Local Coordinate Coding (LCC) prediction. In classification tasks, the algorithm using this bound demonstrates superior performance over other meta-learning methods which are based on PAC-Bayes bounds, but without the proposed prior prediction. ### Strengths - The paper is well written, and maintains a logical flow with the proofs and inference from them. - The idea and intuition for using a learned prior is sound, and is backed by PAC-Bayes theory. - Proposing a tighter generalization bound O(1/m) as opposed to existing bounds of O(1/sqrt(m)) is a meaningful contribution and its efficacy is well shown in the results. ### Weaknesses - Could the authors comment on how their LCC-basedd prior prediction can be extended to other meta learning setups like regression and reinforcement learning? - The baselines compared with are other PAC-Bayes bounds and successfully justifies the contribution. Could the authors provide a comparison with other meta-learning methods (like [1]) to have a holistic view of where this proposed bound gets this line of work? #### Minor: - Spellings: *pratical* -> *practical* (pg1, abstract); *varible* -> *variable* (pg 3); *simplifies* -> *simplify* (pg6, optimization of LLC) - [2] seems to be a related work, as instead of using the global prior, they identify the task first (similar to localized prior), and then utilize it for better performance. ### References [1] Finn, Chelsea, Pieter Abbeel, and Sergey Levine. *Model-agnostic meta-learning for fast adaptation of deep networks.* Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017. [2] Vuorio, R., Sun, S. H., Hu, H., & Lim, J. J. (2018). Toward multimodal model-agnostic meta-learning. arXiv preprint arXiv:1812.07172. ### Score 6 - Weak Accept"}
{"id": "iclr2020_643", "title": "Distance-Based Learning from Errors for Confidence Calibration | OpenReview", "abstract": "Abstract:###Deep neural networks (DNNs) are poorly-calibrated when trained in conventional ways. To improve confidence calibration of DNNs, we propose a novel training method, distance-based learning from errors (DBLE). DBLE bases its confidence estimation on distances in the representation space. We first adapt prototypical learning for training of a classification model for DBLE. It yields a representation space where a test sample*s distance to its ground-truth class center can calibrate the model*s performance. At inference, however, these distances are not available due to the lack of ground-truth label. To circumvent this by approximately inferring the distance for every test sample, we propose to train a confidence model jointly with the classification model, by merely learning from mis-classified training samples, which we show to be highly-beneficial for effective learning. On multiple data sets and DNN architectures, we demonstrate that DBLE outperforms alternative single-modal confidence calibration approaches. DBLE also achieves comparable performance with computationally-expensive ensemble approaches with lower computational cost and lower number of parameters.", "review": "Review:###Summary: The paper proposes a method to do confidence calibration for deep neural networks. It uses standard episodic training for prototypical networks, and first shows empirically that the distances of the embedded test point to its ground truth class center embedding (*not* the predicted class embedding) are indicative of the confidence of the prediction. Further it proposes to exploit this by training an auxiliary confidence prediction MLP carefully. To do so they demonstrate that the training needs to be done of erroneously predicted training examples cf. all the traning examples. They show results with MLP+MNIST, VGG11+CIFAR10, ResNet50+CIFAR100 and ResNet50+TinyImageNet. Detailed comments: The paper is interesting but largely empirical. It shows empirically that: 1. when prototypical networks (and episodic training) is used the distance of test example to true class center reflects the confidence. 2. this does not hold when `vanilla* training is used 3. an auxiliary MLP can be used to learn to predict this while training only with erroneously classified training examples cf. all training examples The results are reported on three networks (MLP, VGG11 and ResNet50) on different benchmarks of image classification. The confidence prediction improvements wrt baselines are non trivial, while keeping the accuracy similar, and the computation cost lower than competing methods. Ablations studies are also convincing. I would have two broad critical comments on the paper: 1. Would this generalize to other image classification tasks and datasets. Generally distance (embedding) based networks perform less than softmax based networks on bigger datasets, so an immediate disadvantage if that happens, is that you would be trading off accuracy cf. vanilla networks, for better confidence prediction using the required distance based network here. 2. The vanilla training is never formally detailed. I am assuming it was softmax + cross entropy loss with gradient descent. Would some other loss be helpful? Specially the metric learning based losses like contrastive or triplet losses come to mind, since they are also distance based. Does the method work with prototypical networks only or it generalizes to other distance based methods as well? Minor comments: The notations are a bit confusing sometimes, and require going back and forth a bit. Eg. mu is used for representation of feature (Eq4) while it usually denotes a mean of some sort (so the reader*s expectation could be that it represents class center). Similarly, boldface p is used for class centers, which is again a bit confusing as being a probability of some sort. In general the notations are different from the original prototypical networks paper (which I needed to revise); keeping them similar would help the reader. The contribution of the present paper is more than that anyway. The erroneously classified training examples are called errors (eg. just before eq8). By errors one could think that it is a difference between some sort of prediction and the ground truth. Explicitly calling them erroneously classified training examples would help the reader as well. The notation odot is not explained (before eq.9)."}
{"id": "iclr2020_644", "title": "Unsupervised Representation Learning by Predicting Random Distances | OpenReview", "abstract": "Abstract:###Deep neural networks have gained tremendous success in a broad range of machine learning tasks due to its remarkable capability to learn semantic-rich features from high-dimensional data. However, they often require large-scale labelled data to successfully learn such features, which significantly hinders their adaption into unsupervised learning tasks, such as anomaly detection and clustering, and limits their applications into critical domains where obtaining massive labelled data is prohibitively expensive. To enable downstream unsupervised learning on those domains, in this work we propose to learn features without using any labelled data by training neural networks to predict data distances in a randomly projected space. Random mapping is a highly efficient yet theoretical proven approach to obtain approximately preserved distances. To well predict these random distances, the representation learner is optimised to learn class structures that are implicitly embedded in the randomly projected space. Experimental results on 19 real-world datasets show our learned representations substantially outperform state-of-the-art competing methods in both anomaly detection and clustering tasks.", "review": "Review:###This paper proposed a method of unsupervised representation by transforming a set of data points into another space while maintaining the pairwise distance as good as possible. The paper is well structured with background literatures, formula, as well as experiments to show the advantage of the proposed method. I find it generally interesting, with the following major concerns. 1. Representation or dimension reduction? If the original space is a structured space like Euclidean space, then effectively this paper*s method coincides with regular distance preserving method in dimension reduction, and Johnson-Lindenstrauss theories. If the original space is not structured or doesn*t naturally have a good distance measure, then the proposed method cannot work. For example, if the original dataset is a set of documents, and the task is to do representation learning to convert each document into a compact vector. However, there*s no good distance metric for the document space. If TF-IDF is used, then the representation space also inherits TF-IDF type features which is not desired. If more advanced similarity is used for the document space, then the role of representation learning is not essential anymore as that similarity measure can already help the downstream tasks. 2. Section 3 the theoretical analysis. This part seems like a collection of previous works and contains minimal information about the proposed method. 3. Some writing issues, like page 4 line 7 about the equation numbering."}
{"id": "iclr2020_645", "title": "Domain Adaptive Multiflow Networks | OpenReview", "abstract": "Abstract:###We tackle unsupervised domain adaptation by accounting for the fact that different domains may need to be processed differently to arrive to a common feature representation effective for recognition. To this end, we introduce a deep learning framework where each domain undergoes a different sequence of operations, allowing some, possibly more complex, domains to go through more computations than others. This contrasts with state-of-the-art domain adaptation techniques that force all domains to be processed with the same series of operations, even when using multi-stream architectures whose parameters are not shared. As evidenced by our experiments, the greater flexibility of our method translates to higher accuracy. Furthermore, it allows us to handle any number of domains simultaneously.", "review": " In this paper, the authors proposed to address the information asymmetry between domains in unsupervised domain adaptation. Innovatively, they resort to a multiflow network where each domain adaptatively selects its own pipeline. I quite appreciate the idea itself, while there are many essential issues to be addressed first. Pros: - The way tackling the information asymmetry, or untie weights, between domains is novel and interesting. - The proposed network/framework can be easily extended to the multi-task setting, or multi-source/multi-target domain adaptation. - The paper is well-written and easy to follow. Cons: - The most critical downside of this paper is its insufficient experiments to support the whole idea, where we will detail in the next. Experimental issues: - Comparison with other state-of-the-art UDA methods (e.g., CDAN) is a must. This paper improves UDA in terms of adaptive parameters sharing, which is completely independent from most of the UDA contributions (including the DANN you compared) which improve the distribution alignment between feature representations. Therefore, it is imperative to compare that line of SOTA methods, otherwise why should we consider adaptative parameters sharing instead of distribution alignment? At best, the proposed multiflow network combined with the SOTA feature alignment method (e.g., CDAN other than DANN) should be considered and expected to beat CDAN itself. - Many ablation studies or hyperparameter sensitivity analyses are missing. o How do you determine the number of parallel flows, i.e., K? Is it possible that 3 or 4, more than 2 flows, are better even in the UDA between two domains? o Do you try any other possibilities of grouping a computational unit, and how will different configurations influence the performance? o Is there a possibility that none of the gates in the final layer is activated? Do you need some constraints? - Since the authors mentioned the potential of the multi-flow network in adaptation between multiple domains, it is necessary to investigate multi-source or multi-target domain adaptation. Only in this case may the significance of different K values be demonstrated. - The baseline results in Table 1 are not comparable to some reported papers, and even lower than those reported in other UDA papers."}
{"id": "iclr2020_646", "title": "Incremental RNN: A Dynamical View. | OpenReview", "abstract": "Abstract:###Recurrent neural networks (RNNs) are particularly well-suited for modeling long-term dependencies in sequential data, but are notoriously hard to train because the error backpropagated in time either vanishes or explodes at an exponential rate. While a number of works attempt to mitigate this effect through gated recurrent units, skip-connections, parametric constraints and design choices, we propose a novel incremental RNN (iRNN), where hidden state vectors keep track of incremental changes, and as such approximate state-vector increments of Rosenblatt*s (1962) continuous-time RNNs. iRNN exhibits identity gradients and is able to account for long-term dependencies (LTD). We show that our method is computationally efficient overcoming overheads of many existing methods that attempt to improve RNN training, while suffering no performance degradation. We demonstrate the utility of our approach with extensive experiments and show competitive performance against standard LSTMs on LTD and other non-LTD tasks.", "review": "Review:###The authors present a novel work to address the problem of signal propagation in the recurrent neural networks. The idea is to build a attractor system for the signal transition from state h_{k-1} to h_k. If the attractor system converges to a equilibrium, then the hidden to hidden gradient is an identity matrix. This idea is elegant. The authors verify the performance of Increment RNN on long-term-dependency tasks and non-long-term-dependency tasks. The work successfully constructs a negative identity hidden to hidden gradient matrix but I still have a few concerns about the theory and the experiments. If the authors can address my concerns in the rebuttal, I am willing to increase my score. Theoretical concerns: Even in the limit sense, the inner ordinary differential equation of g will converge to the equilibrium, it may not converge in the finite steps. Thus, the state-to-state gradient may be slightly away from the identity. And we know that (0.99)^T goes to infinity when T goes to infinity. In practice, the long-term-gradient problem may still exist in the incremental RNN. Experimental concerns: The theorem requires the norm of U to be bounded. But I cannot see how the authors bound the norm of U in the experiment section. Clarity of writing: There are a bunch of typos in the papers, especially the ones in the proof of Theorem 1. The proof of theorem 1 can be polished. The author swap the use of phi and psi several times in the proof. And gradient calculation on equation (5) should be expanded into more details. The authors also missed one U in the equation in the second last line of the proof. Overall, I think the paper is an interesting contribution to the community."}
{"id": "iclr2020_647", "title": "Exploration via Flow-Based Intrinsic Rewards | OpenReview", "abstract": "Abstract:###Exploration bonuses derived from the novelty of observations in an environment have become a popular approach to motivate exploration for reinforcement learning (RL) agents in the past few years. Recent methods such as curiosity-driven exploration usually estimate the novelty of new observations by the prediction errors of their system dynamics models. In this paper, we introduce the concept of optical flow estimation from the field of computer vision to the RL domain and utilize the errors from optical flow estimation to evaluate the novelty of new observations. We introduce a flow-based intrinsic curiosity module (FICM) capable of learning the motion features and understanding the observations in a more comprehensive and efficient fashion. We evaluate our method and compare it with a number of baselines on several benchmark environments, including Atari games, Super Mario Bros., and ViZDoom. Our results show that the proposed method is superior to the baselines in certain environments, especially for those featuring sophisticated moving patterns or with high-dimensional observation spaces.", "review": "Review:###The paper proposes a novel way to formulate intrinsic reward based on optical flow prediction error. The prediction is done with Flownet-v2 architecture and the training is formulated as self-supervision (instead of the ground-truth-based supervised learning in the original Flownet-v2 paper). The flow predictor takes two frames, predicts forward and backward flows, then warps the first/second frame respectively and compares the warped result with real frame. The comparison error serves as the intrinsic reward signal. The results are demonstrated on 7 environments: SuperMario + 5 Atari games + ViZDoom. On those environments, the proposed method performs better or on-par with ICM and RND baselines. I am leaning towards rejecting this paper. Two key factors motivate this decision. First, the motivation for this work is not fully clear: why would the error in flow prediction be a good driving force for curiosity? Optical flow has certain weaknesses, e.g. might not work well for textureless regions because it*s hard to find a match. Why would those weaknesses drive the agent to new locations? Second, the choice of tasks where the largest improvement is shown (i.e. 5 Atari games) seems not well-motivated and rather crafted for the proposed method. Those 5 Atari games are not established hard exploration games. Detailed arguments for the decision above: [major concerns] * Analysis is need on how the method deals with known optical flow problems: occlusion, large displacements, matching ambiguities. Those problems don*t fully go away with learning and it is unclear how correlated corresponding errors would be with state novelty. * *Please note that ri is independent of the action taken by the agent, which distinguishes FICM from the intrinsic curiosity module (ICM) proposed in Pathak et al. (2017)* - but would it then be susceptible to spurious curiosity effects when the agent is drawn to motion of unrelated things? Like leaves trembling in the wind. ICM was proposed to eliminate those effects in the first place, but what is this paper*s solution to that problem? Furthermore, the experiments on BeamRider show that this concern is not a theoretical one but quite practical. * *CrazyClimber, Enduro, KungFuMaster, Seaquest, and Skiing* - none of those Atari environments are known to be hard exploration games (which are normally Gravitar, Montezuma Revenge, Pitfall!, PrivateEye, Solaris, Venture according to Bellemare et al *Unifying count-based exploration and intrinsic motivation*). I understand that every game becomes hard-exploration if the rewards are omitted but then there is a question why those particular games. Moreover, if you omit the rewards the question remains how to select hyperparameters of your method. Was the game reward used for selecting hyperparameters? If not, what is the protocol for their selection? This is a very important question and I hope the authors will address this. * *These games are characterized by moving objects that require the agents to concentrate on and interact with.* - this looks like tailoring the task to suit the method. * Figure 6 - those results are not great compared to the results of Episodic Curiosity: https://arxiv.org/abs/1810.02274 . Maybe this is because of the basic RL solver (A3C vs PPO) but that brings up another question: why are different solvers used for different tasks in this paper? PPO is normally significantly better than A3C, why not use throughout the whole paper? [minor concerns] * Figures are very small and the font in them is not readable. Figure 2 is especially difficult to read because the axes titles are tiny. * *complex or spare reward* -> sparse * *However, RND does not consider motion features, which are essential in motivating an agent for exploration.* - this is unclear, why are those features essential? * *We demonstrated the proposed methodology and compared it against a number of baselines on Atari games, Super Mario Bros., and ViZDoom.* - please state more clearly that only 5 out of 57 Atari games are considered, here and in the abstract. * *Best extrinsic returns on eight Atari games and Super Mario Bros.* - but only 5 games are shown, where are the other 3? Suggestions on improving the paper: 1) Better motivating the approach in the paper would help. Why using the flow prediction error as a curiosity signal? 2) Better motivating the choice of the environments and conducting experiments on more environments would be important for evaluating the impact of the paper."}
{"id": "iclr2020_648", "title": "Understanding Isomorphism Bias in Graph Data Sets | OpenReview", "abstract": "Abstract:###In recent years there has been a rapid increase in classification methods on graph structured data. Both in graph kernels and graph neural networks, one of the implicit assumptions of successful state-of-the-art models was that incorporating graph isomorphism features into the architecture leads to better empirical performance. However, as we discover in this work, commonly used data sets for graph classification have repeating instances which cause the problem of isomorphism bias, i.e. artificially increasing the accuracy of the models by memorizing target information from the training set. This prevents fair competition of the algorithms and raises a question of the validity of the obtained results. We analyze 54 data sets, previously extensively used for graph-related tasks, on the existence of isomorphism bias, give a set of recommendations to machine learning practitioners to properly set up their models, and open source new data sets for the future experiments.", "review": "Review:###The authors discuss here the problem of isomorphism bias in graph dataset, i.e. the overfitting effect in learning networks whenever graph isomorphism features are incorporated within the model. This is a bias which jeopardises the validity and the reproducibility of several studies, and it is theoretically analogous to data leakage effects. The authors fairly discuss the problem in the introduction, with a good coverage of the related literature; the background theory is reasonably discussed, although is not very deep. The experimental part is extensive and well described, and it shows the overfitting effect very clearly. However, the novelty of the work is limited, and also the proposed solutions cannot be claimed as superior to other approaches, due to the small improvement in accuracy."}
{"id": "iclr2020_649", "title": "Learning from Explanations with Neural Module Execution Tree | OpenReview", "abstract": "Abstract:###While deep neural networks have achieved impressive performance on a range of NLP tasks, these data-hungry models heavily rely on labeled data. To make the most of each example, previous work has introduced natural language (NL) explanations to serve as supplements to mere labels. Such NL explanations can provide sufficient domain knowledge for generating more labeled data over new instances, while the annotation time only doubles. However, directly applying the NL explanations for augmenting model learning encounters two challenges. First, NL explanations are unstructured and inherently compositional, which asks for modularized model to represent their semantics. Second, NL explanations often have large numbers of linguistic variants, resulting in low recall and limited generalization ability when applied to unlabeled data. In this paper, we propose a novel Neural Modular Execution Tree (NMET) framework for augmenting sequence classification with NL explanations. After transforming NL explanations into executable logical forms with a semantic parser, NMET employs a neural module network architecture to generalize different type of actions (specified by the logical forms) for labeling data instances, and accumulates the results with soft logic, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks, relation extraction and sentiment analysis, demonstrate its superiority over baseline methods by leveraging NL explanation. Its extension to multi-hop question answering achieves performance gain with light annotation effort. Also, NMET achieves much better performance compared to traditional label-only supervised models in the same annotation time.", "review": "Review:###One recent work that comes to mind from ACL 2019: Leveraging Language Models for Commonsense Reasoning (Rajani et al 2019). In that work, they also have human annotators provide explanations (extending the CommonsenseQA dataset), and they show that by training with these explanations, inference is improved even without them. They also train a language model to generate the explanations, and they show that the language model generated explanations improve performance further at inference time. Seems like a reasonable reference to contrast the more structured approach to using explanations like Srivasta et al (2017), Hancock et al (2018), and this work. I find the method summary beginning with *Human explanations are first converted to machine-actionable logical forms by a semantic parser* until the end of that paragraph to be unnecessary. Actually, as I read it, I find myself asking a lot of questions that get answered below. So I would prefer scrapping that method summary and just getting straight into the Explanation Parsing. *indicates the the logical form matches* redundant *the* I can*t find a definition for LF(E) anywhere, and yet LF(E) is present in many tables. I see that E is mentioned to be the explanations, but this is only in the caption of Table 2 even though the symbol is first used in the first paragraph of Section 4.1. I*m assuming LF logical forms applied directly to explanations, but this should be stated explicitly. Can you elaborate on why it is so dominant on precision In Table 6 and 7 of the Appendix, but low on recall, rather than just saying this is expected in Section 4.1? *For keyword query q, we directly encode it into vector z_q by bi-LSTM and attention.* Can you elaborate on how z_q is constructed? Is it the final state of a forward LSTM concatenated with the final state of a backward LSTM? Why is it so essential that you study the setting in which explanations are low-resource? I*m curious to see what would happen with more explanations. I am surprised that none of the modules or compared methods include any architecture that use a Transformer or a form of contextualized word vectors (McCann et al 2017, Peters et al 2017, Devlin et al 2018). Is there an explanation for this? I would prefer to see a larger suite of tested tasks given that each of these datasets is quite small. Would any other tasks from benchmarks like GLUE or SuperGLUE be amenable to your approach? The tasks you*ve chosen limit the scope of this work and leaves the question of whether it would generally improve across a greater variety of tasks, especially tasks that have seen significant improvement using new methods. Your claim would be much stronger if the explanations were shown to be helpful even to pretrained models like BERT when fine-tuned for a specific task. In particular, it would be interesting to see how the benefits of explanations vary for different kinds of tasks and for different training set sizes. *In the real world, a more realistic problem is that, with limited human-power, should we just annotate more labels or spend time explaining existing annotations.** I think that you mean *should we* makes it sound like this is a question, but there is no question mark, and it makes more sense as a statement. I propose *...human-power, there is a question of whether it is more valuable to ...* *Explanations prove to be an very efficient form for data annotation.* should be *a very* not *an very*. * a vital rule* should be *a vital role* I find the first paragraph of Section 4.3 quite abstruse and bare in its explanation of the method used."}
{"id": "iclr2020_650", "title": "Co-Attentive Equivariant Neural Networks: Focusing Equivariance On Transformations Co-Ocurring in Data | OpenReview", "abstract": "Abstract:###Equivariance is a nice property to have as it produces much more parameter efficient neural architectures and preserves the structure of the input through the feature mapping. Even though some combinations of transformations might never appear (e.g. a face with a horizontal nose) current equivariant architectures consider the set of all possible transformations in the transformation group while generating feature representations. Contrarily, the human visual system is able to attend to the set of relevant transformations occurring in the environment as to assist and improve object recognition. Based on this observation, we modify conventional equivariant feature mappings such that they are able to attend to the set of co-occurring transformations in data. Our experiments show that neural networks utilizing co-attentive equivariant feature mappings consistently outperform those utilizing conventional ones both for fully (rotated MNIST) and partially (CIFAR-10) rotational settings.", "review": "Review:###[Update after rebuttal period] While I still find the paper somewhat hard to parse, the revision and responses have addressed most of my concerns. I think this paper should be accepted, because it presents a novel and non-trivial concept (rotation-equivariant self attention). [Original review] The authors propose a self-attention mechanism for rotation-equivariant neural nets. They show that introduction of this attention mechanisms improves classification performance over regular rotation-equivariant nets on a fully rotational dataset (rotated MNIST) and a regular non-rotational dataset (CIFAR-10). Strengths: + States a clear hypothesis that is well motivated by Figs. 1 & 2 + Appears to accomplish what it claims as contributions + Demonstrates a rotation-equivariant attention mechanism + Shows that its introduction improves performance on some tasks Weaknesses: - Unclear how the proposed attention mechanism accomplishes the goal outlined in Fig. 2d - Performance of the authors* evaluations of the baselines is lower than reported in the original papers, casting some doubt on the performance evaluation - The notation is somewhat confusing and cumbersome, making it hard to understand what exactly the authors are doing - No visualisation or insights into the attention mechanism are provided There are three main issues detailed below that I*d like to see addressed in the authors* response and/or a revised version of the paper. If the authors can address these concerns, I am willing to increase my score. 1. The motivation for the attention mechanism (as discussed in the introduction and illustrated in Fig. 2) seems to be to find patterns of features which commonly get activated together (or often co-occur in the training set). However, according to Eq. (9), attention is applied separately to orientations of the same feature ( is indexed by i, the channel dimension), and not across different features. Since the attention is applied at each spatial location separately, such mechanism only allows to detect patterns of relative orientations of the same feature appearing at the same spatial location. The motivation and utility of such formulation is unclear, as it appears to be unable to solve the toy problem laid out in Fig. 2. Please clarify how the proposed mechanism would solve the toy example in Fig. 2. 2. The only real argument that the proposed mechanism is useful are the numbers in Table 1. However, the experimental results for CIFAR-10 are hard to compare to the baselines because of differences in reported and reproduced results. I would appreciate a clarification about the code used (was it published by the authors of other papers?) and discussion of why the relative improvement achieved by the proposed method is not an artefact of implementation or optimisation issues. 3. The exposition and notation in section 3.1 is very hard to follow and requires substantial improvement. For instance, the sections *Attention and self attention* and *Compact local self attention* seem to abstract from the specific case and use x and y, but it is unclear to me what x and y map to specifically. Maybe also provide a visualization of how exactly attention is applied. Minor comments/questions: - If the attention is applied over the orientations of the same feature, why does it improve the performance on Rotated MNIST (which is rotation invariant)? - I assume the attention matrix is different for each layer, because the features in different layers are different and require different attention mechanisms. However, unlike F and K, A is not indexed by layer l. - It would be good to provide the standard deviation for the reported results on CIFAR-10 to see if the improvement is significant."}
{"id": "iclr2020_651", "title": "Self-Induced Curriculum Learning in Neural Machine Translation | OpenReview", "abstract": "Abstract:###Self-supervised neural machine translation (SS-NMT) learns how to extract/select suitable training data from comparable (rather than parallel) corpora and how to translate, in a way that the two tasks support each other in a virtuous circle. SS-NMT has been shown to be competitive with state-of-the-art unsupervised NMT. In this study we provide an in-depth analysis of the sampling choices the SS-NMT model takes during training. We show that, without it having been told to do so, the model selects samples of increasing (i) complexity and (ii) task-relevance in combination with (iii) a denoising curriculum. We observe that the dynamics of the mutual-supervision of both system internal representation types is vital for the extraction and hence translation performance. We show that in terms of the human Gunning-Fog Readability index (GF), SS-NMT starts by extracting and learning from Wikipedia data suitable for high school (GF=10--11) and quickly moves towards content suitable for first year undergraduate students (GF=13).", "review": " This paper studies how to extract/select suitable training data from comparable —rather than parallel— corpora. The idea sounds reasonable. My major concern is about the evaluation: it didn*t compare with any existing work. Actually there quite a few papers on mining parallel sentences from comparable corpora such as Wikipedia, as shown below. Seems the authors are not aware of those works and didn*t review and compare with them. Without such comparisons, it is difficult to judge the effectiveness of the proposed method and the quality of this work. [1] Finding similar sentences across multiple languages in Wikipedia, Proceedings of the Workshop on NEW TEXT Wikis and blogs and other dynamic text sources. 2006. [2] Method for building sentence-aligned corpus from wikipedia, 2008 AAAI Workshop on Wikipedia and Artificial Intelligence (WikiAI08). 2008. [3] Extracting parallel sentences from comparable corpora using document-level alignment, Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2010. [4] *Improving machine translation performance by exploiting non-parallel corpora.* Computational Linguistics2006. [5] https://www.aclweb.org/anthology/W04-3208.pdf [6] https://openreview.net/pdf?id=ryza73R9tQ Minor issues: 1. *For each language pair, a shared byte-pair encoding (BPE) (Sennrich et al., 2016) of 100k merge operations is applied.* Most papers on neural machine translation don*t use such a large BPE size, which is likely to lead to better performance. It would be better to use the same setting as previous work for fair comparisons. 2. *In the case of SS-NMT, both tasks —data extraction and learning NMT— enable and enhance each other, such that this mutual supervision leads to a self-induced curriculum, which is the subject to our analysis.* Similar idea, mutual boosting between data selection and model training, has been explored in the following paper, although not for machine translation. What*s the difference between these two papers? Learning to Teach, ICLR 2018."}
{"id": "iclr2020_652", "title": "Robust anomaly detection and backdoor attack detection via differential privacy | OpenReview", "abstract": "Abstract:###Outlier detection and novelty detection are two important topics for anomaly detection. Suppose the majority of a dataset are drawn from a certain distribution, outlier detection and novelty detection both aim to detect data samples that do not fit the distribution. Outliers refer to data samples within this dataset, while novelties refer to new samples. In the meantime, backdoor poisoning attacks for machine learning models are achieved through injecting poisoning samples into the training dataset, which could be regarded as “outliers” that are intentionally added by attackers. Differential privacy has been proposed to avoid leaking any individual’s information, when aggregated analysis is performed on a given dataset. It is typically achieved by adding random noise, either directly to the input dataset, or to intermediate results of the aggregation mechanism. In this paper, we demonstrate that applying differential privacy could improve the utility of outlier detection and novelty detection, with an extension to detect poisoning samples in backdoor attacks. We first present a theoretical analysis on how differential privacy helps with the detection, and then conduct extensive experiments to validate the effectiveness of differential privacy in improving outlier detection, novelty detection, and backdoor attack detection.", "review": "Review:###Interesting topic but lacks of novelty #Summary: The paper proposes that by applying differential privacy, the performance on outlier and novelty detection can be improved. It first presents a theoretical analysis, which establishes a lower bound on the prediction performance difference between normal and outlier data. By adding noise into the training process, the outliers in the dataset will be hidden by the noise, which will result in a model that utilizes the normal data. In this way, when deploying the model, the model will find the outlier by observing low confidence. #Strength It is good to see that the paper builds a connection between the privacy parameter and the noise level and the experiments make the theory valid. #Weakness I’m not an expert in differential privacy. But as far as I’m concerned, a typical downside is that the false positive rate will increase and there is no theoretical guarantee that the increase of false-positive rate will be negligible compared with the increase of true positive rate. Its effectiveness in detecting backdoor attacks seems elusive. As we know, the backdoor attacks exist when users want to outsource the task of training the network to a third-party, which may potentially be an attack. Therefore, the training process is out-of-control to the detector. However, the paper proposes to use differential privacy to the model training process, which is not in the settings of a backdoor attack."}
{"id": "iclr2020_653", "title": "Efficient meta reinforcement learning via meta goal generation | OpenReview", "abstract": "Abstract:###Meta reinforcement learning (meta-RL) is able to accelerate the acquisition of new tasks by learning from past experience. Current meta-RL methods usually learn to adapt to new tasks by directly optimizing the parameters of policies over primitive actions. However, for complex tasks which requires sophisticated control strategies, it would be quite inefficient to to directly learn such a meta-policy. Moreover, this problem can become more severe and even fail in spare reward settings, which is quite common in practice. To this end, we propose a new meta-RL algorithm called meta goal-generation for hierarchical RL (MGHRL) by leveraging hierarchical actor-critic framework. Instead of directly generate policies over primitive actions for new tasks, MGHRL learns to generate high-level meta strategies over subgoals given past experience and leaves the rest of how to achieve subgoals as independent RL subtasks. Our empirical results on several challenging simulated robotics environments show that our method enables more efficient and effective meta-learning from past experience and outperforms state-of-the-art meta-RL and Hierarchical-RL methods in sparse reward settings.", "review": "Review:###This paper studies the problem of leveraging past experience to quickly solve new control tasks. The starting point (and perhaps the main contribution) is the observation that some tasks have similar high-level goals, while differing in how those goals are achieved. To that end, the paper introduces an meta-RL algorithm that, given a new task, attempts to solve it by adapting a high-level, goal-setting module, and learn a new, low-level policy to reach each commanded goal. The proposed method might be viewed as a combination of PEARL [Rakelly 19] and HAC [Levy 19]. The proposed method is compared against state-of-the-art hierarchical RL and meta-RL methods on four robotic manipulation tasks. The proposed method outperforms the baselines on each task. While the proposed method is quite strong empirically, I am leaning towards rejecting this paper because many of the claims made in the paper are not empirically validated. While much emphasis is put on the hierarchical aspect of the algorithm, I don*t think that the tasks used in the experiments require hierarchy to solve (see [Plappert 18]). In the introduction, the second claim is that the proposed method *focus[es] on meta learning the overall strategy … [and] provides a simpler and better way for meta RL.* While the experiments show that the proposed method learns better than baselines, I don*t think the paper show that the proposed method learns some sort of *overall strategy.* I don*t think that the proposed method is simpler than the baselines. A second concern is that I*m confused about the experimental protocol. If the high-level policy outputs a desired XYZ position for the gripper (Section 5.1), how can the high-level policy indicate when the gripper should be closed to pick up the block? How is the reward function for the low-level policy defined? PEARL doesn*t have access to this extra information (the reward function), right? A third concern is the large number of grammatical errors in the paper. I would consider increasing my review if (1) a new plot were added to visualize the commanded subgoals (I have a hunch that the high-level policy directly outputs the true goal, obviating the need for hierarchy and contradicting the claim that the method *learns to generate high-level meta-strategies over subgoals*); (2) the experimental protocol were clarified; and (3) the number of grammatical errors were significantly reduced. Other comments: * *inefficient to to directly learn such a meta policy* -- Why? Also, *to to* is repeated. * *Deep Reinforcement learning* -- *Reinforcement* shouldn*t be capitalized. * *failing to generalize* -- Can you add a citation? * *it would be quite inefficient to directly learn such a policy…*: Doesn*t [Plappert 18] do exactly this? * *When the tasks distribution is much wider … these methods can hardly be effective…* -- Where is this claim substantiated? Also, *tasks* should be singular. * *sparse reward settings which is* -> *sparse reward settings, which are* * *the above mentioned problems* -> *the problems mentioned above* * *our algorithm focus on meta learning … which provides a much simpler …* -- Where is it shown that the proposed method is simpler? Also, *focus* should be *focuses* * *1991),which* -- Missing space * *complex tasks which requires* -> *complex tasks that require* * *Nachum et al … set of sub-policies* -- Run on sentence. * *... human leverage…* -- Don*t humans also transfer low-level knowledge across tasks, in addition to high-level knowledge? Also, *human* should be plural. * *algorithms.The* -- Missing a space, I think. * *PEARL leverages … latent variable Z* -- This sentence doesn*t make sense as written. * *z*s* -- This should not be a possessive. * *Good sample efficiency enables fast adaptation … and performs structured exploration …* -- Isn*t the first part true by definition? Why does good sample efficiency perform structured exploration? * *a goal is a 3-d vector* -- If the goal output by the high-level policy is the XYZ coordinates of the * *SAC, such non-hierarchical* -- Grammar doesn*t make sense here. * *such non-hierarchical RL method has been proved to perform badly before on …* -- Can you add a citation? Generally, *proved* is reserved for mathematical proofs. * *In this paper, We have* -- *We* should not be capitalized. ------------------------ UPDATE AFTER AUTHOR RESPONSE ------------------ I thank the authors for at least reading the reviews. My concerns with experiments and clarify remain unaddressed, and are amplified by reading the other reviews. I therefore vote to *reject* this paper."}
{"id": "iclr2020_654", "title": "Shape Features Improve General Model Robustness | OpenReview", "abstract": "Abstract:###Recent studies show that convolutional neural networks (CNNs) are vulnerable under various settings, including adversarial examples, backdoor attacks, and distribution shifting. Motivated by the findings that human visual system pays more attention to global structure (e.g., shape) for recognition while CNNs are biased towards local texture features in images, we propose a unified framework EdgeGANRob based on robust edge features to improve the robustness of CNNs in general, which first explicitly extracts shape/structure features from a given image and then reconstructs a new image by refilling the texture information with a trained generative adversarial network (GAN). In addition, to reduce the sensitivity of edge detection algorithm to adversarial perturbation, we propose a robust edge detection approach Robust Canny based on the vanilla Canny algorithm. To gain more insights, we also compare EdgeGANRob with its simplified backbone procedure EdgeNetRob, which performs learning tasks directly on the extracted robust edge features. We find that EdgeNetRob can help boost model robustness significantly but at the cost of the clean model accuracy. EdgeGANRob, on the other hand, is able to improve clean model accuracy compared with EdgeNetRob and without losing the robustness benefits introduced by EdgeNetRob. Extensive experiments show that EdgeGANRob is resilient in different learning tasks under diverse settings.", "review": " Paper summary: The authors propose to improve the robustness of neural networks by encouraging them to rely more on shape information (as opposed to texture). Specifically, the approach involves training classifiers on processed versions of the input image---either just the output of a (robust) edge detector in EdgeNetRob, or a combination of this with GAN-based infilling in EdgeGANRob. The authors evaluate these approaches in terms of clean and robust accuracy under the threat models of l-infinity adversarial attacks, backdoor attacks, and specific forms of distribution shift on the Fashion MNIST and (binary) CelebA classification tasks. Comments: I find the high-level motivation of the paper compelling. As discussed in the paper, it has been found that there is a mismatch in the kind of features that deep networks rely on for classification, when compared to humans [Geirhos et al, 2019; Ilyas et al., 2019]. Moreover, it has also been shown that encouraging networks to rely more on shape cues may boost robustness to certain corruptions [Geirhos et al., 2019]. This paper aims to put forth a general framework to encourage networks to rely more on global structure in the input images. My main concerns with the paper are as follows: A. The individual components of the approach are poorly motivated. 1. In particular, it is not obvious that preprocessing using edge detection is enough to suppress adversarial perturbations/watermarks/distribution shift in general (more on this in B). The authors claim that edges are robust features, but do not back this up with sufficient evidence. 2. The images generated post GAN-based infilling seem to have similar texture (and thus local structure) to the original input images. The authors do not provide sufficient explanation for why classifiers trained on this generated data would be any more robust (or more broadly, reliant on global structure) than classifiers trained on the original inputs. B. The evaluation of the proposed approach is not sufficiently convincing. 1. Datasets: Experiments are performed only on Fashion MNIST and *binary* CelebA, which are a) not standard datasets in robustness literature and b) simple tasks. In particular, the fact that images produced by EdgeNetRob (basically the output of an edge detector) get comparable clean accuracy to training on the actual data (cf. Table 2) suggests that the classification task is just not hard enough. For instance, on typical deep learning datasets such as ImageNet, it seems improbable that we can get good accuracy using just edge information. Thus, the authors need to evaluate their approach on standard (harder) tasks such as CIFAR-10 or ImageNet classification. 2. Choice of watermarks/distribution shift: What was the motivation behind the specific attacks considered in the backdoor attack section, and in the distribution shift sections? To me, these seem like specific attacks under which the proposed approach is more likely to succeed. - For instance, in the backdoor attack literature, the pattern is often small but somewhat perceptible [Gu et al, 2017: arXiv:1708.06733] and such a pattern would easily bypass the edge detector in EdgeGANRob. Thus, the performance reported in the paper seems somewhat specific to the attack considered (an imperceptible watermark) and would probably not generalize to patterns that are typically considered in the literature. - Similarly, the kinds of distribution shift that are considered are such that the edge information is preserved and thus may not provide a clear picture of the performance of the proposed defense. I would be curious to see how this approach performs in the face of other corruptions such as JPEG compression, fog, snow, etc. from the benchmark in Hendrycks and Dietterich [2019]. 3. Adversarial evaluation: The authors should report accuracy under black-box attacks and also white-box adversarial accuracy as a function of epsilon (e.g., for the Fashion MNIST model, plot adversarial accuracy as a function of the attacker eps from 0/256-256/256). In the Appendix, the authors mention images are scaled to [-1, 1], whereas in the robustness literature it is usually [0, 1]. This matters when you consider the eps used for attack and compare to other work: are all the networks in the paper including the adversarially trained ones also trained with images in this range? Thus, while the problem the paper tries to tackle is an important and interesting one, I am not yet convinced about the effectiveness of the proposed approach. To make the empirical evaluation more rigorous and convincing, I think the authors should: (1) Repeat the adversarial robustness experiments on more commonly-used datasets such as ImageNet and CIFAR, and also evaluate robustness of their models more thoroughly, using techniques like black-box attacks. (2) Repeat the poisoning experiments with watermark patterns which are more standard in the literature. (3) Evaluate their approach on other forms of distribution shift such as the common corruptions benchmark from Hendrycks and Dietterich. As of now, I am recommending rejection, but I would be willing to reconsider my score if the authors performed the above-mentioned experiments."}
{"id": "iclr2020_655", "title": "Reformer: The Efficient Transformer | OpenReview", "abstract": "Abstract:###Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(L^2) to O(L), where L is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.", "review": "Review:###This paper presents a method to make Transformer models more efficient in time and memory. The proposed approach consists mainly of three main operations: - Using reversible layers (inspired from RevNets) in order to prevent the need of storing the activations of all layers to be reused for back propagation; - Using locality sensitive hashing to approximate the costly softmax(QK^T) computation in the full dot-product attention; - Chunking the feed-forward layers computations to reduce their cost. This approach is first applied to a toy dataset to analyze its complexity, then tested on enwik8 language modelling task and imagenet-64 image generation task for ablation study and performance assessment. The problem approached by the paper is interesting and the proposed approach is novel to the best of my knowledge. The paper is well structured and clearly written a part from some small typos (see minor comments below). While the analysis of complexity is sound and convincing, and the fact of being able to train larger Reformers is very interesting, I have some questions and concerns about the approach and experiments. - Effect of reversible layers: It is clear for the experiment of Imagenet64 that the effect is negligible, but the experiment on enwik8 in the paper seems unfinished. Did the authors manage to finish the training, and does it confirm the observation? - Sharing QK: I am a bit confused about the effect and usefulness of this operation. Can the authors comment on why it is needed for LSH attention? It seems to me that the same operations can be achieved with different Q and K. Indeed, doing so, the authors slightly reduce the capacity of the model. The observed non-significantly decreased performance can be an effect of using only 3-layers. This may explain why the results reported for larger models in figure 5 show higher bpc than similar size state of the art models. - Time per iterations: Can the authors report the time per iteration for the larger hash rounds (8 and 16) that are closer to full attention? For the highest reported number (4), from a quick and not precise look at figure 4, it seems that the performance achieved by the proposed method after 140k iterations is achieved by the full attention after ~40k iterations. The gain in time per iteration for this particular number of hash rounds can be lost by the loss in performance. - Can the authors detail how they chose the hyperparameters of their approach? e.g. the size of hash buckets, the distribution used to generate the random matrix R .. - The reported results can be made stronger by reporting average/error bars across several trial to show consistency. Minor: typos: Dimension of matrix R [d_k, d_b/2] -> [d_k, b/2] Last paragraph of page 6: state of these art -> state of the art ——————————————— After rebuttal: I have read the authors answer, and found they addressed my concerns. I*m therefore increasing my score."}
{"id": "iclr2020_656", "title": "What Can Learned Intrinsic Rewards Capture? | OpenReview", "abstract": "Abstract:###Reinforcement learning agents can include different components, such as policies, value functions, state representations, and environment models. Any or all of these can be the loci of knowledge, i.e., structures where knowledge, whether given or learned, can be deposited and reused. Regardless of its composition, the objective of an agent is behave so as to maximise the sum of suitable scalar functions of state: the rewards. As far as the learning algorithm is concerned, these rewards are typically given and immutable. In this paper we instead consider the proposition that the reward function itself may be a good locus of knowledge. This is consistent with a common use, in the literature, of hand-designed intrinsic rewards to improve the learning dynamics of an agent. We adopt a multi-lifetime setting of the Optimal Rewards Framework, and investigate how meta-learning can be used to find good reward functions in a data-driven way. To this end, we propose to meta-learn an intrinsic reward function that allows agents to maximise their extrinsic rewards accumulated until the end of their lifetimes. This long-term lifetime objective allows our learned intrinsic reward to generate systematic multi-episode exploratory behaviour. Through proof-of-concept experiments, we elucidate interesting forms of knowledge that may be captured by a suitably trained intrinsic reward such as the usefulness of exploring uncertain states and rewards.", "review": "Review:###Summary The paper evaluates the intrinsic reward as a way of storing information about episodes. It adopts the optimal intrinsic reward setting (Singh*09), and extends its recent policy gradient implementation, LIRPG, to lifetime settings. The task in the lifetime setting is to learn an intrinsic reward such that when trained with it, the agent maximizes its total return over its lifetime. A lifetime is defined as a sequence of episodes, where the agent does not have memory of previous episodes, however, the function computing the intrinsic reward does. In proof-of-concept experiments, the paper demonstrates that the learned intrinsic reward captures properties of several gridworld environments and induces meaningful behavior in the agent, successfully transferring information from previous episodes. Interestingly, a state-based reward function also generalizes to agents with perturbed action spaces, showing that this way of storing information is agnostic to the agent’s action space. Decision The paper proposal is interesting and adequately evaluated, however, the impact of the paper might be limited by its limited technical novelty and lack of comparisons to strong baselines. I recommend marginal accept. Pros - The paper is well-motivated. - The paper is well-written and the method is clearly explained. The literature review is thorough. - The experimental evaluation demonstrates several interesting and potentially promising phenomena. Cons - The novelty of the paper is limited as it is a somewhat straightforward extension of prior work. - The impact of the paper is hard to judge as the experimental evaluation does not focus on potential usecases. Questions. Here, I will focus on scientific questions, answering which would significantly improve the quality of the paper. - The biggest drawback of the paper is that the proposed method has an unfair advantage as it has a way of transmitting information across episodes, which the baselines do not (as stated on the bottom of page 5). While the findings of this paper are interesting, it is unclear how it compares to methods that have memory of previous episodes, such as agents with non-episodic recurrent policies, or meta-learning agents such as Duan’16, Finn’17. Is it possible that the proposed method e.g. scales better than recurrent policies due to compact representations or provides better generalization to things like action space changes? - How does the method compare to hand-designed intrinsic rewards on hard exploration games (such as montezuma’s revenge or pitfall Atari games)? Since it can only learn to explore on games that it previously successfully solved, it is possible that a hand-designed intrinsic reward such as RND (Burda’19) would perform better on these hard games. On the other hand, it is possible that the method will in fact perform better on these games due to more directed exploration. - How does the method compare to hand-designed intrinsic reward on out-of-distribution tasks? Intuitively, the method should perform the worse the further from the training distribution the task is, while the hand-designed rewards will always perform similarly. However, what is the extent to which the proposed method generalizes? It is possible that this method would be very useful in practice if it generalized well. Other potentially related work. - Xu’18, Learning to Explore with Meta-Policy Gradient, is a relevant work that proposes a meta-learning framework for training an exploration policy. - Metz’19, Meta-Learning Update Rules for Unsupervised Representation Learning, is a conceptually relevant work that proposes to meta-learn loss functions for unsupervised learning (and there is more recent related work on this topic too)."}
{"id": "iclr2020_657", "title": "Exploration via Flow-Based Intrinsic Rewards | OpenReview", "abstract": "Abstract:###Exploration bonuses derived from the novelty of observations in an environment have become a popular approach to motivate exploration for reinforcement learning (RL) agents in the past few years. Recent methods such as curiosity-driven exploration usually estimate the novelty of new observations by the prediction errors of their system dynamics models. In this paper, we introduce the concept of optical flow estimation from the field of computer vision to the RL domain and utilize the errors from optical flow estimation to evaluate the novelty of new observations. We introduce a flow-based intrinsic curiosity module (FICM) capable of learning the motion features and understanding the observations in a more comprehensive and efficient fashion. We evaluate our method and compare it with a number of baselines on several benchmark environments, including Atari games, Super Mario Bros., and ViZDoom. Our results show that the proposed method is superior to the baselines in certain environments, especially for those featuring sophisticated moving patterns or with high-dimensional observation spaces.", "review": "Review:###Pros Solid technical innovation/contribution: - The paper proposed a novel method FICM that bridged the intrinsic reward in DRL with optical flow loss in CV to encourage exploration in an environment with sparse rewards. To the best of my knowledge, this was the first paper proposed to use moving patterns in two consecutive observations to motivate agent exploration. Balanced view: - The authors discussed both the advantages of FICM and settings that FICM might fail to perform well, and conducted experiments to better help the readers understand such nuances. Such balanced view should be valuable to RL communities in both academia and industry. Clarity: - In general this was a very well-written paper, I had no difficulty in following the paper throughout. The proposed method (FICM) was clearly motivated, and the authors provided good coverage of related works. Notably, the authors reviewed two relevant methods upon which FICM was motivated, which made the paper self-contained. Cons Experiments: - Experiments were conducted only using a few recent results as baselines (ICM, forward dynamics, RND). It would be interesting to compare FICM against simpler exploration baselines such as epsilon-greedy or entropy regularization. - I’d also like to see more extensive comparisons between FICM and ICM across different datasets, for example, Super Mario Bros. and the Atari games, instead of only comparing FICM against ICM on ViZDoom. Significance of the innovation: - The proposed exploration method seemed to be applicable with a particular RL setting: the environment changes could be represented through consecutive frames (e.g., video games), and optical flow could be used to interpret any object displacements in such consecutive frames. And as the authors discussed, even under such constraints the applicability of proposed method depends on how much changes of the environment were relevant to the goal. Reproducibility: - Although the authors discussed the experiment setting in detail in supplements, I believe open-sourcing the code / software used to conduct the experiments would be greatly help with the reproducibility of the proposed method for researchers or practitioners. Summary A good paper overall, but the experiments were relatively weak (common for most ICLR submissions) and the novelty was somewhat limited."}
{"id": "iclr2020_658", "title": "Convolutional Conditional Neural Processes | OpenReview", "abstract": "Abstract:###We introduce the Convolutional Conditional Neural Process (ConvCNP), a new member of the Neural Process family that models translation equivariance in the data. Translation equivariance is an important inductive bias for many learning problems including time series modelling, spatial data, and images. The model embeds data sets into an infinite-dimensional function space, as opposed to finite-dimensional vector spaces. To formalize this notion, we extend the theory of neural representations of sets to include functional representations, and demonstrate that any translation-equivariant embedding can be represented using a convolutional deep-set. We evaluate ConvCNPs in several settings, demonstrating that they achieve state-of-the-art performance compared to existing NPs. We demonstrate that building in translation equivariance enables zero-shot generalization to challenging, out-of-domain tasks.", "review": "Review:###The paper introduces ConvCNP, a new member of the neural process(NP) family that models translational equivariance in the data, which uses convolutions and stationary kernels to aggregate the context data into a functional representation. This problem is well-motivated as there are various domains where such an inductive bias is desirable, such as spatio-temporal data and images, and will help especially with predictions for out-of-distribution tasks. This inductive bias was never built into NPs, and it remained unanswered whether the NP can learn such a behaviour. This paper shows that the answer is negative and that one needs to make modifications to create such inductive bias. The architecture of the ConvCNP is motivated by theory that completely characterises the set of translation equivariant functions Phi that maps sets of (x,y) pairs to bounded continuous functions that map x to y (disclaimer: I haven’t read through the proof in the appendix, so will not make any claims on its correctness). Theorem 1 defines the set of such functions using rho, phi and psi, and the choices for each on on-the-grid data and off-the-grid data are listed in Section 4. There are ablation studies in Appendix D.4 that justify the choices. Overall the paper is very well-written and clear for the most part, with helpful pseudo-code and well-laid out quantitative + qualitative results, and a very detailed appendix that allows replicating the setup. The evaluation is extensive, and the results are significant. - The results on 1D synthetic data show a noticeable improvement of the ConvCNP compared to the AttnCNP, with improved interpolation as well as accurate extrapolation for the weakly periodic function. I do think however that a more competitive baseline for AttnCNP would have been to parameterise the logits of the attention weights as a periodic function with learnable length scale (e.g. stationary periodic kernel), since this is another way of building in periodicity into the model. Arguably this is more explicit and restrictive than the translational equivariance built into ConvCNP, but would have made for a more interesting comparison. - Having said that, I like how the evaluation was performed on a variety of stochastic processes - previous literature only used GP + EQ kernel, but here more challenging non-smooth functions such as GP + Matern kernels and sawtooth functions are explored - and it’s very convincing to see the outstanding performance of ConvCNPs here. - It’s also nice to see results on regression tasks on real data (sections 5.2, 5.3), which was never explored in the NP literature as far as I know. 5.2 shows that ConvCNPs can be competitive against other methods that model stochastic processes, and 5.3 shows an instance of where ConvCNPs do a reasonable job whereas (Attn)CNP fails. - The results on images is also extensive, covering 6 different datasets (including the 2 zero shot tasks), and show convincing qualitative and quantitative results. The zero shot tasks are nice examples that explicitly show the consequences of not being able to model translation equivariance in more realistic images composed of multiple objects/faces. I have several comments/questions regarding the disccusion & related work section: - One link that might be worth pointing out regarding functional representation of context is that ANP (or AttenCNP) can also be seen as giving a functional representations of the context; the ANP computes a target-specific representation of the context, which can be seen as a function of the target inputs. - I think it’s incorrect to say that latent-variable extensions enforce consistency. Even with the latent variable, if the encoder is seen as part of the model, then the NP isn’t consistent (pointed out in the last paragraph of section 2.1 in the ANP paper). So there still are issues regarding AR sampling. There does however seem to exist variants of NPs that satisfy consistency e.g. https://arxiv.org/abs/1906.08324 - What is preventing the incorporation of a latent variable in the ConvCNP? Is this just something that can be easily done but you haven’t tried, or do you see any non-trivial issues that arise when doing so e.g. maintaining translation equivariance? Other minor comments: - Are there any guidelines on choice of filter size of CNN in the image case? E.g. have you chosen the filter size of ConvCNP such that the receptive field is smaller than the image, whereas it’s bigger for ConvCNPXL? It’s not clear why having a bigger receptive field allows to capture non-stationarity, and it would be helpful to expand on that, perhaps in the appendix. - Also it’d help for the sake of clarity to explain why AttnCNP uses significantly more memory than ConvCNP, i.e. because memory for self-attention is O(N^2) where N=HW is the number of inputs, whereas for convolutions it’s O(HW). - I think it’d also help to state explicitly in the body that AttnCNP is ANP without the latent path when it is introduced. - typos: first paragraph of Section 2: Z_M <- Z_m (twice), finitely <- infinitely, Appendix D.1: separabe <- separable Overall, I think this is a very strong submission and I vote for its acceptance."}
{"id": "iclr2020_659", "title": "Permutation Equivariant Models for Compositional Generalization in Language | OpenReview", "abstract": "Abstract:###Humans understand novel sentences by composing meanings and roles of core language components. In contrast, neural network models for natural language modeling fail when such compositional generalization is required. The main contribution of this paper is to hypothesize that language compositionality is a form of group-equivariance. Based on this hypothesis, we propose a set of tools for constructing equivariant sequence-to-sequence models. Throughout a variety of experiments on the SCAN tasks, we analyze the behavior of existing models under the lens of equivariance, and demonstrate that our equivariant architecture is able to achieve the type compositional generalization required in human language understanding.", "review": "Review:###Summary --- (motivation) Consider SCAN, a synthetic task where setences like S1=*jump twice and run left* are supposed to be translated into action sequences like A1=JUMP JUMP LTURN RUN. One might replace the word *jump* in S1 with *walk* then translate to get A2=WALK WALK LTURN RUN. If instead S1 is translated into A1 and then the action JUMP is replaced with the action WALK then we should still get the same A2. Such a translation model is equivariant to permutations of *jump* and *walk*. This paper aims to 1) define a general notion of compositionality as equivariance, 2) build a model which is compositional in this general sense, and 3) apply the model to SCAN. (approach - theory) This work considers this kind of compositionality as equivariance to group actions. Previous work (Kondor & Trivedi, 2018) viewed convolution as equivariance to actions by translation groups. This work views language compositionality as equivariance to actions by permutation groups applied to a set of similar words (e.g. verbs in SCAN). (approach - model) The paper proposes G-Embed, G-RNN, G-Attention, and G-Conv (not new) layers that are equivariant to word permuatations (e.g., switching *jump* and *walk*). It then composes these modules in a fairly standard fashion to build a new G-seq2seq model which is invariant to group actions. (experiments) Experiments apply a G-seq2seq model to the SCAN tasks, comparing to strong baselines. G-seq2seq requires slightly more knowledge (a set of related words like verbs) than all the baselines, but less knowledge than Lake 2019. 1. G-seq2seq outperforms all baselines except Lake 2019 (unfair comparison) on basic compositional tasks (*Add Jump* and *Around Right*). 2. Like other models, G-seq2seq fails on the *Length* task, though it is still among the best performers. Strengths --- The theory of compositionality as invariance to actions by permutation groups is new, interesting, and could turn out to be significant. The proposed models are also new, interesting, and could be significant. Experiments on SCAN verify that the proposed models work about as expected, sometimes beating strong baselines in the process. Weaknesses --- It*s hard to know what the impact of this paper will be because 1) it*s unclear whether this model can generalize to more useful domains and 2) the presentation may turn some readers away. While neither of these issues can really be solved, I think paper could be substantially better in both aspects. Corresponding suggestions: 1) How expensive is this? It seems quite expensive because the representation size scales with the number of permutation of the set of words equivariance is with respect to. How will it scale to larger problems in terms of computation/memory costs (especially larger vocab sizes)? What knowledge is required for applying this method to new domains--i.e., how do I choose a set of permutation equivariant verbs in general? More discussion of these issues may help increase the impact of the paper. 2) See next section. Presentation Weaknesses / Points of Confusion / Missing Details --- To mimic a typical decoder RNN there should be another input which copies the word \tilde{w}_{t-1} from the previous iteration as input, somehow fused with the attention feature \tilde{a}_t. How does the G-RNN know what the last word it generated was? The notation in the first equation of section 4.1: I think is supposed to take an integer as input but is a permutation applied to a function. I*m not sure how to apply permutations to functions like w and it doesn*t seem like the output should be an integer in any case so I find this notation confusing. Taking a step back, I find some of the notation (e.g., previous point) a bit confusing. This makes it hard for me to get the main point. I think the idea is that equivariant models can be achieved by tracking a representation (e.g., via rows of the G-Embed matrix) for (almost?) every member of the acting group. It may help the presentation to more frequently demonstrate the general concepts with examples, though doing so may be in conflict with the general nature of the paper*s theoretical contribution. I*m sure this is a familiar tradeoff, but from my perspective the paper would probably be more impactful if the presentation leaned more on examples. Equation numbers would be a really great addition. I found it hard to reference some of the material in writing my review. *and the use of algebraic computation* * This seems specific to the chosen example whereas the rest of the sentence is trying to be general. Suggestions --- * This seems related to [1], which uses group theory to define a notion of disentangled representation similar to compositionality. That may inspire future work and would be useful to mention in the related work. * Why didn*t performance on SCAN get to 100%? It would be useful to spend some time addressing points of failure for the model other than compositionality. * The G-RNN doesn*t have a bias. It*s not necessary, but it may be interesting to describe why this design choice was made. [1]: Higgins, Irina et al. “Towards a Definition of Disentangled Representations.” ArXiv abs/1812.02230 (2018): n. pag. Preliminary Evaluation --- Quality: The theoretical contributions make sense and the experiments show they lead to useful models. Clarity: The technical parts of the paper are somewhat unclear, but the rest of the paper is well written. Significance: As discussed in the Weaknesses section this could turn out to be very significant or not significant at all, but that*s true for a lot of good research. Originality: The general notion of equivariant neural networks and good performance on SCAN are novel. Overall, this is a very clear accept. Post-Rebuttal Update --- There was a lot of agreement between reviewers, though we came to slightly different conclusions about ratings. Though there is significant uncertainty about the impact of this work, I still think 8: Accept is the most appropriate rating. Overall, the other reviews and author responses only increased my confidence that this paper should be accepted."}
{"id": "iclr2020_660", "title": "Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds | OpenReview", "abstract": "Abstract:###We design a new algorithm for batch active learning with deep neural network models. Our algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high-magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between diversity and uncertainty without requiring any hand-tuned hyperparameters. While other approaches sometimes succeed for particular batch sizes or architectures, BADGE consistently performs as well or better, making it a useful option for real world active learning problems.", "review": "Review:###This paper introduces an algorithm for active learning in deep neural networks named BADGE. It consists basically of two steps: (1) computing how uncertain the model is about the examples in the dataset (by looking at the gradients of the loss with respect to the parameters of the last layer of the network), and (2) sampling the examples that would maximize the diversity through k-means++. The empirical results show that BADGE is able to get the best of two worlds (sampling to maximize diversity/to minimize uncertainty), consistently outperforming other approaches in a wide-rage of classification tasks. This is a very well-written paper that seems to make a meaningful contribution to the field with a very good justification for the proposed method and with convincing empirical results. Active learning is not my main area of expertise so I can’t judge how novel the proposed idea is, but from an outsider’s perspective, this is a great paper. It is clear, it does a good job explaining the problem, the different approaches people have used to tackle the problem, and how it fits in this literature. Below I have a couple of (minor) comments and questions: 1. Out of curiosity, it seems that it is standard in the literature, but isn’t the assumption that one can go over the whole dataset, U, at each iteration of the active learning algorithm, limiting? It is not that cheap to go over large datasets (e.g., ImageNet). 2. MARG seems to often outperform the other baselines but it doesn’t have a reference attached to it (bullet points on page 5). Is this a case that a “trivial” baseline outperforms existing methods or is there a reference missing? 3. In some figures, such as Figure 2, there are shaded regions in the plots. It is not clear what they are though. Are they representing confidence intervals? Standard deviation? They are quite tight for a sample size of 5. 4. In the section “Pairwise comparisons” it reads “Algorithm i is said to beat algorithm j in this setting if z > 1.96, and similarly … z < -1.96”. It seems to me that the number 1.96 comes from the z-score table for 95% confidence. However, if that’s the case, it seems z should be much bigger in this context. With a sample-size of 5 (if this is still the sample size, maybe I missed something here), the normal assumptions do not hold and the t-score should’ve been used here. What did I miss? In terms of presentation, Proposition 1 seems to be a very interesting result. I would move it to the main paper instead of leaving it in the Appendix. I also think the paper would read better if it didn’t use references as nouns (e.g., “algorithm of (Derezinski, 2018)”). Finally, there’s also a typo on page 7 (Apppendx). --- >>> Update after rebuttal: I stand by my score after the rebuttal. This is a really strong paper in my opinion. I appreciate the fact that the authors took my feedback into consideration."}
{"id": "iclr2020_661", "title": "Imbalanced Classification via Adversarial Minority Over-sampling | OpenReview", "abstract": "Abstract:###In most real-world scenarios, training datasets are highly class-imbalanced, where deep neural networks suffer from generalizing to a balanced testing criterion. In this paper, we explore a novel yet simple way to alleviate this issue via synthesizing less-frequent classes with adversarial examples of other classes. Surprisingly, we found this counter-intuitive method can effectively learn generalizable features of minority classes by transferring and leveraging the diversity of the majority information. Our experimental results on various types of class-imbalanced datasets in image classification and natural language processing show that the proposed method not only improves the generalization of minority classes significantly compared to other re-sampling or re-weighting methods, but also surpasses other methods of state-of-art level for the class-imbalanced classification.", "review": " This manuscript proposes an over-sampling method for dealing with the imbalanced classification and long-trailed problems. The authors refer to their work as Advserarial Minority Over-sampling (AMO). The interesting aspect of this paper is that it explores adversarial perturbation (possibly of majority class) as a means of over-sampling for the minority class. The findings suggest that it could improve imbalanced learning. However, there are several major issues with the paper in its current form: - There is a recent publication with almost the same topic in ICCV 2019 that also explores using adversarial minority over sampling frameworks (published on arxiv on Apr 3, 2019 https://arxiv.org/pdf/1903.09730.pdf). Although that one is a bit different methodologically, the authors have not mentioned it, compare with it, nor discussed it. It is not clear where the current manuscript stands in comparison with the ICCV 2019 paper. - Given the above paper, the novelties of the proposed technique become marginal. - Another major issue with the paper is its methodological limitations. As the authors have also mentioned, it looks a bit like learning to classify the adversarial examples. It seems like this is a very effective method if we want to classify a majority (normal) class versus a minority (anomaly) class. Because when the model generates adversarial examples for any specific class the adversarial examples may cover all space of the samples minus the samples of that (normal) class. Therefore, the model does not learn the geometry of the minority class (as opposed to many state-of-the-art long-trailed classification models) and only learns the majority class. This is not itself a positive characteristic. - The authors have not provided any theoretical discussions/guarantees why adversarial examples should be a good means of learning the imbalanced distributions. Everything in the paper seems to be experimental and heuristic. - The accuracy metric provided by the authors is a bit misleading. For these cases of long-trailed classification models, Average Class Specific Accuracy and Geometric Mean (analogous to F1-score) are the most relevant ones to report, especially because experiments are conducted on multi-class settings. - The results in Table 2 show a highly imbalanced classification rate between majority and minority classes. This means that neither the proposed method nor the baselines could solve the problem. The differences between the proposed method and the baselines do not seem to be statistically significant. So, what is the purpose of this experiment!?"}
{"id": "iclr2020_662", "title": "Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning? | OpenReview", "abstract": "Abstract:###Hierarchical reinforcement learning has demonstrated significant success at solving difficult reinforcement learning (RL) tasks. Previous works have motivated the use of hierarchy by appealing to a number of intuitive benefits, including learning over temporally extended transitions, exploring over temporally extended periods, and training and exploring in a more semantically meaningful action space, among others. However, in fully observed, Markovian settings, it is not immediately clear why hierarchical RL should provide benefits over standard *shallow* RL architectures. In this work, we isolate and evaluate the claimed benefits of hierarchical RL on a suite of tasks encompassing locomotion, navigation, and manipulation. Surprisingly, we find that most of the observed benefits of hierarchy can be attributed to improved exploration, as opposed to easier policy learning or imposed hierarchical structures. Given this insight, we present exploration techniques inspired by hierarchy that achieve performance competitive with hierarchical RL while at the same time being much simpler to use and implement.", "review": "Review:###This is an interesting paper, as it tries to understand the role of hierarchical methods (such as Options, higher level controllers etc) in RL. The core contribution of the paper is understand and evaluate the claimed benefits often proposed by hierarchical methods, and finds that the core benefit in fact comes from exploration. The paper studies hierarchical methods to eventually draw the conclusion that HRL in fact leads to better exploration based behaviour in complex tasks. While the conclusion suggesting HRL leading to better exploration seems interesting, I am not sure whether this is in fact too surprising? For example, the Options framework has been fundamentally proposed and shows benefits in terms of transfer learning and faster exploration. Options has been fundamentally argued to lead to faster exploration (for example, when the goal changes in the four rooms task). Therefore, isn*t the conclusion that this paper draws already known? Maybe the paper studies HRL methods in different experimental settings, not considerered in existing HRL or Options based frameworks before, but I would imagine similar conclusions can be drawn if a pure HRL system is studied? _x000b_The paper mainly does an experimental ablation study for HRL systems, and draws the conclusion of faster exploration led by HRL methods. However, I am not sure whether this conclusion, while assummably true as argued by previous methods too, is convincing enough based on the proposed suite of tasks? _x000b_It might have been more useful if there were similar theoretical contributions or analysis were also made for a stronger claim for HRL methods. Without lack of theoretical analysis or proofs, and simply based on the ablation studies as in this paper, I am not sure whether this paper is yet ready for acceptance in a venue like ICLR. _x000b_In the tasks considered, also there is no termination condition being learnt, and in fact the HRL systems studied here terminates every c time steps (as in HIRO and most of other HRL papers). This might be a significant limitation though. If we learn the termination condition, as in Option-Critic, would we expect similar behaviour in performance and can attribute the benefits of HRL only to exploration? _x000b_There are a vast amount of approaches based on identifying bottleneck states, and often they have shown better performance in terms of transfer learning. While in these approaches, often the benefits are claimed to be in terms of both exploration and transfer learning, this paper seems to contradict that? _x000b_Experimentally, there are only few specific tasks considered, like the variations of Ant tasks like AntPush and AntMaze. I am not convinced from the set of experimental results that they are sufficient enough to draw the conclusion that HRL methods only excel due to better exploration. _x000b_Why is the conventional Four Rooms domain ignored? What if we take the Four Rooms domain, change the goal states - I would expect the paper to do such analysis on the range of HRL methods, and then propose a convincing argument. _x000b_I think overall the paper needs more work, before such conclusion can be drawn overall. It seems to me like a strong claim that the benefits of HRL is only due to exploration. The paper does not do enough experimental abltation studies to strengthen the claim, mainly lacking fundamental HRL task setups. It does not do a theoretical analysis studying HRL methods and their benefits either. Overall, I think with these contributions, if similar behaviour persists, then it would make a more convincing argument. _x000b_There are a lot of theoretical papers studying provably sufficient exploration methods. Perhaps such approaches can also be taken here to study HRL methods and whether they provably lead to faster exploration? _x000b_Experimentally, I would expect a more wide range of task setups and domains to be studied - since this is mainly a paper based on experimental studies and trying to draw conclusion for existing HRL methods without proposing new approaches. I think without these carefully studied experiments, the conclusions are over-claimed."}
{"id": "iclr2020_663", "title": "GRASPEL: GRAPH SPECTRAL LEARNING AT SCALE | OpenReview", "abstract": "Abstract:###Learning meaningful graphs from data plays important roles in many data mining and machine learning tasks, such as data representation and analysis, dimension reduction, data clustering, and visualization, etc. In this work, we present a scalable spectral approach to graph learning from data. By limiting the precision matrix to be a graph Laplacian, our approach aims to estimate ultra-sparse weighted graphs and has a clear connection with the prior graphical Lasso method. By interleaving nearly-linear time spectral graph sparsification, coarsening and embedding procedures, ultra-sparse yet spectrally-stable graphs can be iteratively constructed in a highly-scalable manner. Compared with prior graph learning approaches that do not scale to large problems, our approach is highly-scalable for constructing graphs that can immediately lead to substantially improved computing efficiency and solution quality for a variety of data mining and machine learning applications, such as spectral clustering (SC), and t-Distributed Stochastic Neighbor Embedding (t-SNE).", "review": "Review:###This paper presents a scalable spectral approach for graph learning. In particular, the authors use graph Laplacian as precision matrix, and show the connection between the proposed method and graphical Lasso. Three tasks, including spectral clustering, graph recovery and t-SNE visualization, are considered in experiments. Pros. 1. Scalable graph learning is an important research topic. This paper presents a practical solution to large-scale graph learning. 2. The connection between the proposed method and graphical Lasso is discussed. Also, theoretical analysis on spectral criticality is provided. 3. Overall the paper is well organized and clearly written. Cons. 1. My major concern is about the experiments. The authors claim that the proposed graph learning approach is highly scalable. It would be more convincing if the authors can evaluate the proposed method on larger datasets. 2. One of the tasks in experiments is t-SNE visualization. There are also some faster versions of t-SNE with a complexity of O(NlogN), such as [a]. For t-SNE, the authors may justify what*s the advantage of using the proposed method over other fast t-SNE algorithms. [a] Accelerating t-SNE using Tree-Based Algorithms, JMLR 2014."}
{"id": "iclr2020_664", "title": "Calibration, Entropy Rates, and Memory in Language Models | OpenReview", "abstract": "Abstract:###Building accurate language models that capture meaningful long-term dependencies is a core challenge in natural language processing. Towards this end, we present a calibration-based approach to measure long-term discrepancies between a generative sequence model and the true distribution, and use these discrepancies to improve the model. Empirically, we show that state-of-the-art language models, including LSTMs and Transformers, are emph{miscalibrated}: the entropy rates of their generations drift dramatically upward over time. We then provide provable methods to mitigate this phenomenon. Furthermore, we show how this calibration-based approach can also be used to measure the amount of memory that language models use for prediction.", "review": "Review:###This paper highlights and studies the problem of *entropy rate drift* for language models: the entropy rate of the language generated by a trained model is much higher than the entropy of ground truth sequences and this discrepancy worsen with the length of generation. The authors interestingly claim that the well-known lack of coherence in long-term model generations is due to this entropy rate drift. Valuably, the entropy rate drift is characterized mathematically. The authors propose a calibration method and prove that their method can interestingly reduce *both* the entropy rate drift and the perplexity of a miscalibrated model, *even though* they assume a rather simplistic model of miscalibration (one that leaks a small amount of mass to all the sequences of a given length). The author quantitatively show that their calibration method reduces the entropy rate drift and qualitatively show that their generations *make sense* in the long-term. As an auxiliary result, they show that a similar calibration method can be used to quantify the past information used by the model by upper-bounding the mutual information between the current prediction and the long-term past, given the short-term past, e.g. I(W_t | W<{t-\tau} | W_{t-\tau:t-1}). I really enjoyed reading this paper and was really intrigued by the author*s solution. However, in the current form, this paper is below the bar of acceptance due to some weaknesses: (i) rather strong assumption for the main derivation; (ii) lack of clarity and computational complexity of the proposed algorithms; (iii) weak experimental results and missing hint to current models / applications / concurring models. I would be more than happy to increase my score if the authors could kindly respond to the points below. 1) About the assumptions: in your theory, you show that the proposed solution is guaranteed to improve a particular constrained form of miscalibrated model P^epsilon, in which a epsilon uniform distribution over all possible sentences of length K is added to the model distribution. This seems a rather strong assumption to me and bumps up to increasing the probability of each word by a small amount for each per-step conditional distribution estimated by the model. 1.1) Could you elaborate on why intuitively this assumption is something that happens in current models like GPT-2 ? Do you have a way of quantifying whether this assumption reasonably holds ? 2) About complexity: 2.1) What’s roughly the computational complexity of Algorithm 2 ? 2.2) Do you need to compute H(W_{t+1}|w_{<= t}) for all possible 1-step continuations w_t ? That seems quite expensive to me in the case of a large vocab. (e.g. sample a word, run forward one step and compute future entropy). 2.3) If this is correct, how to address this issue ? If I am missing something, it would be good to add to the paper some explicit considerations about the technical feasibility of the algorithm. 3) The amazing thing to me is that you reduce entropy-rate drift (although wrt a particular miscalibrated model, cf. 1) by re-minimizing cross-entropy using the ground-truth corpus. If you could give intuition for this , it would be great and make a much stronger paper ! 3.1) Could you explain why intuitively this works ? 3.2) If alpha is positive, Algorithm 2 is basically penalizing the model for producing words that lead to higher entropy in the future. Why this correlates to less cross-entropy with the data distribution ? 3.3) Why would calibrating a model that may not conform to the assumption 1) in general prefer to have an alpha > 0 ? 4) About the experiments: 4.1) Where are the perplexities of the calibrated models ? 4.2) Are the perplexity improving as the theory would suggest ? 4.3) Is there any small , systematic human study that you could perform apart from just showing few examples of generations ? 4.4) Could you report some quantitative metrics of the generations from your models and the baselines, for example as computed in previous papers (https://arxiv.org/abs/1801.07736, https://arxiv.org/abs/1908.04319) 4.5) How do your generations change if you use beam search or arg-max, top-k sampling ? Does your method also help in preventing repetitions ? Minor: - I think you are missing a minus in Algorithm 2 inside the exponential. ==== Updated review: I thank the authors for their answer. I think that overall this paper provides interesting insight on a hard problem. I am raising my score and strongly hope that the authors add the considerations made in their response in the main paper: warning about complexity, optimal alphas for each model and details."}
{"id": "iclr2020_665", "title": "Improving the robustness of ImageNet classifiers using elements of human visual cognition | OpenReview", "abstract": "Abstract:###We investigate the robustness properties of image recognition models equipped with two features inspired by human vision, an explicit episodic memory and a shape bias, at the ImageNet scale. As reported in previous work, we show that an explicit episodic memory improves the robustness of image recognition models against small-norm adversarial perturbations under some threat models. It does not, however, improve the robustness against more natural, and typically larger, perturbations. Learning more robust features during training appears to be necessary for robustness in this second sense. We show that features derived from a model that was encouraged to learn global, shape-based representations (Geirhos et al., 2019) do not only improve the robustness against natural perturbations, but when used in conjunction with an episodic memory, they also provide additional robustness against adversarial perturbations. Finally, we address three important design choices for the episodic memory: memory size, dimensionality of the memories and the retrieval method. We show that to make the episodic memory more compact, it is preferable to reduce the number of memories by clustering them, instead of reducing their dimensionality.", "review": "Review:###This paper proposed to use memory cache (in real implementation - an episodic memory structure) to improve robustness against adversarial image examples. The paper compared using continuous cache (aka soft attention over all items in the cache), with using nearest neighbor approach (hard attention), and concluded that using a large continuous cache is not superior to using a simple 50-nn hard attention (which is as expected since I can imagine the soft weights are extremely sparse over all items in the cache). That seems to say NN is a simple feature extractor, and the contribution of the episodic memory becomes suspicious. What if we simply clustering each ImageNet feature to K clusters and use centroids as cache feature, instead of taking bother to learn such cache contents? And for reducing the dimension, it seems to necessary to compare with NN search with product quantization (Herve Jegou 2011, *Product quantization for nearest neighbor search*) for fair comparison. The generalization capacity is some other concern. Say if the dataset receives new images, is it robust to such images, or simply unseen images would be classified as adversaries? This paper mainly proves some existing conclusions such that nearest neighbor search is effective against adversarial attack. The design and theory seem to be existing already and the novelty is the main concern for acceptance."}
{"id": "iclr2020_666", "title": "Growing Action Spaces | OpenReview", "abstract": "Abstract:###In complex tasks, such as those with large combinatorial action spaces, random exploration may be too inefficient to achieve meaningful learning progress. In this work, we use a curriculum of progressively growing action spaces to accelerate learning. We assume the environment is out of our control, but that the agent may set an internal curriculum by initially restricting its action space. Our approach uses off-policy reinforcement learning to estimate optimal value functions for multiple action spaces simultaneously and efficiently transfers data, value estimates, and state representations from restricted action spaces to the full task. We show the efficacy of our approach in proof-of-concept control tasks and on challenging large-scale StarCraft micromanagement tasks with large, multi-agent action spaces.", "review": "Review:###Based on the intuition that smaller action space should be easier to learn, the author proposes a curriculum learning approach which learns by gradually growing the action space. An agent using simpler action space can generate experiences to be used by all the agents with larger action spaces. The author presents experiments on simple domains and also on a challenging video game. In general, it is an interesting research work. I think the author can improve the paper in the following aspect. 1. Motivation. Curriculum learning is based on the idea that tasks can be arranged in a sequential manner and those tasks learned earlier can be somehow helpful for subsequent tasks. Although it is clear that small action space should be easy to learn, it is unclear why those off-policy samples can be helpful for more complex action space. Smaller action space can correspond to a completely different optimal policy. Imagine that in a tabular environment, two actions A, B are available, and the optimal action is to always take B. Then if the agent with full action space uses the experiences generated by the agent with action space {A} may get completely wrong action values. There must be some constraint of the underlying MDP. The author may provide some experiments in tabular case to illuminate the issue. 2. Relevant works. I think the author should include some discussions regarding large action spaces, since one goal of the proposed method is to handle such situation. There are several works should be discussed. For example, Deep Reinforcement Learning in Large Discrete Action Spaces, Function-valued action space for PDE control. The former handle the large discrete action space by learning an action embedding; while the latter attempts to leverage the regularity in the action space by introducing a convolutional structure for the output of the actor network and hence the proposed method can scale to arbitrary action dimensions."}
{"id": "iclr2020_667", "title": "Learning Surrogate Losses | OpenReview", "abstract": "Abstract:###The minimization of loss functions is the heart and soul of Machine Learning. In this paper, we propose an off-the-shelf optimization approach that can seamlessly minimize virtually any non-differentiable and non-decomposable loss function (e.g. Miss-classification Rate, AUC, F1, Jaccard Index, Mathew Correlation Coefficient, etc.). Our strategy learns smooth relaxation versions of the true losses by approximating them through a surrogate neural network. The proposed loss networks are set-wise models which are invariant to the order of mini-batch instances. Ultimately, the surrogate losses are learned jointly with the prediction model via bilevel optimization. Empirical results on multiple datasets with diverse real-life loss functions compared with state-of-the-art baselines demonstrate the efficiency of learning surrogate losses.", "review": " This paper proposes a method of learning loss functions in addition to the learning of predictors. Since it*s not easy to optimize loss functions that evaluate the accuracy, surrogate loss functions have been widely employed. The design of the surrogate loss is problem-dependent, and handcraft is required. This paper tries to tackle this problem from the viewpoint of meta-learning, i.e., the surrogate loss learning. Typically, deep neural networks (DNN) are used to design a surrogate loss that approximates the original loss while maintaining the tractability of the optimization. Some convergence properties of the proposed method are analyzed. Some empirical studies showed the efficiency of the proposed method to the state-of-the-art baselines. The design of the surrogate loss is important for machine learning problems. However, the proposed method in this paper seems an ad-hoc approach rather. For example, the 0-1 loss is often replaced with convex loss functions such as the hinge loss or logistic loss. Using these surrogate loss functions, the statistical properties of the predictors obtained from 0-1 loss are maintained. See the following paper for details. P. L. Bartlett, et al., (2006), Convexity, Classification, and Risk Bounds, Journal of the American Statistical Association March , Vol. 101, No. 473. On the other hand, the current approach does not have such a theoretical guarantee for each learning problems. Though certainly, the proposed method is widely applicable to many problems, there is no theoretical guarantee. Theorem 1 in page 5 shows the convergence property. However, the number of iterations, K_beta, should tend to infinity. This is not a practical operation in the learning algorithm"}
{"id": "iclr2020_668", "title": "Mixed Setting Training Methods for Incremental Slot-Filling Tasks | OpenReview", "abstract": "Abstract:###Model training remains a dominant financial cost and time investment in machine learning applications. Developing and debugging models often involve iterative training, further exacerbating this issue. With growing interest in increasingly complex models, there is a need for techniques that help to reduce overall training effort. While incremental training can save substantial time and cost by training an existing model on a small subset of data, little work has explored policies for determining when incremental training provides adequate model performance versus full retraining. We provide a method-agnostic algorithm for deciding when to incrementally train versus fully train. We call this setting of non-deterministic full- or incremental training ``Mixed Setting Training*. Upon evaluation in slot-filling tasks, we find that this algorithm provides a bounded error, avoids catastrophic forgetting, and results in a significant speedup over a policy of always fully training.", "review": " This study proposed a heuristic approach to decide whether to use incremental training or full training of a pre-trained model with the availability of new data. The idea is that if the new data is consistent with the previous model (therefore, incremental learning achieves a bounded error), one can resort to incremental learning; otherwise, a full re-training is required. To assess this need, the authors proposed to use reservoir sampling to select data for incremental training. The idea is intuitive, but it is not supported well by the authors, as no theoretical proof was presented, the results don*t show a clear advantage of this method, and it is contradictory with previous results (e.g., that of Golmant et al., 2017). Also, the paper requires more experiments to support the claims in the abstract: 1- Speed up for different tasks should be shown. Also, an analysis of the worst-case scenario (e.g., where the distribution of the samples changes by the time (non-stationary situation) and full training is required for each scenario) is required. 2- Catastrophic forgetting was claimed to be avoided in the abstract, while the authors later stated that they try to reduce it using reservoir sampling. Also, it should be measured using metrics introduced in Kemker et al. 2018 to show that whether the idea is really achieving its goal or not. 3- The threshold for having a bounded error also plays a significant role in the performance of the system. An experiment is required to investigate the effect of this parameter on the speed of the system, and the results should be analyzed and discussed. Furthermore, since the results are somehow contradictory to the previous task, more experiments on other tasks (e.g., NER, sentiment analysis) or domains (computer vision, time series such as stock data) should be conducted to draw a meaningful conclusion from the paper. As a side note, the authors are encouraged to proofread and rewrite some parts of the paper for the next submission (e.g. *Approaches to saving on training...*, *we discuss our approach to informing...*), better use the cite{} and citep{} commands of Latex, and present more readable figures (e.g. by including meaningful legends)."}
{"id": "iclr2020_669", "title": "Acutum: When Generalization Meets Adaptability | OpenReview", "abstract": "Abstract:###In spite of the slow convergence, stochastic gradient descent (SGD) is still the most practical optimization method due to its outstanding generalization ability and simplicity. On the other hand, adaptive methods have attracted much more attention of optimization and machine learning communities, both for the leverage of life-long information and for the deep and fundamental mathematical theory. Taking the best of both worlds is the most exciting and challenging question in the field of optimization for machine learning. In this paper, we take a small step towards such ultimate goal. We revisit existing adaptive methods from a novel point of view, which reveals a fresh understanding of momentum. Our new intuition empowers us to remove the second moments in Adam without the loss of performance. Based on our view, we propose a new method, named acute adaptive momentum (Acutum). To the best of our knowledge, Acutum is the first adaptive gradient method without second moments. Experimentally, we demonstrate that our method has a faster convergence rate than Adam/Amsgrad, and generalizes as well as SGD with momentum. We also provide a convergence analysis of our proposed method to complement our intuition.", "review": "Review:###This paper attempts to remove the use of the second moment in Adam in order to improve the generalization ability of Adam. - Apriori, it is not clear why removing the second moment is important. Does it improve the generalization or decrease the runtime substantially? - Please clarify how our method compares against Yogi (Adaptive Methods for Nonconvex Optimization, Zaheer* 2018) and the more recent RADAM (ON THE VARIANCE OF THE ADAPTIVE LEARNING RATE AND BEYOND, Liu 2019) , both theoretically and empirically. - The paper is meandering and confusing. State your update/algorithm and then explain how it compares against the other methods. Besides, there might be technical problems with it. See the detailed review below: - Section 1: *the generalization results (of adaptive methods) cannot be as good as SGD*. Please cite the relevant papers, (for example, Wilson. 2017) that show this empirically. - Section 1: *the proposed algorithm outperforms Adam in convergence speed* - Please clarify what *convergence speed* refers to. Is it the number of gradient evaluations, the rate of convergence or the wall-clock time. Please state how did you conclude this. - Section 2: The update rule of Adagrad is incorrect. The step-size is constant alpha and it is decreased over time because of the v_t term. - Section 3: There is no guarantee on the approximation in Equation 5. Young*s inequality and the resulting upper bound can be quite loose. - Section 3: In general, it is not possible to have an update that decreases the loss on the current batch, but does not increase the previous batches loss. It is always possible to construct a counter-example to this. - *In practice, the computational cost of computing* the gradient for all i is expensive. Indeed, this is batch gradient descent. I am not sure how this is relevant to the discussion in the paper. - Cite and compare against the variance reduced methods (Stochastic Average Gradient, Schmidt, 2013; SVRG, Johnson, 2013) as these try to *approximate* the full gradient in order to decrease the variance. - The derivation/formulation of Equation 8 is not clear to me. Why is the hat{m} normalized? - In algorithm 1, it seems you need to choose the sequence of step-sizes alpha_t and _x0008_eta_t. How is this an adaptive method then? How are these sequences chosen theoretically and practically? Please clarify this. - Section 4: Please compare the resulting regret bound to that of Adam, Adagrad and AMSgrad. Why does alpha_t = O(1/t)? If we have to decrease the step-size according to a sequence, why should I not use standard SGD? - Section 5: *we decay the learning rate by 0.1 every 50 epochs* This is not aligned with either the theory or the algorithm you proposed."}
{"id": "iclr2020_670", "title": "DOUBLE-HARD DEBIASING: TAILORING WORD EMBEDDINGS FOR GENDER BIAS MITIGATION | OpenReview", "abstract": "Abstract:###Gender bias in word embeddings has been widely investigated. However, recent work has shown that existing approaches, including the well-known Hard Debias algorithm which projects word embeddings to a subspace orthogonal to an inferred gender direction, are insufficient to deliver gender-neutral word embeddings. In our work, we discover that semantic-agnostic corpus statistics such as word frequency are important factors that limit the debiasing performance. We propose a simple but effective processing technique, Double-Hard Debias, to attenuate the effect due to such noise. We experiment with Word2Vec and GloVe embeddings and demonstrate on several benchmarks that our approach preserves the distributional semantics while effectively reducing gender bias to a larger extent than previous debiasing techniques.", "review": "Review:###This paper presents a technique for gender-debiasing word embeddings. The technique consists of applying PCA and k means clustering to word embeddings, in order to reduce the effect of gender biased semantics. The presentation is clear in the sense that the reader can understand the design of the technique and of the experiments; but less clear in the sense that the term bias is overloaded: early on (introduction) the authors state that removing the gender dimension is equivalent to removing gender bias (this is attributed to Bolukbasi et al. 2016 and Prost et al. 2019). This has not been preceded by a definition of gender bias; so in effect, gender bias is defined as the presence of gender in the embeddings. Further down in the introduction, the authors posit that frequency *contaminates the gender dimensions* (this is not further elaborated; as it stands, it is unclear). Even further down, the authors state their intuition that there is another type of bias in word embeddings that is closely intertwined with gender bias: this other type of bias is that words with similar frequencies in training tend to be closer in the vector space even if they do not have similar meanings. So now the core frequentist nature of embeddings is also presented as another type of bias that affects gender bias. After this line of reasoning, the authors state that their technique manages to reduce gender bias and is capable of preserving distributional semantics. But they have just spent two paragraphs defining distributional semantics as bias (in a fuzzy way). Another problem is the lack of several SOTA gender-debiasing baselines in the experimental evaluation. The only gender-debiasing baseline used is the Hard method, but this is the very method that the authors extend with their technique. This is a limitation. The topic of gender bias is important. The authors are encouraged to continue working on this area, and to focus on clarifying their definitions so that they are expressed in precise language. Thorough experimental evaluation against several SOTA baselines is also important."}
{"id": "iclr2020_671", "title": "Efficacy of Pixel-Level OOD Detection for Semantic Segmentation | OpenReview", "abstract": "Abstract:###The detection of out of distribution samples for image classification has been widely researched. Safety critical applications, such as autonomous driving, would benefit from the ability to localise the unusual objects causing the image to be out of distribution. This paper adapts state-of-the-art methods for detecting out of distribution images for image classification to the new task of detecting out of distribution pixels, which can localise the unusual objects. It further experimentally compares the adapted methods on two new datasets derived from existing semantic segmentation datasets using PSPNet and DeeplabV3+ architectures, as well as proposing a new metric for the task. The evaluation shows that the performance ranking of the compared methods does not transfer to the new task and every method performs significantly worse than their image-level counterparts.", "review": "Review:###The paper evaluates a variety of existing pixel-wise out-of-distribution detection methods in the task of semantic segmentation of road scenes. To do so, the paper introduces an evaluation protocol and applies it to two datasets (SUN and IDD) and two models (PSPNet and DeepLabV3+). Strengths: - The paper is well written with high quality visuals and plots - The paper studies an important problem Weaknesses: - The contribution seems to be rather incremental (evaluating existing methods on 2 dataset) and some related work might be missing - Although the analysis is well executed, it is not clear what the community learns from the paper Although I enjoyed reading the paper, I*d lean towards rejection of the paper. My main concern are as follows: It is not clear what the community learns after reading this paper. Is the difference between different approaches significant? Are the class boundary pixels the biggest problem? The paper would benefit strongly from providing some indications of future steps, e. g. how to improve OOD in semantic segmentation. I*m missing a discussion between OOD and and the prior work on uncertainty estimation in semantic segmentation (e. g. https://arxiv.org/pdf/1703.04977.pdf and the follow up works, or https://arxiv.org/pdf/1807.00502.pdf). It seems that uncertainty estimates for semantic segmentation could be applied to the tested scenarios off-the-shelf without the need for additional modifications. Is there any reason the paper did not include approaches for uncertainty estimation in semantic segmentation? In general, it would be useful to connect the tested OOD scenarios to other topics already studied in semantic segmentation such as: uncertainty estimation, outlier detection, and distribution shift in semantic segmentation. A paragraph drawing connections and highlighting the differences would make the paper stronger. Other comments: Section 3.2, *Therefore only the car class as ID...* I*m not sure I understand why car class is the only one considered ID for IDD. From Fig. 2, it seems that other classes such as bus, traffic light, pole, terrain, etc. could be also considered. Could the authors comment on this? *The random normal noise is usually very easily detected by all methods, therefore Perlin noise images are used*. However, when looking at the results Fig 3 and Fig 4 it feels like Normal random noise is harder than Perlin noise. Could the authors comment on this? *All OOD datasets used are mixed with Cityscapes evaluation sets*. Why it is important to add Cityscapes images to evaluation set? Wouldn*t it be enough to use OOD datasets? One suggestion of a plot that could jointly display the information from RQ1 and RQ2 would be to plot both of them in a one scatter plot (e. g. with ID IoU on x-axis and AUROC on the other). Figures 3 and 4 show results for 6 methods, while Table 2 only displays 3 scenarios for 2 models. Table 3 would benefit from including all 6 models and using the same labels as in Figures. Moreover, it would be interesting expand Table 2 by including the performance of the segmentation on in distribution classes from IID and SUN datasets in addition to Cityscapes results."}
{"id": "iclr2020_672", "title": "PDP: A General Neural Framework for Learning SAT Solvers | OpenReview", "abstract": "Abstract:###There have been recent efforts for incorporating Graph Neural Network models for learning fully neural solvers for constraint satisfaction problems (CSP) and particularly Boolean satisfiability (SAT). Despite the unique representational power of these neural embedding models, it is not clear to what extent they actually learn a search strategy vs. statistical biases in the training data. On the other hand, by fixing the search strategy (e.g. greedy search), one would effectively deprive the neural models of learning better strategies than those given. In this paper, we propose a generic neural framework for learning SAT solvers (and in general any CSP solver) that can be described in terms of probabilistic inference and yet learn search strategies beyond greedy search. Our framework is based on the idea of propagation, decimation and prediction (and hence the name PDP) in graphical models, and can be trained directly toward solving SAT in a fully unsupervised manner via energy minimization, as shown in the paper. Our experimental results demonstrate the effectiveness of our framework for SAT solving compared to both neural and the industrial baselines.", "review": "Review:###The paper presents an approach, PDP, to solve Boolean satisfiability (SAT) by decomposing it into Propagation, Decimation and Prediction, where each can be learned with a neural network. Strength: - The proposed approach makes sense to me and allows modularity. - The paper compares the approach to several prior works including the recent NeuroSAT and Glucose, a Conflict-Driven Clause Learning (CDCL) SAT solver for industrial problems. Surprisingly, NeuroSAT cannot handle the problems studied in this work. The proposed PDP performs similar to Glucose on the studied problems. - The paper is clearly written and seem over all solid. Weaknesses: 1. Comparison to prior work 1.1. To allow a better comparison to prior work, I am wondering why the author did not compare in a setting and dataset prior work evaluated SAT solvers. 1.2. It would be interest to know if and how the proposed model performs on the problems evaluated in Selsam 2019. 2. The paper could be improved by including better ablations to understand where the strength comes from, specifically w.r.t. to the design choices of Propagation, Decimation and Prediction and the relation to Selsam 2019. An overall solid paper, which could be improved by better comparison to prior work, by using setups used previously and better analyzed by providing clear ablations which allow better understanding the individual components of the system. I am borderline on this paper, also given my limited knowledge of the field."}
{"id": "iclr2020_673", "title": "SNODE: Spectral Discretization of Neural ODEs for System Identification | OpenReview", "abstract": "Abstract:###This paper proposes the use of spectral element methods citep{canuto_spectral_1988} for fast and accurate training of Neural Ordinary Differential Equations (ODE-Nets; citealp{Chen2018NeuralOD}) for system identification. This is achieved by expressing their dynamics as a truncated series of Legendre polynomials. The series coefficients, as well as the network weights, are computed by minimizing the weighted sum of the loss function and the violation of the ODE-Net dynamics. The problem is solved by coordinate descent that alternately minimizes, with respect to the coefficients and the weights, two unconstrained sub-problems using standard backpropagation and gradient methods. The resulting optimization scheme is fully time-parallel and results in a low memory footprint. Experimental comparison to standard methods, such as backpropagation through explicit solvers and the adjoint technique citep{Chen2018NeuralOD}, on training surrogate models of small and medium-scale dynamical systems shows that it is at least one order of magnitude faster at reaching a comparable value of the loss function. The corresponding testing MSE is one order of magnitude smaller as well, suggesting generalization capabilities increase.", "review": "Review:###This work proposes a new approach for the evolution of Neural ODEs for particle systems. The authors suggest to replace the traditional backpropagation through ODEs or the recent adjoint method for backpropagation and instead solve the problem as an alternating optimization scheme. In particular it is suggested that using spectral methods (Legendre*s polynomials) first compute a minimizer of the trajectory (optimizing a trajectory x(t)) given the Loss/Langrangian (that is based on the data times of training). After an initial trajectory is computed, follow an alternating minimization, where in the first step, minimize the discrepancy between the network (that describes the time change of the ODE) and the time derivative of the current trajectory. In the second step, re-compute the trajectory with the new updated network and modified lagrangian to update the network parameters again. This two step optimization is applied back and forth until a residual condition is reached, i.e. the loss is small. The network*s parameters in the two stages are optimized via SGD and ADAM respectively. Further, to perform the required numerical integration in each step the authors apply Gaussian quadrature and turn the overall optimization scheme into the repeating application of a finite number of gradient updates in an alternating fashion (as has become popular in recent Deep Learning approaches e.g. GANs). The authors test the new method on different particle systems* trajectories and observe a numerical speed-up and improved accuracy as compared with previous approaches. Admittingly, I am not an expert in Neural ODEs and am not too familiar with the literature investigating neural networks for modelling differential equations and control. The paper, however is well written and the presentation is very nice in my opinion. It is hard for me to judge how original some of the ideas presented but from my perspective they seem quite solid. Overall, I am currently voting for weak accept, for solid presentation and content, but with with the following problems: It seems that Neural ODEs are most beneficial, over the alternatives, when used with irregular data times and or sparse number of time points. This is a point that is mostly missing in the discussion and the experimental section, from my understanding the experimental section uses equally spaced intervals. If this is the case, I do not find them sufficient and I would hope to see how does this method perform when trained with sparser and or irregular time points. Currently the method presented is illustrated as an effective algorithm for noisy ODE solvers. For this reason, I am also wondering whether this is also the right venue to present this interesting work. Other small things: In the second sentence *ODE-Nets have been shown to provide superior performance with respect to classic RNNs on time series forecasting with sparse training data.* It could be nice to provide a reference illustrating the improved performance of Neural ODEs over RNNS on a time series forecasting task."}
{"id": "iclr2020_674", "title": "Convolutional Conditional Neural Processes | OpenReview", "abstract": "Abstract:###We introduce the Convolutional Conditional Neural Process (ConvCNP), a new member of the Neural Process family that models translation equivariance in the data. Translation equivariance is an important inductive bias for many learning problems including time series modelling, spatial data, and images. The model embeds data sets into an infinite-dimensional function space, as opposed to finite-dimensional vector spaces. To formalize this notion, we extend the theory of neural representations of sets to include functional representations, and demonstrate that any translation-equivariant embedding can be represented using a convolutional deep-set. We evaluate ConvCNPs in several settings, demonstrating that they achieve state-of-the-art performance compared to existing NPs. We demonstrate that building in translation equivariance enables zero-shot generalization to challenging, out-of-domain tasks.", "review": "Review:###The paper describes a method for model neural for neural processes considering translation-equivariant embeddings. The paper seems to be quite specific topic. Maybe, the author could add more empirical results to it to show the impact on translation-equivariant examples. The theoretical claims seem to be valid. So the question is a bit open what are the applications. The empirical results are also narrow as there is not much other competitive work. The results seem to be increment extension to previous work. The work looks solid to me, currently I am probably not able to appreciate and judge relevance to its full extend. I would judge, it is more of interest to view specific people working on this - maybe, the authors could for the final version make this more clear. The questions that should be more addressed maybe is also the applications - why is this relevant and how does it improve your specific cases. Why do we want to develop this. State of the art is quite relative if authors come from a quit narrow area which not much papers on the topic and data sets. One of the main points of the paper did not get clear how does translation-equivariant helps to solve or improve the empirical results. Could you add some examples where this improves results. I remain ambivalent. It seems to be solid work with not much convincing applications and somewhat incremental. Maybe the authors might address this in their introduction more. The motivation remains unclear to me and hence difficult to judge its potential and impact."}
{"id": "iclr2020_675", "title": "Irrationality can help reward inference | OpenReview", "abstract": "Abstract:###Specifying reward functions is difficult, which motivates the area of reward inference: learning rewards from human behavior. The starting assumption in the area is that human behavior is optimal given the desired reward function, but in reality people have many different forms of irrationality, from noise to myopia to risk aversion and beyond. This fact seems like it will be strictly harmful to reward inference: it is already hard to infer the reward from rational behavior, and noise and systematic biases make actions have less direct of a relationship to the reward. Our insight in this work is that, contrary to expectations, irrationality can actually help rather than hinder reward inference. For some types and amounts of irrationality, the expert now produces more varied policies compared to rational behavior, which help disambiguate among different reward parameters -- those that otherwise correspond to the same rational behavior. We put this to the test in a systematic analysis of the effect of irrationality on reward inference. We start by covering the space of irrationalities as deviations from the Bellman update, simulate expert behavior, and measure the accuracy of inference to contrast the different types and study the gains and losses. We provide a mutual information-based analysis of our findings, and wrap up by discussing the need to accurately model irrationality, as well as to what extent we might expect (or be able to train) real people to exhibit helpful irrationalities when teaching rewards to learners.", "review": "Review:###This paper studies reward inference from demonstrations of irrational experts. More specifically, a set of semantically meaningful experts* behaviors is considered which is derived from modifying the Bellman update. The quality of reward inference from these different experts is measured by two different scores and analyzed regarding properties of the demonstrator. The main finding is that irrationality can be helpful for inferring rewards. The problem addressed by the paper is very interesting and relevant to a reasonable part of the community but I argue that the paper is not ready for publication in its current form. In particular, the experiments are too limited to thoroughly support the claims (this requires at least the consideration of more different environments; and to really make the paper impactful some parts of the *future work* section should be conducted) and the write-up should be improved from Section 3 onwards to provide more clarity. A few more detailed points: * I see the paper in its current form as a theoretical study on reward inference from irrational experts. To provide insights here, and as these only involves simulations, a rich set of different MDPs should be considered. In the current form it is unclear how general the results are (although I assume that certain findings hold more generally but there is no supporting evidence for that). Maybe even formal theoretical insights can be derived. * Regarding the Bellman update, on the RHS it should be . * Regarding the presentation of the irrational experts. Is there a simpler way of presenting the irrational experts through modified MDPs that the expert tries to solve optimally? Are all updates actually convergent, in particular the pessimistic one? * Please provide a formal specification of the reward model used in experiments. * Please provide a describe how you do the computation of the posteriors in the main paper (or at least provide a forward reference to the appendix). Which prior on are you using (put in main paper)? * What is the precise nature of ? I would assume it is a sequence of state-actions but that is not consistent with the definition of the log-loss which suggests it is only actions. * Probably more interesting than the log-loss and L^2 loss is the actual performance of an optimal policy using the inferred reward parameters. It would be good to report this numbers. How do irrational experts compare looking at this metric? * The discussion of related work should be extended. For instance, R. Shah et al.*s paper *On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference* should be discussed in more detail and similarities and differences clarified. Minor comments and suggestions for improving the paper: * The definition of Boltz in 2.2.2 can be made more clear. Maybe define the function using actions to connect to the above equation. * Correct *update on the the trajectory *. * Please check the usage of and and make it consistent. I think it would also help to make the log-loss and L^2 distance not look like a function of . * Figure 4/5: Explain what we see. I guess the black square is the starting position? * Regarding figure 5: Comparing different values seems more sensible if the norm of is normalized."}
{"id": "iclr2020_676", "title": "Skew-Explore: Learn faster in continuous spaces with sparse rewards | OpenReview", "abstract": "Abstract:###In many reinforcement learning settings, rewards which are extrinsically available to the learning agent are too sparse to train a suitable policy. Beside reward shaping which requires human expertise, utilizing better exploration strategies helps to circumvent the problem of policy training with sparse rewards. In this work, we introduce an exploration approach based on maximizing the entropy of the visited states while learning a goal-conditioned policy. The main contribution of this work is to introduce a novel reward function which combined with a goal proposing scheme, increases the entropy of the visited states faster compared to the prior work. This improves the exploration capability of the agent, and therefore enhances the agent*s chance to solve sparse reward problems more efficiently. Our empirical studies demonstrate the superiority of the proposed method to solve different sparse reward problems in comparison to the prior work.", "review": " This paper proposes a new exploration algorithm by proposing a new way of generating intrinsic rewards. Specifically, the authors propose to maintain a *novelty frontier* which consists of states that have low-likelihood under some likelihood model trained on their replay buffer. The authors propose to sample from the novelty frontier using a scheme similar to a prior method called Skew-Fit, but replace the VAE with a kernel-based density model. To construct an exploration reward, the authors estimate the KL divergence between the resulting policy state distribution and the desired `state distribution, where the desire state distribution is a Gaussian centered around a point sampled from the novelty frontier. Overall, the paper tackles an important questions of exploration, and while the concept of a frontier set is not novel, the authors propose a concrete instantiation that has promising results on continuous state spaces. I*m skeptical that this exact algorithm would work on domains with complex state spaces (e.g. images), where adding Gaussian noise to your state won*t produce reasonable nearby states. That said, the general idea of fitting a new model to the latest trajectory and using KL as reward seems like a promising principle that could on its own scale. However, the theory seems a bit off and there are a few experimental details that make me hesitant to increase my score. In details: Theory: I found the proof surprisingly long given that it amounts to saying that if (1) S = Z + N and (2) Z and N are independent, then H(S) >= H(N) and so H(S | Z) - H(Z | S) = H(S) - H(Z) >= H(N) - H(Z) Perhaps more worrisome is the statement, *we consider to maximize h(S|Z) - h(Z|S)*. Unless I misread the paper, the authors do not maximize this quantity. Instead, they *fix* this quantity by choosing a fixed entropy of N. Worse yet, this quantity is actually minimizes since, while h(N) is fixed for the duration of the experiment, h(Z) is maximized (*To increase h(Z), we need to add...*). It would be good for the authors to address this concern, given that the claim of the paper is that they are *maximizing the entropy of the visited states.* It seems like a simple answer is the following: given that S = Z + N, if N is fixed to some Gaussian distribution, then the authors simply need to maximize H(Z), which they are already doing. I*m not sure why the authors need to reason about H(S | Z) - H(Z | S). Experiments: Can Table 1 be replaced with the learning curves? The numbers 90% success and standard deviation of 3% seem like arbitrary numbers. It doesn*t preclude the possibility that (e.g.) Skew-Fit or RND receives a 99% success rate with standard deviation of 3.1%. Figure 11 and 12 of the Appendix don*t convince me that threshold at 90% and 3% is a particularly good choice. Can the authors summarize the difference between coverage and entropy in the main paper? It seems like an important distinction. Given that the authors did not use all 8 pages, it would be good to explain it there rather than in the Appendix. How sensitive is the method to the hyperparameter alpha? How was it chosen? Is it the same alpha chosen for Skew-Fit? How was N chosen for the door environment? Is Figure 7 (left) showing the performance on the simulated or real-world robot? If it was done on the real-world robot, were there any important details in getting sim-to-real-world to work? In Figure 5, why does there seem to be discrete jumps in the learning curves for *DoorOpen Coverage*? I would be inclined to raise my score if: 1. The authors clarify why studying the quantity H(S | Z) - H(Z | S) is particularly important. 2. Address the concerns raised over the experiments. 3. Discuss more explicitly under what assumption they expect for this method (with a Gaussian KDE) to work well"}
{"id": "iclr2020_677", "title": "Siamese Attention Networks | OpenReview", "abstract": "Abstract:###Attention operators have been widely applied on data of various orders and dimensions such as texts, images, and videos. One challenge of applying attention operators is the excessive usage of computational resources. This is due to the usage of dot product and softmax operator when computing similarity scores. In this work, we propose the Siamese similarity function that uses a feed-forward network to compute similarity scores. This results in the Siamese attention operator (SAO). In particular, SAO leads to a dramatic reduction in the requirement of computational resources. Experimental results show that our SAO can save 94% memory usage and speed up the computation by a factor of 58 compared to the regular attention operator. The computational advantage of SAO is even larger on higher-order and higher-dimensional data. Results on image classification and restoration tasks demonstrate that networks with SAOs are as effective as models with regular attention operator, while significantly outperform those without attention operators.", "review": "Review:###The authors introduce a novel self-attention operator for neural networks. Their self-attention operator computes similarity between elements a and b as (a+b)^Tw where w is a learned parameter and does not use the softmax operator. This leads to improvements in space and time complexity compared to regular self-attention which uses the dot product (a^Tb). They show that concatenating their operation with convolution brings improvements over the MobileNetv2 baseline on ImageNet classification and over U-Net on restoration tasks. Attention has been empirically shown to bring improvements in many visual tasks but certain methods (such as self-attention) can be quite expensive in computations and memory. Identifying cheaper attention mechanisms that obtain similar accuracy performance as expensive attention mechanisms is therefore an important direction for work. However, I take several crucial issues with this work and especially the evaluation/presentation of the methods: - Although this is the focus of the work, the authors do not report actual memory consumption and latency times for MobileNetv2, SANet and the other attention mechanisms. (There is no need for the simulated scenarios of Table 2 since we already know the theoretical complexities of the different methods). - The authors only compare their methods against regular self-attention (without or with pooling/softmax), and ignore a longstanding literature of other (potentially cheaper in terms of memory and computations) attention mechanisms in vision (see below). Without comparison to at least Squeeze-and-Excite, it is hard to evaluate the significance of the method presented in the draft. - The motivation for naming the method *siamese* is quite poor. Siamese networks typically are more complex than a single layer feed forward (which is just a dot-product). Furthermore, the siamese similarity (as introduced by the authors) does not respect the usual properties of similarity functions. For example siasim(a, 0) = a^tw = 1/2 siasim(a,a) can take arbitrary values including negative values (*a can be dissimilar with itself*) - X vs Q, K, V? Self-attention is incorrectly described as *a special case of the attention operator with Q = K = V*, instead of Q = XW^Q, K=XW^K, V = XW^V. - In Table 7, shouldn*t SANet w/o params have less params than SANet? In summary, the paper addresses an important challenge and proposes a technically sound method. However, the current draft has fundamental experimental flaws in its evaluation/presentation and lacks comparison against relevant cheap channelwise attention mechanisms (such as Squeeze-and-Excitation). I argue for rejection. Relevant literature: - channelwise attention: Squeeze-and-Excitation, Gather-Excite - Channelwise and spatial attention: Bottleneck Attention Module, Convolutional Block Attention Module - Relative Self-attention for vision: Attention Augmented Convolutional Networks, An Empirical Study of Spatial Attention Mechanisms in Deep Networks."}
{"id": "iclr2020_678", "title": "Multi-Task Adapters for On-Device Audio Inference | OpenReview", "abstract": "Abstract:###The deployment of deep networks on mobile devices requires to efficiently use the scarce computational resources, expressed as either available memory or computing cost. When addressing multiple tasks simultaneously, it is extremely important to share resources across tasks, especially when they all consume the same input data, e.g., audio samples captured by the on-board microphones. In this paper we propose a multi-task model architecture that consists of a shared encoder and multiple task-specific adapters. During training, we learn the model parameters as well as the allocation of the task-specific additional resources across both tasks and layers. A global tuning parameter can be used to obtain different multi-task network configurations finding the desired trade-off between cost and the level of accuracy across tasks. Our results show that this solution significantly outperforms a multi-head model baseline. Interestingly, we observe that the optimal resource allocation depends on both the task intrinsic characteristics as well as on the targeted cost measure (e.g., memory or computing cost).", "review": " Overview This paper presents a model that can perform multiple audio classification task using a number of shared layers. The main motivation for the proposed model is the fact that it consumes the same input (audio) to estimate various properties (like language ID, musical pitch, bird song) and therefore it might be useful to share some of the lower level computations while also having some task specific parameters/layers. The main contribution of the paper is an additive cost term that learns to distribute parameters between the various tasks while penalising parameters/number of operations. The main feature of the proposed model is task specific layers that are placed in parallel to the shared embedding layers that are used by all tasks. The task specific layers receive inputs from all the shared layers and the task specific layers in the layer below. The design is such that the task specific layers/parameters for each task are independent and therefore during inference only the computations for a given task need to performed (in addition to the shared computations). The cost function is then used to distribute parameters between these layers given a fixed compute budget. Although I think the idea and the area of application are extremely interesting and relevant, there are some shortcomings that should be addressed before the paper is accepted. My main criticism is the fact that the comparison between the multi-head model and the proposed architectures is not fair. The multi-head architecture does not have any task-specific parameters and contains N softmax layers for the N tasks considered, on top of the final shared encoder layer. This architecture is quite restrictive because not only does the multi-head layer have fewer parameters, it is also forced to share the same fixed-dimensional encoding across all N-tasks. It is therefore unsurprising that the multi-head architecture performs worse since it uses the same fixed-dimensional embedding in the final layer to represent information using for all N tasks, some of which are quite unrelated, for example detecting musical pitches and bird songs. A better baseline architecture would be to have a number of shared layers, followed by a number of tasks specific layers in series rather than parallel. This architecture might yield similar accuracies, would also have independent parameters for each task and the same cost function could then be used to distributed parameters between tasks. Secondly, the authors do not comment on the relationship between tasks. Some of the tasks like speaker identification and language identification are clearly related. However, detecting musical pitches, bird songs, instruments and environmental sounds etc are quite unrelated and it doesn*t make sense for a single network to be good at performing all these tasks simultaneously. This is clearly reflected in the evaluation where the single-task accuracy is always better than any of the multi-task results for some tasks. The paper does not provide any insight into what tasks can be usefully shared. I think this needs to be addressed first before investigating the best method for sharing computation between tasks. Finally, I found the evaluation section quite difficult to follow. The figures should be improved for clarity and the main arguments and inferences drawn from these figures are not clear. Overall I think the proposed cost function has limited novelly and although the area of application is extremely interesting, the paper fails to address some important basic questions about the nature of the tasks. Furthermore, the baseline comparison isn*t fair for the reasons outlined above. Minor Comments: Introduction 1. “characterized by a large number of parameters and floating point operations (FLOPs)”, the authors mention floating point operations however all models that are deployed on embedded devices are quantised to have integer weights. Floating point operations are only performed during training. Given that this paper is about on-device inference, this sentence is a bit confusing. The authors should replace FLOPs with number of operations. 2. “the audio embeddings might fail to capture all the information needed to solve all tasks”, the authors here argue that the shared embeddings might fail to capture information related across all tasks, which is why I believe that baseline multi-head system should contain task specific layers/computations in series after the shared embedding layers. Methods 1. I found the description of the gating mechanism quite hard to follow and had to read the section several times to understand what exactly is going on, even though the overall function is quite simple. The description should be improved for clarity. 2. “The slope of the non-linearity s is progressively increased during training”, what schedule was used to do this? The experiments later also do not shed any light on how the slope was changed. Experiments 1. Why use 64 bins while computing the STFT? The standard practice is to use 40 bins (in speech and other audio tasks). Do any of the tasks require extra resolution along the frequency axis? Lowering the number of frequency bins is a useful way of reducing the input size and therefore the number of computations performed in the first/input layer. 2. “Note that the choice of the tasks used for the evaluation is consistent with the selected temporal granularity”. I’m not sure what temporal granularity means in this content. 3. “As such, we do not consider speech recognition tasks, which generally require a much finer temporal granularity” I’m not exactly what is meant here. Temporal granularity in feature extraction? Or in the labels that are assigned to the input audio? This statement should also be clarified. 4. “The number of parameters of the output softmax layer depends on the number of output classes.” What is the sum of parameters in all output layers combined? Surely this is non-trivial compared to the number of parameters in the encoder (65k and 125k) 5. Figures 2b,c and 3b,c should be enlarged for clarity. The text says that the curves should not start at 0 since they consider the number of parameters in the task specific layers, but many points on curve 2b look like they start at 0. Also Figures 2 and 3 are quite difficult to read. Its not the most clear presentation of the argument. The tables are much more clear though. Summary The paper presents a novel model for performing multiple audio related tasks using joint/tied layers. The main novelty of the paper is to present an additive term in cost function and a gating mechanism that penalises large models. I think the multi-head baseline against which the results are presented is quite restrictive since it doesn’t have any task specific parameters and also has to share the same vectors for encoding all the information related to all the different tasks. Although the number of different tasks and experiments presented is commendable, the evaluation section does not present a convincing argument. The evaluation section also needs to be reworked in order to present the arguments and results more clearly. Finally, the authors should consider the relationship between the tasks considered and whether they expect all of them to be benefit from a joint/multi-task approach."}
{"id": "iclr2020_679", "title": "At Your Fingertips: Automatic Piano Fingering Detection | OpenReview", "abstract": "Abstract:###Automatic Piano Fingering is a hard task which computers can learn using data. As data collection is hard and expensive, we propose to automate this process by automatically extracting fingerings from public videos and MIDI files, using computer-vision techniques. Running this process on 90 videos results in the largest dataset for piano fingering with more than 150K notes. We show that when running a previously proposed model for automatic piano fingering on our dataset and then fine-tuning it on manually labeled piano fingering data, we achieve state-of-the-art results. In addition to the fingering extraction method, we also introduce a novel method for transferring deep-learning computer-vision models to work on out-of-domain data, by fine-tuning it on out-of-domain augmentation proposed by a Generative Adversarial Network (GAN). For demonstration, we anonymously release a visualization of the output of our process for a single video on https://youtu.be/Gfs1UWQhr5Q", "review": "Review:###The paper is a nice piece of works which clearly articulates the objective and the subsequent discussion. The focus of the paper--i.e. disclose the difficulties of piano fingering data annotation and the proposal of automating this process by automatically extracting fingerings from public videos and MIDI files, using computer-vision DNN-based algorithms —although not really mainstream, it does provide some practical insights using a couple of experimental settings (piano fingering model and prediction) to help the readers. I really enjoyed reading this paper. I think that it can be considered a relevant and interesting piece of work, very well written and clear. Furthermore, providing new benchmarks/datasets/competitions for the AI community is always refreshing. Also, the results seem believable and solid, and potentially useful. My only concern is that, although the rationale and utility of the paper is clear, the rest of the paper is somewhat incremental/engineering piece which depends somehow on previous works (see Nakamura,2019). I fail to see much novel scientific contribution to the area of research (apart from the dataset) and I’m not sure whether there are enough scientific technical advancements. Furthermore, the experimental setting is somewhat limited, and it is not clear whether results are statistically significant."}
{"id": "iclr2020_680", "title": "Extracting and Leveraging Feature Interaction Interpretations | OpenReview", "abstract": "Abstract:###Recommendation is a prevalent application of machine learning that affects many users; therefore, it is crucial for recommender models to be accurate and interpretable. In this work, we propose a method to both interpret and augment the predictions of black-box recommender systems. In particular, we propose to extract feature interaction interpretations from a source recommender model and explicitly encode these interactions in a target recommender model, where both source and target models are black-boxes. By not assuming the structure of the recommender system, our approach can be used in general settings. In our experiments, we focus on a prominent use of machine learning recommendation: ad-click prediction. We found that our interaction interpretations are both informative and predictive, i.e., significantly outperforming existing recommender models. What*s more, the same approach to interpreting interactions can provide new insights into domains even beyond recommendation.", "review": "Review:###This paper proposed a model for extracting global feature interactions from the source model which was later being encoded in the target model to enhance its prediction performance. Strong points: 1. The paper laid out the necessary background knowledge very clear even for the audiences outside this area. 2. The paper performed reasonable amount of experiments to compare the proposed model with various baseline models with various data sets, which are strong enough to support the claims made in this paper. Comments: 1. The theoretical innovation of this paper is trivial. The local detection model for feature interactions, i.e. MADEX is simply employing the previous work, LIME and NID. Its global extension, i.e. GLIDER and the proposed encoding method are straightforward. 2. The descriptions on the feature dimensions of the original data and the generated binary representation data x’ are rather confusing and inconsistent throughout the paper. If I understand it correctly, the data sample from the original feature space x in R^p, and a binary representation x’ in R^d, where d <= p. However, when d first appears in the first paragraph of section 3, it is defined as the dimension of the original feature space and later in that paragraph the dimension became p in “f(.): R^p -> R”. There is no clarification on the differences between d and p till section 4.1 where it states “x in R^p ”. However, the dimension of the data instance from the original feature space changed to d again in section 4.2 where it states “x =[x_1, x_2, …, x_d]”. 3. “What’s more, the same approach to interpreting interactions can provide new insights into domains even beyond recommendation.” should be “What’s more, the same approach to interpret interactions can provide new insights into domains even beyond the recommendation.” 4. “An interaction, I, is a subset of all input features...”, but according to the following sections, I is the indices of a feature subset instead of the features themselves. 5. “...by requiring the same cross feature ID to occur more that T times in a batch of samples,...”, ‘that’ should be ‘than’. Overall, although there is no significant theoretical innovation, it is a decent application paper."}
{"id": "iclr2020_681", "title": "Statistical Adaptive Stochastic Optimization | OpenReview", "abstract": "Abstract:###We investigate statistical methods for automatically scheduling the learning rate (step size) in stochastic optimization. First, we consider a broad family of stochastic optimization methods with constant hyperparameters (including the learning rate and various forms of momentum) and derive a general necessary condition for the resulting dynamics to be stationary. Based on this condition, we develop a simple online statistical test to detect (non-)stationarity and use it to automatically drop the learning rate by a constant factor whenever stationarity is detected. Unlike in prior work, our stationarity condition and our statistical test applies to different algorithms without modification. Finally, we propose a smoothed stochastic line-search method that can be used to warm up the optimization process before the statistical test can be applied effectively. This removes the expensive trial and error for setting a good initial learning rate. The combined method is highly autonomous and it attains state-of-the-art training and testing performance in our experiments on several deep learning tasks.", "review": "Review:###This paper proposes a new way of automatically scheduling the learning rate in stochastic optimization algorithms: Stochastic Approximation with Line-search and Statistical Adaptation (SALSA). By first introducing a necessary condition for stationarity, the authors use this condition to make a simple statistical test for non-stationarity. Using this test, the authors propose the following strategy for the learning rate schedule in stochastic optimization problems: (1) They first apply a new Line-Search algorithm (Smoothed Stochastic Line-Search - SSLS) to increase a small initial learning rate until the process becomes stationary according to their statistical test. At this stage, the learning rate is assumed to be optimally initialized for the objective function considered. (2) The second step is to decrease the learning rate gradually every time the process is stationary again. For this, the authors derived their own version of a Statistical Adaptive Stochastic Approximation algorithm called SASA+ based on their statistical test. The resulting strategy benefits from being simpler than previous statistical tests while being equally effective empirically. The authors empirically demonstrated that their new learning rate scheduling mechanism achieves comparable, if not better, accuracy on two image classification tasks with ResNet-18 neural networks. It is important to note that the compared baselines got their parameters slightly fine-tuned, while (according to the authors) the proposed approach was not fine-tuned, and only the default parameters were used. This shows the robustness of the proposed approach against its various parameter settings. I would accept this submission because the authors propose a new learning rate scheduling mechanism that seems to perform well empirically while being robust against its different initial parameter settings (including the choice of the initial learning rate). This paper has several novelties: first, it proposes a simpler, yet as effective as previous approaches, Statistical Adaptive Stochastic Approximation (SASA) algorithm based on a new statistical test for non-stationarity. Second, it manages to relax the dependence of SASA algorithms on their optimal initial learning rate by introducing a Smoothed Stochastic Line Search (SSLS) algorithm that is responsible for finding such an optimal initial learning rate. Combined together, these two sub-routine provide a robust mechanism to schedule the learning rate in stochastic optimization problems. One improvement I could suggest to better motivate the proposed approach is to experiment it not only on Convolutional-based networks with image classification tasks but also on Recurrent-based networks with text datasets. For instance, keeping the ImageNet experiments, the CIFAR-10 experiments could be replaced by an NLP task. This would show that the proposed approach is robust to different types of deep learning problems. Overall I found the paper well written, and relatively easy to follow, even for non-theoretical practitioners. A few details listed below could improve even more the quality of this paper: - At the top of page 2, the last sentence of the top paragraph (*However, these learning rate schedules are insufficient ...*) requires a citation. - In Figure 5: it is a little confusing to have the SASA+(NAG) algorithm in both rows: once in the top row, twice in the bottom row. The difference between the two SASA+(NAG) in the bottom row is well explained, but is there any difference between SASA+(NAG) in the top row and the ones in the bottom row? - In Figure 5 - bottom row, left graph: if all lines are SASA+ algorithm, the legend should be consistent: either add *sasa+* to all lines or remove it from all lines. - On page 9, ImageNet paragraph: a small typo: *On the other hand, both both SASA+ and SALSA...*."}
{"id": "iclr2020_682", "title": "Pushing the bounds of dropout | OpenReview", "abstract": "Abstract:###We push on the boundaries of our knowledge about dropout by showing theoretically that dropout training can be understood as performing MAP estimation concurrently for an entire family of conditional models whose objectives are themselves lower bounded by the original dropout objective. This discovery allows us to pick any model from this family after training, which leads to a substantial improvement on regularisation-heavy language modelling. The family includes models that compute a power mean over the sampled dropout masks, and their less stochastic subvariants with tighter and higher lower bounds than the fully stochastic dropout objective. The deterministic subvariant*s bound is equal to its objective, and the highest amongst these models. It also exhibits the best model fit in our experiments. Together, these results suggest that the predominant view of deterministic dropout as a good approximation to MC averaging is misleading. Rather, deterministic dropout is the best available approximation to the true objective.", "review": "Review:###This paper proposes a new understanding of dropout on top of variational dropout, which shows that training with dropout equals to maximizing an empirical variational lower bound on the log-likelihood. This paper shows that the log posterior have the same lower bound when the inference model p(y|x) is defined by different methods, i.e., the arithmetic mean of predictions with different dropout masks, the geometric mean, and a power-mean family as an interpolation between these two cases. This indicates that with the same training objective, different inference methods have different gaps to the posterior lower bound. Intuitively, a smaller gap might lead to better performance. The paper then uses an existing result from Liao & Berg (2017) to show that the gap can be bounded by the variance of prediction probability. With empirical observations, the paper gives an unrigorous conclusion that the deterministic inference with dropout rate 0 achieves the smallest gap. However, this does not hold theoretically due to the extra bias on expectation. The optimality of deterministic inference does not hold empirically due to class imbalance or discrepancy between training and test sets. The paper then proposes two practical solutions for better inference: 1) tuning dropout rate, softmax temperature, and the power mean parameter; and 2) deterministic inference with tuned softmax temperature. By using the first inference solution, the performance on PTB and Wikitext2 LM can be improved by 2-3 on perplexity but is still slightly worse than the SOTA achieved by the mixture of softmaxes. The idea of analyzing the gap to variational posterior lower bound for different dropout inference model is interesting. The derivations are correct. The organization is not perfect and readers might find it hard to follow here and there, but the main idea is understandable. Experiments show that the suggested tuning of inference hyperparameters can bring improvements to LM tasks, which is convincing. However, there are still major gaps between the theoretical analysis, the conclusion and the empirical solution (please see the detailed comments). Such gaps make the main contribution questionable and make it as a pure empirical paper on its value. Detailed comments: 1) Reducing the variance of output prediction can reduce the gap on variational posterior, but how does the gap relate to the generalization error? The current paper only indicates that a small gap gives more consistency between the true objective and the optimized objective defined on the training set: they can be still far away from the expected posterior over data distribution. Hence, it is hard to directly relate *reducing the gap* and *improve the test-set performance*. 2) As the author mentioned in Section 3.4, reducing the dropout rate causes a bias issue on the expectation. So it is not clear whether deterministic inference with zero dropout rate can achieve the smallest gap or not. In this way, the conclusion is only supported by the empirical observations but not the presented theoretical analysis. 3) One main contribution of this paper is the power-mean family of dropout. However, only one member (alpha=0.5) from the family has been evaluated in the experiments, and it does not achieve the best performance in most experiments. So this contribution seems not practically useful according to the empirical result. 4) It is not clear how the prediction variance is reduced gradually in order to generate the results in Figure 1(b). I guess reducing dropout rate is not the correct way to do so since it causes the bias issue and the tightness will be influenced."}
{"id": "iclr2020_683", "title": "Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps | OpenReview", "abstract": "Abstract:###Modern neural network architectures use structured linear transformations, such as low-rank matrices, sparse matrices, permutations, and the Fourier transform, to improve inference speed and reduce memory usage compared to general linear maps. However, choosing which of the myriad structured transformations to use (and its associated parameterization) is a laborious task that requires trading off speed, space, and accuracy. We consider a different approach: we introduce a family of matrices called kaleidoscope matrices (K-matrices) that provably capture any structured matrix with near-optimal space (parameter) and time (arithmetic operation) complexity. We empirically validate that K-matrices can be automatically learned within end-to-end pipelines to replace hand-crafted procedures, in order to improve model quality. For example, replacing channel shuffles in ShuffleNet improves classification accuracy on ImageNet by up to 5%. Learnable K-matrices can also simplify hand-engineered pipelines---we replace filter bank feature computation in speech data preprocessing with a kaleidoscope layer, resulting in only 0.4% loss in accuracy on the TIMIT speech recognition task. K-matrices can also capture latent structure in models: for a challenging permuted image classification task, adding a K-matrix to a standard convolutional architecture can enable learning the latent permutation and improve accuracy by over 8 points. We provide a practically efficient implementation of our approach, and use K-matrices in a Transformer network to attain 36% faster end-to-end inference speed on a language translation task.", "review": "Review:###Summary The authors introduce kaleidoscope matrices (K-matrices) and propose to use them as a substitute for structured matrices arising in ML applications (e.g. circulant matrix used for the convolution operation). The authors prove that K-matrices are expressive enough to capture any structured matrix with near-optimal space and matvec time complexity. The authors demonstrate that learnable K-matrices achieve similar metrics compared to hand-crafted features on speech processing and computer vision tasks, can learn from permuted images, achieve performance close to a CNN trained on unpermuted images and demonstrate the improvement of inference speed of a transformer-based architecture for a machine translation task. Review The overall quality of the paper is high. The main contribution of the paper is the introduction of a family of matrices called kaleidoscope matrices (or K-matrices) which can be represented as a product of block-diagonal matrices of a special structure. Because of the special structure, the family allows near-optimal time matvec operations with near-optimal space complexity for structured matrices which are commonly used in deep architectures. The proposed approach is novel. It gives a new characterization of sparse matrices with optimal space complexity up to a logarithmic term. Moreover, the proposed characterization is able to learn any structured matrix and matvec time complexity of the K-matrix representation is near-optimal matvec time complexity of the structured matrix. Even though in the worst-case complexity is not optimal, the authors argue that for matrices that are commonly used in machine learning architectures (e.g. circulant matrix in a convolution layer) the characterization is optimal. This results in a new differentiable layer based on a K-matrix that can be trained with the rest of an architecture using standard stochastic gradient methods. However, it is worth noting that the reviewer is not an expert in the field, and it is hard for him to compare the proposed approach with previous work. The paper is generally easy to follow. Even though the introduction of K-matrices requires a lot of definitions, they are presented clearly and Figure 1 helps to understand the concept of K-matrices. The experimental pipeline is also clear. Given the special structure of the family, the reviewer might guess that having K-matrices can slow down the training, i.e. it might require more epochs to achieve the reported results compared to baselines. Providing training plots might increase the quality of the paper. The experimental results are convincing. First, the authors show that K-matrices can be used instead of a handcrafted MFSC featurization in an LSTM-based architecture on the TIMIT speech recognition benchmark with only a 0.4% loss of phoneme error rate. Then, the authors evaluate K-matrices on ImageNet dataset. In order to do so, they compare a lightweight ShuffleNet architecture which uses a handcrafted permutation layer to the same architecture but with a learnable K-matrix instead of the permutation layer. The authors demonstrate the 5% improvement of accuracy over the ShuffleNet with 0.46M parameters with only 0.05M additional parameters of the K-matrix and the 1.2% improvement of accuracy over the ShuffleNet with 2.5M parameters with only 0.2M additional parameters of the K-matrix. Next, the authors show that K-matrices can be used to train permutations in image classification domains. In order to demonstrate so, they take the Permuted CIFAR-10 dataset and ResNet-18 architecture, insert a trainable K-matrix at the beginning of the architecture and compare against ResNet-18 with an inserted FC-layer (attempting to learn the permutation as well) and ResNet-18 trained on the original, unpermuted CIFAR-10 dataset. With K-matrix, the authors achieve a 7.9% accuracy improvement over FC+ResNet-18 and only a 2.4% accuracy drop compared to ResNet-18 trained on the original CIFAR-10. Finally, the authors demonstrate that K-matrices can be used instead of the decoder’s linear layers in a Transformer-based architecture on the IWSLT-14 German-English translation benchmark which allows obtaining 30% speedup of the inference using a model with 25% fewer parameters with 1.0 drop of BLEU score. Overall, the analysis and the empirical evaluations suggest that K-matrices can be a practical tool in modern deep architectures with a variety of potential benefits and tradeoffs between a number of parameters, inference speed and accuracy, and ability to learn complex structures (e.g. permutations). Improvements 1. Even though K-matrices are aimed at structured matrices, it would be curious either to empirically compare K-matrices to linear transformations in fully-connected networks (i.e. dense matrices) or to provide some theoretical analysis. 2. Section 3.3 argues that K-matrices allow to obtain an improvement of inference speed, however, providing the results of convergence speed (e.g. training plots with a number of epochs) will allow a better understanding of the proposed approach and will improve the quality of the paper."}
{"id": "iclr2020_684", "title": "Interpretability Evaluation Framework for Deep Neural Networks | OpenReview", "abstract": "Abstract:###Deep neural networks (DNNs) have attained surprising achievement during the last decade due to the advantages of automatic feature learning and freedom of expressiveness. However, their interpretability remains mysterious because DNNs are complex combinations of linear and nonlinear transformations. Even though many models have been proposed to explore the interpretability of DNNs, several challenges remain unsolved: 1) The lack of interpretability quantity measures for DNNs, 2) the lack of theory for stability of DNNs, and 3) the difficulty to solve nonconvex DNN problems with interpretability constraints. To address these challenges simultaneously, this paper presents a novel intrinsic interpretability evaluation framework for DNNs. Specifically, Four independent properties of interpretability are defined based on existing works. Moreover, we investigate the theory for the stability of DNNs, which is an important aspect of interpretability, and prove that DNNs are generally stable given different activation functions. Finally, an extended version of deep learning Alternating Direction Method of Multipliers (dlADMM) are proposed to solve DNN problems with interpretability constraints efficiently and accurately. Extensive experiments on several benchmark datasets validate several DNNs by our proposed interpretability framework.", "review": "Review:###This paper gathers various requirements of the interpretability, and formulates constraints as regularizations in the learning procedure to ensure the interpretability of the model. Strengths: * The stabilities of several kinds of NN are proven. * The requirements of the interpretability is formulated into one unified optimization procedure. Weaknesses: * The clearance of the paper should be improved. How the constraints are modeled as G_l(W_l) is not clearly formulated. The difference between the proposed work and the work of Wang et al., 2019 is not emphasized. * The definitions about the stabilities is just (local) Lipchitz conditions, which should be clearly stated. * In the experiments, it seems that no baseline (that is, no interpretability constraint) evaluation score is represented. And, how the constraints affect each other is not analyzed. * There is no analysis about how the unified framework helps to interpret the behavior of the neural network better than previous work."}
{"id": "iclr2020_685", "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning | OpenReview", "abstract": "Abstract:###Universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. A common paradigm is to pre-train a feature extractor on large amounts of data then fine-tune it as part of a deep learning model on some downstream task (i.e. transfer learning). While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.", "review": "Review:###This work is an empirical study of testing how pruning at the pre-training stage affects subsequent transfer learning (through fine-tuning) stage. The main idea is to carefully control the amount of sparsity injected into BERT through weight magnitude pruning and study the impact on accuracy. The experimental setup is mostly well done, especially the part that disentangles the complexity restriction and information deletion. During the exploration, the authors made several interesting observations, such as 30-40% model weights do not encode any useful inductive bias, which could help shed some light for future work on both training and compressing BERT-like models. Overall, the paper is well written and explained. The goal is meaningful, and this is a sensible contribution to the ongoing interests of compressing BERT-like large models for efficient training and inference. My major concern is on its novelty and how directly it can provide benefit to computation. First, although the findings are interesting, the methods used in this paper are not new. Various pruning techniques have been explored in prior work, which makes the novelty contribution of this paper somewhat limited. Furthermore, the study has mostly focused on the impact of random sparsity to accuracy. However, as it is known that it is really difficult for modern hardware to benefit from random sparsity because it leads to irregular memory accesses, which negatively impact the performance. It has been observed that speedups are very limited or can be negative even the random sparsity is >95% [1]. Therefore, it is hard to judge how inference or training can benefit from 30-40% weight sparsity. Going forward, the authors are encouraged to choose pruning methods that lead to regular memory access to avoid adversely impacting practical acceleration in modern hardware platforms. [1] Learning Structured Sparsity in Deep Neural Networks. Wen et al. NeurIPS 2016"}
{"id": "iclr2020_686", "title": "Supervised learning with incomplete data via sparse representations | OpenReview", "abstract": "Abstract:###This paper addresses the problem of training a classifier on incomplete data and its application to a complete or incomplete test dataset. A supervised learning method is developed to train a general classifier, such as a logistic regression or a deep neural network, using only a limited number of observed entries, assuming sparse representations of data vectors on an unknown dictionary. The proposed method simultaneously learns the classifier, the dictionary and the corresponding sparse representations of each input data sample. A theoretical analysis is also provided comparing this method with the standard imputation approach, which consists on performing data completion followed by training the classifier based on their reconstructions. The limitations of this last *sequential* approach are identified, and a description of how the proposed new *simultaneous* method can overcome the problem of indiscernible observations is provided. Additionally, it is shown that, if it is possible to train a classifier on incomplete observations so that its reconstructions are well separated by a hyperplane, then the same classifier also correctly separates the original (unobserved) data samples. Extensive simulation results are presented on synthetic and well-known reference datasets that demonstrate the effectiveness of the proposed method compared to traditional data imputation methods.", "review": "Review:###1. Summary The authors propose a scheme for simultaneous dictionary learning and classification based on sparse representation of the data within the learned dictionary. Their goal is achieved via optimization of a three-part training cost function that explicitly models the accuracy and sparsity of the sparse model, simultaneously with usual classification error minimization. An alternating optimization algorithm is used, alternating between sparse representations and other parameters. The problem they want to address with this scheme is training over incomplete/partial feature vectors. A clean theoretical statement is provided that provides conditions under which a classifier trained via partial feature vectors would do no better in terms of accuracy, had it been trained on complete feature vectors. The authors claim this condition can be checked after training, although this ability is not validated/illustrated numerically. 2. Decision and Arguments Weak Accept a) Very nice mathematical result and justification. The proof is clear. However, why wasn’t the result used in the numerical section? You claim that the condition (4) can be evaluated to test optimality: how do your trained dictionaries compare to say another dictionary learning scheme? After all, this doesn’t seem to inform your actual learning scheme and is not used to evaluate your results. It would be nice to see a numerical validation/illustration of this result. Also is delta_K really that easy to compute? b) The numerical results are good—but lack error bars and comparators. I don’t understand why you considered so many comparators on synthetic data and none on more ‘realistic’ benchmark data. c) Also I don*t feel very satisfied doing image examples, it would be more interesting to work on difficult (eg medical) classification problems with large feature vectors 4. Additional Feedback a) Very well and clearly written with intuitive examples and clean math. My only suggestion is to clarify in the abstract that there are missing *features*-- when I first read *incomplete data* I think of entire data samples that are missing from the training set. That makes no sense, but it became clear when I got to section 2. b) Please use markers and dashes etc. with *every* line in your plots. It is hard (or impossible for many) to compare as is with such tiny thin lines. 5. Questions a) Could you comment on statistical significance of your results? For synthetic data it should be easy to perform the experiments on a number of mask realizations and include error bars. b) Why no comparators on benchmark datasets? c) The proof of Thm 3.2 is nice—but how reasonable is the assumption that you have two dictionaries each with the exact same RIP constant delta_K? Can that property be enforced (even approximately) during training? Or is it trivial that, given one such dictionary, there exists a second one? d) See 2.a"}
{"id": "iclr2020_687", "title": "A shallow feature extraction network with a large receptive field for stereo matching tasks | OpenReview", "abstract": "Abstract:###Stereo matching is one of the important basic tasks in the computer vision field. In recent years, stereo matching algorithms based on deep learning have achieved excellent performance and become the mainstream research direction. Existing algorithms generally use deep convolutional neural networks (DCNNs) to extract more abstract semantic information, but we believe that the detailed information of the spatial structure is more important for stereo matching tasks. Based on this point of view, this paper proposes a shallow feature extraction network with a large receptive field. The network consists of three parts: a primary feature extraction module, an atrous spatial pyramid pooling (ASPP) module and a feature fusion module. The primary feature extraction network contains only three convolution layers. This network utilizes the basic feature extraction ability of the shallow network to extract and retain the detailed information of the spatial structure. In this paper, the dilated convolution and atrous spatial pyramid pooling (ASPP) module is introduced to increase the size of receptive field. In addition, a feature fusion module is designed, which integrates the feature maps with multiscale receptive fields and mutually complements the feature information of different scales. We replaced the feature extraction part of the existing stereo matching algorithms with our shallow feature extraction network, and achieved state-of-the-art performance on the KITTI 2015 dataset. Compared with the reference network, the number of parameters is reduced by 42%, and the matching accuracy is improved by 1.9%.", "review": "Review:###This paper presents an algorithm for stereo image matching that attempts to capture improved representations of detailed spatial structure information, in particular, by increasing the size of the receptive field. The paper shows that this leads to a major reduction in the number of model parameters (42% in one case) with comparable performance on the KITTI2015 data set. I like the driving principle of the authors* approach (that stereo image matching relies more heavily on low-level features, and that higher level *semantic* features are not as critical) compelling. I would have really like to have seen the authors do some analysis of the features that they do extract, so that the reader can get a deeper insight into why their method works. The paper could be improved by providing more this kind of analysis and by adding more motivate for why low-level features are more important for stereo matching. I*m concerned that the paper only present results on one, small (200+200 images) data set. The paper would be much stronger if the authors tested on more, and varied data sets. Is it simply a network complexity issues or is there something else? The related work section appears to be just a laundry list of methods. The paper would be stronger if the authors provided more interpretation of the strengths and weaknesses of these methods, some insight into why they work, and why the proposed method is better. The authors* method claims to use a 1x1 convolution layer. Is that correct? Sounds like simple multiplication. Explain what it different. The authors* reporting of their results appears muddled. They claim the error rate was *reduced 3.4% and 1.9%* in Table 5. I could not figure out which numbers they were talking about. In most cases, the authors* method was not the best. Minor point: The authors say *conclusion* when I think they mean *oclusion*."}
{"id": "iclr2020_688", "title": "Clustered Reinforcement Learning | OpenReview", "abstract": "Abstract:###Exploration strategy design is one of the challenging problems in reinforcement learning~(RL), especially when the environment contains a large state space or sparse rewards. During exploration, the agent tries to discover novel areas or high reward~(quality) areas. In most existing methods, the novelty and quality in the neighboring area of the current state are not well utilized to guide the exploration of the agent. To tackle this problem, we propose a novel RL framework, called underline{c}lustered underline{r}einforcement underline{l}earning~(CRL), for efficient exploration in RL. CRL adopts clustering to divide the collected states into several clusters, based on which a bonus reward reflecting both novelty and quality in the neighboring area~(cluster) of the current state is given to the agent. Experiments on several continuous control tasks and several Atari-2600 games show that CRL can outperform other state-of-the-art methods to achieve the best performance in most cases.", "review": " This paper proposed a clustering based algorithm to improve the exploration performance in reinforcement learning. Similar to the count based approaches, the novelty of a new state was computed based on the statistics of the corresponding clusters. This exploration bonus was then combined with the TRPO algorithm to obtain the policy. The experimental results showed some improvement, compare with its competitors. Although the proposed method is somewhat similar to the earlier hash based approaches, I think it is still interesting by using the clustering, instead of computing the hash code with neural networks. On the other hand, the motivation and explanation of this method are not well presented. I also have some concern regarding the fairness of the comparison in experiments. The English usage could be improved as well. My detailed comments and questions are as follows. 1. The new proposal for the exploration bonus is provided in Equation (3). The denominator there is essentially the count, which is consistent with previous count based approaches (though not with the square root). For the numerator, I am a bit confused about the choice, as if *N* is small, the accumulated *R* could be small as well, which may offset the bonus based on count. I also didn*t understand the author*s claim that *...it is highly possible that all states in cluster phi (s) have zero reward*, just below Equation (3). Unless the authors provide more details, I am not convinced that the enumerator could be a good choice. 2. Given the proposed bonus, I am wondering how sensitive could it be to the choice of hyperparameters, especially w.r.t eta. The authors may need to provide more ablation studies on their effect. 3. Another concern is regarding the scalability of the proposed method. Algorithm 1 implies that k-means needs to be conducted in every iteration, which could be very slow. So how about the running time of the proposed method, when compared with baselines? 4. In the experiments, the authors claimed that the code for TRPO-Hash is provided by its authors. However, the scores for TRPO-Hash were much worse than the numbers in the TRPO-Hash paper (see their Table 1). Do you have any explanation?"}
{"id": "iclr2020_689", "title": "LabelFool: A Trick in the Label Space | OpenReview", "abstract": "Abstract:###It is widely known that well-designed perturbations can cause state-of-the-art machine learning classifiers to mis-label an image, with sufficiently small perturbations that are imperceptible to the human eyes. However, by detecting the inconsistency between the image and wrong label, the human observer would be alerted of the attack. In this paper, we aim to design attacks that not only make classifiers generate wrong labels, but also make the wrong labels imperceptible to human observers. To achieve this, we propose an algorithm called LabelFool which identifies a target label similar to the ground truth label and finds a perturbation of the image for this target label. We first find the target label for an input image by a probability model, then move the input in the feature space towards the target label. Subjective studies on ImageNet show that in the label space, our attack is much less recognizable by human observers, while objective experimental results on ImageNet show that we maintain similar performance in the image space as well as attack rates to state-of-the-art attack algorithms.", "review": " This paper proposes a method for constructing adversarial attacks that are less detectable by humans, by changing the target class to be a class similar to the original class of the image. The resulting attack methodology is then studied in terms of its imperceptibility in label space, and shown to be less perceptible in label space to human observers, while not coming at a cost in image space. The paper presents compelling evaluation of the method and does seem to succeed in proving that their proposed attack satisfies the stated goal. However, it appears as though this goal is somewhat counter to the main point of adversarial examples---indeed, if the label is reasonable to a human, then what makes the adversarial example adversarial? The main threat in adversarial examples research seems to be that it is possible to induce predictions that are arbitrarily different from humans* on natural-looking in puts. Thus, changing the label to something that a human actually agrees with would actually reduce the impact of the adversarial attack. In order to improve the paper, I would suggest applying the same (or similar) methodologies to other areas of ML security where imperceptibility in label space is commonly desired---for example, in data poisoning attacks or backdoor attacks. In general, such attacks are much more likely to be *inspected* by humans, and so imperceptibility in both label and image space is very desirable. However, I suspect that this would require significant effort and changes to the paper, and so for now I recommend rejection."}
{"id": "iclr2020_690", "title": "Switched linear projections and inactive state sensitivity for deep neural network interpretability | OpenReview", "abstract": "Abstract:###We introduce switched linear projections for expressing the activity of a neuron in a ReLU-based deep neural network in terms of a single linear projection in the input space. The method works by isolating the active subnetwork, a series of linear transformations, that completely determine the entire computation of the deep network for a given input instance. We also propose that for interpretability it is more instructive and meaningful to focus on the patterns that deactive the neurons in the network, which are ignored by the exisiting methods that implicitly track only the active aspect of the network*s computation. We introduce a novel interpretability method for the inactive state sensitivity (Insens). Comparison against existing methods shows that Insens is more robust (in the presence of noise), more complete (in terms of patterns that affect the computation) and a very effective interpretability method for deep neural networks", "review": "Review:###This manuscript introduces a novel method to explain activities of ReLU-based deep networks by constructing a linear subnetwork which only contains neurons activated by the input. The status of each neuron can be obtained given any input sample. Moreover, the author applies the notion of “neuron’s center”, which is a neutral data point that is similar to actual input x, but with differences in particular objects to cause f(x) be positive. The activity of each neuron can be decomposed into the attribution of each input pixel, and this decomposition can also be used to measure the contribution of each pixel to the network stability. Overall, the proposed methodology is intuitive and distinctive to the state-of-the-art interpretability methods. However, the application constraint on the ReLU-based deep neural network prevents this method from being a model agnostic approach: the problem formulation would be much different if other non-linear activation functions are used. Although the experiment part visualizes the superiority of switched linear projections over other prevalent approaches, the evaluations contain mostly subjective assessment and the arguments are monotonous. I would suggest adding more experiments with quantitative analysis, or mathematically demonstrate why the proposed method is better than, say purely gradient-based method, in the linear case. In addition, additional experiments on a broader set of input data (e.g., tabular, text) could avoid the evaluations look cherry-pick. Minor issues: 1. In figure 2, I think it would be better to write down explicitly the connections between v, hat{b} and hat{w} for each neuron given any input. Just seeing v and hat{b} on top of each subfigure is a bit confusing. 2. I spent a long time to understand the *neuron’s center* concept, it might be better to add some background or mathematical formulation. 3. In figure 4, when the digits get misclassified, the Insens explanation should highlight the patterns of wrongly predicted digits, but the patterns of neurons* inactive state sensitivity still look like the correct digits. 4. It would also be interesting to show how the Insens explanation would change when the input is under various kinds of adversarial attacks rather than adding simple Gaussian noise."}
{"id": "iclr2020_691", "title": "Data-dependent Gaussian Prior Objective for Language Generation | OpenReview", "abstract": "Abstract:###For typical sequence prediction problems like language generation, maximum likelihood estimation (MLE) has been commonly adopted as it encourages the predicted sequence most consistent with the ground-truth sequence to have the highest probability of occurring. However, MLE focuses on a once-for-all matching between the predicted sequence and gold-standard consequently, treating all incorrect predictions as being equally incorrect. We call such a drawback {it negative diversity ignorance} in this paper. Treating all incorrect predictions as equal unfairly downplays the nuance of these sequences* detailed token-wise structure. To counteract this, we augment the MLE loss by introducing an extra KL divergence term which is derived from comparing a data-dependent Gaussian prior and the detailed training prediction. The proposed data-dependent Gaussian prior objective (D2GPo) is defined over a prior topological order of tokens, poles apart from the data-independent Gaussian prior (L2 regularization) commonly adopted for smoothing the training of MLE. Experimental results show that the proposed method can effectively make use of more detailed prior in the data and significantly improve the performance of typical language generation tasks, including supervised and unsupervised machine translation, text summarization, storytelling, and image caption.", "review": " This paper introduces the use of data-dependent Gaussian prior, to overcome negative diversity ignorance problem that includes the exposure bias problem for sequence generation models. In addition to the usual MLE (teacher forcing) criteria, the authors add the KL divergence between the prediction and the Gaussian PDF on the word embedding space. Experimental results show that the proposed method consistently improves the performance of the state-of-the-art methods for neural machine translation, text summarization, storytelling, and image captioning. I lean to accept this paper. The proposed method is well motivated and shown to be effective in several tasks for language generation. I have some major comments about the evaluation function . The authors propose to define it as a Gaussian distribution. - While this choice seems to be reasonable, I would like to know how its standard deviation can be defined. If it is a hyperparameter, the sensitivity of different deviations for the performance should be experimentally reported. A small valued deviation would make the KL divergence close to zero, while a large one makes its convergence slow. - Another way to remedy the problem of KL divergence above is applying Wasserstein distance instead of KL divergence. I would like to know if the authors have investigated the use. Minor comments: - White space should be inserted between *sequence-to-sequence* and *(seq2seq)* on the third page. - If the authors define a sequence using a bold and italic font as , each token can be represented using an italic font to distinguish each token and the entire sequence: . Otherwise, the sequence can be defined as if the authors like to represent each token as a vector. - There is a typo on the fifth page. The word *dada-independent* should be *data-independent.*"}
{"id": "iclr2020_692", "title": "Multi-hop Question Answering via Reasoning Chains | OpenReview", "abstract": "Abstract:###Multi-hop question answering requires models to gather information from different parts of a text to answer a question. Most current approaches learn to address this task in an end-to-end way with neural networks, without maintaining an explicit representation of the reasoning process. We propose a method to extract a discrete reasoning chain over the text, which consists of a series of sentences leading to the answer. We then feed the extracted chains to a BERT-based QA model to do final answer prediction. Critically, we do not rely on gold annotated chains or ``supporting facts:** at training time, we derive pseudogold reasoning chains using heuristics based on named entity recognition and coreference resolution. Nor do we rely on these annotations at test time, as our model learns to extract chains from raw text alone. We test our approach on two recently proposed large multi-hop question answering datasets: WikiHop and HotpotQA, and achieve state-of-art performance on WikiHop and strong performance on HotpotQA. Our analysis shows the properties of chains that are crucial for high performance: in particular, modeling extraction sequentially is important, as is dealing with each candidate sentence in a context-aware way. Furthermore, human evaluation shows that our extracted chains allow humans to give answers with high confidence, indicating that these are a strong intermediate abstraction for this task.", "review": " The paper is tacking the problem of multi-hop machine reading. The method is evaluated on hotpotqa and wikihop datasets. The model consists of two steps: First, the model learns to retrieve supporting facts. To do so, the authors propose a heuristic method to retrieval *reasoning graph* based on entities present in the question and the text. In the method, The nodes are basically the sentences, the edges are the adjacency of sentences and the presence of shared entities. Each reasoning path starts from the question and ends at the reach of the answer. Finally, 2 strategies are used to select the path *shortest* and *ROUGE*. From this dataset, an RNN based extractor is trained. A first question is why not using the supporting facts training signal which is present in both datasets. It would have been useful to compare this heuristic extraction method for the training dataset extraction to the sole use of the supporting facts as this first step of the method is considered as the originality. Then, based on the supporting fact extraction, an SoA BertQA model is used to perform the extraction. As a second step of the method, the method seems to produce a quite marginal improvement compared to the current state of the art."}
{"id": "iclr2020_693", "title": "MIST: Multiple Instance Spatial Transformer Networks | OpenReview", "abstract": "Abstract:###We propose a deep network that can be trained to tackle image reconstruction and classification problems that involve detection of multiple object instances, without any supervision regarding their whereabouts. The network learns to extract the most significant top-K patches, and feeds these patches to a task-specific network -- e.g., auto-encoder or classifier -- to solve a domain specific problem. The challenge in training such a network is the non-differentiable top-K selection process. To address this issue, we lift the training optimization problem by treating the result of top-K selection as a slack variable, resulting in a simple, yet effective, multi-stage training. Our method is able to learn to detect recurrent structures in the training dataset by learning to reconstruct images. It can also learn to localize structures when only knowledge on the occurrence of the object is provided, and in doing so it outperforms the state-of-the-art.", "review": "Review:###The article introduces the novel MIST architecture which tries to solve the problem of multiple-instance classification and image generation from multiple objects. It employs two submodels, where the first generates a heatmap of intereting region and the second model is a task-specific model that works on image-patches, for example a classifier or an autoencoder. Both models are connected by a patch-extraction routine. The main contribution of this paper is to provide a way to propagate errors through this non-differentiable patch-extraction scheme. This is done by introducing slack-variables. ------------------ The paper is overall relatively easy to follow and the results are very good. However, it suffers from the fact that it does not differentiate between model-architecture and the overall approach. While the main contribution is described in Section 4, the paper spends a lot of space beforehand to introduce the task-dependent models as well as the heatmap architecture - things that i can imagine will vary a lot in different applications. The real important part is how to train the model and this is unfortunately only half described. A good deal of abstraction from the network architecture would have made the paper a lot better. Further, I think that the loss-function for the classification task does not work in the general case. On my first read-through, i completely misunderstood Section 4. Here is an unsorted list of issues i had with this: - since E_K is not truly invertible, writing the approximate inverse as E_K^{-1} is misleading. - It might help to stress that you treat {x_k} as continuous and the sampling as differentiable. - In (7) it would be better to explicitly write E_K^{-1}(x_k) instead of introducing _x0008_ar{h}. The line below is not clear. - It is also misleading, because the choice of _x0008_ar{h}=E_K^{-1}(x_k) is not the minimizer of (5) given that all other variables are fixed. You can see this by observing that assuming that when {x_k}=E_K(H(I)) holds, we can choose all other pixels to be exactly the value returned by H(I). - I am not sure where the alternating part comes from because this usually involves taking your solution from (7) and feeding it into (6). - I am pretty sure that in (6)+(7), as well as lines 3+6 of the algorithm, you actually don*t want to optimize for tau or eta from scratch but only perform a single SGD step. I think that is what you are doing, but right now it is written as *find a complete new model for each batch*. - Since this is performed batch-wise: is x_k a variable kept between iterations or do you use H(I) for an initial estimate of x_k for the batch? Regarding the classification objective: -since (3) uses the MSE of the mean class-label and the mean-prediction, a dataset where all objects always appear with the exact same amount will not work since than for each image the mean label is identical. - Therefore, the MNIST-easy dataset should be unsolvable for the proposed architecture since every digit occurs exactly once."}
{"id": "iclr2020_694", "title": "Differentiable Bayesian Neural Network Inference for Data Streams | OpenReview", "abstract": "Abstract:###While deep neural networks (NNs) do not provide the confidence of its prediction, Bayesian neural network (BNN) can estimate the uncertainty of the prediction. However, BNNs have not been widely used in practice due to the computational cost of predictive inference. This prohibitive computational cost is a hindrance especially when processing stream data with low-latency. To address this problem, we propose a novel model which approximate BNNs for data streams. Instead of generating separate prediction for each data sample independently, this model estimates the increments of prediction for a new data sample from the previous predictions. The computational cost of this model is almost the same as that of non-Bayesian deep NNs. Experiments including semantic segmentation on real-world data show that this model performs significantly faster than BNNs, estimating uncertainty comparable to the results of BNNs.", "review": "Review:###Summary of the Paper: This paper describes a method for training Bayesian neural networks in the context of stream data. The method proposed is based on using a quantization approach with some techniques to estimate the change in probability distributions. The proposed approach is compared on some tasks, including real and synthetic datasets. Detailed comments: The paper needs to improve the writing. For example, the sentence *it has been unable to estimate the uncertainty of the predictions until recently* sounds awkward. The authors have to better explain the features and challenges of stream data. The paper is unclear. There are several steps of the proposed method that are not well described. How is the posterior distribution of the weights computed? The notation x_0 and x_1 for the test point and the NN weights is confusing. Figure 2 is not clear. The description of the baselines the authors compare with is not clear. What do you mean by degenerated in section 5.1.? The paper is missing a related work section describing state of the art methods to address stream data. It seems the real experiments of section 5.1. only consider one baseline MU. I believe this is insufficient. In table 2 the benefits of the proposed approach are not very significant. The experiments are missing error bars. It is not possible to extract conclusions of significance without them. My overall impression is that the paper needs to better explain the approach followed and improve the notation to facilitate the reading. I believe that this paper needs for work and is not yet suitable for acceptance."}
{"id": "iclr2020_695", "title": "Collaborative Filtering With A Synthetic Feedback Loop | OpenReview", "abstract": "Abstract:###We propose a novel learning framework for recommendation systems, assisting collaborative filtering with a synthetic feedback loop. The proposed framework consists of a ``recommender** and a ``virtual user.** The recommender is formulizd as a collaborative-filtering method, recommending items according to observed user behavior. The virtual user estimates rewards from the recommended items and generates the influence of the rewards on observed user behavior. The recommender connected with the virtual user constructs a closed loop, that recommends users with items and imitates the unobserved feedback of the users to the recommended items. The synthetic feedback is used to augment observed user behavior and improve recommendation results. Such a model can be interpreted as the inverse reinforcement learning, which can be learned effectively via rollout (simulation). Experimental results show that the proposed framework is able to boost the performance of existing collaborative filtering methods on multiple datasets.", "review": "Review:###The paper proposes to learn a *virtual user* while learning a *recommender* model, to improve the performance of the recommender system. The *virtual user* is used to produce the *reward* signal for training the recommendation system (which is trained using RL). Jointly learning the recommender and the *virtual user* provides a synthetic feedback loop that helps to improve the performance of the recommendation system. The paper formulates the training of the *virtual user* as an inverse RL problem and uses adversarial regularizer. The paper proposes an interesting idea but more experiments (and explanation) is needed to bring out the usefulness of the work. In general, the writing needs to be polished as well. =============== Following are the items that need to be improved: ## Significance of the results * The results in Tables 3 and 4 provide only marginal improvements over the baseline. These improvements do not appear to be statistically significant. It would help if the authors comment on why these results appear significant and also provide variance values/curves for the results. ## Role of the feedback general F * Is there any separate loss for training F or is it always trained along with pi? * Is the feedback capturing some sort of *memory* or *past preferences/behaviors* of the user? If yes, wouldn*t using a recurrent recommender also capture these effects without needing the feedback model F? Note that I am not criticizing the choice of F. I am trying to understand the role played by F (in addition to the recommender pi). * If there is no separate loss for F, I wonder how would the performance change if the F network was to be removed and the reward value was to be fed into a recurrent recommended. (The paper seems to have considered a special case where a non-recurrent recommender is used with the reward value). The reason I am stressing on this is that one of the key distinctions of the authors* work is the use of feedback generator and it would be useful to quantify the benefits on this modification. ## Others * How is the static representation, x, computed? From eq 16 (appendix), it appears that x is a binary vector that captures what items have been purchased/reviewed. Is that correct? If yes, wouldn*t x have an enormous dimensionality? * The recommendation system is operating in a sequential decision-making setup. The formulation of R and F do no consider any sequential information. For example, let us say that the recommender recommends the items a1, a2 and a3 in 3 timesteps. The reward at time 3 is a function of x and a3 and the information of a1 and a2 is not used. * The loss function has many components and I want to make sure I understand what gradients flow where. So please correct me if I missed something; * supervised learning loss (from the real data) trains pi and F (equation 6). * pi and F collaborate to get a high reward from R (since we do not have the ground truth corresponding to R). No gradient flows to R. * Adversarial game between pi and R - which is used to update R. * I understand that the loss is defined according to the output of the last step but do the gradients flow through all the intermediate steps? * The training/inference procedure seems to be doing something strange. Let us assume that T = 5. So the 5 items are recommended to the *virtual user* and only the 5th item is recommended to the real user. Now the recommendation of the 5th item depends on the first 4 items that the real user has not seen. * In equation 15, what does the subscript F stand for? * What algorithm is used to train the policy pi? ================= The following are the items that should be corrected in the future iterations of the paper. Please note that these things did not impact the score. * It seems that the irrelevance of an item (for a user) is treated the same way as missing information about the relevance of the item. Could the authors discuss this more in the paper? * Is there any reason why this approach is only used with CF methods? * Some writing choices seem to make the problem statement more complex than it is. For example, the authors discuss how their work is different than traditional RL instead of simply stating that their work is set up as a POMDP. * Articles are missing (or mixed) up at many places."}
{"id": "iclr2020_696", "title": "A Generalized Training Approach for Multiagent Learning | OpenReview", "abstract": "Abstract:###This paper investigates a population-based training regime based on game-theoretic principles called Policy-Spaced Response Oracles (PSRO). PSRO is general in the sense that it (1) encompasses well-known algorithms such as fictitious play and double oracle as special cases, and (2) in principle applies to general-sum, many-player games. Despite this, prior studies of PSRO have been focused on two-player zero-sum games, a regime wherein Nash equilibria are tractably computable. In moving from two-player zero-sum games to more general settings, computation of Nash equilibria quickly becomes infeasible. Here, we extend the theoretical underpinnings of PSRO by considering an alternative solution concept, ?-Rank, which is unique (thus faces no equilibrium selection issues, unlike Nash) and tractable to compute in general-sum, many-player settings. We establish convergence guarantees in several games classes, and identify links between Nash equilibria and ?-Rank. We demonstrate the competitive performance of ?-Rank-based PSRO against an exact Nash solver-based PSRO in 2-player Kuhn and Leduc Poker. We then go beyond the reach of prior PSRO applications by considering 3- to 5-player poker games, yielding instances where ?-Rank achieves faster convergence than approximate Nash solvers, thus establishing it as a favorable general games solver. We also carry out an initial empirical validation in MuJoCo soccer, illustrating the feasibility of the proposed approach in another complex domain.", "review": "Review:###This paper extends the original PSRO paper to use an -Rank based metasolver instead of the projected replicator dynamics and Nash equilibria based metasolvers in the original. To this end, the paper modifies the original idea of Best-Response (BR) oracle since it can ignore some strategies in -Rank defining SSCC to introduce the idea of _preference-based_ Best-Response (PBR) oracle. The need for a different oracle is well justified especially with the visualization in the Appendix. The main contributions that the paper seems to be going for is a theoretical analysis of -Rank based PSRO compared to standard PSRO. From the PBR*s description (especially in Sec 4.3) it seems the paper is intereseted in expanding the population with novel agents rather than finding the *best* single agent which is not well defined for complex games with intransitivities. Nevertheless, it seems that BR is mostly compatible with PBR for symmetric zero-sum two-player games. The paper performs empirical experiments on different versions of poker. First set of experiments compare BR and PBR with -Rank based metasolver on random games and finds that PBR does better than BR at population expansion as defined. The second set of experiments compare the metasolvers. -Rank performs similarly to Nash where applicable. Moreover it*s faster than Uniform (fictitious self-play) on Kuhn. Then the paper tacks on the MuJoCo soccer experiment as a teaser for ICLR crowd. Overall the paper is quite interesting from the perspective of multiagent learning and I would lean towards accepting. However the paper needs to clarify a lot of details to have any chance of being reproducible. ** Clarifications needed: - Tractability of PBR-Score and PCS-Score It*s unclear how tractable these are. Moreover these were only reported for random games. What did these scores look like for the Poker games? Could you clarify how exactly these were computed? - It*s somewhat unclear what the lack of convergence without novelty-bound oracle implies. Does this have to do with intransitivities in the game? - Dependence of ? The original -Rank paper said a lot about the importance of choosing the right value for . How were these chosen? Do you do the sweep after every iteration of PSRO? - Oracle in experiments? The paper fails to mention the details about the Oracles being used in the experiments. They weren*t RL oracles but more details would be useful. - BR not compatible with PBR, albeit not the other way around, meaning one of the solutions you get from PBR might be BR, but can we say which one? - For MuJoCo soccer was it true PSRO or cognitive hierarchy. In general, the original PSRO paper was partly talking about the scalable approach via DCH. This paper doesn*t mention that at all. So were the MuJoCo experiments with plain PSRO? What was the exact protocol there? From the appendix it*s unclear how the team-vs-team meta game works with individual RL agents. Moreover how are the meta-game evaluation matrices computed in general? How many samples were needed for the Poker games and MuJoCo soccer? - The counterexamples in Appendix B3 are quite interesting. Do you have any hypotheses about the disjoint support from games* correlated equilibria?"}
{"id": "iclr2020_697", "title": "Set Functions for Time Series | OpenReview", "abstract": "Abstract:###Despite the eminent successes of deep neural networks, many architectures are often hard to transfer to irregularly-sampled and asynchronous time series that occur in many real-world datasets, such as healthcare applications. This paper proposes a novel framework for classifying irregularly sampled time series with unaligned measurements, focusing on high scalability and data efficiency. Our method SeFT (Set Functions for Time Series) is based on recent advances in differentiable set function learning, extremely parallelizable, and scales well to very large datasets and online monitoring scenarios. We extensively compare our method to competitors on multiple healthcare time series datasets and show that it performs competitively whilst significantly reducing runtime.", "review": "Review:###This paper considers the problem of supervised classification of time-series data that are irregularly sampled and asynchronous, with a special focus on the healthcare applications in the experiments. Inspired by the recent progress on differentiable set function learning, the paper proposes an approach called Set Functions for Time Series (SEFT), which views the time series as sets, and use a parametrized sum-decomposing function f as the model for representing the probabilities of different classes, with the sets as the inputs. The problem then reduces to learning the finite dimensional parametrization of the function f under a given loss, which is a differentiable optimization problem that can be learned via standard optimization methods. Together with a positional embedding of the timestamps and an attention-based aggregation, the paper reports improved performance of the proposed approach on a few healthcare time series with asynchronous and irregularly sampled data. In particular, the runtime is largely shortened, while the final accuracy remains competitive to other methods compared in the paper. The idea of SEFT is novel and the results are also showing its promise. In addition, the interpretability shown in section 4.3 is also attractive. However, there are several issues that limit the contribution and maturity of this paper. Firstly, the paper proposes to model time series as a set. But this loses the information of the order of the time series, which can be extremely important in those datasets with long history dependence. In such cases, I*m not convinced that the set modeling would work. The authors should double check the characteristics of the datasets that are used, and see if they lack long history dependence properties in intuition. If so, this should be mentioned clearly. The authors should also make a more fair comparison with other approaches (like those based on RNN) on datasets with strong history dependence, e.g., Memetracker datasets of web postings and limit-order books datasets. Otherwise, it would be not clear whether this set modeling is generally applicable for general time series data. Secondly, the authors missed a large amount of related literature for approaching asynchronous and irregularly sampled time series, namely (marked) point-process based approaches. See papers like [1, 2, 3], to name just a few. The authors should at least include some of the recent approaches in this direction for comparison before claiming the superiority of SEFT. Thirdly, there are a few parts that are not very clear. 1) The discussion about complexity (order m and mlog m) at the bottom of page 1 is weird -- what does this complexity refer to? Does it include the learning of the unknown parameters in the models (like training of the neural networks in this paper)? 2) The loss function in formula (5) is not specified later in the paper (at least hard to find). 3) The Table 1 should be explained in much more details. In particular, why don*t we include SEFT-ATTN for H-MNIST? The comment after * is also not clear to me -- is it relevant to why SEFT-ATTN is not included? And what are MICRO/MACRO/WEIGHTED AUC? And why are we using different sets of performance criteria for the first two and last two datasets? Finally, some minor comments: 1) On page 2, *the following methods* should be *the above methods*; 2) on page 3, the meaning of *channels* should be specified clearer; 3) on page 4, in formulae (3) and (4), should there be pi or 2pi in the formula? [1] Mei, Hongyuan, and Jason M. Eisner. *The neural hawkes process: A neurally self-modulating multivariate point process.* Advances in Neural Information Processing Systems. 2017. [2] Xiao, Shuai, et al. *Joint modeling of event sequence and time series with attentional twin recurrent neural networks.* arXiv preprint arXiv:1703.08524 (2017). [3] Yang, Yingxiang, et al. *Online learning for multivariate Hawkes processes.* Advances in Neural Information Processing Systems. 2017. ############## post rebuttal ############### After reading the authors* rebuttal, I decide to improve the rating to 5 (reflected as 6 due to the ICLR rating system limitation this year)."}
{"id": "iclr2020_698", "title": "Faster Neural Network Training with Data Echoing | OpenReview", "abstract": "Abstract:###In the twilight of Moore*s law, GPUs and other specialized hardware accelerators have dramatically sped up neural network training. However, earlier stages of the training pipeline, such as disk I/O and data preprocessing, do not run on accelerators. As accelerators continue to improve, these earlier stages will increasingly become the bottleneck. In this paper, we introduce “data echoing,” which reduces the total computation used by earlier pipeline stages and speeds up training whenever computation upstream from accelerators dominates the training time. Data echoing reuses (or “echoes”) intermediate outputs from earlier pipeline stages in order to reclaim idle capacity. We investigate the behavior of different data echoing algorithms on various workloads, for various amounts of echoing, and for various batch sizes. We find that in all settings, at least one data echoing algorithm can match the baseline*s predictive performance using less upstream computation. We measured a factor of 3.25 decrease in wall-clock time for ResNet-50 on ImageNet when reading training data over a network.", "review": "Review:###This paper discusses the use of data echoing (re-passing data fetched from drive or cloud) to maximize GPU usage and reduce reliance on data transportation time. The schemes basically are: reusing data at the example level, after data augmentation, or after batching. The experiments measure how much fresh data is needed in order to reach the same level of validation accuracy, with significant speedup when echoing is used. I thought the paper is very nicely motivated, although this is out of my area so I cannot comment on how thoroughly the problem of data fetching is investigated in other works. The evaluations are also nice, and appropriately uses a large model (Resnet 50) and dataset (Imagenet). The simplicity of the method is a plus, but I question a fundamental part, especially if batch echoing is used--isn*t this just the same as running SGD twice, and therefore halving the stepsize and doubling the number of steps? From the optimization viewpoint, it seems that if less data was used and a good validation error level is reached, then how do we not know that less data wouldn*t work well in the first place? I understand that all step sizes and decay rates were chosen independently per experiment; can those numbers be shared in a way to see if this is happening or not? Figure 8 also suggests that though it may take a long time for the baseline to reach the same level as that with batch echoing, everyone reaches a pretty low error rate at about the same point, and the difference may be in the *slow converging* phase of the optimization; thus measuring how long it takes to reach a specific low error rate may be an exaggerated measure. Basically, what I am saying is that the idea is nice, but the results look a bit magical. I*m happy to increase my score if the authors can upload more intermediary results, like plots of form figure 8, decay rates and schedules, batch sizes, exact repetition schedules, etc. minor: page 3 end of paragraph 2: *repeated data would NOT be more valuable than fresh data?* After rebuttal: The authors have addressed my concerns and I more-or-less believe the results. I see why the contribution can be viewed as minor, but it is well-motivated and looks like a nice set of experiments. I encourage the authors to make their code available so that it can be easily incorporated in applications."}
{"id": "iclr2020_699", "title": "A Copula approach for hyperparameter transfer learning | OpenReview", "abstract": "Abstract:###Bayesian optimization (BO) is a popular methodology to tune the hyperparameters of expensive black-box functions. Despite its success, standard BO focuses on a single task at a time and is not designed to leverage information from related functions, such as tuning performance metrics of the same algorithm across multiple datasets. In this work, we introduce a novel approach to achieve transfer learning across different datasets as well as different metrics. The main idea is to regress the mapping from hyperparameter to metric quantiles with a semi-parametric Gaussian Copula distribution, which provides robustness against different scales or outliers that can occur in different tasks. We introduce two methods to leverage this estimation: a Thompson sampling strategy as well as a Gaussian Copula process using such quantile estimate as a prior. We show that these strategies can combine the estimation of multiple metrics such as runtime and accuracy, steering the optimization toward cheaper hyperparameters for the same level of accuracy. Experiments on an extensive set of hyperparameter tuning tasks demonstrate significant improvements over state-of-the-art methods.", "review": " The authors propose a new way of normalizing the labels of the meta-data. They propose a Thompson sampling strategy as a new hyperparameter optimization warmstarting strategy and an optimization method that leverages transfer learning. The transfer learning baselines are surprisingly weak and show no improvement over the random search baseline for two of your tasks. You should consider stronger baselines. One interesting work is *Collaborative hyperparameter tuning* by Bardenet et al. which overcomes the problem of different scales by considering the problem as a ranking problem. You discussed further works in your related work. Another interesting work based on GPs is *Scalable Gaussian Process based Transfer Surrogates for Hyperparameter Optimization*. The warm-start GP currently seems to be your strongest baseline. However, I have doubts that it is implemented correctly. You say that you estimate the most similar dataset and then evaluate its best 100 hyperparameter configuration. First of all, I don*t understand why you decided to choose 100 (DeepAR only has 220 configurations in total!). Second, this is not how this method works. Instead, you estimate the k most similar tasks and evaluate its best hyperparameter configuration. In your case k is upper bounded by 10 (number of tasks). This is notably smaller than 100 and gives more time to the Bayesian optimization which will likely improve your results. You observe worse results on XGBoost and good ones on FCNet. You refer to Table 6 and connect it to the RMSE. This might be true but I have a much simpler explanation: the search space of FCNet is orders of magnitudes larger and it contains many configurations that lead to very high MSE and only few with low. Therefore, a random search will on average provide poor results where a warmstarted search will obtain decent results for the first iterations. XGBoost is less sensitive to hyperparameters such that the overall variance in the losses is smaller. Maybe you can provide some insights into the complexity of the optimization tasks (plot the distributions )and add it to the appendix? Table 6 contains more datasets than Table 2. Why did you drop some tasks? The aggregated results in Table 2 are nice but actually we are interested in the outcome after the search after a given budget. Can you add such a table? I have few suggestions to improve the readability of the paper: That there is a choice of copula estimators is mentioned at the very end of the paper. Can you add it to the section where you describe them first? In section 5.1 you already argue by referring to Table 6. However, Table 6 is explained first in section 5.2 which makes it hard to follow your argumentation. You use the term *metric* to refer to objectives. This is confusing, you might consider changing this. You propose to use scalarization to address the multi-objective optimization problem. Why would an average of both objectives be the optimal solution? Does the unit you use to measure the time matter? What if you use machine learning algorithms that scale super-linear in the number of data points? How is this novel and why can*t your baselines employ the same idea? A discussion of related work on autoML for multiple objectives is missing. The second paragraph of section 5 is confusing. You cite some work, discuss it and then conclude that your setup is entirely different. Would any information be lost if you say that you precomputed the values?"}
{"id": "iclr2020_700", "title": "Learnable Group Transform For Time-Series | OpenReview", "abstract": "Abstract:###We propose to undertake the problem of representation learning for time-series by considering a Group Transform approach. This framework allows us to, first, generalize classical time-frequency transformations such as the Wavelet Transform, and second, to enable the learnability of the representation. While the creation of the Wavelet Transform filter-bank relies on the sampling of the affine group in order to transform the mother filter, our approach allows for non-linear transformations of the mother filter by introducing the group of strictly increasing and continuous functions. The transformations induced by such a group enable us to span a larger class of signal representations. The sampling of this group can be optimized with respect to a specific loss and function and thus cast into a Deep Learning architecture. The experiments on diverse time-series datasets demonstrate the expressivity of this framework which competes with state-of-the-art performances.", "review": " A typical Wavelet Transform is built through the dilation and/or rotation of a mother wavelet, which can been viewed as a group action on a mother wavelet. This work proposes to extend this construction beyond the Euclidean group, and to supervisedly learn operators that will be applied on a mother wavelet. Competitive numerical performances are obtained. Overall, I think that re-thinking the way a Wavelet Transform is designed, is an interesting direction of research, but I think some of the theoretical tools developed in this paper are not dedicated to achieve this purpose. In particular, the group/representation properties seem to not be used, and the authors could simply consider a specific subset of invertible mapping on which would be applied on the mother wavelet and lead to a Wavelet Transform. In other words, the overall formulation could be simplified. Pros: - In general, the numerical experiments are at the level of the state of the art. - Parametrizing a subset of the group of increasing function and its application to signal processing tools is novel, to my knowledge. Cons: - Some very relevant elements in the literature review are missing. Learning or using an underlying group of symmetry that will be combined with a deep neural network is not novel, cf: https://arxiv.org/abs/1601.04920 ; in particular for reducing the number of parameters, filters or samples: https://arxiv.org/abs/1809.06367 ; http://proceedings.mlr.press/v48/cohenc16.pdf ; https://arxiv.org/abs/1809.10200 ; https://arxiv.org/abs/1605.06644 - I think the authors should discuss at least one or two of those papers, if not all. - The performance on the bird detection task is good but the improvement compared to other work is not clear, given that some supervision in the first layer is incorporated. - Subsections 2.2 and 2.3 are difficult to parse because the authors introduce a lot of equations or notion that are not useful to understand their algorithm/method. The equation (6) seems wrong to me (one should consider t->s_i(-t) and not t->s_i(t) and b seems missing in the second line). - Figure 2 is difficult to read because of the illustrative graphics. Maybe a block schema would be easier to parse. - It seems to me that no-where the group properties are used, such as the stability to composition. In this paper, the authors simply try to parametrize a diffeomorphism to dilate the mother wavelet. From my understanding of 3.3, the subset of function used to approximate do not form a subgroup as well, contrary to the Euclidean case, where for instance discrete rotations in the case of images are a finite group. - In subsection 4.1(Table 1), a comparison with a wavelet transform followed by a linear operator is compared with the proposed method. I find this result surprising :?LGT/nLGT/cLGT/cnLGT and the WT are some linear methods whereas the STFT is non linear. As the WT should be unitary, if the linear classifier method is reasonably trained, then both methods should lead to the same result, except if the data are poorly conditioned. In which case, this experiment would not be meaningful. I think the authors should comment more this result because it is surprising. - I slightly disagree with the sentence *in the case of WT, the precision in frequency degrades as the frequency increases*. Actually, the heisenberg principle is optimally optimized by wavelets, meaning that the area of the frequency/spatial support on a spectrogram is constant. On the contrary, the STFT has a lack of localisation (and thus the *precision* is not constant along frequencies). Maybe this could be rephrased slightly. - Given the filter learned in Figure 5, one can wonder if a foveal approach (i.e., Foveal Wavelets) could perform similarly? It would be interesting to display the littlewood-paley plot(i.e., the sum of the modulus of the filters in the Fourier domain) of this representation to understand the nature of this operator in the Fourier domain. - I think group actions could be considered instead of representations: it would be simpler to understand for a potential reader. Typos: - abstract *in order to transform the mother ..* > *in order to transform a mother..* - page 7 *this variation is as not captured as well* > *this variation is not captured as well*."}
{"id": "iclr2020_701", "title": "Learning Robust Representations via Multi-View Information Bottleneck | OpenReview", "abstract": "Abstract:###The information bottleneck method provides an information-theoretic view of representation learning. The original formulation, however, can only be applied in the supervised setting where task-specific labels are available at learning time. We extend this method to the unsupervised setting, by taking advantage of multi-view data, which provides two views of the same underlying entity. A theoretical analysis leads to the definition of a new multi-view model which produces state-of-the-art results on two standard multi-view datasets, Sketchy and MIR-Flickr. We also extend our theory to the single-view setting by taking advantage of standard data augmentation techniques, empirically showing better generalization capabilities when compared to traditional unsupervised approaches.", "review": "Review:###This is a good multiview representation learning paper with new insights. The authors propose to learn variables z_1 and z_2, which are consistent, contain view-invariant information but discard as much view-specific information as possible. The paper relies on mutual information estimation and is reconstruction-free. It is mentioned in some previous works (e.g. Aaron van den Oord et al. 2018), that reconstruction loss can introduce bias that has a negative effect on the learned representation. Comparing to existing multiview representation learning approaches that try to maximize the mutual information between learned representation and the view(s), this paper clearly defines superfluous information that we should try to throw away and figure out how to obtain sufficiency learned representation for output. The authors also draw clear connections between a few existing (multiview) representation learning methods to their proposed approaches. The experimental results on the right side of Figure 3, deliver a very interesting conclusion. In low-resource case, robust feature (obtained by using the larger beta, discarding more superfluous information) is crucial for achieving good performance. While when the amount of labeled data samples is enough, vice-versa. Here are my major concerns: 1. In the paper, the authors said the original formulation of IB is only applicable to supervised learning. That is true, but the variational information bottleneck paper [Alexander A. Alem et al. 2017] already showed the connection of unsupervised VIB to VAE in the appendix. 2. I would not consider the data augmentation used to extend single-view data to “pseudo-multiview” as a contribution. This has been done before (e.g. in the multiview MNIST experiment part of the paper *On Deep Multi-View Representation Learning*). 3. Which MV-InfoMax do you really compare to? You listed a few of them: (Ji et al., 2019; Henaff et al., ´ 2019; Tian et al., 2019; Bachman et al., 2019) in the related work section. 4. I think the authors should also make a more careful claim on their results in MIR-Flickr. I’d rather not saying MIB generally outperforms MV-InfoMax on MIR-Flickr, as MIB does not (clearly) outperform MV-InfoMax when enough labeled data is available for training downstream recognizers. But MIB does clearly outperform MV-InfoMax when scaling down the percentage of labeled samples used. 5. Regarding baselines/experiments a. In Figure 4, it seems that VAE (with beta=4) outperforms MV-InfoMax. Why the ``*pseudo-second view* does not help Mv-Infomax in this scenario? Why VAE is clearly better than Infomax? b. In Figure 3, you might also tune beta for VCCA and its variants, like what you did for VAE/VIB in a single view. 6. Do you think your approach can be extended to more than two views easily? For me, it seems the extension is not trivial, as it requires o(n^2) terms in your loss for n views. But this is minor."}
{"id": "iclr2020_702", "title": "On the Reflection of Sensitivity in the Generalization Error | OpenReview", "abstract": "Abstract:###Even though recent works have brought some insight into the performance improvement of techniques used in state-of-the-art deep-learning models, more work is needed to understand the generalization properties of over-parameterized deep neural networks. We shed light on this matter by linking the loss function to the output’s sensitivity to its input. We find a rather strong empirical relation between the output sensitivity and the variance in the bias-variance decomposition of the loss function, which hints on using sensitivity as a metric for comparing generalization performance of networks, without requiring labeled data. We find that sensitivity is decreased by applying popular methods which improve the generalization performance of the model, such as (1) using a deep network rather than a wide one, (2) adding convolutional layers to baseline classifiers instead of adding fully connected layers, (3) using batch normalization, dropout and max-pooling, and (4) applying parameter initialization techniques.", "review": "Review:###This paper examines generalization performance of various neural network architectures in terms of a sensitivity metric that approximates how the error responds to perturbations of the input. A crude argument is presented for how the proposed sensitivity metric captures the variance term in the standard bias-variance decomposition of the loss. A number of experimental results are presented that show strong correlation between the sensitivity metric and the empirical test loss. Understanding the distinguishing characteristics of networks that generalize well versus networks that generalize poorly is a central challenge in modern deep learning research, so the topic and analyses presented in this paper are salient and will be of interest to most of the community. The experimental results are intriguing and the presentation is clear and easy to read. While some may object to the egregious simplifications utilized in *deriving* the sensitivity metric, I believe this kind of analysis should be welcomed if it produces new insights and helps explain otherwise opaque empirical phenomena. All told, if taken in isolation from prior work, I think the insights and empirical results presented in this paper are quite interesting and certainly sufficient for acceptance to ICLR. However, there is significant overlap with prior work that severely detracts from the novelty of the results presented here, and I think the community is already familiar with the paper*s main conclusions. From the empirical viewpoint, [1] performs a very similar (and actually quite a bit more thorough) analysis, and reaches very similar conclusions. The authors do cite [1], but unless I missed something, their main argument for uniqueness is basically *in experiments, we prefer S to the Jacobian, because in order to compute S it is enough to look at the network as a black box that given an input, generates an output, without requiring further knowledge of the model.* While this may be useful from the practical standpoint for some non-differentiable models, I*m not convinced that this distinction is really significant in terms of building insights or new understanding. One additional way this paper is distinct from [1] is that it includes a theoretical *derivation* for the sensitivity metric. While I found the argument interesting, from the theoretical perspective, [2] gives much more rigorous and insightful arguments that help explain the observed phenomena. Overall, I*m just not convinced this paper is novel enough to merit publication. But perhaps I*ve overlooked something, in which case I hope the author*s response can highlight their unique contributions relative to prior work. [1] Novak, Roman, et al. *Sensitivity and generalization in neural networks: an empirical study.* arXiv preprint arXiv:1802.08760 (2018). [2] Arora, Sanjeev, et al. *Stronger Generalization Bounds for Deep Nets via a Compression Approach.* International Conference on Machine Learning. 2018."}
{"id": "iclr2020_703", "title": "Symmetric-APL Activations: Training Insights and Robustness to Adversarial Attacks | OpenReview", "abstract": "Abstract:###Deep neural networks with learnable activation functions have shown superior performance over deep neural networks with fixed activation functions for many different problems. The adaptability of learnable activation functions adds expressive power to the model which results in better performance. Here, we propose a new learnable activation function based on Adaptive Piecewise Linear units (APL), which 1) gives equal expressive power to both the positive and negative halves on the input space and 2) is able to approximate any zero-centered continuous non-linearity in a closed interval. We investigate how the shape of the Symmetric-APL function changes during training and perform ablation studies to gain insight into the reason behind these changes. We hypothesize that these activation functions go through two distinct stages: 1) adding gradient information and 2) adding expressive power. Finally, we show that the use of Symmetric-APL activations can significantly increase the robustness of deep neural networks to adversarial attacks. Our experiments on both black-box and open-box adversarial attacks show that commonly-used architectures, namely Lenet, Network-in-Network, and ResNet-18 can be up to 51% more resistant to adversarial fooling by only using the proposed activation functions instead of ReLUs.", "review": "Review:###This paper proposes a learnable piece-wise linear activation unit whose hinges are placed symmetrically. It gives a proof on the universality of the proposed unit on a certain condition. The superiority of the method is empirically shown. The change of the activation during training is analyzed and insight on the behavior is provided. The robustness to adversarial attacks is also empirically examined. This paper discusses a very basic component of neural network models: activation function. Thus, it should be of interest to many researchers. The proposed method is simple and seems easy to use in real settings. A number of experiments are conducted to validate the method and the results look promising. The experiments in Section 5 is particularly interesting. It might give some hints for the following studies. However, there are several things to be addressed for acceptance. 1) What is actually proposed is not very clear. S-APL is formulated in Equation 2. However, there are some discussion after that which changes or restricts the equation. For example, it seems that b_i^s^+ = b_i^s^- is assumed throughout the paper. In that case, it should be just reflected in Equation 2. In the third paragraph of Section 3.2, it is mentioned that h_i^s(x) = h_i^s(-x) with b_i^s^+ = b_i^s^-. However, it should also assume that a^s^+ = a^s^-. From the experiments. apparently, a^s^+ = a^s^- is not assumed. It seems that the method has symmetry only for the hinge locations. In the first paragraph of Section 3.2, it is implied that parameters are shared across layers. It is not very clear what is shared and what is not. Please make that part clear. It will make it easier to understand the experimental settings, too. 2) Theorem 3.1 does not seem to prove the approximation ability of S-APL. It is clear that g(x, S) can represent arbitrary h(x, S), but I am not sure if it is clear that h(x, S) can represent arbitrary g(x, S). It should also depend on the conditions on a^s^+, a^s^-, b_i^s^+, b_i^s^-. I think it needs to prove that h(x, S) can approximate arbitrary piecewise linear function (i.e., g(x, S)) if you want to prove the approximation ability of h(x, S). Equation 4 seems to assume that all intervals are the same (i.e., ?i, B_i - A_i = (B-A) / S). It should be stated explicitly. This relates to the problem 1). I may not understand some important aspect. I am happy to be corrected. 3) Experimental conditions are not clear. Please cite the papers which describe the architecture of the models used in the experiments. The effectiveness of the proposed method should depend on the network architecture and it is importable to be able to see the details of the models. 4) On the sensitivity of optimization on the initial value. It is interesting to see that *fixed trained S-APL* is not comparable with *S-APL positive*. If the hypothesis in the paper is correct, it is natural to assume that *fixed trained S-APL* also has some issue on training. It would be interesting to see experimental results with *initialized with trained-S-APL* and *S-APL positive with non-zero initial value*. It is a bit weird to observe that *S-APL positive* never becomes non-zero for x < 0. 5) Comparison results with other activation units in Section 6. The proposed method is compared only with ReLU. It is important to see comparisons with other activations such as the plain APL. Some other minor comments: It is quite interesting that objects are actually modified for adversarial attack for the proposed method in Figure 5. It would be interesting to have some consideration on it."}
{"id": "iclr2020_704", "title": "Using Logical Specifications of Objectives in Multi-Objective Reinforcement Learning | OpenReview", "abstract": "Abstract:###In the multi-objective reinforcement learning (MORL) paradigm, the relative importance of each environment objective is often unknown prior to training, so agents must learn to specialize their behavior to optimize different combinations of environment objectives that are specified post-training. These are typically linear combinations, so the agent is effectively parameterized by a weight vector that describes how to balance competing environment objectives. However, many real world behaviors require non-linear combinations of objectives. Additionally, the conversion between desired behavior and weightings is often unclear. In this work, we explore the use of a language based on propositional logic with quantitative semantics--in place of weight vectors--for specifying non-linear behaviors in an interpretable way. We use a recurrent encoder to encode logical combinations of objectives, and train a MORL agent to generalize over these encodings. We test our agent in several grid worlds with various objectives and show that our agent can generalize to many never-before-seen specifications with performance comparable to single policy baseline agents. We also demonstrate our agent*s ability to generate meaningful policies when presented with novel specifications and quickly specialize to novel specifications.", "review": "Review:###This paper proposes using logical specifications to facilitate Q-learning in multi-objective reinforcement learning (MORL). Empirically the proposed method can generalize to unseen reward specifications with performance competitive to agents being fully trained in the new environment. The proposed setting employs a more expressive objectives space induced by propositional logic. The proposed method uses a recurrent encoder to embed specifications into vectors and uses them to parametrize the Q-function. Overall, I weakly recommend accepting the submission for the following reasons: (+) it proposes using propositional logic to specify reward functions, which broadens the objective space in an interpretable way, (+) the learned objective embedding demonstrates the ability to generalize to unseen environments(rewards). However, there is still room for improvement. I will raise my score if the following problems are addressed: (-) Needs more diverse experiments (instead of grid worlds) to support the paper (-) It might be hard to express objectives using logic formulas in real-world applications. More specifically, the problems of the paper are, (-) The scalability issue. The most complicated problem in the experiments has a 20x20 state space, which is pretty small for a typical RL problem. I wonder whether the model is still generalizable for larger problems. Even in the case of 20x20 grids, we can notice the gap between the baseline and the proposed method. (-) The assumption. This paper assumes the logical specifications are given by human. However, we usually don*t know how to describe the true objective with logic formulas. Sometimes, finding the specification(reward) itself is as difficult as finding a good policy. Is it possible that we can relax this assumption? Minor comments: Figure 6: The word *baseline* here is misleading. A better choice would be *upper-bound*?"}
{"id": "iclr2020_705", "title": "MUSE: Multi-Scale Attention Model for Sequence to Sequence Learning | OpenReview", "abstract": "Abstract:###Transformers have achieved state-of-the-art results on a variety of natural language processing tasks. Despite good performance, Transformers are still weak in long sentence modeling where the global attention map is too dispersed to capture valuable information. In such case, the local/token features that are also significant to sequence modeling are omitted to some extent. To address this problem, we propose a Multi-scale attention model (MUSE) by concatenating attention networks with convolutional networks and position-wise feed-forward networks to explicitly capture local and token features. Considering the parameter size and computation efficiency, we re-use the feed-forward layer in the original Transformer and adopt a lightweight dynamic convolution as implementation. Experimental results show that the proposed model achieves substantial performance improvements over Transformer, especially on long sentences, and pushes the state-of-the-art from 35.6 to 36.2 on IWSLT 2014 German to English translation task, from 30.6 to 31.3 on IWSLT 2015 English to Vietnamese translation task. We also reach the state-of-art performance on WMT 2014 English to French translation dataset, with a BLEU score of 43.2.", "review": " The paper describes a variation of the Transformer sequence2sequence architecture for neural machine translation. The proposed innovations are: 1) moving the token-wise feed-forward blocks of the transformer to be in parallel with the self-attention blocks rather than interleaved between them as in the original architecture. 2) combining in parallel the self-attention and FFN blocks with dynamic convolution blocks. The methods are evaluated on WMT14 En->Fr, IWSLT2014 De->En and IWSLT2015 En->Vi. For 1) a small improvement of 0.5 BLEU over the original Transformer is reported on a single task, with no significance analysis. Given that the improvement is small and results from a single experiment, it*s not possible to draw strong conclusions from it. For 2) the model shows strong improvements over the original Tansformer, but it*s on pair with the Dynamic convolution Transformer of Wu et al. 2019 from which this work is based, despite having more parameters. Overall it*s not clear from the reported evidence that the methods proposed in this paper represent an improvement over Wu et al. 2019."}
{"id": "iclr2020_706", "title": "BERT Wears GloVes: Distilling Static Embeddings from Pretrained Contextual Representations | OpenReview", "abstract": "Abstract:###Contextualized word representations such as ELMo and BERT have become the de facto starting point for incorporating pretrained representations for downstream NLP tasks. In these settings, contextual representations have largely made obsolete their static embedding predecessors such as Word2Vec and GloVe. However, static embeddings do have their advantages in that they are straightforward to understand and faster to use. Additionally, embedding analysis methods for static embeddings are far more diverse and mature than those available for their dynamic counterparts. In this work, we introduce simple methods for generating static lookup table embeddings from existing pretrained contextual representations and demonstrate they outperform Word2Vec and GloVe embeddings on a variety of word similarity and word relatedness tasks. In doing so, our results also reveal insights that may be useful for subsequent downstream tasks using our embeddings or the original contextual models. Further, we demonstrate the increased potential for analysis by applying existing approaches for estimating social bias in word embeddings. Our analysis constitutes the most comprehensive study of social bias in contextual word representations (via the proxy of our distilled embeddings) and reveals a number of inconsistencies in current techniques for quantifying social bias in word embeddings. We publicly release our code and distilled word embeddings to support reproducible research and the broader NLP community.", "review": "Review:###This paper introduces methods for distilling pretrained contextual representation to a static embeddings for the faster use. It proposes subword pooling and context combination, and its contribution is demonstrated with a suite of classic experiments for static word embeddings. My score for this paper is weakly rejected because (1) the motivation of the proposed approach is not clear. The paper title is about “distill”, but I can’t find out what the methods try to distill, what is the objective; (2) the goal of the paper is to obtain a static embedding performing like ELMO or BERT. As we know, the power of the contextual embedding is demonstrated by different NLP downstreaming task like MT, QA and a bunch of text classification tasks. In this paper, I don’t find those experiments; (3) this paper compared the results with word2vec and glove, but not clear which version to compare. And there are some re-embedding approaches that improve word2vec and glove such as “Word Re-Embedding via Manifold Dimensionality Retention”, the table in this re-embedding paper shows different version of glove and some of them are better than the proposed methods distilled from BERT."}
{"id": "iclr2020_707", "title": "MONET: Debiasing Graph Embeddings via the Metadata-Orthogonal Training Unit | OpenReview", "abstract": "Abstract:###Are Graph Neural Networks (GNNs) fair? In many real world graphs, the formation of edges is related to certain node attributes (e.g. gender, community, reputation). In this case, any GNN using these edges will be biased by this information, as it is encoded in the structure of the adjacency matrix itself. In this paper, we show that when metadata is correlated with the formation of node neighborhoods, unsupervised node embedding dimensions learn this metadata. This bias implies an inability to control for important covariates in real-world applications, such as recommendation systems. To solve these issues, we introduce the Metadata-Orthogonal Node Embedding Training (MONET) unit, a general model for debiasing embeddings of nodes in a graph. MONET achieves this by ensuring that the node embeddings are trained on a hyperplane orthogonal to that of the node metadata. This effectively organizes unstructured embedding dimensions into an interpretable topology-only, metadata-only division with no linear interactions. We illustrate the effectiveness of MONET though our experiments on a variety of real world graphs, which shows that our method can learn and remove the effect of arbitrary covariates in tasks such as preventing the leakage of political party affiliation in a blog network, and thwarting the gaming of embedding-based recommendation systems.", "review": "Review:###TLDR: split node embeddings into medatadata and graph structure, force them to be orthogonal. The paper proposes to split node embeddings in a graph into two parts: 1. graph structure embeddings: Es 2. known node metadata embeddings: Em To prevent Es from containing information about Em, the authors propose a scheme which puts Es into the Nullspace of Em through repeated SVD factorizations. This prevents linear classifiers that operate on Es to reliably predict information in Em. The weakness of the paper stems from the proposed definition of debiasing. Just like two random variables can be dependent, but have a linear correlation coefficient of 0, in the proposed method the two embeddings may be linearly unrelated, but have a strong non-linear relationship. This is an important caveat that should be highlighted in the papers* abstract, not burried deep on p4, under Theorem 2. In fact, looking at Fig 3c information about party affiliation follows a XOR-like pattern in the PCA space. This means that a linear classifier will fail (indeed the linear SVM in Table 1 fails), but a non-linear one should work OK. Thus, contrary to the abstract, the proposed method doesn*t remove the effect of arbitrary covariates, but removes a LINEAR dependence. Thus the paper proposes to solve an important problem and proposes a partial solution, but overstates the results in the abstract and hides the true efficiency of the method. Action items ot correct the paper: - be more honest about the true result. Decorrelation does not imply independence. - redo Table 1 with strong non-linear classifiers such a Gaussian SVM or Random Forest to show how much is not filtered out by your linear decorrelation method Finally, contrast with the adversarial information removal [1] and the information bottleneck [2], both of which also promise to remove non-linear dependencies. It may happen that the you method works better, even though it only guarantees no linear dependencies. [1] https://arxiv.org/abs/1505.07818 [2] D. Moyer, S. Gao, R. Brekelmans, A. Galstyan, and G. Ver Steeg, “Invariant Representations without Adversarial Training,” in Advances in Neural Information Processing Systems 31, 2018"}
{"id": "iclr2020_708", "title": "R-TRANSFORMER: RECURRENT NEURAL NETWORK ENHANCED TRANSFORMER | OpenReview", "abstract": "Abstract:###Recurrent Neural Networks have long been the dominating choice for sequence modeling. However, it severely suffers from two issues: impotent in capturing very long-term dependencies and unable to parallelize the sequential computation procedure. Therefore, many non-recurrent sequence models that are built on convolution and attention operations have been proposed recently. Notably, models with multi-head attention such as Transformer have demonstrated extreme effectiveness in capturing long-term dependencies in a variety of sequence modeling tasks. Despite their success, however, these models lack necessary components to model local structures in sequences and heavily rely on position embeddings that have limited effects and require a considerable amount of design efforts. In this paper, we propose the R-Transformer which enjoys the advantages of both RNNs and the multi-head attention mechanism while avoids their respective drawbacks. The proposed model can effectively capture both local structures and global long-term dependencies in sequences without any use of position embeddings. We evaluate R-Transformer through extensive experiments with data from a wide range of domains and the empirical results show that R-Transformer outperforms the state-of-the-art methods by a large margin in most of the tasks.", "review": " Summary: This paper proposes a new architecture, R-Transformer, that blends the Transformer networks and the recurrent networks, so as to better capture both the long- and short-term features. By injecting a local RNN layer at every level of the network, the authors hoped to enhance the Transformer*s ability to model locality structure. To demonstrate the modeling power of R-Trasnformer, the paper evaluates the effectiveness of R-Transformer on 4 different sequence tasks (seqMNIST, polyphonic music, character- and word-level PTB). Contribution: The authors propose an architecture that combines the practices of recurrent and feed-forward sequence models. However, I have major concerns regarding the novelty this paper, the various claims it makes, as well as its experiment setting. ---------------------------------------------------------------------------- Major issues/questions: 1. The techniques proposed by this paper lack novelty. For instance, the entire section 3.2 is simply the original design of the multi-head self-attention by Vaswani et al. The major difference between R-Transformer and the original Transformer is the replacement of positional embedding with an RNN layer, but (in my opinion) the authors did not demonstrate sufficiently its effectiveness via ablative studies (see below). Moreover, some prior works have already exploited the locality structure in Transformers. For instance, [1] showed that a sparse, local Transformer can work extremely well and be very efficient (they achieved SOTA on large-scale char-level language modeling tasks). 2. The experiments do not entirely convince me. i) The authors use the *same hidden size for R-Transformer and Transformer.* But in fact, as the R-Transformer has one extra RNN/LSTM/GRU layer at every level of the network, the tests were carried out (in effect) using a larger model than the baselines. I think the authors should instead control the # of model parameters, especially since you are running only on small tasks with small-sized models. ii) It is nice that the authors tested R-Transformer on a variety of tasks--- this is important. However, in no way do these number achieve the levels of the *state-of-the-art*, which the authors claim at the end of Section 1 (e.g., [2] has better number on seqMNIST and character-level PTB, and the Transformer-XL actually achieves <55 perplexity on word-level PTB). Therefore, the numbers don*t look particularly appealing to me. iii) Lack of more challenging, or large-scale experiments. Sequential MNIST is known to be a relatively simple task, and alternatives such as sequential CIFAR-10 or permuted sequential MNIST are more valuable *small sequence tasks.* (e.g., prior works studying long-term sequence model dependencies [3]) I also think that benchmarking R-Transformer on large-scale tasks like WikiText-103 or 1Bword (which prior works like QRNN, TrellisNet, RMC and Transformer-XL all explored) would be much more indicative of its usefulness (and should not be left out in such a paper). For instance, it would be useful to compare on small models with controlled model size, even if larger models that require TPUs are not available. iv) Lack of ablative study. Table 4 seems to suggest that Transformer-XL is better than R-Transformer. Is it because of their usage of the relative positional embedding? How does the *finite window size* affect the performance of R-Transformer? Why don*t you use Transformer-XL for the other 3 tasks? Lots of interesting questions are unanswered here. More details in (3) below. 3. Regarding the motivation to insert a local RNN at the start of each layer. The authors claim (by citing Al-Rfou et al.) that the positional embedding (PE) have only limited effect. But in fact, Al-Rfou only says that the PE features (which is added only at the first layer, by the design of Vaswani et al.) can get lost once the transformer gets very deep--- which is totally expected. Therefore, Al-Rfou et al. propose to use a learnable embedding for each layer. This in no way suggests that positional embedding has *limited effect* (Note that in their ablative study, when they turned off the learnable PE, they also added back the original PE). Moreover, the authors didn*t use convolution to capture local data, because it *completely ignores the sequential information of positions within the local window*. That is true. However, **stacking** convolutional layers does capture sequential information. Prior works like [2] showed exactly how temporal convolutions are related to finite-window RNNs. I would suggest the authors to at least compare these different options with ablative studies. I would at least expect a comparison of i) temporal convolution; 2) fixed-window RNN (LocalRNN); 3) unlimited-window RNN; 4) positional embedding; 5) relative PE (which Transformer-XL uses; is that why it*s better in Table 4?). 4. The authors claim that the finite-window RNN captures local features. But doesn*t that claim only applies to the first layer? Once the first layer multi-head attention mixes all input elements across the sequence, the *local* features fed into the second layer RNN will be, actually, **global** features? Doesn*t that *defeat* the purpose of using a local RNN though? ============================ Minor issues that have mild or zero impact on the score: 5. Inconsistent notations. Equation (2) and (3) both describe the LocalRNN(...) function, but clearly have a different input-output signature. Instead of using italics, it*s better to have well-defined notations. Another case is Equation (7): you have *FeedForward(mt)*, but *mt* is not on the right-hand side of the equation at all. Moreover, the symbols used in Eq. (7) are currently inconsistent with Eq. (8). 6. Show the # of parameters in the model in Table 1-4. 7. According to the code released in the dropbox URL, you precompute the index in the finite window and later called *torch.index_select* to produce a tensor that is *ksize* larger than the original sequence (cf. Line 130-143 in models/RTransformers.py in the dropbox folder). How does R-Transformer compare to Transformers in terms of speed and memory? Since you convert the batch dimension to (batch_size * seq_len) at every level of the network, I imagine that could slow down the process, especially in high-dimensional/large-scale experiments? 8. There are some typos (e.g., Sec. 4.2, LTSM) and grammatical mistakes in the paper (Sec. 3.3). 9. In Section 4.2, the authors claim that both LSTM and TCN performed better than Transformers on the polyphonic music dataset because *these music tunes exhibit strong local structures*. However, the difference between Transformer and LSTM is actually very small, and it*s even better than GRU/vanilla RNN. Is that too big a claim to make? In polyphonic music datasets like JSB or Nottingham, there are still longer sequences with longer dependencies... ============================ Overall, I feel that this paper has a good motivation to combine different sequence model families (Transformers, TCNs, RNNs) to improve their modeling power. But at the same time, I feel that the experiments can be a lot stronger, and the paper has limited novelty when compared to prior works. I*m happy to consider adjusting my score if my concerns above are addressed. [1] *Generating Long Sequences with Sparse Transformers*, https://arxiv.org/abs/1904.10509 [2] *Trellis Networks for Sequence Modeling*, https://arxiv.org/abs/1810.06682 [3] *Learning Longer-term Dependencies in RNNs with Auxiliary Losses*, https://arxiv.org/abs/1803.00144"}
{"id": "iclr2020_709", "title": "Semi-Supervised Named Entity Recognition with CRF-VAEs | OpenReview", "abstract": "Abstract:###We investigate methods for semi-supervised learning (SSL) of a neural linear-chain conditional random field (CRF) for Named Entity Recognition (NER) by treating the tagger as the amortized variational posterior in a generative model of text given tags. We first illustrate how to incorporate a CRF in a VAE, enabling end-to-end training on semi-supervised data. We then investigate a series of increasingly complex deep generative models of tokens given tags enabled by end-to-end optimization, comparing the proposed models against supervised and strong CRF SSL baselines on the Ontonotes5 NER dataset. We find that our best proposed model consistently improves performance by F1 in low- and moderate-resource regimes and easily addresses degenerate model behavior in a more difficult, partially supervised setting.", "review": " This work applies CRF autoencoder to the task of semi-supervised named entity recognition. The latent variables are the discrete tag sequences; and to facilitate end-to-end training, an amortized variational objective is used. Different generative model (i.e., generating input tokens conditioning on the latent tags) architectures are explored. Experiments with NER with the Ontonotes 5 dataset show that, with proper architectures and prior distribution, the proposed model outperforms strong baselines. Overall I find the paper clearly presented and well executed. However there is not much to learn from the technical part: most of the key components have been around for a while, and the paper does not seem to attempt to solve any key challenges. To be more constructive, I think the paper can be improved by: - Evaluating the proposed method with other tagging tasks, and discuss whether the architecture and prior distribution choices for the generative model are task-dependent. - Discussing how the model (especially the generative part) translate to other structured latent variables, e.g., syntactic trees and semantic graphs. - More carefully evaluating the partially supervised learning objective, which seems interesting can could potentially benefit future research. I*m happy to adjust the score if the authors can justify the technical contribution and take the effort to improve the paper."}
{"id": "iclr2020_710", "title": "AN EFFICIENT HOMOTOPY TRAINING ALGORITHM FOR NEURAL NETWORKS | OpenReview", "abstract": "Abstract:###We present a Homotopy Training Algorithm (HTA) to solve optimization problems arising from neural networks. The HTA starts with several decoupled systems with low dimensional structure and tracks the solution to the high dimensional coupled system. The decoupled systems are easy to solve due to the low dimensionality but can be connected to the original system via a continuous homotopy path guided by the HTA. We have proved the convergence of HTA for the non-convex case and existence of the homotopy solution path for the convex case. The HTA has provided a better accuracy on several examples including VGG models on CIFAR-10. Moreover, the HTA would be combined with the dropout technique to provide an alternative way to train the neural networks.", "review": " In this paper, the authors propose the Homotopy Training Algorithm (HTA) for neural network optimization problems. They claim that HTA starts with several simplified problems and tracks the solution to the original problem via a continuous homotopy path. They give the theoretical analysis and conduct experiments on the synthetic data and the CIFAR-10 dataset. My major concerns are as follows. 1. The authors may want to give more detailed explanations of HTA. For example, they may want to give the pseudocode for HTA and explain its advantages compared to other optimization methods. 2. The theoretical analysis is trivial. The proof of Theorem 3.1 is to verify Assumptions 4.1 and 4.3 in [1]. Moreover, the proof of Theorem 3.2 is similar to the analysis for the convergence of SGD for convex problems in [2]. 3. The experiments do not show the efficiency of HTA, as the original quasi-newton method is faster than the quasi-newton method with the homotopy setup. 4. The authors make a mistake in the proof of Theorem 3.1. The claim that “{\theta_k} is contained in an open set which is bounded. Since that g is continuous, g is bounded.” is incorrect. We can find a counterexample g(x) = frac{1}{x}, xin (0,1). [1] L. Bottou, F. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223–311, 2018. [2] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009."}
{"id": "iclr2020_711", "title": "Adapting Behaviour for Learning Progress | OpenReview", "abstract": "Abstract:###Determining what experience to generate to best facilitate learning (i.e. exploration) is one of the distinguishing features and open challenges in reinforcement learning. The advent of distributed agents that interact with parallel instances of the environment has enabled larger scale and greater flexibility, but has not removed the need to tune or tailor exploration to the task, because the ideal data for the learning algorithm necessarily depends on its process of learning. We propose to dynamically adapt the data generation by using a non-stationary multi-armed bandit to optimize a proxy of the learning progress. The data distribution is controlled via modulating multiple parameters of the policy (such as stochasticity, consistency or optimism) without significant overhead. The adaptation speed of the bandit can be increased by exploiting the factored modulation structure. We demonstrate on a suite of Atari 2600 games how this unified approach produces results comparable to per-task tuning at a fraction of the cost.", "review": "Review:###This paper presents an adaptive exploration scheme that can reduce the complexity of per-task tuning. This goal is achieve by formulating the adapting scheme as a multi-arm bandit problem with the actual *learning progress* as a feedback signal. The paper is well written and easy to be understood. The strength of this paper is that 1) the proposed method is new in the sense that it invents an automatic way for exploration. 2) The algorithm is simple yet effective by the experiment results the authors provide. Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear. More explanations are needed."}
{"id": "iclr2020_712", "title": "Neural Arithmetic Units | OpenReview", "abstract": "Abstract:###Neural networks can approximate complex functions, but they struggle to perform exact arithmetic operations over real numbers. The lack of inductive bias for arithmetic operations leaves neural networks without the underlying logic needed to extrapolate on tasks such as addition, subtraction, and multiplication. We present two new neural network components: the Neural Addition Unit (NAU), which can learn to add and subtract; and Neural Multiplication Unit (NMU) that can multiply subsets of a vector. The NMU is to our knowledge the first arithmetic neural network component that can learn multiplication of a vector with a large hidden size. The two new components draw inspiration from a theoretical analysis of recent arithmetic components. We find that careful initialization, restricting parameter space, and regularizing for sparsity is important when optimizing the NAU and NMU. Our results, compared with previous attempts, show that the NAU and NMU converges more consistently, have fewer parameters, learns faster, does not diverge with large hidden sizes, obtains sparse and meaningful weights, and can extrapolate to negative and small numbers.", "review": "Review:###The authors extend the work of Trask et al 2018 by developing alternatives to the Neural Accumulator (NAC) and Neural Arithmetic Logic Unit (NALU) which they dub the Neural Addition Unit (NAU) and Neural Multiplication Unit (NMU), which are neural modules capable of performing addition/subtraction and multiplication, respectively. The authors show that their proposed modules are capable of performing arithmetic tasks with higher accuracy, faster convergence, and more theoretically well-grounded foundations. The new modules modules are relatively novel, and significantly outperform their closest architectural relatives, both in accuracy and convergence time. The authors also go to significant lengths to demonstrate that the parameters in these modules can be initialized and learned in a more theoretically well-grounded manner than their NAC/NALU counterparts. For these reasons I believe this paper should be accepted. General advice/feedback: - should provide an explanation of the row in Table 2 showing that a simple linear transformation is able to achieve accuracy and convergence times comparable to those of the NAU - should provide an explanation of the universal 0% success rate on the U[1.1,1.2] sampling interval in Figure 3 - inconsistent captioning in Figure 2c, missing *NAC• with* - should clarify in Section 4.1 that the *arithmetic dataset* task involves summing only *contiguous* vector entries; this is implied by the summation notation, and made explicit in Appendix Section C, but not specified in Section 4.1 - it is unclear what experiments you performed to obtain Figure 3, and the additional explanation in Appendix Section C.4 regarding interpolation/extrapolation intervals only adds to the confusion; please clarify the explanation of Figure 3, or else move it to the Appendix - the ordering of some of the sections/figures is confusing and nonstandard: Section 1.1 presents results before explaining what exactly is being measured, Figure 1 shows an illustration of an NMU 2 pages before it is defined, Section 3 could be merged with the Introduction Grammatical/Typesetting errors: - *an theoretical* : bottom of pg 2 - *also found empirically in (see Trask et al. (2018)* : top of pg 4 - *seamlessly randomly* : middle of pg 5 - *We choice* : middle of pg 6 - inconsistent typesetting of *NAC* : bottom of pg 6 - *hindre* : middle of pg 8 - *to backpropergation* : bottom of pg 8 - *=?* : top of pg 17 - *mathcalR* : bottom of pg 23 - *interrest* : bottom of pg 24 - *employees* : bottom of pg 24 - *models, to* : bottom of pg 24 - *difference, is* : bottom of pg 24 - *consider them* : bottom of pg 24 - *model, is* : top of pg 25 - *task, is* : top of pg 25 - *still struggle* : top of pg 25 - *seam* : top of pg 27 - *inline* : top of pg 27 - inconsistent typesetting of *NAC* : top of pg 27"}
{"id": "iclr2020_713", "title": "HOW IMPORTANT ARE NETWORK WEIGHTS? TO WHAT EXTENT DO THEY NEED AN UPDATE? | OpenReview", "abstract": "Abstract:###In the context of optimization, a gradient of a neural network indicates the amount a specific weight should change with respect to the loss. Therefore, small gradients indicate a good value of the weight that requires no change and can be kept frozen during training. This paper provides an experimental study on the importance of a neural network weights, and to which extent do they need to be updated. We wish to show that starting from the third epoch, freezing weights which have no informative gradient and are less likely to be changed during training, results in a very slight drop in the overall accuracy (and in sometimes better). We experiment on the MNIST, CIFAR10 and Flickr8k datasets using several architectures (VGG19, ResNet-110 and DenseNet-121). On CIFAR10, we show that freezing 80% of the VGG19 network parameters from the third epoch onwards results in 0.24% drop in accuracy, while freezing 50% of Resnet-110 parameters results in 0.9% drop in accuracy and finally freezing 70% of Densnet-121 parameters results in 0.57% drop in accuracy. Furthermore, to experiemnt with real-life applications, we train an image captioning model with attention mechanism on the Flickr8k dataset using LSTM networks, freezing 60% of the parameters from the third epoch onwards, resulting in a better BLEU-4 score than the fully trained model. Our source code can be found in the appendix.", "review": "Review:###This paper studies the importance of a neural networks weights and to which extend do they need to be updated. Particularly, the authors show that freezing weights which have small gradient in the very beginning of the training only results in a very slight drop in the final accuracy. This paper should be rejected because (1) the paper only provides some empirical results on freezing network network weights, I don*t think there are much insights and useful information; (2) To my knowledge, the phenomenon that only a few parameters are important has been observed before by many papers. Given that, I vote for a rejection."}
{"id": "iclr2020_714", "title": "Mesh-Free Unsupervised Learning-Based PDE Solver of Forward and Inverse problems | OpenReview", "abstract": "Abstract:###We introduce a novel neural network-based partial differential equations solver for forward and inverse problems. The solver is grid free, mesh free and shape free, and the solution is approximated by a neural network. We employ an unsupervised approach such that the input to the network is a points set in an arbitrary domain, and the output is the set of the corresponding function values. The network is trained to minimize deviations of the learned function from the PDE solution and satisfy the boundary conditions. The resulting solution in turn is an explicit smooth differentiable function with a known analytical form. Unlike other numerical methods such as finite differences and finite elements, the derivatives of the desired function can be analytically calculated to any order. This framework therefore, enables the solution of high order non-linear PDEs. The proposed algorithm is a unified formulation of both forward and inverse problems where the optimized loss function consists of few elements: fidelity terms of L2 and L infinity norms, boundary conditions constraints and additional regularizers. This setting is flexible in the sense that regularizers can be tailored to specific problems. We demonstrate our method on a free shape 2D second order elliptical system with application to Electrical Impedance Tomography (EIT).", "review": "Review:###This paper presents an unsupervised learning approach to solve forward and inverse problems represented by partial differential equations. A framework is proposed based on the minimisation of loss functions that enforce the boundary conditions of the PDE solution and promote its smoothness. The method is then applied to solve forward and inverse problems in electrical impedance tomography. Flexible machine learning approaches to solving partial differential equations is a subject of ongoing research. While the paper presents an elegant solution folding forward and inverse problems into a single framework, the presentation is missing a few important details which difficult the assessment of the contribution and favour a rejection of the paper. The main issues are insufficient experimental comparisons and a lack of theoretical support for the method. Major issues: 1. When compared to DGM (Sirignano & Spiliopoulos, 2017), for the forward problem, the method only differs in the form of the loss function, which is almost identical, with the exception of the additional L-infinity term and the optional user-defined regularisers. The argument for the inclusion of the L-infinity loss is its high sensitivity to outliers, enforcing a smooth solution. (a) Why PDE solutions learned using the original loss from DGM, which also yields continuous functions, should present outliers in the first place? Moreover, the possibility of adding a user-defined regularizer seems to be a relatively simple extension. (b) What should the theoretical or practical implications for the extra user-defined regularisation term be? 2. The loss function for the inverse problem, which seems to be one of the paper’s contributions, misses a dedicated discussion. An important detail in this loss is the third term, which enforces boundary conditions for the coefficients at the boundary of the domain. In Equation 2, however, the coefficients only affect the PDE through the Lu term over the domain, not its boundary. So what does \theta_0 mean in Equation 4 then? 3. The paper proposes a general framework, but experimental results are presented for only one specific problem, the electrical impedance tomography. The generalisability of the method to more complex problems, such as PDEs with time components and a high-dimensional spatial domain, cannot be inferred. Adding experimental comparisons on higher-dimensional domains would strengthen the paper. 4. Experiments only present comparisons to relevant state-of-the-art methods (DGM) in the forward problem. There are no comparisons against other methods for the inverse and the free-shape geometry problems. For example, have the authors considered the method in [A]? [A] Xu, Kailai, and Eric Darve. *The neural network approach to inverse problems in differential equations.* arXiv preprint arXiv:1901.07758 (2019). Minor issues: 1. The background on PDEs is relatively short for a machine learning conference. (a) There lacks an explanation on what the operator mathcal{L} means. (b) Equation 1 lacks an explicit use of “u(x)”, instead of simply “u”, causing confusion with the dependence of the coefficients on “x”. (c) The meaning of the index subscripts on the partial derivatives is also not made clear, especially if “u” could be interpreted as a vector-valued function for someone unfamiliar with PDEs. Replacing “some u” by “some u:R^d\toR” would already help. 2. What does “n” mean in the electrical current equations in Sec. 3? 3. The derivative of a scalar “u(x)” with respect to a vector “x” should be a vector. So what are the plots in figures 4, 5 and 8 showing when referring to du/dx? Is that the magnitude of the vector or the partial derivative with respect to a single spatial component? 4. What does “PSNR” stand for? 5. Indirect citations in the text should be enclosed by brackets using something like the “citep” command from the package “natbib”. 6. In Table 1, there is a typo: “GDM”->”DGM”. 7. The context contains a few minor grammatical issues that can be distracting at times, but should be solvable by revision."}
{"id": "iclr2020_715", "title": "ProxNet: End-to-End Learning of Structured Representation by Proximal Mapping | OpenReview", "abstract": "Abstract:###Underpinning the success of deep learning is the effective regularization that allows a broad range of structures in data to be compactly modeled in a deep architecture. Examples include transformation invariances, robustness to adversarial/random perturbations, and correlations between multiple modalities. However, most existing methods incorporate such priors either by auto-encoders, whose result is used to initialize supervised learning, or by augmenting the data with exemplifications of the transformations which, despite the improved performance of supervised learning, leaves it unclear whether the learned latent representation does encode the desired regularities. To address these issues, this work proposes an emph{end-to-end} representation learning framework that allows prior structures to be encoded emph{explicitly} in the hidden layers, and to be trained efficiently in conjunction with the supervised target. Our approach is based on proximal mapping in a reproducing kernel Hilbert space, and leverages differentiable optimization. The resulting technique is applied to generalize dropout and invariant kernel warping, and to develop novel algorithms for multiview modeling and robust temporal learning.", "review": "Review:###The paper proposes looking at many existing and some new layers on neural networks as proximal operations (where a proximal operator is defined as P_f(z) = argmin_{xin C} f(x) + 1/2 ||x-z||^2). Trivially many existing layers can be written in this way, but the model can be applied to novel fairly interesting setups. The analogies and extensions of existing methods are overall very dense and hard to follow, specially with the amount of liberties and approximations spread throughout the paper. For example, at one point when talking about kernel spaces the paper suggests that a proximal operator computes X^-1 y and draws a close analogy to X y, *modulo the exponent in X*, when in fact multiplying something by a matrix or its inverse tend to have quite the opposite effect (unless the matrix is its own inverse, which does not seem to be the case here). Similarly in the dropout section a b^2 is treated as the same as (a+lambda)^2 b^2, which again has quite different behavior (and IIUC the algorithm does not work with lambda=0). Overall it*s very hard to believe that the proximal methods which claim to approximate and extend existing methods do indeed approximate and extend those methods, at least from reading the mathematics of the paper. The experimental section does show that the methods perform reasonably well, and the two particular variants evaluated in the paper body proper are two of the better-justified one. I would strongly encourage the authors to substantially rework the paper to remove the more dubious equivalence claims (or, instead, strengthen them) and focus on simplifying the explanation and derivation of the things which do perform well."}
{"id": "iclr2020_716", "title": "Model-free Learning Control of Nonlinear Stochastic Systems with Stability Guarantee | OpenReview", "abstract": "Abstract:###Reinforcement learning (RL) offers a principled way to achieve the optimal cumulative performance index in discrete-time nonlinear stochastic systems, which are modeled as Markov decision processes. Its integration with deep learning techniques has promoted the field of deep RL with an impressive performance in complicated continuous control tasks. However, from a control-theoretic perspective, the first and most important property of a system to be guaranteed is stability. Unfortunately, stability is rarely assured in RL and remains an open question. In this paper, we propose a stability guaranteed RL framework which simultaneously learns a Lyapunov function along with the controller or policy, both of which are parameterized by deep neural networks, by borrowing the concept of Lyapunov function from control theory. Our framework can not only offer comparable or superior control performance over state-of-the-art RL algorithms, but also construct a Lyapunov function to validate the closed-loop stability. In the simulated experiments, our approach is evaluated on several well-known examples including classic CartPole balancing, 3-dimensional robot control and control of synthetic biology gene regulatory networks. Compared with RL algorithms without stability guarantee, our approach can enable the system to recover to the operating point when interfered by uncertainties such as unseen disturbances and system parametric variations to a certain extent.", "review": "Review:###In this work the authors studied the model-free RL approach for learning a policy with stability guarantees. Leveraging the Lyapunov stochastic stability criterion, instead if minimizing the cumulative cost (plus a soft entropy), they propose optimizing an objective function with a specific Lyapunov critic, which is a specific critic function that satisfies the Lyapunov criterion to guarantee stability. They also show in several Cartpole, Mujoco, and Repressilator experiments that this approach is more robust to perturbations (such as sinusoids), where the agent are more robust to dynamic uncertainties and disturbances. In general, the topic of guaranteeing stability is a topic in safe RL, and I find this work of enforcing stability in model-free RL interesting. Through the specific parameterization of quadratic Lyapunov function (in the latent space), the authors proposed learning a new critic function that is a value function but at the same time (almost) satisfies the Lyapunov constraints. While this is an interesting idea, and the experimental results look promising, I do have several questions. First, regarding the learning problem of Lyapunov function, how does the proposed way of learning L differ from the one in Richard*18: The lyapunov neural network: Adaptive stability certification for safe learning of dynamic systems, where the problem is formulated as a classification (while in here it is a regression problem)? Second, while this approach is intuitive, since the approach is penalty-based (Lagrangian based), I do not see how the Lyapunov criteria in Theorem 1 is guaranteed, in this case is stability guaranteed by the policy learning algorithm? If not, what do the authors do to enforce that? Third, if one formulates the immediate constraint cost of the CMDP to be the distance of the state to the equilibrium point, then the (undiscounted, shortest-path type) CMDP total cost constraint should guarantee stability (because the total distance cumulative cost is bounded, meaning that the distance cost converges to zero). Then, one can use the Lyapunov approach by Chow*19 (in modulo to their setting in discounted MDPs) to enforce stability (which is a specific notion of safety in this case). How does the proposed method compare with this approach? Can the authors provide numerical comparisons with the method proposed by Chow*19 as well?"}
{"id": "iclr2020_717", "title": "Improving Sample Efficiency in Model-Free Reinforcement Learning from Images | OpenReview", "abstract": "Abstract:###Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. The agent needs to learn a latent representation together with a control policy to perform the task. Fitting a high-capacity encoder using a scarce reward signal is not only extremely sample inefficient, but also prone to suboptimal convergence. Two ways to improve sample efficiency are to learn a good feature representation and use off-policy algorithms. We dissect various approaches of learning good latent features, and conclude that the image reconstruction loss is the essential ingredient that enables efficient and stable representation learning in image-based RL. Following these findings, we devise an off-policy actor-critic algorithm with an auxiliary decoder that trains end-to-end and matches state-of-the-art performance across both model-free and model-based algorithms on many challenging control tasks. We release our code to encourage future research on image-based RL.", "review": " The paper aims to tackle the problem of improving sample efficiency of model-free, off-policy reinforcement learning in an image-based environment. They do so by taking SAC and adding a deterministic autoencoder, trained end-to-end with the actor and critic, with the actor and critic trained on top of the learned latent space z. They call this SAC-AE. Experiments in the DeepMind control suite demonstrate that the result models train much faster than SAC directly on the pixels, in some cases reaching close to the performance of SAC on raw state. Ablation studies demonstrate their approach is most stable with deterministic autoencoders proposed by (Ghosh et al, 2019), rather than the beta-VAE autoencoder proposed in (Nair et al, 2018), end-to-end learning of the autoencoder gives improved performance, and the encoder transfers to some similar tasks. I thought the paper was written well, and its experiments were done quite carefully, but it was lacking on the novelty front. At a high level, the paper has many similarities with the UNREAL paper (Jaderberg et al, 2017), which is acknowledged in the related work. This paper says it differs from UNREAL because they use an off-policy algorithm, and that UNREAL*s auxiliary tasks are based off real-world inductive priors. I don*t see the off-policy distinction as very relevant, because in the end, both UNREAL and SAC-AE are actor-critic algorithms (using A3C and SAC respectively). The way that SAC is used in the paper always collects data in a near on-policy manner, and UNREAL includes experience replay from a replay buffer, which introduces some off-policy nature to UNREAL as well. Therefore this doesn*t feel like a strong argument. Furthermore, although some of the auxiliary tasks in UNREAL are based off human intuition for what makes sense in those environments, they also include task-agnostic auxiliary tasks: reward prediction and pixel-level control. These do not depend on real-world inductive priors, and are shown to improve performance. Overall, this doesn*t feel like a strong enough contribution for ICLR. More specific comments: * Section 6.1 examines the representation power of the encoder by reconstructing proprioceptive state from the encoder. I am not sure the comparison between SAC+AE and SAC is particularly meaningful here. The predictors are learned on top of the encoder output, and in SAC+AE we would expect task information to be encoded in the learned z. But in baseline SAC, there is no reason to expect this to be true - task information is more likely to be distributed across the entire network architecture. The case for SAC+AE seems much stronger from the reward curves, rather than these plots. * The paper argues that their approach is stable and sample-efficient, but when looking at the reward curves, it looked about as stable as SAC. Figure 3 (where they do not train the VAE end-to-end in the red curve) has a similar story. This makes me believe that any claims of added stability are more thanks to SAC, rather than proposed methods. Edit: I would like to clarify that the rating system only provides a 3 for Weak Reject and 6 for Weak Accept. On a 1-10 scale I would rate this as a 5, I feel it is closer to Weak Accept than Weak Reject. Edit 2: I*ve read the other author*s comments. I*m not particularly convinced by the case for novelty, but I didn*t realize that UNREAL*s replay buffer was only 2k transitions instead of 1 million transitions. On reflection, I believe the main contribution here is showing that deterministic autoencoders are more reliable than stochastic ones for the RL setting, and this isn*t the biggest contribution, but it*s enough to make me update to weak accept."}
{"id": "iclr2020_718", "title": "AMUSED: A Multi-Stream Vector Representation Method for Use In Natural Dialogue | OpenReview", "abstract": "Abstract:###The problem of building a coherent and non-monotonous conversational agent with proper discourse and coverage is still an area of open research. Current architectures only take care of semantic and contextual information for a given query and fail to completely account for syntactic and external knowledge which are crucial for generating responses in a chit-chat system. To overcome this problem, we propose an end to end multi-stream deep learning architecture which learns unified embeddings for query-response pairs by leveraging contextual information from memory networks and syntactic information by incorporating Graph Convolution Networks (GCN) over their dependency parse. A stream of this network also utilizes transfer learning by pre-training a bidirectional transformer to extract semantic representation for each input sentence and incorporates external knowledge through the neighbourhood of the entities from a Knowledge Base (KB). We benchmark these embeddings on next sentence prediction task and significantly improve upon the existing techniques. Furthermore, we use AMUSED to represent query and responses along with its context to develop a retrieval based conversational agent which has been validated by expert linguists to have comprehensive engagement with humans.", "review": "Review:###This paper presents a model for dialogue understanding, named AMUSED (A MUlti-Stream vector representation method for USE in natural Dialogue). The method has three main components to understand a sequence of dialogue utterances. A) syntactic component based on dependency parse of utterances. B) knowledge base module based on entities linked from the utterances using off-the-shelf entity linker. C) memory network module which conveys information from previous utterances. The method uses a triplet loss, which compares the correct question - answer pair, with the negative example (question - randomly sampled answer) pair to train the network parameter. I don’t think this paper is strong enough for ICLR as it is now because (0) the proposed method lacks novelty (1) the evaluation task is not well motivated (2) the experimental result doesn’t support comparison to existing methods, (3) the paper is not very clearly written. (0) The proposed method lacks novelty: the paper basically combines many existing ideas and stacks them together for the task of answer retrieval for conversational question answer dialog. While the paper argues using “multi-head attention” as contribution at the end of introduction, this is more or less standard practice these days. Overall, I don’t see a technical novelty for this paper. (1) I don’t think “Next Dialogue Prediction Task” is very well motivated, and this is main evaluation measure of this paper. It’s not very realistic scenario, and proposing this new task makes the work hard to compare with existing work. The authors should motivate this task more clearly. (2) The experimental result doesn’t support valid comparison to existing methods. Moreover, important experimental details and ablations are missing. While the paper puts together many modules, the experiments don’t justify those modules. For instance, what was the benefit of using KB? Table 1 does not isolate its effect as it’s conflated with memory network component. I’m skeptical whether it does anything. Similarly with GCN. The paper should present the model’s best performance, and best performance - GCN component to show its effectiveness. The paper is also missing many details: for example, for knowledgeable module, how frequently entity mentions were detected and used? (Because it must have an entity name embedding in GLoVE vocabulary?) Human evaluation section also needs more rigor. It isn’t clear how many examples were manually evaluated, and how reliable were the annotators. How would you describe “expert linguists”? Any measure of inter-annotator agreement? (3) The paper is not very clearly written, leaving readers confused. For example, “distribution bias” at the end of the second paragraph of page 1 should be better explained and defined. So is “Interesting response retrieval” at the end of Section 1. What is “Multi-stream”? Table 2 is really confusing, from what I understand, the last two rows are of the same system on different datasets. Other comments: In section 4.1., the paper talks about how syntax information have been helpful. I think it’s a bit dishonest to stress it here, given most state-of-the-art NLP models (pre-trained LMs) doesn’t use any syntax. Throughout the paper, capitalization is inconsistent. The paper uses “next sentence prediction” , “next dialogue prediction” interchangeably. Please be consistent. Precision@1 isn’t the accurate or intuitive description of what’s happening."}
{"id": "iclr2020_719", "title": "BERTScore: Evaluating Text Generation with BERT | OpenReview", "abstract": "Abstract:###We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task and show that BERTScore is more robust to challenging examples compared to existing metrics.", "review": " This paper presents a simple application of BERT-based contextual embeddings for evaluation of text generation models such as machine translation and image caption generation. An extensive set of experiments have been carried out to show that the proposed BERTScore metric achieves better correlation with human judgments than the existing metrics. Overall, the paper is well-written and the motivations are clear. However, I am not sure about the technical novelty of the paper as the proposed approach is a natural application of BERT along with traditional cosine similarity measures and precision, recall, F1-based computations, and simple IDF-based importance weighting. Other comments: - It would be interesting to see how the proposed metric performs to evaluate paraphrase generation and text simplification models as the models need to follow specific constraints such as semantic equivalence, novelty, simplicity etc. with respect to source and reference sentences. - Another limitation of the proposed metric is memory and time complexity as it takes relatively more time to evaluate the sentences compared to BLEU, as authors acknowledged in Section 5."}
{"id": "iclr2020_720", "title": "Quantum algorithm for finding the negative curvature direction | OpenReview", "abstract": "Abstract:###We present an efficient quantum algorithm aiming to find the negative curvature direction for escaping the saddle point, which is a critical subroutine for many second-order non-convex optimization algorithms. We prove that our algorithm could produce the target state corresponding to the negative curvature direction with query complexity O(polylog(d)?^(-1)), where d is the dimension of the optimization function. The quantum negative curvature finding algorithm is exponentially faster than any known classical method which takes time at least O(d?^(?1/2)). Moreover, we propose an efficient algorithm to achieve the classical read-out of the target state. Our classical read-out algorithm runs exponentially faster on the degree of d than existing counterparts.", "review": "Review:###This paper proposes a new quantum algorithm which can efficiently estimate the eigen vectors of a Hessian matrix with negative eigenvalue. Decision I would vote for a weak accept although this is somewhat an educated guess. Finding efficient ways to estimate eigen-vectors of the Hessian would have a dramatic impact on second-order optimization techniques (neglecting the practical implications of it being a quantum algorithm). Since the paper is submitted to a machine learning conference I believe more efforts should be done to make the paper accessible to researchers unfamiliar to quantum computing. Comments I appreciate the clarification of the notation at section 2.1, however many exotic notation for machine learning researchers are presented before this section without any reference to 2.1. The operation `propto` (latex) is never explained. My understanding is that it is the update of a quantum state? The section 2.2 should give a brief overview of why both techniques will be useful for the many parts of the contributed algorithms. I could not find definition of T_H anywhere."}
{"id": "iclr2020_721", "title": "Good Semi-supervised VAE Requires Tighter Evidence Lower Bound | OpenReview", "abstract": "Abstract:###Semi-supervised learning approaches based on generative models have now encountered 3 challenges: (1) The two-stage training strategy is not robust. (2) Good semi-supervised learning results and good generative performance can not be obtained at the same time. (3) Even at the expense of sacrificing generative performance, the semi-supervised classification results are still not satisfactory. To address these problems, we propose One-stage Semi-suPervised Optimal Transport VAE (OSPOT-VAE), a one-stage deep generative model that theoretically unifies the generation and classification loss in one ELBO framework and achieves a tighter ELBO by applying the optimal transport scheme to the distribution of latent variables. We show that with tighter ELBO, our OSPOT-VAE surpasses the best semi-supervised generative models by a large margin across many benchmark datasets. For example, we reduce the error rate from 14.41% to 6.11% on Cifar-10 with 4k labels and achieve state-of-the-art performance with 25.30% on Cifar-100 with 10k labels. We also demonstrate that good generative models and semi-supervised results can be achieved simultaneously by OSPOT-VAE.", "review": " The paper proposes to combine a VAE model with the Optimal Transport to approximate some components of the model. The authors evaluate their approach on semi-supervised problems and claim to obtain very competitive results compared to literature. Unfortunately, the paper is very unclear and hard to follow. The authors make some claims that are not true, for instance, learning so called M1+M2 architecture end-to-end is hard, however, there are papers that successfully train such model. Moreover, the authors reported their results as SOTA, however, they missed many other papers with much better scores. Remarks: - The authors claim in the introduction that training a two-level VAE with a classifier (so called M1+M2 architecture) is not robust. However, there are papers that were able to train such model without any reported problems, for instance: * Louizos, C., Swersky, K., Li, Y., Welling, M., & Zemel, R. (2015). The variational fair autoencoder. arXiv preprint arXiv:1511.00830. * Davidson, T. R., Falorsi, L., De Cao, N., Kipf, T., & Tomczak, J. M. (2018). Hyperspherical variational auto-encoders. arXiv preprint arXiv:1804.00891. (published at UAI 2018) * Ilse, M., Tomczak, J. M., Louizos, C., & Welling, M. (2019). DIVA: Domain Invariant Variational Autoencoders. arXiv preprint arXiv:1905.10427. - The authors report SOTA results on MNIST (100 labels). However, there are papers that report better scores, for instance 2.6 in: Davidson, T. R., Falorsi, L., De Cao, N., Kipf, T., & Tomczak, J. M. (2018). Hyperspherical variational auto-encoders. arXiv preprint arXiv:1804.00891. (published at UAI 2018) In this paper they used a VAE model (the M1+M2 architecture) that was trained end-to-end. - A rather minor remark, but I do not fully see a reason why the authors spent a lot of space on Section 2. I believe most of this text could be included in the Appendix. - Equation 8: The authors claim that this decomposition was introduced in (Zhao et al., 2017), however, it was done earlier: Hoffman, M. D., & Johnson, M. J. (2016, December). ELBO surgery: Yet another way to carve up the variational evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference, NIPS. - Equation (16) is very vague and hard to follow. Moreover, the idea of using Optimal Transport is also hard to follow. The authors present a very generic description of the Optimal Transport, and then refer to an algorithm (a pseudocode). It is very unreadable. ===== AFTER REBUTTAL ===== I would like to thank the authors for the rebuttal. I read the comments carefully. However, I am not still convinced by claims of the paper. It is still vague to me why the proposed training procedure is necessary. Therefore, I decided to keep my original score."}
{"id": "iclr2020_722", "title": "Relational State-Space Model for Stochastic Multi-Object Systems | OpenReview", "abstract": "Abstract:###Real-world dynamical systems often consist of multiple stochastic subsystems that interact with each other. Modeling and forecasting the behavior of such dynamics are generally not easy, due to the inherent hardness in understanding the complicated interactions and evolutions of their constituents. This paper introduces the relational state-space model (R-SSM), a sequential hierarchical latent variable model that makes use of graph neural networks (GNNs) to simulate the joint state transitions of multiple correlated objects. By letting GNNs cooperate with SSM, R-SSM provides a flexible way to incorporate relational information into the modeling of multi-object dynamics. We further suggest augmenting the model with normalizing flows instantiated for vertex-indexed random variables and propose two auxiliary contrastive objectives to facilitate the learning. The utility of R-SSM is empirically evaluated on synthetic and real time series datasets.", "review": "Review:###* Summary: The paper presents a relational state-space model that simulates the joint state transitions of correlated objects which are hierarchically coordinated in a graph structure. A structured posterior approximation is developed based on sequential graph neural networks. Two auxiliary contrastive predictive losses are proposed to help circumvent the posterior collapse problem. Graph normalizing flow is further incorporated into the framework to make the joint state transition density more expressive. The proposed R-SSM shows performance gains over state-of-the-arts in three benchmarks. * Comments: The paper is generally well written and technically sound. The framework, including formulation of each of its components, is well defined. It is also helpful that the authors included preliminaries of the literature. The number of experiments are adequate. However, there are a few parts that require more extensive clarification and analysis. 1. Different parts of Section 3 appear to be rather disconnected, the reader still has a hard time figuring out how the learning of the whole framework is carried out. It is desirable to include a sketch of learning algorithm. 2. In the formulation GNF, what is the intuition or principle to decouple the state Z_t into two parts Z_a and Z_b? How does the mapping of Z_b into Z*_b help to make the state transition distribution more expressive? 3. In Section 5.3, the authors mention that GNF was not used due to memory cost. Could it be discussed more thoroughly about the complexities of learning R-SSM and GNF? 4. The model keeps track of a global state z^g, but it is not analyzed in experiments. It is strongly recommended that the authors discuss about the (global and individual) states and their transitions. It would provide great insights on how multiple objects interact with each other. Minor point: 1. Please clarify what is X_{t-h} in Table 3?"}
{"id": "iclr2020_723", "title": "Attraction-Repulsion Actor-Critic for Continuous Control Reinforcement Learning | OpenReview", "abstract": "Abstract:###Continuous control tasks in reinforcement learning are important because they provide an important framework for learning in high-dimensional state spaces with deceptive rewards, where the agent can easily become trapped into suboptimal solutions. One way to avoid local optima is to use a population of agents to ensure coverage of the policy space, yet learning a population with the ``best* coverage is still an open problem. In this work, we present a novel approach to population-based RL in continuous control that leverages properties of normalizing flows to perform attractive and repulsive operations between current members of the population and previously observed policies. Empirical results on the MuJoCo suite demonstrate a high performance gain for our algorithm compared to prior work, including Soft-Actor Critic (SAC).", "review": "Review:###[Note to authors: I made profit of a small edit to add a new important comment, which starts with a star below] This paper builds on SAC-NF, an extension of SAC with Normalizing Flows which is shown to improve exploration at a low cost (Mazoure et al., 2019). In this paper, the authors build a population of SAC-NF agents with different parameters for the NF, and use an attraction-repulsion approach to ensure diversity and performance of the population. The resulting combination is shown to be competitive to or better than state-of-the-art algorithms. This is a nice paper with nice ideas, and the resulting ARAC algorithm shows state-of-the-art performance on difficult continuous control tasks. Thus I*m in favor of accepting it. However, it should be improved before final acceptance. Here are a few random remarks: About related work, *balancing the objective and diversity* is also the central concern of Quality-Diversity (QD) algorithms (see e.g. Cully&Demiris for a survey). * Actually, in QD as well as Novelty Search (NS), the algorithms look for diversity in a so called *behavior space*, which is also called *outcome space in Goal Exploration Processes (GEPs), see e.g. *O Sigaud, F Stulp (2108) Policy Search in Continuous Action Domains: an Overview, Neural Networks*, Section 4 for a unifying view. According to eq. (8) and (9), ARAC is looking for diversity in *fitness space*, which in my opinion is weaker. I would be glad to see a comment on that. I had to have a look at (Rezende&Mohamed, 2015) and to read (Mazoure et al., 2019) to get the normalizing flow part. An effort could be made in the beginning of Section 2.3 to make the paper more self-contained. The account of SAC corresponds to an early version of the algorithm. In the most recent one, the value function approximator has been removed (see *Soft actor-critic algorithms and applications*). I had a hard time to figure out whether the _x0008_eta of the NF had something to do with the _x0008_eta_pi of the AR loss. Fig. 4 is of much interest. I would be curious to see the performance of ARAC on Swimmer, as this benchmark has been shown to suffer from a deceptive gradient effect. Reproducibility: *See github.com* => Can you be more specific ? :) The caption of Fig 6 describes 3 things, but I can only see two curves. Fig 7 seems to be a mere repetition of Fig 4. Late edit before submitting? ;) Fig.8 suffers from a poor choice of colors: even magnifying a lot the pdf, I cannot tell which is TD3 and which is SAC. Again, Fig.9 is the same as Fig. 2. It would be nice to move Alg 1 to the main part if possible. typos: p3: without risk (of) loss of information"}
{"id": "iclr2020_724", "title": "Autoencoders and Generative Adversarial Networks for Imbalanced Sequence Classification | OpenReview", "abstract": "Abstract:###We introduce a novel synthetic oversampling method for variable length, multi- feature sequence datasets based on autoencoders and generative adversarial net- works. We show that this method improves classification accuracy for highly imbalanced sequence classification tasks. We show that this method outperforms standard oversampling techniques that use techniques such as SMOTE and autoencoders. We also use generative adversarial networks on the majority class as an outlier detection method for novelty detection, with limited classification improvement. We show that the use of generative adversarial network based synthetic data improves classification model performance on a variety of sequence data sets.", "review": "Review:###The paper is well-written. The idea is good, but it seems like GANs have been suggested for Imbalanced data sequences before. A quick search on google, I found this paper: *Multi-Task Generative Adversarial Network for Handling Imbalanced Clinical Data* by Mina Rezaei et al., arXiv:1811.10419v1 Moreover, the paper doesn*t seem to be comparing their results with other state of the art imbalanced sequence classification methods. The comparisons are all between different proposed GAN methods. For these two reasons, I do not recommend this paper for publication at this point."}
{"id": "iclr2020_725", "title": "VIDEO AFFECTIVE IMPACT PREDICTION WITH MULTIMODAL FUSION AND LONG-SHORT TEMPORAL CONTEXT | OpenReview", "abstract": "Abstract:###Predicting the emotional impact of videos using machine learning is a challenging task. Feature extraction, multi-modal fusion and temporal context fusion are crucial stages for predicting valence and arousal values in the emotional impact, but have not been successfully exploited. In this paper, we proposed a comprehensive framework with innovative designs of model structure and multi-modal fusion strategy. We select the most suitable modalities for valence and arousal tasks respectively and each modal feature is extracted using the modality-specific pre-trained deep model on large generic dataset. Two-time-scale structures, one for the intra-clip and the other for the inter-clip, are proposed to capture the temporal dependency of video content and emotional states. To combine the complementary information from multiple modalities, an effective and efficient residual-based progressive training strategy is proposed. Each modality is step-wisely combined into the multi-modal model, responsible for completing the missing parts of features. With all those above, our proposed prediction framework achieves better performance with a large margin compared to the state-of-the-art.", "review": "Review:###This work design a framework to predict valence and arousal values in the emotional impact. Although its performance is much better than previous works, I have some questions about the current submission: -- The writing is not clear enough. I have to guess the technical details based on the context. For example, in Equation 1, how to estimate y_{i}? And why put 1/m behind sum? Is there any particular reason for this? It is a classification problem here, why use MSE loss rather than cross-entropy loss? -- *If modality i plays an important role, it would push the mapping to complete the f_{i-1} towards f_i to get better performance.* Are you suggesting that sometimes H() will be about zero? -- Figure 2 is not clear enough for me. I still can not fully understand what ``progressive* means here. I will appreciate it if the authors can make this more clear. -- What concerns me the most is the novelty of this work. It seems solid and effective. However, based on the current content, I can not fully appreciate its novelty. Using deep learning models to extract features from different modalities has been used before. Using deep learning to consider inter-clip and intra-clip relations also has been conducted. Using LSTM for fusion is not new, either. The authors claimed they propose a progressive training strategy for effectively training and information fusion. However, given the current version, I can not fully appreciate it. I can change my point if the authors would provide more details and explanations later, which can help me to understand the novelty of this work fully. Thanks."}
{"id": "iclr2020_726", "title": "A Deep Recurrent Neural Network via Unfolding Reweighted l1-l1 Minimization | OpenReview", "abstract": "Abstract:###Deep unfolding methods design deep neural networks as learned variations of optimization methods. These networks have been shown to achieve faster convergence and higher accuracy than the original optimization methods. In this line of research, this paper develops a novel deep recurrent neural network (coined reweighted-RNN) by unfolding a reweighted l1-l1 minimization algorithm and applies it to the task of sequential signal reconstruction. To the best of our knowledge, this is the first deep unfolding method that explores reweighted minimization. Due to the underlying reweighted minimization model, our RNN has a different soft-thresholding function (alias, different activation function) for each hidden unit in each layer. Furthermore, it has higher network expressivity than existing deep unfolding RNN models due to the over-parameterizing weights. Moreover, we establish theoretical generalization error bounds for the proposed reweighted-RNN model by means of Rademacher complexity. The bounds reveal that the parameterization of the proposed reweighted-RNN ensures good generalization. We apply the proposed reweighted-RNN to the problem of video-frame reconstruction from low-dimensional measurements, that is, sequential frame reconstruction. The experimental results on the moving MNIST dataset demonstrate that the proposed deep reweighted-RNN significantly outperforms existing RNN models.", "review": "Review:###Strength: This paper proposes a new reweighted-RNN by unfolding a reweighted L1-L1 minimization problem. It develops an iterative algorithm to solve the reweighted L1-L1 minimization problem, where the soft-thresholding functions can be adaptively learned. This paper provides the generalization error bound for deep RNNs and shows that the proposed reweighted-RNN has a lower generalization error bound. In addition, the paper shows that the proposed algorithm can be applied to video-frame reconstruction and achieves favorable results against state-of-the-art methods. The paper is well organized, and the motivation is clear. Weakness: The effectiveness of the reweighted L1-L1 minimization method should be better explained and evaluated. It is not clear why the reweighted L1-L1 regularization is better than the L1-L1 regularization. In addition, the experimental evaluation does not support this claim well. The authors should compare the baseline method which uses the L1-L1 regularization in their framework instead of directly comparing the proposed algorithm with [Le et al., 2019] as there exist differences in the algorithm design. This is an important baseline. As claimed by the authors, the proposed reweighted-RNN has different sets of {W_l;U_l} for each hidden layer. This will definitively increase the model size when the depth increases. The authors should clarify whether the performance gains due to the only use of large model parameters. Overall, this paper proposes an effective reweighted-RNN model based on the solver of a reweighted L1-L1 minimization. Theoretical analysis and experimental results are provided. I would be willing to increase the score if these problems are solved in the authors’ response."}
{"id": "iclr2020_727", "title": "BatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong Learning | OpenReview", "abstract": "Abstract:###Ensembles, where multiple neural networks are trained individually and their predictions are averaged, have been shown to be widely successful for improving both the accuracy and predictive uncertainty of single neural networks. However, an ensemble*s cost for both training and testing increases linearly with the number of networks. In this paper, we propose BatchEnsemble, an ensemble method whose computational and memory costs are significantly lower than typical ensembles. BatchEnsemble achieves this by defining each weight matrix to be the Hadamard product of a shared weight among all ensemble members and a rank-one matrix per member. Unlike ensembles, BatchEnsemble is not only parallelizable across devices, where one device trains one member, but also parallelizable within a device, where multiple ensemble members are updated simultaneously for a given mini-batch. Across CIFAR-10, CIFAR-100, WMT14 EN-DE/EN-FR translation, and contextual bandits tasks, BatchEnsemble yields competitive accuracy and uncertainties as typical ensembles; the speedup at test time is 3X and memory reduction is 3X at an ensemble of size 4. We also apply BatchEnsemble to lifelong learning, where on Split-CIFAR-100, BatchEnsemble yields comparable performance to progressive neural networks while having a much lower computational and memory costs. We further show that BatchEnsemble can easily scale up to lifelong learning on Split-ImageNet which involves 100 sequential learning tasks.", "review": "Review:###The paper proposes a new efficient ensembling method that has smaller memory footprint than naive ensembling and allows a simple parallelization on one device. The authors’ idea is based on sharing weights between individual ensembling models. The weights of each model can be represented as element-wise product of two matrices: shared one and matrix with rank 1 that can be efficiently stored. The idea is quite interesting despite its simplicity. The experimental part is quite broad. I would like to highlight the lifelong learning as the strongest experimental result achieved by the authors. Despite the significant improvement on top of the baselines, this approach has one drawback described by the authors themselves. This method is difficult to generalize for the case of very diverse tasks despite its scalability. Nevertheless, I would not consider it as a large problem. I have a concern regarding ensembling. Do I understand correctly that in Figure 1 the method achieves almost constant test time cost only in the case of one device parallelization? If yes, then Figure 1 is slightly misleading and the description of this figure should be improved. In the classification section the authors compare their approach only with MC-dropout. I would recommend adding other ensembling methods that have small memory footprint: e.g. [1], and can be better than MC-dropout. The same is true for machine translation section. The authors emphasize that their approach is faster than consequently training independent models. However, since these models are independent, it is possible to train them in parallel on multiple devices. The restriction to one device during training seems in general a bit artificial. [1] Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall, and Dhruv Batra. Whym heads are better than one: Training a diverse ensemble of deep networks.arXiv preprintarXiv:1511.06314, 2015b Overall, it is an interesting paper, that has several drawbacks."}
{"id": "iclr2020_728", "title": "Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness | OpenReview", "abstract": "Abstract:###Recently -norm maximization has been proposed to solve the sparse dictionary learning (SDL) problem. The simple MSP (matching, stretching, and projection) algorithm proposed by cite{zhai2019a} has shown to be surprisingly efficient and effective. This paper aims to better understand this algorithm from its strong geometric and statistical connections with the classic PCA and ICA, as well as their associated fixed-point style algorithms. Such connections provide a unified way of viewing problems that pursue {em principal}, {em independent}, or {em sparse} components of high-dimensional data. Our studies reveal additional good properties of the -maximization: not only is the MSP algorithm for sparse coding insensitive to small noise, but also robust to outliers, and resilient to sparse corruptions. We provide preliminary statistical justification for such inherently nice properties. To corroborate the theoretical analysis, we also provide extensive and compelling experimental evidence with both synthetic data and real images.", "review": "Review:###This paper presents results on Dictionary Learning through l4 maximization. The authors base this paper heavily off of the formulation and algorithm in Zhai et. al. (2019) *Complete dictionary learning via l4-norm maximization over the orthogonal group*. The paper draws connections between complete dictionary learning, PCA, and ICA by pointing out similarities between the objectives functions that are optimized as well as the algorithms used. The paper further presents results on dictionary learning in the presence of different types of noise (AWGN, sparse corruptions, outliers) and show that the l4 objective is robust to different types of noise. Finally the authors apply different types of noise to synthetic and real images and show that the dictionaries that they learn are robust to the noise applied. Overall this paper makes significant contributions by extending the work in the paper referenced above to noisy dictionary learning settings and I would vote to accept based on these results. The connections between Complete Dictionary Learning, PCA and ICA are interesting, but the algorithmic analogies seem superficial in my opinion. There are a lot of algorithms which follow a projected/proximal gradient descent scheme. If there are any deeper connections between the specific algorithms discussed, they should be spelled out more clearly. One point of clarification that I would like to raise is the similarity between the kurtosis and l4 objectives. This paper could be strengthened by delineating the conditions under which one would learn an ICA basis vs a Complete Dictionary. It seems to me that the only difference is in the generative model, and that maximizing the same objective under different data conditions could return an ICA basis or a Complete Dictionary. The robustness theory and experiments on synthetic data are reasonable and demonstrate that complete dictionary learning is robust to the different noise conditions. I would like to how this technique compares to other complete dictionary learning algorithms (ER-SpUD, Complete dictionary learning over the sphere - Sun, Qu, Wright 2015) and whether the l4 objective is unique in providing this robustness. Another central claim of Zhai et. al. 2019 seems to be that l4 maximization is able to recover the entire dictionary at once, vs other algorithms that recover the dictionary one column at a time. To test this, I would like to see runtime evaluations and comparisons to other algorithms. While the claim of recovering the entire dictionary is true, it seems to me that requiring an SVD at each iteration would be very expensive. I am not completely convinced that the approach of estimating the entire dictionary would indeed be faster. To summarize, I believe this paper would be a good addition to the literature on l4 maximization algorithms for dictionary learning. I am willing to adjust my score based on responses to the above concerns."}
{"id": "iclr2020_729", "title": "EnsembleNet: A novel architecture for Incremental Learning | OpenReview", "abstract": "Abstract:###Deep neural networks are used in many state-of-the-art systems for machine perception. Once a network is trained to do a specific task, it cannot be easily trained to do new tasks as it leads to catastrophic forgetting of the previously learned tasks. We propose here a novel architecture called EnsembleNet that accommodates for newer classes of data without having to retrain previously trained sub-models. The novelty of our model lies in the fact that only a small portion of the network has to be retrained which makes it extremely computational efficient and also results in high performance compared to other architectures in the literature. We demonstrated our model on MNIST Handwritten digits, MNIST Fashion, and CIFAR10 datasets. The proposed architecture was benchmarked against other models in the literature on Omega-new, Omega-base, Omega-all metrics for MNIST- Handwritten dataset. The experimental results show that Ensemble Net on overall outperformed every other model in the literature.", "review": "Review:###This paper presents an approach to avoiding catastrophic forgetting in neural networks trained on classification tasks in an incremental manner. The authors term their approach “EnsembleNet”. The approach is very straightforward: the network architecture utilizes one sub-network for each class that outputs an “opinion” score for an input, which indicates whether the input is a member of the class associated with that sub-network. A multi-layer perceptron then takes the opinion scores of each of the sub-networks and classifies the input. Whenever a new class is introduced, a new sub-network is trained up. The authors provide data showing that this approach does better at avoiding catastrophic forgetting than other approaches that have been proposed. This paper was reasonably easy to read, and the motivations clear. However, it suffers from some major flaws that make its contributions quite limited, in my opinion: 1) This approach can only ever work on classification tasks where we know the correct label for the data, or tasks where there is an explicit signal of learning from a new distribution occurring. If we do not know that we are now receiving data from a new category/task, then the network has no way to accommodate the new data. In other words, this approach essentially puts all the onus on the programmer/dataset for indicating whether to use new weights or adapt old ones, unlike other approaches that try to do this in an automated manner (e.g. Kirkpatrick et al., 2017). That effectively makes the problem trivial, by assuming that the hard work has already been done. This is essentially just a standard ensemble approach. Indeed, one could take this approach to the extreme and simply train a new neural network every time data from a new class is encountered. As such, I don’t see how this paper makes any truly interesting theoretical or practical contribution to solving the problem of catastrophic forgetting. 2) The claim that there would be time savings is not well supported. First, the time savings only happen if one assumes that the training is done in parallel on different machines. But, if training of each sub-network is incremental, then it can’t be done in parallel! Moreover, the mathematical analysis in the supplementary is ridden with errors. Aside from numerous typos and errors in writing (see for example the equations at the bottom of page 10, or the broken equation reference on page 11), the basic analysis is wrong in several places, from what I can tell. Consider, “Lemma” 7.1.3: the authors start by assuming alpha <=1, they then demonstrate that this can work for any value of c or |S|, since -4*c/|S| < 0 for all c and |S| > 0. Fine, but that doesn’t prove that alpha <= 1! All that shows is that if alpha <= 1 it does not lead to a contradiction. Indeed, they can’t assume that alpha <=1, because alpha is how the training time scales with the number of datapoints, and there is zero guarantee that this is sublinear. From what I can tell, the “proof” that T_en<T_ord is, in fact, invalid. Some slightly more minor points: - Critical measures like Omega_new, alpha_ideal, etc. should be defined, at least in the supplemental, not left for readers to go and look up in another paper. - I am not convinced that the benchmarking results from Kemker et al. (2018) provide fair comparisons. How well were these networks optimized in that work? Did EWC really only achieve 0.001 for Omega_new? That strikes me as potentially a hyperparameter tuning problem. - Figure 4 seems to tell us nothing beyond the easily stated: we can train each sub-network on a different computer (but, see my first major point above). - Is the final classifier only trained after all the sub-networks have been trained? That’s what Algorithm 1 seems to imply. In which case, it’s not truly incremental training! Altogether, given these various considerations, I cannot recommend this paper for publication at ICLR."}
{"id": "iclr2020_730", "title": "Generalized Zero-shot ICD Coding | OpenReview", "abstract": "Abstract:###The International Classification of Diseases (ICD) is a list of classification codes for the diagnoses. Automatic ICD coding is in high demand as the manual coding can be labor-intensive and error-prone. It is a multi-label text classification task with extremely long-tailed label distribution, making it difficult to perform fine-grained classification on both frequent and zero-shot codes at the same time. In this paper, we propose a latent feature generation framework for generalized zero-shot ICD coding, where we aim to improve the prediction on codes that have no labeled data without compromising the performance on seen codes. Our framework generates pseudo features conditioned on the ICD code descriptions and exploits the ICD code hierarchical structure. To guarantee the semantic consistency between the generated features and real features, we reconstruct the keywords in the input documents that are related to the conditioned ICD codes. To the best of our knowledge, this works represents the first one that proposes an adversarial generative model for the generalized zero-shot learning on multi-label text classification. Extensive experiments demonstrate the effectiveness of our approach. On the public MIMIC-III dataset, our methods improve the F1 score from nearly 0 to 20.91% for the zero-shot codes, and increase the AUC score by 3% (absolute improvement) from previous state of the art. We also show that the framework improves the performance on few-shot codes.", "review": "Review:###The paper proposes a method to do zero-shot ICD coding, which comes down to determining which elements from a set of natural language labels (ICD codes) apply to a given text (diagnostic summary). The main problem is that many codes have zero or very few examples. The proposed solution for this problem is to learn a feature-space generator for examples which can be conditioned on a code. This generator is trained using a GAN, which moves the few-/zero-shot problem to training the discriminator. Here, the tree structure of the ICD codes is used, by using examples with sibling labels as approximate examples for codes with few or on labelled data points. Additionally, a keyword reconstruction loss is used, based on the idea that the keywords of the corresponding ICD code can be reconstructed from a good feature vector. The paper is written and executed well; the setup, which involves a fair number of components, is described and motivated clearly. The experiments include comparisons to state of the art results, finding that applying the GAN-based technique they choose (introduced in Xian et al., 2018 for computer vision problems) produces significant gains in recall of low-data classes (few or zero examples). The precision decreases, probably due to a shift from false negatives to false positives, while the AUC shows modest gains. The main potential issue with the paper is the degree of (effective) novelty. The bulk of the gain relative to the SotA seems to be achievable by applying the GAN-based method, which is described in the Xian et al. (2018) reference, or by applying the label-distribution-aware margin, which are known techniques even if they have not been applied to this particular dataset yet. The additional elements introduced by the authors - the use of sibling codes and the keyword reconstruction loss - are good ideas, and it is worthwhile having a documented test of the benefit they provide, but they don’t seem to have a major influence on the quality of the model. All in all, I would argue for the paper to be accepted. The work done here is valuable to have on record, and the presentation and execution are well done. Two questions for the authors: 1. Could the model benefit from having a self-attention model, like a transformer? This applies mostly to the diagnostic text encoder, as interpreting the ICD code descriptions seems not to depend strongly on structure or context. From the text of the paper it appears that a 1D convolution was used to process the diagnostic texts, but I would expect that longer-distance links between words can be quite relevant there. 2. Could the precision-recall curves be added to the supplementary material?"}
{"id": "iclr2020_731", "title": "CopyCAT: Taking Control of Neural Policies with Constant Attacks | OpenReview", "abstract": "Abstract:###We propose a new perspective on adversarial attacks against deep reinforcement learning agents. Our main contribution is CopyCAT, a targeted attack able to consistently lure an agent into following an outsider*s policy. It is pre-computed, therefore fast inferred, and could thus be usable in a real-time scenario. We show its effectiveness on Atari 2600 games in the novel read-only setting. In the latter, the adversary cannot directly modify the agent*s state -its representation of the environment- but can only attack the agent*s observation -its perception of the environment. Directly modifying the agent*s state would require a write-access to the agent*s inner workings and we argue that this assumption is too strong in realistic settings.", "review": "Review:###This paper proposes a targeted attack on reinforcement learning agents. The goal of the attacker is to add small perturbations masks to an agent’s observations in order to encourage the agent to follow a specific (targeted) set of actions. Each mask corresponds to one specific action, and thus can be thought of as a universal perturbation to induce the agent into taking that specific action. The attack, named CopyCAT, is experimentally compared with current attacks in RL such as the FGSM attack of Huang et al. I would argue to reject the paper for two key reasons. First, the threat model is not well-motivated or clearly explained. Second, I am not convinced by the soundness of the experimental results because the FGSM baseline seems much worse than it should be. I am also concerned by the lack of ablation studies for the L2 regularization term in the author’s loss function. The threat model the authors propose is that the attacker should only have read-access to the agent that it wants to attack, and not write-access. In RL terms, this means that the attacker can only attack the current observation, and not the internal state of the agent. However, in the actual experiments, the authors use networks like DQN which take as input 4 frames at a time, and then the authors only apply the mask to the final (4th) state. This seems like a very strange threat model - if an attacker can already attack every 4th state by applying a mask to it, why can’t the attacker just attack every state? In my opinion, the authors should reconsider the threat model, and perhaps focus more on the fact that they are generating universal perturbations for RL agents via action-specific masks, which does seem interesting. Next, I am worried about the soundness of the experimental results, especially the FGSM baseline. First, in Fig. 4, I do not think the L2 norm of the FGSM-L_inf attack should be on the x-axis - I would not expect the FGSM-L_inf attack to have low L2 norm at all, so it seems unfair to compare something that you explicitly regularize to have low L2 norm (CopyCAT) with something that has no L2 norm constraint. Second, I do think that it is fair to have FGSM-L2 (I did not see this explicitly discussed anywhere in the text, which should also be addressed, but I assume this is an FGSM attack which is projected to a bounded L2 norm ball) in such a plot. However, it is worrying that the FGSM-L2 attack gets worse as the L2 norm of the allowed perturbation increases. Intuitively, allowing a larger norm attack should make it easier to force the targeted action; as a result, I am worried that the implementation is simply incorrect. Third, I would like to see ablation studies on the L2 norm regularization term added to CopyCAT. The intuitive explanation for why it was added (we should minimize energy) doesn’t really make much sense to me - why do we want masks of low energy? Couldn’t we just have a separate version of CopyCAT that just does L2 ball projections? Also, in Fig 4., it seems that the attacks with extremely low weight on the L2 norm regularization term perform better, so why have the term at all? Finally, I am confused why FGSM is so much worse. It seems to me that CopyCAT is very similar to FGSM, but it is computed over an entire training dataset as opposed to for one image. However, I would expect that FGSM on a single datapoint x to result in lower loss on that datapoint than the mask (which maximizes the loss over the whole dataset, not just x). I don’t think it’s a problem if CopyCAT performs slightly worse than FGSM, since the universality of the masks is the interesting feature; I almost think it’s a problem if CopyCAT performs better. If it does indeed perform better, it’s important to figure out which element of CopyCAT makes it perform better. Some additional feedback: - In the abstract, you should say what your contributions and results are. - I think having a section that very clearly outlines the differences (maybe by writing the two algorithms side-by-side) between FGSM and CopyCAT would be immensely helpful for understanding what your contributions are. - I think having more plots about how the masks transfer to out-of-distribution inputs would be really interesting. For example, you could explicitly start the agent off at states not seen in the data used for training CopyCAT and check if you can still force a specific action using the mask. - I would also add an additional baseline beyond just DQN vs Rainbow; while the two are different, they are trained to do similar things. I would also consider doing something like Randomly Initialized DQN vs trained DQN, since those have very different goals."}
{"id": "iclr2020_732", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations | OpenReview", "abstract": "Abstract:###Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations, longer training times, and unexpected model degradation. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large.", "review": "Review:###Summary: This paper investigates improving upon BERT by reducing complexity in terms of free parameters and memory footprint as well as computation steps. They propose 2 strategies for doing this: 1) Splitting the embedding matrix into two smaller matrices (going from V x A to V x B + B x A where B <<<< A); 2) layer-wise parameter sharing. They also utilize sentence order prediction to help with training. These coupled with a bunch of other choices such as using the lamb optimizer, certain hyperparameters etc help show dramatic empirical gains across the board on a wide variety of NLP/NLU tasks. Positives: This paper has a dramatic, seemingly statistically significant reduction in error across a wide-variety of tasks. It provides a thorough experimental plan and approaches the few addendums to training (splitting the embedding matrix, the layer-wise parameter sharing, and the sentence order prediction). Concerns & Questions: There*s a lot of experimentation here and a lot of seemingly deliberate choices after seeing empirical results during the research phase. How crucial are the choices of optimizer and other specific hyperparameters? Were there ones you observed that were more brittle than others? Any specific *reasonable* configurations/settings that caused degenerate solutions?"}
{"id": "iclr2020_733", "title": "Quadratic GCN for graph classification | OpenReview", "abstract": "Abstract:###Graph Convolutional Networks (GCN) have been extensively used to classify nodes in graphs and have been shown to outperform most other node classification methods. However, there are currently few GCN based formalism for graph classification tasks. In this task, graphs of different sizes (e.g. graphs representing different protein structures) belong to different classes, and one attempts to predict the graph class. We here propose a solution combing GCN and methods from knowledge graphs to produce a quadratic GCN (Q-GCN). We extend the GCN formalism by adding a quadratic layer to a standard GCN to classify full graphs. Such a layer produces an output with dimensions independent of the graph node number. This output is then passed through a softmax to classify full graphs. We applied this method to a wide range of graph classification problems and show that such a straightforward formalism outperforms state of the art methods for binary graph classification with or without external input on each graph.", "review": "Review:###The paper proposes a simple modification to existing graph NN methods for classifying the entire graph of possibly variable size. The problem is of interest. The proposed method is on the other hand fairly simple and straightforward, as shown in Fig.1 and eq(4), as a simple function of the adjacency matrix A that incorporate the entire graph structure. The results are verified on four datasets with promising results. I have a number of concerns with the paper: *The presentation: The paper seems to come out in a rush, with many critical details missing. For example, the main method, which is the main technical contribution, is described within less 1-page length. The figure 1 is also not very helpful in understanding the details of the method. In experiment section, four datasets are presented, where many of them are introduced without any info about the dataset size, graph size, etc. The comparison methods used in Table 1 are also minimal (only 1 method is compared with in each dataset), and many of the methods are decade old. Some of the comparison methods are even without any formal reference so there is literally no precise info about where the methods are exactly from. Overall it is very difficult for an average reader to be fully convinced of the significance and professionalism of the work. **empirical evaluation: As aforementioned, the experiments are not compared with the recent work and detailed info is critically missing. **algorithmic contribution: this is also a question mark."}
{"id": "iclr2020_734", "title": "Information Geometry of Orthogonal Initializations and Training | OpenReview", "abstract": "Abstract:###Recently mean field theory has been successfully used to analyze properties of wide, random neural networks. It gave rise to a prescriptive theory for initializing feed-forward neural networks with orthogonal weights, which ensures that both the forward propagated activations and the backpropagated gradients are near `2 isometries and as a consequence training is orders of magnitude faster. Despite strong empirical performance, the mechanisms by which critical initializations confer an advantage in the optimization of deep neural networks are poorly understood. Here we show a novel connection between the maximum curvature of the optimization landscape (gradient smoothness) as measured by the Fisher information matrix (FIM) and the spectral radius of the input-output Jacobian, which partially explains why more isometric networks can train much faster. Furthermore, given that orthogonal weights are necessary to ensure that gradient norms are approximately preserved at initialization, we experimentally investigate the benefits of maintaining orthogonality throughout training, and we conclude that manifold optimization of weights performs well regardless of the smoothness of the gradients. Moreover, we observe a surprising yet robust behavior of highly isometric initializations— even though such networks have a lower FIM condition number at initialization, and therefore by analogy to convex functions should be easier to optimize, experimentally they prove to be much harder to train with stochastic gradient descent. We propose an explanation for this phenomenon by exploting connections between Fisher geometry and the recently introduced Neural Tangent Kernel.", "review": "Review:###This paper analyses the training behavior of wide networks and argues orthogonal initialization helps the training. They suggest projections to the manifold of orthogonal weights during training and provide analysis. Their main result seems to be a bound on the eigen-values of the Fisher information matrix for wide networks (Theorem on pg 6). In their experiments they train Stiefel and Oblique networks as examples of manifold constrained networks and claim they converge faster than unconstrained networks. Cons: - Page 6, the main theorem of the paper, Theorem (bound on the fisher) doesn’t have a proof. - Fig 1. What’s the overhead wall-clock time of manifold constraint?, on cifar10 the two manifold don’t have the same rate. Euclidean on cifar10 has higher test accuracy. Test accuracy after 200 epochs is below 90 and below 60. - There are claims in the paper for providing explanations by making connections to Neural Tangent Kernel but it is mentioned only in the discussion section and they reiterate previously known results. - Fig 3: is the training plot for cifar10 in this figure the one in figure1? Where is the training curves for svhn? Where should we see the rate of reduction in training loss for these methods? - Section B.4: To show that FIM and NTK have the same spectrum you need \nabla^2 L to be identity which is only true for L2 loss function. This does not apply to other loss functions such as cross-entropy. After rebuttal: I raise my rating to weak accept. The writing has improved a lot and most of my concerns are addressed. It would be nice if authors could incorporate the timing plots in the appendix."}
{"id": "iclr2020_735", "title": "Policy Tree Network | OpenReview", "abstract": "Abstract:###Decision-time planning policies with implicit dynamics models have been shown to work in discrete action spaces with Q learning. However, decision-time planning with implicit dynamics models in continuous action space has proven to be a difficult problem. Recent work in Reinforcement Learning has allowed for implicit model based approaches to be extended to Policy Gradient methods. In this work we propose Policy Tree Network (PTN). Policy Tree Network lies at the intersection of Model-Based Reinforcement Learning and Model-Free Reinforcement Learning. Policy Tree Network is a novel approach which, for the first time, demonstrates how to leverage an implicit model to perform decision-time planning with Policy Gradient methods in continuous action spaces. This work is empirically justified on 8 standard MuJoCo environments so that it can easily be compared with similar work done in this area. Additionally, we offer a lower bound on the worst case change in the mean of the policy when tree planning is used and theoretically justify our design choices.", "review": "Review:###This paper claims to present a method for combining model based and model free approaches. The paper I find very poorly written hence my certainty about understanding the method cannot be very high. In training the method seems to build up a backup tree using transition operators and a policy and using them as targets for learning. In training it is not quite as clear what they are doing. The paper seems novel and sensible and has some experimental results that are not trivial but the writing is so difficult to follow that it makes it impossible for me to assess the contributions and even check correctness. I also think that readers would find it too difficult to understand as well. This is making a complete rewrite mandatory. I have added some initial pointers that would help making this more readable but implementing these would only allow us to assess what is being done rather than guarantee acceptance. Since the rebuttal is not intended as a deadline extension I recommend rejecting this paper! Major points: * I find the related work quite badly written. There is content but what the reader cares about it situating the paper in the landscape of existing methods. There is none of that here: why do we care in this work about PPO and not say Q(lambda). It should build up the components that were existing in the literature not just present some other methods. It needs to tell us roughly what is similar in this work to what was previously existing (roughly at least). * If PPN is so central it has to be presented before PTN and notation should be introduced there. Introducing formulas without explaining notation like eq (1-4) serves only to alienate the reader and the (well-intentioned) reviewer. * Please separately present how inference works and present the learning all in one place instead of losses in 3.1 and how to construct targets spread out until 3.2.2 * The fact that you need so many *note that * should be a red flag that the writing is not right (12 times). * The algorithm was the most useful thing in the paper but even there it should be much clearer e.g. what is Q, how come we can write Q[j] = in consecutive lines. The second one should probably be Q[j] +=. I can*t be sure because everything else is so hard to track. * To me if you have a network that takes in previous step and action and produces a latent next step that is an explicit transition model. How is it not ?"}
{"id": "iclr2020_736", "title": "Learning to Control PDEs with Differentiable Physics | OpenReview", "abstract": "Abstract:###Predicting outcomes and planning interactions with the physical world are long-standing goals for machine learning. A variety of such tasks involves continuous physical systems, which can be described by partial differential equations (PDEs) with many degrees of freedom. Existing methods that aim to control the dynamics of such systems are typically limited to relatively short time frames or a small number of interaction parameters. We show that by using a differentiable PDE solver in conjunction with a novel predictor-corrector scheme, we can train neural networks to understand and control complex nonlinear physical systems over long time frames. We demonstrate that our method successfully develops an understanding of complex physical systems and learns to control them for tasks involving multiple PDEs, including the incompressible Navier-Stokes equations.", "review": "Review:##### Summary The authors propose a method for training physical systems whose behavior is governed by partial differential equations. They consider situations where only partial observations available, and where control is indirect. Indirectly controlling physical systems is a very important problem with applications throughout engineering. In fact, much of the field of robotics can be described in these terms. The authors employ a number of interesting methods, including a predictor-corrector framework and the adjoint sensitivity method for differentiating through differential equation solvers. The paper is generally very clear, organized and well written. There are only a few places where I think clarification is needed (see detailed comments below). I also have a few questions about the losses and training procedure. On the whole, I think the paper is inventive, well-written and potentially very impactful. I think it would be a great addition to ICLR. ## Clarifications * Page 4: I found the statement *an agent trained in supervised fashion will then learn to average over the modes instead of picking one of them* a little confusing. Could you clarify the reasoning here? * Page 5: I think the description of predictor-corrector could be clearer. In particular, I found the phrase *the correction uses o(t + ?t) to obtain o(t + ?t)* unclear. * Page 8: Could you add a description of what is observable to the body of the paper (I see it is included in the supplement)? * Page 8: I think ?p needs to be divided by density in your NS equation, right? * Page 8 - 9: Is there a limit on the size of the force that can be applied at any point? I know the total force is penalized, but what about the maximum force applied at any point? ## Losses and Training * I think *differentiable physics* losses need a more detailed explanation in the body of the paper. * In the supplement, it is defined using B_r, but I don*t think B_r is defined. * It seems like the differential physics loss requires a differential solver (in this case, for Burger/Navier-Stokes). If I have understood this correctly, I think this needs to be discussed in the body of the paper. In particular, it would be nice to discuss what happens when the physics is a black box (i.e. we can interact with the system by applying control and observing, but we don*t know the rules governing the physical system). Is this exactly when we are restricted to the *supervised* loss? Is there some middle ground? What if we had black box access to the exact physics, along with an approximate differentiable solver? This seems like a realistic scenario for e.g. large fluid flow scenarios."}
{"id": "iclr2020_737", "title": "On Global Feature Pooling for Fine-grained Visual Categorization | OpenReview", "abstract": "Abstract:###Global feature pooling is a modern variant of feature pooling providing better interpretatability and regularization. Although alternative pooling methods exist (eg. max, lp norm, stochastic), the averaging operation is still the dominating global pooling scheme in popular models. As fine-grained recognition requires learning subtle, discriminative features, we consider the question: is average pooling the optimal strategy? We first ask: ``is there a difference between features learned by global average and max pooling?** Visualization and quantitative analysis show that max pooling encourages learning features of different spatial scales. We then ask ``is there a single global feature pooling variant that*s most suitable for fine-grained recognition?** A thorough evaluation of nine representative pooling algorithms finds that: max pooling outperforms average pooling consistently across models, datasets, and image resolutions; it does so by reducing the generalization gap; and generalized pooling*s performance increases almost monotonically as it changes from average to max. We finally ask: ``what*s the best way to combine two heterogeneous pooling schemes?** Common strategies struggle because of potential gradient conflict but the ``freeze-and-train** trick works best. We also find that post-global batch normalization helps with faster convergence and improves model performance consistently.", "review": "Review:###This paper reports experimental results on global feature pooling for fine-grained visual categorization. The main experiments are (1) Comparison of features learned by average pooling and max pooling. (2) Comparison of nine pooling methods (3) Combination of two different pooling schemes. The finding on the features learned by average pooling and max pooling is interesting. However, this paper is not suitable for publication because the experimental comparison of two different pooling schemes lacks details and does not support a significant contribution. The hypothesis of the little improvement of a conventional combination of two different pooling is claimed that lower layers* filters become confused by gradient signals coming from the two different pooling schemes. However, such a high-level explanation is not convincing well. The hypothesis should be verified by equations and/or experiments. The detail of “channel split“ is unclear. How is the feature map split into different splits? The detail of the “freeze-and-train” trick is unclear. Does “a new linear layer with global max pooling” means that a linear layer is added after or before the pooling? p.10, the authors wrote that “This trick guarantees that …”. Why the training of a new linear layer before training whole network can guarantee that final performance becomes not worse? This paper also proposed post-global batch normalization for achieving performance improvement and faster convergence. However, all experiments are conducted using this normalization; thus, the effects of this normalization cannot be confirmed."}
{"id": "iclr2020_738", "title": "Low Bias Gradient Estimates for Very Deep Boolean Stochastic Networks | OpenReview", "abstract": "Abstract:###Stochastic neural networks with discrete random variables are an important class of models for their expressivity and interpretability. Since direct differentiation and backpropagation is not possible, Monte Carlo gradient estimation techniques have been widely employed for training such models. Efficient stochastic gradient estimators, such Straight-Through and Gumbel-Softmax, work well for shallow models with one or two stochastic layers. Their performance, however, suffers with increasing model complexity. In this work we focus on stochastic networks with multiple layers of Boolean latent variables. To analyze such such networks, we employ the framework of harmonic analysis for Boolean functions. We use it to derive an analytic formulation for the source of bias in the biased Straight-Through estimator. Based on the analysis we propose emph{FouST}, a simple gradient estimation algorithm that relies on three simple bias reduction steps. Extensive experiments show that FouST performs favorably compared to state-of-the-art biased estimators, while being much faster than unbiased ones. To the best of our knowledge FouST is the first gradient estimator to train up very deep stochastic neural networks, with up to 80 deterministic and 11 stochastic layers.", "review": "Review:###***Score updated to weak accept after the rebuttal.*** Straight-Through is a popular, yet not theoretically well-understood, biased gradient estimator for Bernoulli random variables. The low variance of this estimator makes it a highly useful tool for training large-scale models with binary latents. However, the bias of this estimator may cause divergence in training, which is a significant practical issue. The paper develops a Fourier analysis of the Straight-Through estimator and provides an expression for the bias of the estimator in terms of the Fourier coefficients of the considered function. Motivated by this expression, the paper proposes two modifications of Straight-Through which may reduce the bias of the estimator, at the cost of the variance. The experimental results show advantage of this improved estimator over Gumbel-Softmax and DARN estimator. While I really like the premise of the paper, I feel that it needs a significant amount of additional work. The text is currently fairly hard to read. The theoretical part of the paper does not quantify the variance of the estimator. The experiments are a bit unfinished and do not include ablations of the proposed modifications of Straight-Through. Most importantly, I think that in the current form the theoretical and the empirical parts of the papers are not well-connected. Because of this, I believe that the paper should currently be rejected, but I encourage the authors to continue this line of work. Pros: 1. Theoretical analysis and empirical improvement of the Straight-Through estimator is an important avenue of work. 2. The paper makes a solid contribution of deriving the Fourier expansion of the Straight-Through estimator bias. 3. Based on this expansion, the paper proposes an algorithm with reduced bias. The algorithm is simple to implement, practical and appears to work slightly better than DARN. Cons: 1. The key weakness of the theoretical part of the paper is that it focuses on the bias of the estimator, but does not quantify the variance, especially after the modifications. If reducing the bias was the only goal, one could use unbiased (but high-variance) estimators such as REINFORCE or VIMCO. 2. The final algorithm appears to be the DARN estimator combined with relaxation by uniform noise (“Bernoulli splitting uniform”) and scaling. The paper does not have an ablation showing how the uniform noise and scaling perform on their own. 3. There are a few incorrect statements that I’ve noticed. * “As a side contribution, we show that the gradient estimator employed with DARN (Gregor et al., 2013), originally proposed for autoregressive models, is a strong baseline for gradient estimation.” - MuProp paper compared to this estimator under the name 1/2-estimator * In Lemma 1 the “REINFORCE gradient” is just the exact gradient of the expectation, not a stochastic REINFORCE gradient. * “ To the best of our knowledge, FouST is the first gradient estimate algorithm that can train very deep stochastic neural networks with Boolean latent variables.” This paper uses up to 11 latent variable layers, while [1] has trained models with >20 latent variable layers (although their “layers” have just one unit). 4. The derivation of “Bernoulli splitting uniform” trick is confusing and contains a lot of typos. For instance, the text before eqn. (14) implies that the distribution of u_i is U[-1, 1], which cannot be right and does not correspond to Algorithm 1. The statement that this trick does not lead to a relaxation is odd, since the function is being evaluated at non-discrete points. 5. There are generally many typos and some poor formatting in the math. For example, in eqn. (6) the coefficients are off by one: it should be c0 + c1 z1 + c2 z2^2 + … . The equations (10) and (11) are poorly formatted. The notation partial_z1 f(u_1, u_2) in eqn. (14) is strange. In many places p^{i->½} is denoted as p^{1->½}. 5. I don’t think I understood the idea of representation scaling (Section 4.4). The eqn. (16) would suggest that the scaling should optimally be set to zero, which is just saying that the gradient is unbiased when the model does not use the latents. There is no other practical guidance on choosing this coefficient. Furthermore, one can always absorb the global scaling factor into the succeeding weights layer of the model, so this trick can probably be replaced by a modification of the weights initialization. 6. The experiments are missing a comparison to the Straight-Through Gumbel-Softmax estimator, introduced in the original Gumbel-Softmax paper. This is a popular biased estimator for Bernoulli latents, e.g. used in [1] [2]. Another interesting comparison would be [3] which proposes a lower-bias version of Gumbel-Softmax. 7. Figure 2 is missing the line for REBAR, even though this line is referred to on Page 8. Figure 2 and Figure 4 are both labeled as training ELBOs, despite the plots being different. [1] Andreas Veit, Serge Belongie “Convolutional Networks with Adaptive Inference Graphs” ECCV 2018 [2] Patrick Chen, Si Si, Sanjiv Kumar, Yang Li, Cho-Jui Hsieh “Learning to Screen for Fast Softmax Inference on Large Vocabulary Neural Networks” ICLR 2019 https://openreview.net/forum?id=ByeMB3Act7 [3] Evgeny Andriyash, Arash Vahdat, Bill Macready “Improved Gradient-Based Optimization Over Discrete Distributions” https://arxiv.org/abs/1810.00116"}
{"id": "iclr2020_739", "title": "Uncertainty - sensitive learning and planning with ensembles | OpenReview", "abstract": "Abstract:###We propose a reinforcement learning framework for discrete environments in which an agent optimizes its behavior on two timescales. For the short one, it uses tree search methods to perform tactical decisions. The long strategic level is handled with an ensemble of value functions learned using -like backups. Combining these two techniques brings synergies. The planning module performs \textit{what-if} analysis allowing to avoid short-term pitfalls and boost backups of the value function. Notably, our method performs well in environments with sparse rewards where standard backups fail. On the other hand, the value functions compensate for inherent short-sightedness of planning. Importantly, we use ensembles to measure the epistemic uncertainty of value functions. This serves two purposes: a) it stabilizes planning, b) it guides exploration. We evaluate our methods on discrete environments with sparse rewards: the Deep sea chain environment, toy Montezuma*s Revenge, and Sokoban. In all the cases, we obtain speed-up of learning and boost to the final performance.", "review": " Uncertainty-Sensitive Learning and Planning with Ensembles ===================================================== This paper investigates the use of uncertainty-aware estimates in solving planning problems (RL with access to simulator). The proposed algorithm combines a learned model-free value estimate with MCTS planning. An ensemble of neural networks is used to model posterior uncertainty in the value estimate and drive efficient exploration. There are several things to like about this paper: - The paper takes on several core issues in RL/planning research, most notably the synthesis of dealing with model-based and model-free uncertainty in RL. - The general flavour of the paper + algorithm seems to be reasonable. The proposal to use ensemble uncertainty estimates to drive model-based MCTS is interesting, natural, and I think it*s a good one. - The proposed structure of the paper is quite nice, there is mostly a linear and logical progression of complexity in the experiments. This is nice to see clear benefits of the approach on the simplest possible settings and build up from there. - The effort to open source code + implementation details is laudable. However, there are several places where this paper falls short: - In general, the claims and results of the paper are far too vague to be fully understood and replicated. Take the main algorithm 1, it really seems like more of a *sketch* of a very general family of algorithms, rather than a specific description of a clear algorithm. - This vagueness is spread throughout the plots and figures as well... note that Figure 1 has no indication of how many steps have been evaluated, and Figure 2 has no indication for what value K > 0 was actually used. The clarity does not improve in Sections 3.2 and 3.3 where quite inconsistent performance metrics and presentations are presented. - Generally, the writing could be tightened quite a lot. In particular I would encourage you to think about whether each statement you make is clearly supported by some theorem, experiment or plot in your paper. For example, on page 3 *We found this mechanism to be beneficial... see Section 3.3* but then it*s not clear exactly what statement shows that particular part of the mechanism was helpful, versus other issues associated with ensemble learning. There are more than a few typos... the on(e) in Osband... akin to ??... might be obtained by choosing from (the) ensemble... - It would be very helpful to clarify that the agent is given access to a simulator... so that this is not exactly the typical RL setting of sequential decision making. This should appear early in the paper. - The code that is released with the paper is also quite confusing, it is not structured with a clear README and includes many sections of dead/commented code. I was hoping the code might rescue some of the clarity, but I think that still needs work. Overall, I do think there is some interesting material here... It*s an important problem, and the core building blocks of combining model, value and uncertainty for better exploration is interesting. However, I just think the actual paper is not clear enough on the details. My belief is that going through this paper very methodically and carefully to make sure that every single detail + claim is rigorously supported would help this paper immensely. For that reason I have to say that I think it*s a *reject* in its current form."}
{"id": "iclr2020_740", "title": "Adversarial Neural Pruning | OpenReview", "abstract": "Abstract:###Despite the remarkable performance of deep neural networks (DNNs) on various tasks, they are susceptible to adversarial perturbations which makes it difficult to deploy them in real-world safety-critical applications. In this paper, we aim to obtain robust networks by sparsifying DNN*s latent features sensitive to adversarial perturbation. Specifically, we define vulnerability at the latent feature space and then propose a Bayesian framework to prioritize/prune features based on their contribution to both the original and adversarial loss. We also suggest regularizing the features* vulnerability during training to improve robustness further. While such network sparsification has been primarily studied in the literature for computational efficiency and regularization effect of DNNs, we confirm that it is also useful to design a defense mechanism through quantitative evaluation and qualitative analysis. We validate our method, emph{Adversarial Neural Pruning (ANP)} on multiple benchmark datasets, which results in an improvement in test accuracy and leads to state-of-the-art robustness. ANP also tackles the practical problem of obtaining sparse and robust networks at the same time, which could be crucial to ensure adversarial robustness on lightweight networks deployed to computation and memory-limited devices.", "review": "Review:###Adversarial neural pruning This paper proposes “adversarial neural pruning” method and vulnerability suppression loss to defend against adversarial attacks. For adversarial neural pruning, the authors train a pruning mask in an adversarial way that benefits both the clean accuracy and the adversarial robustness. The authors also propose a new loss function “vulnerability loss” that measures the robustness of intermediate features in neural networks. By using both techniques, the authors demonstrate through experiments that it is an effective method to defend against adversarial attacks. I have the following questions: Q1: Don’t understand definition 1 and definition 2? I don’t quite understand the meaning of definition 1 and 2. It seems useless for the presentation of the paper to me. Can the authors illustrate why you define these two notions here? Is there any usage in this paper for defining (epsilon, delta)-robust and (epsilon, delta)-vulnerable? Q2: How do you compute the vulnerability of a network? The authors define the vulnerability of a feature in equation (1). However, I can not find how such vulnerability measure is computed exactly. Can the authors specify how they compute this vulnerability measure? Q3: In page 4 “We emphasize that our proposed method can be extended to any existing or new sparsification method in a similar way” This claim is not carefully discussed in this paper. Can your approach be applied to [1]? Q4: Title does not properly cover the content of this paper? The author proposed two approaches: adversarial neural pruning and vulnerability loss. However, the title seems to only cover one of them. It is misleading since vulnerability loss seems also to be a major part of the proposed method. Therefore, either the title needs to be changed or the structure of this paper should be altered to highlight adversarial neural pruning approach. [1] Learning both Weights and Connections for Efficient Neural Networks. Song Han, Jeff Pool, John Tran, William J. Dally, NIPS 2015."}
{"id": "iclr2020_741", "title": "Abductive Commonsense Reasoning | OpenReview", "abstract": "Abstract:###Abductive reasoning is inference to the most plausible explanation. For example, if Jenny finds her house in a mess when she returns from work, and remembers that she left a window open, she can hypothesize that a thief broke into her house and caused the mess, as the most plausible explanation. While abduction has long been considered to be at the core of how people interpret and read between the lines in natural language (Hobbs et al., 1988), there has been relatively little research in support of abductive natural language inference and generation. We present the first study that investigates the viability of language-based abductive reasoning. We introduce a challenge dataset, ART, that consists of over 20k commonsense narrative contexts and 200k explanations. Based on this dataset, we conceptualize two new tasks – (i) Abductive NLI: a multiple-choice question answering task for choosing the more likely explanation, and (ii) Abductive NLG: a conditional generation task for explaining given observations in natural language. On Abductive NLI, the best model achieves 68.9% accuracy, well below human performance of 91.4%. On Abductive NLG, the current best language generators struggle even more, as they lack reasoning capabilities that are trivial for humans. Our analysis leads to new insights into the types of reasoning that deep pre-trained language models fail to perform—despite their strong performance on the related but more narrowly defined task of entailment NLI—pointing to interesting avenues for future research.", "review": "Review:###This paper introduces two new natural language tasks in the area of commonsense reasoning: natural language abductive inference and natural language abductive generation. The paper also introduces a new dataset, ART, to support training and evaluating models for the introduced tasks. The paper describes the new language abductive tasks, contrasting it to the related, and recently established, natural language inference (entailment) task. They go on to describe the construction of baseline models for these tasks. These models were primarily constructed to diagnose potential unwanted biases in the dataset (e.g., are the tasks partially solved by looking at parts of the input, do existing NLI models far well on the dataset, etc.), demonstrating a significant gap with respect to human performance. The paper, and the dataset specifically, represent an important contribution to the area of natural language commonsense reasoning. It convincingly demonstrates that the proposed tasks, while highly related to natural language entailment, are not trivially addressed by existing state-of-the-art models. I expect that teaching models to perform well on this task can lead to improvements in other tasks, although empirical evidence of this hypothesis is currently absent from the paper. Below are a set of more specific observations about the paper. Some of these comments aim to improve the presentation or content of the paper. 1. In Section “5.1 - Pre-trained Language Models” and attendant Table 1describe results of different baselines on the ART inference task. The results in the table confused me for quite some time, I’d appreciate some clarifications. With respect to the differences with columns 1 (GPT AF) and 2 (ART, also BERT AF) I would like the comparison to be made more clear. As far as I understand it, there are 2 parts of the dataset that can be varied: (1) the train+dev sets and (2) the test set. Furthermore, it seems that it makes sense to vary each of these at a time, if we are to compare results with variants. For example: we can fix the test set, and vary how we generate training and dev examples. If a model does better with the same test set, we can assume the train+dev examples were better for the model (for whatever reasons, closer distribution to test, harder or more informative training examples, etc). We can also keep the train+set constant, and vary the test set. This allows us to evaluate which test set is harder with respect to the training examples. The caption of Table 1 implies that both columns are evaluations based on the “ART” test set. If that is correct, then the train+dev set generated from the GPT adversarial examples is of better quality, generating a BERT-ft (fully connected) model that is 3% better. But the overall analysis seems to indicate that this is not what was done in the experiment. Rather, it seems that *both* the train+dev _and_ the test sets were modified concurrently. If that is the case, I would emphasize that the text needs to make this distinction clear. Furthermore, I would say that varying both train and test sets concurrently is sub-optimal, and makes it a bit harder to draw the conclusion that BERT adversarial filtering leads to a stronger overall dataset. 2. Along the lines of the argument in (1), above, I would urge the authors to publish the *entire* set of generated hypotheses (plausible and implausible) instead of only releasing the adversarially filtered pairs. Our group’s experience with training inference models is that it is often beneficial to train using “easy” examples, not only hard examples. I suspect the adversarially filtered set will focus on hard examples only. While this is fine to do in the test set, I think if the full set of annotated/generated hypotheses are released, model designers can experiment with combining pairs of hypothesis in different ways. 3. Furthering the argument of (2): in certain few-shot classification tasks, one is typically asked to identify similarity between one test example and different class representatives. Experience shows that it is often beneficial to train the model on a larger number of distractor classes than what the model is eventually evaluated on (e.g., https://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning). In the alpha-NLI setting, have you experimented with training using multiple distractors, instead of only 1, during training (even if you end up evaluating over 2 hypotheses)? 4. One potential argument for introducing a new natural language task is of transfer learning: learning to perform a complex natural language task should lead to better natural language models more generally, or, for some other related tasks. This paper does not really touch on this aspect of the work. But, potentially, one investigation that could be conducted is through a reversal of the paper’s existing NLI entailment experiment. The paper shows that NLI entailment models do not perform well on alpha-NLI. But it would be interesting to see if a model trained on alpha-NLI, and fine-tuned or multi-tasked on NLI entailment, does better on NLI entailment (i.e., is there transfer from alpha-NLI to entailment NLI?). 5. Another option is to evaluate whether alpha-NLI helps with other commonsense tasks. One other example is Winograd Schema Challenge, which current systems also perform well below human performance. It also seems that the Winograd Schema Challenge questions are not too far from abductive inference. 6. In the abstract of the paper, before the paper defines the abductive inference/generation task specifically, the claim that abductive reasoning has had much attention in research seemed awkward. Informally, most commonsense reasoning (including NLI entailment) could be cast as abductive reasoning. 7. In at least one occasion, I found an acronym which was hard to find the definition for (“AF” used in Section “5.1 - Pre-trained Language Models”; I assumed it was “adversarial filtering”.) 8. In Section “5.1 - Pre-trained Language Models” it seems that the text quotes an accuracy for BERT-ft (fully connected) of 68.9%, but Table 1 indicates 69.6%. 9. In Section “5.1 - Learning Curve and Dataset Size”, there is a claim that the performance of the model plateaus at ~10,000 instances. This does not seem supported by Figure 5. There appears to be over 5-7% accuracy (absolute) improvements from 10k to 100k examples. Maybe the graph needs to be enhanced for legibility? 10. It is great that the paper includes human numbers for both tasks, including all the metrics for generation. 11. Period missing in footnote 8. 12. The analysis section is interesting, it is useful to have in the paper. However, Table 3 is a bit disappointing in that ~26% of the sampled examples fit into one of the categories. It would be great if the authors could comment on the remaining ~74% of the sampled dataset."}
{"id": "iclr2020_742", "title": "PCMC-Net: Feature-based Pairwise Choice Markov Chains | OpenReview", "abstract": "Abstract:###Pairwise Choice Markov Chains (PCMC) have been recently introduced to overcome limitations of choice models based on traditional axioms unable to express empirical observations from modern behavior economics like framing effects and asymmetric dominance. The inference approach that estimates the transition rates between each possible pair of alternatives via maximum likelihood suffers when the examples of each alternative are scarce and is inappropriate when new alternatives can be observed at test time. In this work, we propose an amortized inference approach for PCMC by embedding its definition into a neural network that represents transition rates as a function of the alternatives’ and individual’s features. We apply our construction to the complex case of airline itinerary booking where singletons are common (due to varying prices and individual-specific itineraries), and asymmetric dominance and behaviors strongly dependent on market segments are observed. Experiments show our network significantly outperforming, in terms of prediction accuracy and logarithmic loss, feature engineered standard and latent class Multinomial Logit models as well as recent machine learning approaches.", "review": " 1.The goal of the paper is to connect flexible choice modeling with a modern approach to ML architecture to make said choice modeling scalable, tractable, and practical. 2. The approach of the paper is well motivated intuitively, but could more explicitly show that PCMC-Net is needed to fix inferential problems with PCMC and that e.g. SGD and some regularization + the linear parameterization suggested by the original PCMC authors isn*t scalable in itself. 3. The approximation theorem is useful and clean, and the empirical results are intriguing. While consideration of more datasets would improve the results, the metrics and baselines considered demonstrate a considerable empirical case for this method. My *weak accept* decision is closer to *accept* than *weak reject.* (Edit 11/25/19: I raised my score in to accept in conjunction with the author*s improvements in the open discussion phase) Improvement areas(all relatively minor): - While I personally enjoy the choice axioms focused on by the PCMC model and this paper, stochastic transitivity, IIA, and regularity are probably more important to emphasize than Contractibility. Because the properties of UE and contractibility were not used, it may be more appropriate to use this space to introduce more of the literature on neural-nets-as-feature-embeddings stuff. - This paper could be improved by generalizing to a few other choice models- in particular the CDM (https://arxiv.org/abs/1902.03266) may be a good candidate for your method. This is more a suggestion for future work if you expand this promising initial result. - Hyper-parameter tuning: I noticed that several of your hyper parameters were set to extremal values for the ranges you considered. If you tuned the other algorithms* hyper parameters the same way, it could be the case that the relative performance is explained by the appropriateness of those ranges. Would be interesting to have a more in-depth treatment of this, but I do understand that it*s a lot of work. Specific Notes: Theorem 1 is nice, and the proof is clean, but doesn*t explicitly note that a PCMC model jointly specifies a family of distributions pi_S for each S in 2^U obtained by subsetting a single rate matrix Q indexed by U. It*s clear that PCMC-Net will still approximate under this definition, as hat q_ij approximates each q_ij because hat q_ij doesn*t depend on S. While the more explicit statement is true with the same logic in the theorem, the notational choice to have *X_i* represent the *i-th* element in S is confusing at first, as e.g. X_1 is a different feature vector for S = {2,3} and S={1,3}. I don*t see this issue as disqualifying, but it took me a while to realize that there wasn*t more than a notational abuse problem when I returned to the definitions where the indexing depended on the set S under consideration. Typos/small concerns: -Above equation (1), the number of parameters in Q_S is |S|(|S|-1) rather than (|S|-1)^2, as each of the |S| alternatives has a transition rate to the other |S|-1 alternatives. -Below equation (3), I think you mean j in S rather than 1<= j <= |S|, as S may not be {1,2,...,|S|}. Later I noticed that you always index S with {1,dots,|S|}, but using i in S in combination with 1<=j<=|S| was a bit confusing. -X_i as the i-th element of S is a bit of an abuse of notation, as it surpasses dependence on S -In Figure 1, you show X_0 in a vector that is referred to as *S.* It is my understanding that X_0 represents user features. As the user is not in the set, this is confusing. The use of a vertical ellipsis to connect \rho(X_0) to \rho(X_1) is also confusion, as \rho(X_1) is input into the Cartesian product while X_0 is input into the direct sum. Overall, nice job! Really enjoyed the paper and approach, good to see connections made between these literatures so that progress in discrete choice can be used at scale."}
{"id": "iclr2020_743", "title": "Towards Stable and comprehensive Domain Alignment: Max-Margin Domain-Adversarial Training | OpenReview", "abstract": "Abstract:###Domain adaptation tackles the problem of transferring knowledge from a label-rich source domain to an unlabeled or label-scarce target domain. Recently domain-adversarial training (DAT) has shown promising capacity to learn a domain-invariant feature space by reversing the gradient propagation of a domain classifier. However, DAT is still vulnerable in several aspects including (1) training instability due to the overwhelming discriminative ability of the domain classifier in adversarial training, (2) restrictive feature-level alignment, and (3) lack of interpretability or systematic explanation of the learned feature space. In this paper, we propose a novel Max-margin Domain-Adversarial Training (MDAT) by designing an Adversarial Reconstruction Network (ARN). The proposed MDAT stabilizes the gradient reversing in ARN by replacing the domain classifier with a reconstruction network, and in this manner ARN conducts both feature-level and pixel-level domain alignment without involving extra network structures. Furthermore, ARN demonstrates strong robustness to a wide range of hyper-parameters settings, greatly alleviating the task of model selection. Extensive empirical results validate that our approach outperforms other state-of-the-art domain alignment methods. Additionally, the reconstructed target samples are visualized to interpret the domain-invariant feature space which conforms with our intuition.", "review": "Review:###This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives. Pros: - The writing is good - Satisfactory empirical results Cons: - The proposed method is very similar to certain methods in the literature Detail comments: (1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited. (2) Experiments - How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available. - What does the * in DRCN* mean in Table 1? - ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin. (3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition. Typos: - In Eq.(1), there is a missing D in the first term. D_s should be mathcal{D}_s to match previous notation. - In Eq.(2), the *0,* is not meaningful given the definition of []^+. Refs - Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR*06) (Vol. 2, pp. 1735-1742). IEEE. - Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham. - Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236. # Update after rebuttal Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are *totally different* can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly."}
{"id": "iclr2020_744", "title": "RNA Secondary Structure Prediction By Learning Unrolled Algorithms | OpenReview", "abstract": "Abstract:###In this paper, we propose an end-to-end deep learning model, called E2Efold, for RNA secondary structure prediction which can effectively take into account the inherent constraints in the problem. The key idea of E2Efold is to directly predict the RNA base-pairing matrix, and use an unrolled constrained programming algorithm as a building block in the architecture to enforce constraints. With comprehensive experiments on benchmark datasets, we demonstrate the superior performance of E2Efold: it predicts significantly better structures compared to previous SOTA (29.7% improvement in some cases in F1 scores and even larger improvement for pseudoknotted structures) and runs as efficient as the fastest algorithms in terms of inference time.", "review": "Review:###RNA Secondary Structure Prediction by Learning Unrolled Algorithms This paper proposes E2Efold, which is an RNA secondary structure prediction algorithm based on an unrolled algorithm. Previous methods rely on dynamic programming (which does not work for molecular configurations that do not factorize) or rely on energy-based models (which require a minimization step, e.g. by using MCMC to traverse the energy landscape and find minima). The former does not work for all molecules and the latter can be difficult to optimize. The method presented here is novel, shows strong SOTA performance, and would be of interest to the wider deep learning community. The method is based on an unrolled algorithm, which is motivated by the inclusion of three inductive biases / constraints important underlying RNA folding. These constraints limit the wide RNA search space. The first component of the method is a “Deep Score Network” which uses a stack of Transformer encoders (with relative and exact positional embeddings) followed by 2D convolutional layers to output a L x L symmetric matrix describing the “scores” of base pairing. As these scores may not obey the rules of RNA folding, a second post-processing network is trained end-to-end together with the “Deep Score Network” to enforce constraints. This network starts with a transformation that symmetrizes the matrix and applies a constraint-enforcing mask. The problem is transformed into an unconstrained problem by using Lagrange multipliers; it is then solved using a proximal gradient. Finally, a recurrent cell is defined that implements this algorithm in a deep learning framework. This method is creative, could be applied to other tasks with constraints, and would be interesting to the wider deep learning community. In addition to developing the deep score network and post-processing network, the authors also develop a differentiable F1 loss, so that the network can directly optimize for precision and recall on the task. The performance of this method significantly outperforms previous methods. There was a fruitful discussion on OpenReview regarding whether this was a result of overfitting on the task. Indeed, it is critical in deep learning applications to carefully construct train/test sets to avoid high performance by memorization alone. To address this, the authors train on RNAStralign and test on ArchiveII. As the original ArchiveII dataset contains subsequences of other RNA sequences, which can result in overfitting, the authors re-ran their experiment with that removed, and similar results were achieved. To support the hypothesis that ArchiveII and RNAStralign capture different distributions, they perform a permutation test on the unbiased empirical Maximum Mean Discrepancy estimator, finding that the distributions are different. I do wonder why they did not check if P(ArchiveII) = P(ArchiveII) as they do check if P(RNAStr_train) = P(RNAStr_train). On the specific task of pseudoknot prediction, the method also performs well (F1 is >0.23 over the baseline). On sequence length-weighted F1, the model does even better. The paper is rich with ablations. The analysis of the number of unrolling iterations T helps support the use of an unrolled method and builds intuition for its importance - it would be useful to include this in the appendix of the paper. I also appreciated the visualizations, which are a good sanity check that the model correctly handles pseudoknots. The performance of the method is broken down by RNA family, which is also quite interesting -- the method outperforms LinearFold on all classes, besides 5S RNA, SRP, and Group I intron. Further analysis is required to better understand why the method is weaker on those datasets. Additionally, further work should explore training on one set of families and testing on a held-out set of families. This was pointed out by public comments on this paper. This is potentially a limitation of E2EFold (the authors do not seem to have tried this suggested experiment) and further exploration is required. Exploring this limitation (even if it is not overcome) would make this paper even more rich. That said, I recommend acceptance of this paper due to the extensive experiments, polished writing, novel method, and strong results, which can inspire future research."}
{"id": "iclr2020_745", "title": "Pragmatic Evaluation of Adversarial Examples in Natural Language | OpenReview", "abstract": "Abstract:###Attacks on natural language models are difficult to compare due to their different definitions of what constitutes a successful attack. We present a taxonomy of constraints to categorize these attacks. For each constraint, we present a real-world use case and a way to measure how well generated samples enforce the constraint. We then employ our framework to evaluate two state-of-the art attacks which fool models with synonym substitution. These attacks claim their adversarial perturbations preserve the semantics and syntactical correctness of the inputs, but our analysis shows these constraints are not strongly enforced. For a significant portion of these adversarial examples, a grammar checker detects an increase in errors. Additionally, human studies indicate that many of these adversarial examples diverge in semantic meaning from the input or do not appear to be human-written. Finally, we highlight the need for standardized evaluation of attacks that share constraints. Without shared evaluation metrics, it is up to researchers to set thresholds that determine the trade-off between attack quality and attack success. We recommend well-designed human studies to determine the best threshold to approximate human judgement.", "review": "Review:###With the recently growth in interest in adversarial examples for natural language, this paper takes an important step back in asking *do the adversarial examples we generate actually respect invariances of meaning*. The authors first give a list of constraints which attacks against NLP systems might have to satisfy, and then evaluate 2 recently proposed attacks on NLP classifiers to examine whether they satisfy some of the proposed constraints. The authors show that the attacks actually make more errors of a type that are detectable by automatic grammar checkers, and that humans do not reliably confirm that the proposed attacks preserve the semantics of the original sentence. Further, humans achieve higher than chance performance when telling perturbed text from original text. Finally, the authors show that it is important to control how much tolerance is allowed when considering attack success. While I am very sympathetic to the aims of this paper, and I feel like it makes an important message (that there is no such thing as an imperceptible manipulation of text), I feel the execution of the paper is somewhat lacking, and the experiments need a lot of tightening up before the paper is ready to be accepted. Further, the writing of the paper is very imprecise at times which is an issue especially if this paper is setting out to standardise terminology. In addition, I feel like the authors are too willing to abandon hope of semi-automated measurement of constraint satisfaction. While it is true that human evaluation is the gold standard, many of the constraints the authors propose could be amenable to automation, and I feel like this deserves further discussion. For this reason, I feel like this paper needs one revision cycle before resubmission, but with tighter writing would be a good addition to the adversarial text literature. We will start with the imprecise writing: this mainly concerns section 3: The whole of section 3.1 uses the word *morphology* in a very loose sense. The authors seem to mainly use *morphology* as a synonym for *surface distance*, but in linguistics, morphology has a strict meaning; importantly, small surface changes to a word can dramatically change its morphological categories, or even its root word. To say, therefore, that small surface distances are *morphology-preserving* is highly inaccurate. I highly recommend not using the word *morphology* for this kind of constraint. Further, the first sentence of this section: *the attacker is willing to chance the semantics of the input as long as all changed sentences read the same to a human*, is difficult to understand. By definition, if the semantics is noticeably different, the sentences no longer *read the same* to a human. Perhaps the intended meaning is that humans are able to repair surface errors using context? Evaluating whether generated text fulfills some semantic specification is well-studied problem in NLG. The crafters of the message know exactly what semantic content they wish to convey. Depending on the form of the semantic representation, one can attempt to parse the output message back into a semantic representation, and compare this with the desired input semantic representation. Or, if the attacker has examples of output with the intended semantics, one can use well-beloved overlap based metrics like BLEU, ROUGE or METEOR to evaluate whether the generated text has the intended semantics. While all automatic metrics have their problems, they are used too widely to be ignored completely. The non-suspicious constraint can also similarly be automatically measured. For instance, recent work on detecting computer-generated news articles have used large pretrained language models to discrminate between human-written and computer-generated articles [1], which is a technique which shows some promise. Therefore, claiming that *evaluation of the non-suspicious constraint must be done by humans* seems wide of the mark. I feel the experimental section could do with a lot more examples. For instance, examples of each category of grammatical error for both models should be included, and examples of sentences which annotators judged to consistent have changed meaning. In addition, in Section 4.2, satisficing behaviour is well known in crowdsourcing surveys, and not including check questions to prevent this kind of behaviour can throw doubt on the conclusions that can be drawn. Also, one notes that when the intent of the question is reversed, the average human response is roughly 5-x, where x is the original human response. This, to me, shows that exactly how one asks the question is somewhat irrelevant. [1] Defending Against Neural Fake News, Zellers et al. 2019"}
{"id": "iclr2020_746", "title": "Robust Learning with Jacobian Regularization | OpenReview", "abstract": "Abstract:###Design of reliable systems must guarantee stability against input perturbations. In machine learning, such guarantee entails preventing overfitting and ensuring robustness of models against corruption of input data. In order to maximize stability, we analyze and develop a computationally efficient implementation of Jacobian regularization that increases classification margins of neural networks. The stabilizing effect of the Jacobian regularizer leads to significant improvements in robustness, as measured against both random and adversarial input perturbations, without severely degrading generalization properties on clean data.", "review": " The main contribution of this paper is that it proposed an estimator of Jacobian regularization term for neural networks to reduce the computational cost reduced by orders of magnitude, and the estimator is mathematically proved unbiased. In details, the time consumed for the application of Jacobian regularizer and the unbiasedness of the proposed estimator are proved mathematically. Then the author experimentally demonstrated that the proposed regularization term retains all the practical benefits of the exact method but with a low computation cost. Quantitative experiments are provided to illustrate that the proposed Jacobian regularizer does not adversely affect the model, can be used simultaneously with other regularizers and effectively improve the model*s robustness against random and adversarial input perturbations. In general, this paper was well organized and it is also great that an efficient approximation of the Jacobian regularizer can be derived. However, the paper was written in a quite misleading or over-claimed way. The major comments are as follows: (1) It is not new to use Jacobian regularizer for improving the robustness learning. Such idea has been elaborated in [1] (though it was cited in the paper). The main contribution is, the paper proposed an efficient approximated way to the exact Jacobian term. All the benefits of robust learning are rooted in the Jacobian regularization. (2) In light of point (1), the title of this paper was first quite misleading. Robust learning with Jacobian was not proposed by the paper, so the title makes no sense. Instead, efficient approximation should be emphasized. (3) Most of the advantages shown in the quantitative experiments are the benefits of Jacobian regularization! The authors should focus on the approximating algorithm rather than the merits of Jacobian regularization which has been discussed in [1]. In another word, it is better to add more comparison based on running time so as to illustrate the significant performance between the proposed regularizer and the exact one. (4) In section 3, it may be interesting to see more comparison results if two regularizers were combined, e.g., (L2+Dropout, L2+Jacobianor Dropout+Jacobian). [1] Varga, Dániel, Adrián Csiszárik, and Zsolt Zombori. *Gradient regularization improves accuracy of discriminative models.* arXiv preprint arXiv:1712.09936 (2017). =============== I have read the authors* response. I would have to say that, though I enjoy reading the paper and the other parts of this paper are very good, the paper*s contribution/novelty may be limited because Jacobian regularization has been earlier thoroughly discussed in [1] and even another recent paper published in ECCV 2018. I am afraid that I would firmly keep my original rating."}
{"id": "iclr2020_747", "title": "Towards a Unified Evaluation of Explanation Methods without Ground Truth | OpenReview", "abstract": "Abstract:###This paper proposes a set of criteria to evaluate the objectiveness of explanation methods of neural networks, which is crucial for the development of explainable AI, but it also presents significant challenges. The core challenge is that people usually cannot obtain ground-truth explanations of the neural network. To this end, we design four metrics to evaluate the explanation result without ground-truth explanations. Our metrics can be broadly applied to nine benchmark methods of interpreting neural networks, which provides new insights of explanation methods.", "review": "Review:###The manuscript proposes an evaluation methodology to assess objectively methods for model explanation without the need of ground truth. This methodology is based on four metrics evaluating different aspects of the generated explanation. These four metrics evaluated the bias of the explanation map at the pixel level, quantify the unexplainable features components, robustness of the explanation to spatial masking, and mutual verification of different explanation methods. Experiments on the CIFAR10 an Pascal VOC 2012 considering a good set of representative explanation methods show the strenghts and weaknesses of the proposed method. Given the recent proposal of a significant number of works aiming a model explanation, the manuscript touches the critical point of assessing objectively the performance of these methods. Overall the manuscript has a good presentation, and its content, to great extent, is clear and easy to follow and good formal preentation of the proposed method is given. I appreciate that the experiments section covers two datasets and various landmark methods for model explanation. My main concerns with the manuscript are as follows: In several parts of the manuscript they were sentences refering to some explanation methods as ables to reflect/diagnose the *logic of a CNN/model/network*. I could not help but all the time that I encountered the expression *logic* ideas related to reasoning or internal decision-making process came to my mind. This really affected the flow of the content, and with all due respect, none of the explanations generated by existent methods is able to faithfully reflect the decision-making process of the models being explained. The majority are oriented towards generating approximations and indicating relevant regions of the input data. Along the direction of objective evaluation of explanation methods, Oramas et al. ICLR*19 and more recently Yang and Kim, arXiv:1907.09701 proposed methodologies aimed at quantitative evaluations of explanation methods. Given the common objectives that these works have with the manuscript, it would be strenghten the manuscript to positition itself wrt. these methods in the related work section. In Sec.3.3 the proposed method quantifies the amount of unexplainable regions, i.e. regions with low absolute attribution, of different methods. However, unless I missed something, it seems that these unexplainable regions are native of the explanation methods but native by the masking step introduced by the proposed metric. In addition, this metric relies on a pre-defined threshold which determines the number of pixels to be considered. Could you motivate how this threshold value was selected? what would be the effect of selecting different values? The metric proposed in Sec. 3.4 works under the assumption that a explanation method which is robust to spatial masking, can be considered more convincing. More specifically, it assumes that explanations from occluded versions of the same input should be similar. Do this metric takes into account the prediction produced by the model to be explained? if these predictions differ, assumming that the explanations should be similar might not be correct. Could you comment on this? The multual verification metric proposed in Sec. 3.5: assumes that methods for explanation are more reliable if they produce similar heatmaps. I think this assumption might be incorrect. For instance, Adebayo et al., NIPS*18 showed that several methods were able to produce very similar explanations. Yet, all these explanations were wrong since they were all biases towards edge-like structures of the input data and were not faithful to the decision-making process of the model being explained. Under the proposed mutual verification metric, these biased methods would have been flagged as objectively reliable. In the evaluation, when conducting experiments on PascalVOC 2012, could you motivate why cropping the images is necessary? Isn*t this removing some background clutter that could pose an interesting scenario on which the performance of explanation methods could be analyzed? The manuscript had a good detailed and motivated beginning. In comparison to its beginning its conclusion was somewhat abrupt. Discussions presented in the evaluation section related to the four proposed metrics were very shallow and most of the time pointed towards the supplementary material. In this regard, it is not clear how much of an insight we got from the evaluated explanation methods. Moreover, having this relevant part in the supplementary, gives the impression that the manuscript is not self-contained. Perhaps some re-organization of the content might be needed. Finally, the manuscript claims the proposed method to be sufficiently general to be applied to different method. However, as was evidenced in the experiments, there were some methods, e.g. CAM, grad-CAM, Pert, where some of the proposed metrics were not applicable. Therefore the claims related to the generality of the proposed method should be relaxed."}
{"id": "iclr2020_748", "title": "Differentially Private Survival Function Estimation | OpenReview", "abstract": "Abstract:###Survival function estimation is used in many disciplines, but it is most common in medical analytics in the form of the Kaplan-Meier estimator. Sensitive data (patient records) is used in the estimation without any explicit control on the information leakage, which is a significant privacy concern. We propose a first differentially private estimator of the survival function and show that it can be easily extended to provide differentially private confidence intervals and test statistics without spending any extra privacy budget. We further provide extensions for differentially private estimation of the competing risk cumulative incidence function. Using nine real-life clinical datasets, we provide empirical evidence that our proposed method provides good utility while simultaneously providing strong privacy guarantees.", "review": "Review:###The authors present a method that guarantees differential privacy for survival function estimates using Kaplan-Meier. They show their results in nine different datasets, and their survival curves are similar to the original survival curves while protecting patient privacy. There have been a number of survival functions with differential privacy discussion since Nguyen and Hui. Please check other papers that cite it. Overall, the writing is choppy, where ideas that should be a single sentence is separated into multiple sentences. For example, in the first few sentences of the Introduction, we have *An adversary, with only access to the published estimates... Effectively leading to the disclosure of sensitive information*, which should just be a single sentence. There are many instances of this throughout the paper, and they should be edited prior to re-submission. The authors claim that GANs do not work well for their application, but it would be more convincing if there are citations or experiments tha t can back this up. The authors write that their method *provides good utility*. They should explicitly state the metric of evaluation, as well as the increase in performance clearly in both the abstract and the introduction. Section 2.1 introducing the survival function is not necessary. Maybe just 2-3 sentences on the K-M estimator."}
{"id": "iclr2020_749", "title": "Unaligned Image-to-Sequence Transformation with Loop Consistency | OpenReview", "abstract": "Abstract:###We tackle the problem of modeling sequential visual phenomena. Given examples of a phenomena that can be divided into discrete time steps, we aim to take an input from any such time and realize this input at all other time steps in the sequence. Furthermore, we aim to do this \textit{without} ground-truth aligned sequences --- avoiding the difficulties needed for gathering aligned data. This generalizes the unpaired image-to-image problem from generating pairs to generating sequences. We extend cycle consistency to \textit{loop consistency} and alleviate difficulties associated with learning in the resulting long chains of computation. We show competitive results compared to existing image-to-image techniques when modeling several different data sets including the Earth*s seasons and aging of human faces.", "review": "Review:###This paper proposes loop consistency on the base of cycle consistency and alleviate difficulties associated with learning in the resulting long chains of computation. Like CycleGAN, the method proposed in this paper does not require data to be specifically matched. This paper still has the following problems? 1?For the time series images, the reason for processing with a single generator is not clearly stated. In CycleGAN, the two transformations use two different generators, respectively, and use the cycle consistency loss to force the results to converge. In the examples presented in this paper, such as Face Aging, it isn*t a process that can be reversed. This paper does not explain why the use of a single generator has been effective. 2?To be more convincing, this article needs to be tested on more baselines. The indicators provided in this article are not objective enough. 3?This article also does not give the training computational complexity and testing time cost of the proposed method."}
{"id": "iclr2020_750", "title": "Natural Language State Representation for Reinforcement Learning | OpenReview", "abstract": "Abstract:###Recent advances in Reinforcement Learning have highlighted the difficulties in learning within complex high dimensional domains. We argue that one of the main reasons that current approaches do not perform well, is that the information is represented sub-optimally. A natural way to describe what we observe, is through natural language. In this paper, we implement a natural language state representation to learn and complete tasks. Our experiments suggest that natural language based agents are more robust, converge faster and perform better than vision based agents, showing the benefit of using natural language representations for Reinforcement Learning.", "review": "Review:###This work advocates for using natural language instead of visual features to represent state in reinforcement learning. Using VizDoom as a testbed, the authors extract semantic states (e.g. how many entities of which type are where) from the game. These semantic states are then edited using a grammar into language descriptions for the learning agent. The authors compare models that rely on these language representations to those that rely on raw pixels and semantic maps and find that for most cases, the language representations outperform visual representations. While the hypothesis that natural language representations is an effective way to parametrize the state space is interesting, the experiments in this paper do not test this hypothesis. This is due to the following reasons: 1. The authors do not use *natural* language, since the language here is generated from a simple template. 2. Because the language is not natural, they correspond to precise, unambiguous, state representations (e.g. there is no difference from the model*s perspective whether the utterance is *you have high health* vs. *feat[*high health*] == 1*), which fundamentally simplifies the learning process --- the vision-based models have to learn how to extract precise state representations, whereas the *language* models are given these precise state representations. The second point is particular important. For many learning problems (including this VizDoom experiment in the paper), language descriptions do not occur naturally. There are some works that assume that the model can produce language descriptions, which it then uses to facilitate learning (see https://arxiv.org/abs/1806.02724). However, in this case, the language descriptions are being provided by humans. If I understand correctly, the authors are comparing models that learn from raw pixels (e.g. baselines) to models that are fed precise, hand-extracted states (e.g. proposed). The results then are not surprising, nor do they test the hypothesis. The takeaway for me from this paper is that the authors engineered a clever feature extractor for VizDoom, and show that the state composed from the extracted features are more sample efficient to learn from compared to raw pixel values. I also have some feedback regarding the writing: - The introduction is too verbose. The paragraphs are disjoint, with independent overviews on deep learning, semantics, and NLP. Out of the 5 paragraphs, only the last one talks about the content of the paper. The introduction does not go into any detail about what experiments were actually run, and what the results are."}
{"id": "iclr2020_751", "title": "Attributes Obfuscation with Complex-Valued Features | OpenReview", "abstract": "Abstract:###The paper studies the possibility of hiding sensitive input information from the intermediate-layer features without too much accuracy degradation. We propose a generic method to revise a conventional neural network to boost the challenge of adversarially inferring about the input but still yields useful outputs. In particular, the method transforms real-valued features into complex-valued ones, in which the input information is hidden in a randomized phase of the transformed features. The knowledge of the phase acts like a key, with which any party can easily recover the prediction from the processing result, but without which the party can neither recover the output nor distinguish the original input. Preliminary experiments on various datasets and network structures have shown that our method significantly diminishes the adversary*s ability in inferring about the input while largely preserves the accuracy of the predicted outcome.", "review": "Review:###After rebuttal, I really appreciate the authors* effort during the rebuttal, and most of my concerns are addressed well. === Summary: This paper proposed a complex-valued neural network to protect the input data from hidden features of DNNs. Specifically, the authors introduce (1) encoder: producing a complex-valued feature, (2) processing module: extracting useful features for a decoder and (3) decoder: making a final decision from processed features. Using various deep architectures and datasets, the authors showed that the proposed method can hide the input data from hidden features while maintaining the performance of DNNs. Detailed comments: The research topic and main idea of this paper (i.e. introducing a complex-valued neural network for hiding sensitive input data) are interesting and the authors showed that the proposed idea indeed works well using various neural architectures and datasets. It would be more interesting if the authors can consider NLP datasets or other tasks instead of classification. Overall, the paper is well-written and the ideas are novel."}
{"id": "iclr2020_752", "title": "Octave Graph Convolutional Network | OpenReview", "abstract": "Abstract:###Many variants of Graph Convolutional Networks (GCNs) for representation learning have been proposed recently and have achieved fruitful results in various domains. Among them, spectral-based GCNs are constructed via convolution theorem upon theoretical foundation from the perspective of Graph Signal Processing (GSP). However, despite most of them implicitly act as low-pass filters that generate smooth representations for each node, there is limited development on the full usage of underlying information from low-frequency. Here, we first introduce the octave convolution on graphs in spectral domain. Accordingly, we present Octave Graph Convolutional Network (OctGCN), a novel architecture that learns representations for different frequency components regarding to weighted filters and graph wavelets bases. We empirically validate the importance of low-frequency components in graph signals on semi-supervised node classification and demonstrate that our model achieves state-of-the-art performance in comparison with both spectral-based and spatial-based baselines.", "review": "Review:###Despite reading the paper multiple times, I am not sure I have the background to know whether what is written is significant or not. I*m aware of work on general semi-supervised learning and see the much better performance of this approach compared to things like label propagation, but cannot say for sure whether the idea is novel/significant. One q for authors -- I don*t understand the core component of the proposal, is the key ingredient that have different weighting between low vs high that causes the better performance on tasks ? or is that we have less dependencies across variables (as reflected in the computational costs) that gets the better performance ?"}
{"id": "iclr2020_753", "title": "Learn Interpretable Word Embeddings Efficiently with von Mises-Fisher Distribution | OpenReview", "abstract": "Abstract:###Word embedding plays a key role in various tasks of natural language processing. However, the dominant word embedding models don*t explain what information is carried with the resulting embeddings. To generate interpretable word embeddings we intend to replace the word vector with a probability density distribution. The insight here is that if we regularize the mixture distribution of all words to be uniform, then we can prove that the inner product between word embeddings represent the point-wise mutual information between words. Moreover, our model can also handle polysemy. Each word*s probability density distribution will generate different vectors for its various meanings. We have evaluated our model in several word similarity tasks. Results show that our model can outperform the dominant models consistently in these tasks.", "review": "Review:###Summary: This paper proposed a variational word embedding method, by using vMF distribution as prior and adding an entropy regularization term. Strengths: [+] In the experiment parts, the authors describe the experiment settings in detail. [+] Detailed descriptions for each equation is given. Weaknesses: [-] Template: It seems that the authors use the wrong template, which is not the template for ICLR2020. [-] Appendix: Although the author mentioned *appendix* many times, I cannot see the appendix. [-] Motivation: The connection between motivation and proposed method seems weak. The authors argue *interpretable* in their title and abstract, but their method and experiments do not show this point explicitly. Questions [.] The experiments seem a little weak. The vocabulary size is 10K and the corpus is not so big, and I wonder whether the performance of the proposed method will be better for large corpus and longer training time. [.] For Equation 8, we should have a guarantee on the concentration of partition functions. Is it still true for vMF distribution? [.] What is the advantage of vMF distribution?"}
{"id": "iclr2020_754", "title": "A critical analysis of self-supervision, or what we can learn from a single image | OpenReview", "abstract": "Abstract:###We look critically at popular self-supervision techniques for learning deep convolutional neural networks without manual labels. We show that three different and representative methods, BiGAN, RotNet and DeepCluster, can learn the first few layers of a convolutional network from a single image as well as using millions of images and manual labels, provided that strong data augmentation is used. However, for deeper layers the gap with manual supervision cannot be closed even if millions of unlabelled images are used for training. We conclude that: (1) the weights of the early layers of deep networks contain limited information about the statistics of natural images, that (2) such low-level statistics can be learned through self-supervision just as well as through strong supervision, and that (3) the low-level statistics can be captured via synthetic transformations instead of using a large image dataset.", "review": "Review:###The paper studies self-supervised learning from very few unlabeled images, down to the extreme case where only a single image is used for training. From the few/single image(s) available for training, a data set of the same size as some unmodified reference data set (ImageNet, Cifar-10/100) is generated through heavy data augmentation (cropping, scaling, rotation, contrast changes, adding noise). Three popular self-supervised learning algorithms are then trained on this data sets, namely (Bi)GAN, RotNet, and DeepCluster, and the linear probing accuracy on different blocks is compared to that obtained by training the same methods on the reference data sets. The linear probing accuracy from the first few conv layers of the network trained on the single/few image data set is found to be comparable to or better than that of the same model trained on the full reference data set. I enjoyed the paper; it addresses the interesting setting of an extremely small data set which complements the large number of studies on scaling up self-supervised learning algorithms. I think it is not extremely surprising that using the proposed strategy allows to learn low level features as captured by the first few layers, but I think it is worth studying and quantifying. The experiments are carefully described and presented, and the paper is well-written. Here are a few questions and concerns: - How much does the image matter for the single-image data set? The selected images A and B are of very high entropy and show a lot of different objects (image A) and animals (image B). How do the results change if e.g. a landscape image or an abstract architecture photo is used? - How general is the proposed approach? How likely is it to generalize to other approaches such as Jigsaw (Doersch et al., 2015) and Exemplar (Dosovitskiy et al., 2016)? It would be good to comment on this. - [1] found that the network architecture for self-supervised learning can matter a lot, and that by using a ResNet architecture, performance of SSL methods can be significantly improved. In particular, the linear probing accuracy appears to be often monotonic as a function of the depth of the layer it is computed from. This is in contrast to what is observed for AlexNet in Tables 2 and 3, where the conv5 accuracy is lower than the conv4. It would therefore be instructive to add experiments for ResNet to see how well the results generalize to other network architectures. - Does the MonoGAN exhibit stable training dynamics comparable to training WGAN on CIFAR-10, or do the training dynamics change on the single-image data set? Overall, I’m leaning towards accepting the paper, but it would be important to see how well the experiments generalize to i) ResNet and ii) other (lower entropy) input images. [1] Kolesnikov, A., Zhai, X. and Beyer, L., 2019. Revisiting self-supervised visual representation learning. arXiv preprint arXiv:1901.09005. --- Update after rebuttal: I thank the authors for their detailed response. I appreciate the efforts of the authors into investigating the issues raised, the described experiments sound promising. Unfortunately, the new results are not presented in the revision. I will therefore keep my rating."}
{"id": "iclr2020_755", "title": "Learning from Partially-Observed Multimodal Data with Variational Autoencoders | OpenReview", "abstract": "Abstract:###Learning from only partially-observed data for imputation has been an active research area. Despite promising progress on unimodal data imputation (e.g., image in-painting), models designed for multimodal data imputation are far from satisfactory. In this paper, we propose variational selective autoencoders (VSAE) for this task. Different from previous works, our proposed VSAE learns only from partially-observed data. The proposed VSAE is capable of learning the joint distribution of observed and unobserved modalities as well as the imputation mask, resulting in a unified model for various down-stream tasks including data generation and imputation. Evaluation on both synthetic high-dimensional and challenging low-dimensional multi-modality datasets shows significant improvement over the state-of-the-art data imputation models.", "review": " The paper proposed variational selective autoencoders (VSAE) to learn from partially-observed multimodal data. Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws. See below for detailed comments. [Pros] 1. The main idea of the paper is to propose a generative model that can handle partially-observed multimodal data during training. Specifically, prior work considered non-missing data during training, while we can*t always guarantee that all the modalities are available. Especially in the field of multimodal learning, we often face the issue of imperfect sensors. This line of work should be encouraged. 2. In my opinion, the idea is elegant. The way the author handles the missingness is by introducing an auxiliary binary random variable (the mask) for it. Nevertheless, its presentation and Figure 1 makes this elegant idea seems over-complicated. [Cons] 1. [The claim] One of my concerns for this paper is the assumption of the factorized latent variables from multimodal data. Specifically, the author mentioned Tsai et al. assumed factorized latent variables from the multimodal data, while Tsai et al. actually assumed the generation of multimodal data consists of disentangled modality-specific and multimodal factors. It seems to me; the author assumed data from one modality is generated by all the latent factors (see Eq. (11)), then what is the point for assuming the prior of the latent factor is factorized (see Eq. (4) and (5))? One possible explanation is because we want to handle the partially-observable issues from multimodal data, and it would be easier to make the latent factors factorized (see Eq. (6)). The author should comment on this. 2. [Phrasing.] There are too many unconcise or informal phrases in the paper. For example, I don*t understand what does it mean in *However, if training data is complete, ..... handle during missing data during test.* Another example would be the last few paragraphs on page 4; they are very unclear. Also, the author should avoid using the word *simply* too often (see the last few paragraphs on page 5). 3. [Presentation.] The presentation is undesirable. It may make the readers hard to follow the paper. I list some instances here. a. In Eq. (3), it surprises me to see the symbol epsilon without any explanation. b. In Eq. (6), it also surprises me to see no description of phi and psi. The author should also add more explanation here, since Eq. (6) stands a crucial role in the author*s method. c. Figure 1 is over-complicated. d. What is the metric in Table 1 and 2? The author never explains. E.g., link to NRMSE and PFC to the Table. e. What are the two modalities in Table 2? The author should explain. f. The author completely moved the results of MNIST-SVHN to Supplementary. It is fine, but it seems weird that the author still mentioned the setup of MNIST+SVHN in the main text. g. The author mentioned, in Table , the last two rows serve the upper bound for other methods. While some results are even better than the last two rows. The author should explain this. h. Generally speaking, the paper does require a significant effort to polish Section 3 and 4. 4. [Experiments.] The author presented a multimodal representation learning framework for partially-observable multimodal data, while the experiments cannot corraborrate the claim. First, I consider the tabular features as multi-feature data and less to be the multimodal data. Second, the synthetic image pairs are not multimodal in nature. These synthetic setting can be used for sanity check, but cannot be the main part of the experiments. The author can perhaps consider the datasets used by Tsai et al. There are seven datasets, and they can all be modified to the setting of partially-observable multimodal data. Also, since the synthetic image pairs are not multimodal in nature, it is unclear to me for what the messages are conveyed in Figure 3 and 4. I do expect the paper be a strong submission after a significant effort in presentation and experimental designs. Therefore, I vote for weak rejection at this moment."}
{"id": "iclr2020_756", "title": "Implicit Rugosity Regularization via Data Augmentation | OpenReview", "abstract": "Abstract:###Deep (neural) networks have been applied productively in a wide range of supervised and unsupervised learning tasks. Unlike classical machine learning algorithms, deep networks typically operate in the overparameterized regime, where the number of parameters is larger than the number of training data points. Consequently, understanding the generalization properties and the role of (explicit or implicit) regularization in these networks is of great importance. In this work, we explore how the oft-used heuristic of data augmentation imposes an implicit regularization penalty of a novel measure of the rugosity or “roughness” based on the tangent Hessian of the function fit to the training data.", "review": "Review:###This paper shows that a penalty term called rugosity captures the implicit regularization effect of deep neural networks with ReLU (and piecewise affine in general) activation. Roughly, rugosity measures how far the function parametrized as a deep network deviates from a locally linear function. The paper starts by showing that the amount of training loss increased from adding data augmentation is upper bounded in terms of (roughly) a Monte Carlo approximate to a Hessian based measure of rugosity. It then formally derives this measure of rugosity for networks with continuous piecewise affine activations. Finally, experimental evaluation for classification tasks on MNIST, SVHN and CIFAR shows that data augmentation indeed reduces the rogusity by a significant amount particularly when using the ResNet structure. A somehow surprising message is, however, that if one imposes explicit regularization with rugosity in lieu of data augmentation, then the better generalization usually seen from data augmentation no longer presents, though one does get a network with smaller rugosity. Comments: It is quite interesting to see that the rugosity measure proposed in the paper captures at least some aspects of the implicit regularization effect of data augmentation both in terms of theory (i.e. Theorem 1) and practical observations. My feeling is that rugosity is mostly a measure of the smoothness of the function parametrized by the neural network. From that perspective, how is the rugosity as a smoothness measurement for neural networks with piecewise affine activations different from the Lipschitz constant for general neural networks? My guess is that data augmentation also decreases the Lipschitz constant of a neural network near the training data points, but regardless of whether this is true or not, it is not clear if and how rugosity is better than Lipschitz constant for characterizing the implicit regularization of data augmentation. In addition, there have been many recent studies on showing that gradient penalty / Lipschitz regularization are useful for achieving better generalization and adversarial robustness, see e.g. [a,b,c]. The results in this paper on showing that regularizing rugosity does not improve accuracy seem to contradict with the conclusion of these prior studies. It is unclear to me whether this is caused by insufficient experimentation or if there is any fundamental difference between rugosity and Lipschitz regularization that I am missing. [a] Finlay et al., Lipschitz regularized deep neural networks generalize and are adversarially robust [b] Gouk et al., Regularisation of Neural Networks by Enforcing Lipschitz Continuity [c] Thanh-Tung et al., Improving generalization and stability of GANs"}
{"id": "iclr2020_757", "title": "The intriguing role of module criticality in the generalization of deep networks | OpenReview", "abstract": "Abstract:###We study the phenomenon that some modules of deep neural networks (DNNs) are more emph{critical} than others. Meaning that rewinding their parameter values back to initialization, while keeping other modules fixed at the trained parameters, results in a large drop in the network*s performance. Our analysis reveals interesting properties of the loss landscape which leads us to propose a complexity measure, called {em module criticality}, based on the shape of the valleys that connects the initial and final values of the module parameters. We formulate how generalization relates to the module criticality, and show that this measure is able to explain the superior generalization performance of some architectures over others, whereas earlier measures fail to do so.", "review": "Review:###This paper introduces a new way to reason about neural network generalization using a module criticality measure. The measure is tangible and intuitive. It leads to some formal bounds on the generalization of deep networks, and is able to better rank trained image classification architectures than previous measures. I am leaning to accept, as I expect this to be a significant theoretical contribution with several potential practical applications. With a few additional details, this could be a very strong submission: (1) Choice of module decomposition. Having each module be a single convolutional or fully-connected layer makes intuitive sense, but is there some theoretical motivation for this choice? If the only requirement for a module is that it includes some linear transformation, in the extreme, a module could consist of a single weight, or the entire network. Would those choices change the generalization bounds or relative criticality across different architectures? (2) Scope of experimental results. The ranking results would be much more compelling if they included a broader range of architectures, including more recent models with more branching, e.g., DenseNet. Is there some reason ResNet101 has higher generalization error than 18 and 34? Net. Criticality for ResNets is inversely correlated with the number of layers; is there an explanation for this? Is this true for other very deep models? (3) Practical use. To compute the criticality measure, we must train the model; but, if we train the model, we can compute generalization directly. So, what is the practical application of the measure? Is there some way it could be used to save computation? Could it help in the case of a small validation dataset, which we do not want to look at many times during model selection? Minor typos: - Section 2.2: “An stable phenomena” - Section 2.3: “…an the…” - In appendix: “ResNet101: ResNet34 architectures…” ---------------------------- After rebuttal: The authors have addressed my concerns, and I*ve increased my rating. There are still a few points I*d like to see addressed in the final version: 1. The fact that the approach cannot yet be applied to batch normalization is a big practical drawback. Some discussion of approaches you tried, why they didn*t work, and possible future directions for overcoming this would be appreciated. 2. Clarify in the paper that the *PAC Bayes* approach used for comparison (Table 1) is a your method, i.e., an ablated version of criticality. As is, someone reading the paper quickly may think all you*ve done is add an alpha parameter to an existing *PAC Bayes* approach, which does fairly well on its own. 3. Visualizing the experimental tables as scatterplots could make them easier for a reader to interpret."}
{"id": "iclr2020_758", "title": "Critical initialisation in continuous approximations of binary neural networks | OpenReview", "abstract": "Abstract:###The training of stochastic neural network models with binary ( ) weights and activations via continuous surrogate networks is investigated. We derive, using mean field theory, a set of scalar equations describing how input signals propagate through surrogate networks. The equations reveal that depending on the choice of surrogate model, the networks may or may not exhibit an order to chaos transition, and the presence of depth scales that limit the maximum trainable depth. Specifically, in solving the equations for edge of chaos conditions, we show that surrogates derived using the Gaussian local reparameterisation trick have no critical initialisation, whereas a deterministic surrogates based on analytic Gaussian integration do. The theory is applied to a range of binary neuron and weight design choices, such as different neuron noise models, allowing the categorisation of algorithms in terms of their behaviour at initialisation. Moreover, we predict theoretically and confirm numerically, that common weight initialization schemes used in standard continuous networks, when applied to the mean values of the stochastic binary weights, yield poor training performance. This study shows that, contrary to common intuition, the means of the stochastic binary weights should be initialised close to close to for deeper networks to be trainable.", "review": " The paper addresses a very important and relevant topic of initialisation of weights of neural networks. It builds up on highly celebrated results of Poole, Schoenholz and others, using the language of mean field theory and an approach rooted in Dynamical Systems. What the authors propose is an extension of the approach to other settings. The paper is very scientific and math-heavy. A good practice in such cases is to adhere to a format of a scientific mathematical paper and organise the material using Theorema, Lemmata, Propositions and Corollaries , Definitions and Proofs. Such language and framework exists for a reason - to structure the material and make the paper readable. The paper as is a stream of equations and discussion making it very unclear what the point is. In order for this paper to be suitable for publication the reviewer would like to strongly suggest: - Organise the material in a way that would make it clear what is claimed, what is proven etc. - Make more specific what the added value of the paper is. Also - for the contribution of the paper to be less incremental it would be valuable to add more formality to the original results, for example - Gaussian approximation is claimed without any sort of verification of assumptions of any version of CLT or Law of Large Numbers."}
{"id": "iclr2020_759", "title": "Domain-invariant Learning using Adaptive Filter Decomposition | OpenReview", "abstract": "Abstract:###Domain shifts are frequently encountered in real-world scenarios. In this paper, we consider the problem of domain-invariant deep learning by explicitly modeling domain shifts with only a small amount of domain-specific parameters in a Convolutional Neural Network (CNN). By exploiting the observation that a convolutional filter can be well approximated as a linear combination of a small set of basis elements, we show for the first time, both empirically and theoretically, that domain shifts can be effectively handled by decomposing a regular convolutional layer into a domain-specific basis layer and a domain-shared basis coefficient layer, while both remain convolutional. An input channel will now first convolve spatially only with each respective domain-specific basis to ``absorb* domain variations, and then output channels are linearly combined using common basis coefficients trained to promote shared semantics across domains. We use toy examples, rigorous analysis, and real-world examples to show the framework*s effectiveness in cross-domain performance and domain adaptation. With the proposed architecture, we need only a small set of basis elements to model each additional domain, which brings a negligible amount of additional parameters, typically a few hundred.", "review": " Summarize what the paper claims to do/contribute. * The paper proposes to perform domain adaptation via separating domain-specific and cross-domain features, by what is referred to as *domain-adaptive filter decomposition*. Each domain contributes its own share of features to be combined by a subsequent common layer. The method was benchmarked against competing methods on simple classification tasks and a hard semantic segmentation task. Clearly state your decision (accept or reject) with one or two key reasons for this choice. Weak Accept. * I think the paper was very well written, the explanations were clear and the technical contributions seem sound. * The experiments were satisfying for the most part. I would have wanted to see MNIST->SVHN for the unsupervised case as well, as this is a particularly hard one. Provide supporting arguments for the reasons for the decision. * Please do not use the Office dataset, it is commonly used in unsupervised domain adaptation papers, especially older ones, but it*s hard to tell anything about proposed methods from this dataset as there is label pollution and not enough samples per class to be used with neural nets. * For GTA->Cityscapes you are missing a few works eg, the CYCADA work. Also please use citations in the tables if you did not yourselves run experiments (as to make it clear that experimental protocols also might be slightly different etc). Provide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment. * Tables 1 & 2: It*d be better to not refer to methods as A1,2,3 but rahter with some specific names or explicitly describe what these abbreviations mean in the caption of the table."}
{"id": "iclr2020_760", "title": "Differentiable Bayesian Neural Network Inference for Data Streams | OpenReview", "abstract": "Abstract:###While deep neural networks (NNs) do not provide the confidence of its prediction, Bayesian neural network (BNN) can estimate the uncertainty of the prediction. However, BNNs have not been widely used in practice due to the computational cost of predictive inference. This prohibitive computational cost is a hindrance especially when processing stream data with low-latency. To address this problem, we propose a novel model which approximate BNNs for data streams. Instead of generating separate prediction for each data sample independently, this model estimates the increments of prediction for a new data sample from the previous predictions. The computational cost of this model is almost the same as that of non-Bayesian deep NNs. Experiments including semantic segmentation on real-world data show that this model performs significantly faster than BNNs, estimating uncertainty comparable to the results of BNNs.", "review": "Review:###The paper proposes a differentiable Bayesian neural network. Traditional BNN can model the model uncertainty and data uncertainty via adding a prior to the weight and assuming a Gaussian likelihood for the output y. However, it*s slow in practice since evaluating the loss function of BNN involves multiple runs over the entire network. Also when the input data is non-stationary, the output function can not be differentiated with respect to the input data. The paper proposes to use an online code vector histogram (OCH) method attached to the input and output of a classical DNN. Input OCH captures the distribution of both input data and network weights, and the output OCH captures the distribution of the predictions. Since these OCHs are differentiable, the proposed DBNN model can be used for streaming input data with time-variant distributions. I think the idea is interesting and novel. It explores a different way of modeling distributions with DNN. Instead of adding priors, DBNN relies on histograms, which is usually used to describe distributions for discrete observed input data. So the paper is well-motivated. 1. The paper needs more literature review in the area of data streaming. Papers, such as [1], have proposed to use a vector quantization process that can be applied online to a stream of inputs. This paper introduces the vector quantization but doesn*t mention the use of it in streaming data in related work, which kind of blurs the contribution a bit. Moreover, it would be helpful for readers to learn about useful techniques for streaming data from this paper. [1] Hervé Frezza-Buet. Online computing of non-stationary distributions velocity fields by an accuracy controlled growing neural gas 2. I think the paper might need a bit more explanation about codevector, since it*s not a very well-acknowledged concept in this field. The main issue for me to understand it is how to get these codevectors. When DBNN deals with streaming data and starts from no input, is the set of codevector empty at the beginning? The input data points are accumulated as codevectors? I hope the authors could clarify this process a bit more. 3. Given the insufficient understanding of codevector, figure 2 is a bit hard to read. 1) (a)-(d) are figures for x0 at t=0, which is not time-variant. 2) what are these codevectors picked. 3) It seems that the codevectors are out of the regime of the distribution of y. But according to algorithm 2, y_*<-T(c_*), would that be a problem? I think (a)-(d) are informative but not straightforward to read. The authors need to put more text to explain these figures, since this simulated example can help readers to understand what is codevector and how it helps for uncertainty estimation. Overall I think the paper is well-written. The idea is novel and practical in the scenario of DNN. I would vote for accept."}
{"id": "iclr2020_761", "title": "Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples | OpenReview", "abstract": "Abstract:###Few-shot classification refers to learning a classifier for new classes given only a few examples. While a plethora of models have emerged to tackle it, we find the procedure and datasets that are used to assess their progress lacking. To address this limitation, we propose Meta-Dataset: a new benchmark for training and evaluating models that is large-scale, consists of diverse datasets, and presents more realistic tasks. We experiment with popular baselines and meta-learners on Meta-Dataset, along with a competitive method that we propose. We analyze performance as a function of various characteristics of test tasks and examine the models’ ability to leverage diverse training sources for improving their generalization. We also propose a new set of baselines for quantifying the benefit of meta-learning in Meta-Dataset. Our extensive experimentation has uncovered important research challenges and we hope to inspire work in these directions.", "review": "Review:###The paper presents Meta-Dataset, a benchmark for few-shot classification that combines various image classification data sets, allows the number of classes and examples per class to vary, and considers the relationships between classes. It performs an empirical evaluation of six algorithms from the literature, k-NN, FineTune, Prototypical Networks, Matching Networks, Relation Networks, and MAML. A new approach combining Prototypical Networks and first-order MAML is shown to outperform those algorithms, but there is substantial room for improvement overall. Many papers in the literature have observed that current algorithms achieve very high performance on existing few-shot classification data sets such as Omniglot, making it difficult to compare them. This paper fills the need for a more challenging data set. This data set is also more realistic in that 1. The training and test data may come from different data sets. In the real world, we often encounter locations, weather conditions, etc. that are unseen during training, and agents must be able to adapt quickly to these new situations. 2. Classes are selected based on their semantic structure. This allows control of the difficulty of the task, as it is easier to adapt to a semantically similar class, and thus enables a more nuanced comparison of few-shot classification algorithms. Suggestions for clarifying the writing: 1. Section 3 should also discuss choosing the data set to sample from. 2. In figures 1c and 1d, it would be helpful to include standard error regions, like in the rest of figure 1. 3. Maybe the paragraph on Proto-MAML should be moved to section 3, as it is not from previous literature. In addition, steps 2b and 2c in section 3.2 overlap in content, and so it may be clearer to combine them."}
{"id": "iclr2020_762", "title": "Gram-Gauss-Newton Method: Learning Overparameterized Neural Networks for Regression Problems | OpenReview", "abstract": "Abstract:###First-order methods such as stochastic gradient descent (SGD) are currently the standard algorithm for training deep neural networks. Second-order methods, despite their better convergence rate, are rarely used in practice due to the pro- hibitive computational cost in calculating the second-order information. In this paper, we propose a novel Gram-Gauss-Newton (GGN) algorithm to train deep neural networks for regression problems with square loss. Our method draws inspiration from the connection between neural network optimization and kernel regression of neural tangent kernel (NTK). Different from typical second-order methods that have heavy computational cost in each iteration, GGN only has minor overhead compared to first-order methods such as SGD. We also give theoretical results to show that for sufficiently wide neural networks, the convergence rate of GGN is quadratic. Furthermore, we provide convergence guarantee for mini-batch GGN algorithm, which is, to our knowledge, the first convergence result for the mini-batch version of a second-order method on overparameterized neural net- works. Preliminary experiments on regression tasks demonstrate that for training standard networks, our GGN algorithm converges much faster and achieves better performance than SGD.", "review": "Review:###The authors propose a scalable second order method for optimization using a quadratic loss. The method is inspired by the Neural Tangent kernel approach, which also allows them to provide global convergence rates for GD and batch SGD. The algorithm has a computational complexity that is linear in the number of parameters and requires to solve a system of the size of the minibatch. They also show experimentally the advantage of using their proposed methods over SGD. - The paper is generally easy to read except section 3.1 which could be clearer when establishing the connexion between the proposed algorithm and NTK. - The proposed algorithm seems to be literally a regularized Gauss-Newton with Woodbury matrix inversion lemma applied to equation (7). Additional simplifications occur due to the pre-multiplication by the jacobian and give (9). However, this is not clear in the paper, instead section 3.1, is a bit vague about the derivation of (9). - In terms of theory, the proofs of thm 1 and 2 seem sound. They rely essentially on the convergence results established for NTK in [Jacot2018, Chizat2018]. The main novelty is that the authors provide faster rates for the Gauss-Newton pre-conditioner which leads to second-order convergence. The second theoretical contribution is to extend the proof to batched gradient descent. Both are somehow expected, although the second one is more technical. - However, the convergence rates provided for batched gradient descent (thm 2) rely on a rather unrealistic assumption: the size of the network should grow as n^18 where n is the sample size. This makes the result less appealing as in practice this is highly unlikely to be the case. - The convergence analysis for the NTK dynamics, which is essential in the proof, relies on a particular scaling 1/sqrt(M) of the function with the number of parameters. In [Chizat2018], it is discussed that although it leads to convergence in the training loss, generalization can be bad. Is there any reason to think in this case, things would be different? - Experiments: Experiments were done on two datasets to solve a regression task. They show that training loss decreases indeed faster than SGD and finds better solutions. A more fair comparison would be against other second-order optimizers like KFAC. - How was the learning rate chosen for the other methods? Was the same lr used? - The authors say that the algorithm has the same cost of one backward pass, could they be more specific about the implementation? - What are the test results for the second dataset? Could they be reported somewhere (in the appendix?) - Both tasks are univariate regression, can the method be applied successfully in a multivariate setting? I don*t see how the proposed method is different from exactly doing regularized gauss newton, so to me the algorithm is not novel in itself. Besides the method seems to require a quadratic loss function which limits its application. ---------------------------------------------------------------------------------- Revision: I*ve read the author*s response and other reviews. I think the paper will be stronger if extended to more general cases (multivariate output + more general losses), thus I encourage the authors to resubmit the paper with stronger experiments."}
{"id": "iclr2020_763", "title": "Unsupervised Learning of Node Embeddings by Detecting Communities | OpenReview", "abstract": "Abstract:###We present Deep MinCut (DMC), an unsupervised approach to learn node embeddings for graph-structured data. It derives node representations based on their membership in communities. As such, the embeddings directly provide interesting insights into the graph structure, so that the separate node clustering step of existing methods is no longer needed. DMC learns both, node embeddings and communities, simultaneously by minimizing the mincut loss, which captures the number of connections between communities. Striving for high scalability, we also propose a training process for DMC based on minibatches. We provide empirical evidence that the communities learned by DMC are meaningful and that the node embeddings are competitive in different node classification benchmarks.", "review": "Review:###Although this paper seems to only combine existing techniques in community detection and node embedding into a co-train process. The idea is simple and easy understood and the paper is well-written. Theoretical analysis is provided for the approximation error for the sampling strategy. However, major concerns are: 1. Experimental results show that co-training node embedding and community detection can improve the performance for node classification. The improvements may result from the assumption that papers with the same class label are associated with the same community in the citation graph. However, in the dataset, there are many cases that there are not dense connections among the same labeled papers. The authors should check the correlation between the detected communities and the original paper labels. 2. No comparison with other community-preserving node embedding methods, such as *Community Preserving Network Embedding* in AAAI17 3. Since this paper aims to combine community detection and node embedding process, a set of baseline should be considered. For example, if considering the downstream node classification of node embedding as an evaluation task, then how about the performance of the following two-step method. We can first detect communities based on the node features then do graph node embedding by considering the communities* membership and node features together (e.g. simply concatenating both community membership features and node features). 4. Efficiency and scalability evaluations are needed. Spectral clustering has a scalability issue when meeting big graphs. Since the spectral process is also applied in the proposed method, efficiency and scalability evaluations are encouraged to provide, especially for big graphs which are not covered in the selected datasets in this paper. 5. In Sec 5.3 and Fig 2, it*s mentioned that trends of the three datasets are different. For the increasing trend, how about the performance for an extreme case where all nodes are considered in one batch. On the other hand, adding more nodes in one minibatch could provide more information, but why there exists a decreasing trend? Though the authors provide a reason in Sec 5.3, it*s better to analyze the reason directly from the datasets."}
{"id": "iclr2020_764", "title": "Visual Imitation with Reinforcement Learning using Recurrent Siamese Networks | OpenReview", "abstract": "Abstract:###It would be desirable for a reinforcement learning (RL) based agent to learn behaviour by merely watching a demonstration. However, defining rewards that facilitate this goal within the RL paradigm remains a challenge. Here we address this problem with Siamese networks, trained to compute distances between observed behaviours and the agent’s behaviours. Given a desired motion such Siamese networks can be used to provide a reward signal to an RL agent via the distance between the desired motion and the agent’s motion. We experiment with an RNN-based comparator model that can compute distances in space and time between motion clips while training an RL policy to minimize this distance. Through experimentation, we have had also found that the inclusion of multi-task data and an additional image encoding loss helps enforce the temporal consistency. These two components appear to balance reward for matching a specific instance of a behaviour versus that behaviour in general. Furthermore, we focus here on a particularly challenging form of this problem where only a single demonstration is provided for a given task – the one-shot learning setting. We demonstrate our approach on humanoid agents in both 2D with 10 degrees of freedom (DoF) and 3D with 38 DoF.", "review": "Review:###This paper presents an imitation learning method that deploys previously well-studied techniques such as siamese networks, inverse RL, learning distance functions for IRL and tracking. + the paper studies an important problem of IL using visual data. + I found the ablation studies in the appendix quite useful in understanding the efficiency of the proposed method. -In terms of novelty, the proposed approach is a combination of several past works so the technical novelty is limited. Additionally, it is not clear how impactful the proposed method can be given that it is only tested on a synthetic domain which is the same as the train domain. So, from the current experimental results it is not clear if this approach would be effective to be applied in a real system (e.g. robots) on the practical side. -There are not enough evaluation done to compare with the most updated state-of-the-art baselines. The evaluations are done on just a single synthetic domain with a single character. Therefore, the train and test videos are very similar."}
{"id": "iclr2020_765", "title": "Learning Effective Exploration Strategies For Contextual Bandits | OpenReview", "abstract": "Abstract:###In contextual bandits, an algorithm must choose actions given observed contexts, learning from a reward signal that is observed only for the action chosen. This leads to an exploration/exploitation trade-off: the algorithm must balance taking actions it already believes are good with taking new actions to potentially discover better choices. We develop a meta-learning algorithm, MELEE, that learns an exploration policy based on simulated, synthetic contextual bandit tasks. MELEE uses imitation learning against these simulations to train an exploration policy that can be applied to true contextual bandit tasks at test time. We evaluate on both a natural contextual bandit problem derived from a learning to rank dataset as well as hundreds of simulated contextual bandit problems derived from classification tasks. MELEE outperforms seven strong baselines on most of these datasets by leveraging a rich feature representation for learning an exploration strategy.", "review": "Review:###• Summary This paper introduced a meta-learning algorithm for the contextual bandit problem, MELEE, which learns an exploration policy based on simulated and synthetic contextual bandit tasks. The training is mainly divided into two steps. In step one, they proposed to train a policy optimizer, which maps features and actions to rewards. This policy optimizer could be used to reveal the most valuable action to take according to the modeled reward. All possible actions and their corresponding values are revealed to the policy optimizer because of the existing ground-truth labels in the synthetic dataset. The policy optimizer would then suggest which action to take. The algorithm takes the action in an greedy fashion, i.e. with probability it will follow the suggestion and with probability it will sample it uniformly at random. The policy optimizer, historical actions and the taken actions are appended to the training set for training the exploration policy in the next step. The procedure in step one is proposed to be done in rounds. In step two, the training set is used for training an exploration policy . During testing, the contexts are drawn from the real world, the policy optimizer will first evaluate the whole history and the exploration policy will generate actions with the input from the policy optimizer and the context. The algorithm suggests the action to explore in an greedy fashion. The proposed algorithm is evaluated on a dataset for learning to rank, and 300 synthetic datasets. It shows better performances in most cases. I am critical about the paper because 1) the experiments show that all more recently published exploration methods are worse than weak baselines, for which it lacks enough justification and convincing explanations. 2) the experiments are difficult to understand with only citation to publications. Detailed information of the datasets, tasks, procedures is missing. • Main arguments The paper is in general hard to follow because of too many citations of the previous works without simple explanations. It refers to the imitation algorithm, AggreVate, which is an instantiate of meta-learning for contextual bandits. Meanwhile, they failed to clarify the difference between the proposed algorithm and the existing one, making the training algorithm part confusing. The major concern lies in the experiment section, I can not see big performance difference between the proposed method and the greedy based methods in Figure 1 (left) as 1) the variances are large 2) there are overlapping . It is surprising to see all recently published method are worse than the classical greedy method. These results may require deeper investigations. In addiction, the used datasets in experiments are mentioned without any details and task definitions, which makes the experiments part unclear and the result not that convincing. Below are some other inconsistencies in the paper: 1. I had a hard time to understand what the function means. In Section 2, is used to map user feature to predicted rewards for actions, i.e. is a function with user features and actions as input and reward as output. However, in equation 1, it only takes user feature as input. In Section 5.1, is called a classifier (I regard it as a variant of function ). Both are not consistent with the mapping definition when first formally defined. This point confuses me so that I cannot fully understand what the POLOPT does as the function is the output of it. 2. By the end of section 2, the paper claims that they used direct method for it simplicity and unbiased property. However, as verified in [1], the direct method is biased with low variance whereas IPS is unbiased with high variance. [1] Dud?k, Miroslav, John Langford, and Lihong Li. “Doubly Robust Policy Evaluation and Learning.” • Things to improve that did not impact the score a. clear definition of the introduced notations, including the ones in algorithms 1. • Questions: a. What is ? b. What is POLOPT? c. Why do you choose AggreVate? d. Will the performance change if using any other methods instead of direct method for policy optimizer? e. What is roll-out policy in line 8 of Algorithm 1? (In text only the roll-out value is defined.) f. What will happen if the roll-in action is different from the behavior in test time? g. How is exploration policy trained?"}
{"id": "iclr2020_766", "title": "PROVABLY BENEFITS OF DEEP HIERARCHICAL RL | OpenReview", "abstract": "Abstract:###Modern complex sequential decision-making problem often both low-level policy and high-level planning. Deep hierarchical reinforcement learning (Deep HRL) admits multi-layer abstractions which naturally model the policy in a hierarchical manner, and it is believed that deep HRL can reduce the sample complexity compared to the standard RL frameworks. We initiate the study of rigorously characterizing the complexity of Deep HRL. We present a model-based optimistic algorithm which demonstrates that the complexity of learning a near-optimal policy for deep HRL scales with the sum of number of states at each abstraction layer whereas standard RL scales with the product of number of states at each abstraction layer. Our algorithm achieves this goal by using the fact that distinct high-level states have similar low-level structures, which allows an efficient information exploitation and thus experiences from different high-level state-action pairs can be generalized to unseen state-actions. Overall, our result shows an exponential improvement using Deep HRL comparing to standard RL framework.", "review": "Review:###This paper studies the theoretical aspects of HRL. It provides theoretical analysis for the complexity of Deep HRL. The idea is to exploit a given action hierarchy, and known state decomposition, the fact that the high-level state space shares similar low-level structures. The final result is an exponential improvement of HRL to flat RL. Overall, the paper pursues an ambitious goal that analyses the complexity of Deep HRL. The writing is not easy to follow. I some questions and concerns as follows - I wonder why the state space must be defined in a product form? If a standard RL is used, then it could be applied directly to the state space ( ) on that primitive actions operate. Hence L-1 state spaces will be discarded? I don*t see why a flat RL must estimate policies for states at all levels. It looks like many later derivations based on the assumption of factored state spaces and factored transitions on different levels. In the case of factored representation, the authors should make clear assumptions and find a better way to describe the overall algorithm. - Section 3.2: the authors use time index for Q and V, does that mean all analysis is for non-stationary MDPs? This is not the assumption in Jaksch et al. (2010) and this paper. The description in this section is very confusing and contains a lot of imprecise definitions e.g. should H = prod {i=1} H_i?? is h =(h_1,...h_L) not in [1,H]? what is the definition of the immediate next lexicographical tuple? etc. The definition of sigma is also unclear and hard to understand. - The analysis in Section 4. and Algorithm 1 are not for Deep HRL as said in Abstract and Introduction. The analysis is based on PAC-MDP learning for models at each action level. This paper*s contributions might be clearer if the authors made clearer assumptions, e.g. on action hierarchy, abstract state space structures etc.."}
{"id": "iclr2020_767", "title": "Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards | OpenReview", "abstract": "Abstract:###Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In this work, we propose a method that can learn to learn from both demonstrations and trial-and-error experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. In comparison to meta-reinforcement learning, we can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration. Our experiments show that our method significantly outperforms prior approaches on a set of challenging, vision-based control tasks.", "review": "Review:###This work focuses on meta learning from both demonstrations and rewards. The proposed method has certain advantages over previous methods: (1) In comparison to meta-imitation, it enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. (2) In comparison to meta-reinforcement learning, it can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration. I am a bit confused by the writings and don*t clearly understand the algorithm. 1. In eq. 4, should \theta be phi? 2. What does pi_\theta(a_t|s_t, {d_i,k}) mean? I*d like to see an example formulation of the policy. When the parameter \theta is well trained, does the policy still take demonstrations {d_i,k} as inputs? If yes, why are demonstrations needed? 3. In meta-testing (Algo. 2), will \theta and phi be updated? Currently it looks like that the two parameters are fixed in meta testing. If so, this is a bit strange. Why not update the policies after sampling demonstrations (step 4) and collecting trials (step 5)? 4. *The state space S, reward ri, and dynamics Pi may vary across tasks.* I think all the tasks should share the same input space; otherwise, the meta-trained two policies cannot be applied to a test task with different input space. For example, if the states are 100x100 images in meta training, how to apply the policies to a meta-testing task with 100x200 images as states?"}
{"id": "iclr2020_768", "title": "COMBINED FLEXIBLE ACTIVATION FUNCTIONS FOR DEEP NEURAL NETWORKS | OpenReview", "abstract": "Abstract:###Activation in deep neural networks is fundamental to achieving non-linear mappings. Traditional studies mainly focus on finding fixed activations for a particular set of learning tasks or model architectures. The research on flexible activation is quite limited in both designing philosophy and application scenarios. In this study, we propose a general combined form of flexible activation functions as well as three principles of choosing flexible activation component. Based on this, we develop two novel flexible activation functions that can be implemented in LSTM cells and auto-encoder layers. Also two new regularisation terms based on assumptions as prior knowledge are proposed. We find that LSTM and auto-encoder models with proposed flexible activations provides significant improvements on time series forecasting and image compressing tasks, while layer-wise regularization can improve the performance of CNN (LeNet-5) models with RPeLu activation in image classification tasks.", "review": "Review:###This paper proposed combined form of flexible activation functions with carefully designed principle of choosing activation functions. It shows some gains on stock price and one standard image task over the baseline. I*m leaning to reject or give borderline for this paper because (1) the paper don*t have comparison with neural architecture search. For example, https://arxiv.org/abs/1710.05941 (Searching for Activation Functions). I don*t know what the advantage of this approach compared to searched activation. I guess it*s less computation heavy and maybe better motivated. But at least the author should give some pros/cons. (2) the paper has two benchmark, stock price prediction and CIFAR-10. I don*t understand why as arch paper, it use such non standard benchmark (stock) and non standard arch (LeNet?). I don*t think based these benchmark we can make solid conclusion. (3) These model seems introduce quite a bit more hyper-parameters. It*s unclear if it is better than tuning other architecture e.g., batch norm/layer norm/dropout or even just optimizer. For example, *flexible is 0.032* does this parameter generalize to other dataset? Like if the gain is really significant, like resnet over AlexNet, hyperparam doesn*t matter. But if it*s marginal win over a weak baseline, the how to get the results is important. Some comments: In section 2, * back propagation of these activation parameters by stochastic gradient descent can be done as follows* Why we need to list the detail backprop formulation here? Are these special? Isn*t just autograd? Can the author explain more for principle 1? What is the *same domain* means here?"}
{"id": "iclr2020_769", "title": "Unified Probabilistic Deep Continual Learning through Generative Replay and Open Set Recognition | OpenReview", "abstract": "Abstract:###We introduce a unified probabilistic approach for deep continual learning based on variational Bayesian inference with open set recognition. Our model combines a joint probabilistic encoder with a generative model and a linear classifier that get shared across tasks. The open set recognition bounds the approximate posterior by fitting regions of high density on the basis of correctly classified data points and balances open set detection with recognition errors. Catastrophic forgetting is significantly alleviated through generative replay, where the open set recognition is used to sample from high density areas of the class specific posterior and reject statistical outliers. Our approach naturally allows for forward and backward transfer while maintaining past knowledge without the necessity of storing old data, regularization or inferring task labels. We demonstrate compelling results in the challenging scenario of incrementally expanding the single-head classifier for both class incremental visual and audio classification tasks, as well as incremental learning of datasets across modalities.", "review": "Review:###This paper combines replay and openMax approach to help continual learning. The results shows robustness on different dataset include image and audio in the continual learning condition, where the new come data has a different distribution but the model still able to maintain reasonable quality for the previously and newly come examples. To my understanding, this approach was not ground-breaking but seems a reasonable combinations. I*m learning to give weak reject for this paper because of it*s poorly written. (1) It*s very hard to align the contribution claimed by the paper and previous work in the introduction section. I highly suggest the author re-write this part and has a separate section about related work and explicit describe the difference compare to others. (2) The contribution seems over-claimed, it said it*s a unified framework, but I don*t understand what it has been unified. (3) In the experimental part, it use audioMNIST. Any reason to use this dataset? There are more well-defined audio task such as TIMIT for phoneme classification or Aurora for digital recognition. They are more easy to understand since they have well established benchmark. Given my limited knowledge on this the literature of this topic, I*m happy to change the score if the written being improved and the following question being addressed. (1) Introduction. I take most of space to describe previous work, but hard to find out what*s difference of this paper. My understanding it combines A and B and apply it to C. But it claim it*s a unified framework. (2) *We fully share our model across tasks and automatically expand the linear classifier with additional units when encountering new classes, thus not requiring explicit task labels.* I cannot link *automatic* with the proposed method. Is that doable because of the proposed framework? (3) Why use AudioMNIST which is an unusual task for audio? (4) For the giant Table 1, I suggest link each acronym with the reference paper. So it can easily get how it associate with different approach. Highlight some numbers can also help the reader understand what*s going on in this giant table. (5) Can the author give me some insights, what these KL loss demonstrated in the table? I feel since you use the beta-vae version, the kl scale is depend on different approach, not really comparable for these different models."}
{"id": "iclr2020_770", "title": "On the Distribution of Penultimate Activations of Classification Networks | OpenReview", "abstract": "Abstract:###This paper considers probability distributions of penultimate activations in deep classi?cation networks. We ?rst identify a dual relation between the activations and the weights of the ?nal fully connected layer: learning the networks with the cross-entropy loss makes their (normalized) penultimate activations follow a von Mises-Fisher distribution for each class, which is parameterized by the weights of the ?nal fully-connected layer. Through this analysis, we derive a probability density function of penultimate activations per class. This generative model allows us to synthesize activations of classi?cation networks without feeding images forward through them. We also demonstrate through experiments that our generative model of penultimate activations can be applied to real-world applications such as knowledge distillation and class-conditional image generation.", "review": "Review:###This paper studies the distribution of the second-to-last layers of neural networks. The paper argues that, under certain assumptions, minimizing the standard cross-entropy loss of a classification task corresponds to minimizing the KL divergence between the distribution of the penultimate activations and a von Mises-Fisher (vMF) distribution. They assess their assumptions through experiments, and use this interpretation of the penultimate activations as vMF distributions on two tasks: knowledge distillation and class-conditional image generation. They argue that this interpretation outperforms other knowledge distillation methods under domain shift, and report a similar success for class conditional image generation. This is an important problem, especially since these penultimate activations have become more frequently used as object representations in downstream tasks. Their result could also help us understand better interpretations of neural networks. The writing is clear throughout, and it*s straightforward to read. The results are based on assumptions that I will try to list below: * normalizing the penultimate activations does not affect classification performance * the class-specific vectors in the final weight matrix have constant normalizations * minimizing cross-entropy loss is equivalent to minimizing the KL divergence between the penultimate activation distribution and a vMF distribution (i.e. the entropy term in equation 6 is negligible) In light of these assumptions, theoretical claims like *learning the networks with the cross-entropy loss makes their (normalized) penultimate activations follow a von Mises-Fisher distribution for each class* in the abstract are false; even if the normalization assumptions are true, this is an approximate result, due to 1) no guarantee that the KL-divergence will be zero and 2) the entropy term in Equation 6. Additionally, Equation 6 is only applied to the numerator of the objective in Equation 4, so the objectives aren*t equivalent. These results are thus necessarily based on approximations and some heuristics. This is not inherently a problem, but I would argue that it necessitates that experiments prove these assumptions to be useful. I find Table 1 to be convincing of the assumption that the weight and activation normalizations are negligible. Figure 1 is also a nice visualization for arguing that penultimate activations are approximately vMF. The MNIST experiment in Table 4 is a red flag, since the presented results do not match the results in the HVAE paper. The paper claims the experimental setup is identical to the HVAE paper, so it is concerning that the results do not match, especially since the results in the HVAE paper appear to outperform the proposed method. It is possible that the results in the original paper are not reproducible or that the experiments differ, but this should be stated in the paper, since the natural assumption would be that the baseline was not adequately reproduced. Small comment: In Table 2 it is not clear from the figure or the caption that CKD is the method the paper is proposing. It would also make sense to bold the best performing method since you do that for the other tables in the paper. I like the paper and think the results are interesting, but because they are based on heuristics and assumptions, the paper currently promises too much. This puts the burden on presenting experiments that are convincing of improvement, which is not currently the case with the HVAE experiment. I would be open to changing my score if these reviews are addressed."}
{"id": "iclr2020_771", "title": "NORML: Nodal Optimization for Recurrent Meta-Learning | OpenReview", "abstract": "Abstract:###Meta-learning is an exciting and powerful paradigm that aims to improve the effectiveness of current learning systems. By formulating the learning process as an optimization problem, a model can learn how to learn while requiring significantly less data or experience than traditional approaches. Gradient-based meta-learning methods aims to do just that, however recent work have shown that the effectiveness of these approaches are primarily due to feature reuse and very little has to do with priming the system for rapid learning (learning to make effective weight updates on unseen data distributions). This work introduces Nodal Optimization for Recurrent Meta-Learning (NORML), a novel meta-learning framework where an LSTM-based meta-learner performs neuron-wise optimization on a learner for efficient task learning. Crucially, the number of meta-learner parameters needed in NORML, increases linearly relative to the number of learner parameters. Allowing NORML to potentially scale to learner networks with very large numbers of parameters. While NORML also benefits from feature reuse it is shown experimentally that the meta-learner LSTM learns to make effective weight updates using information from previous data-points and update steps.", "review": "Review:###This paper proposes a meta-learner that learns how to make parameter updates for a model on a new few-shot learning task. The proposed meta-learner is an LSTM that proposes at each time-step, a point-wise multiplier for the gradient of the hidden units and for the hidden units of the learner neural network, which are then used to compute a gradient update for the hidden-layer weights of the learner network. By not directly producing a learning rate for the gradient, the meta-learner’s parameters are only proportional to the square of the number of hidden units in the network rather than the square of the number of weights of the network. Experiments are performed on few-shot learning benchmarks. The first experiment is on Mini-ImageNet. The authors build upon the method of Sun et al, where they pre-train the network on the meta-training data and then do meta-training where the convolutional network weights are frozen and only the fully-connected layer is updated on few-shot learning tasks using their meta-learner LSTM. The other experiment is on Omniglot 20-way classification, where they consider a network with only full-connected layers and show that their meta-learner LSTM performs better than MAML. The closest previous work to this paper is by Ravi & Larochelle, who also propose a meta-learner LSTM. The submission states about this work that “A challenge of this approach is that if you want to optimize tens of thousands of parameters, you would have a massive hidden state and input size, and will therefore require an enormous number of parameters for the meta-learner…In Andrychowicz at al. an alternative approach is introduced that avoids the aforementioned scaling problem by individually optimizing each parameter using a separate LSTM…” I don’t believe this is true. As stated in the work by Ravi & Larochelle, “Because we want our meta-learner to produce updates for deep neural networks, which consist of tens of thousands of parameters, to prevent an explosion of meta-learner parameters we need to employ some sort of parameter sharing. Thus as in Andrychowicz et al. (2016), we share parameters across the coordinates of the learner gradient. This means each coordinate has its own hidden and cell state values but the LSTM parameters are the same across all coordinates.” Thus, Ravi & Larochelle employ something similar to Andrychowicz at al., meaning that the number of parameters in the LSTM meta-learner there is actually a constant relative to the size of the learner network. Thus, the paper’s contribution relative to Ravi & Larochelle is to propose a LSTM meta-learner with more parameters relative to the learner model. Firstly, I think this comparison should be stated and explained clearly in the paper. Additionally, in order to prove the benefit of such an approach, I think a comparison to the work of Ravi & Larochelle with the exact experimental settings used in the submission (pre-training the convolutional network and only using the meta-learner LSTM to tune the last fully-connected layer) would be helpful in order to validate the usefulness of the extra set of parameters in their proposed meta-learner LSTM. The submission also states that “In many cases it is preferred to have an inner loop that consists of multiple sequential updates. However the inner loop’s computational graph can become quite large if too many steps are taken. This often results in exploding and vanishing gradients since the outer loop still needs to differentiate through the entire inner loop (Aravind Rajeswaran (2019), Antoniou et al. (2018)). This limits MAML to domains where a small amount of update steps are sufficient for learning. The LSTM-based meta-learner proposed in this work, allow gradients to effectively flow through a large number of update steps. NORML can therefore be applied to a wide array of domains.” I think this statement should be validated if it is stated. Can it be shown that when making a lot of inner-loop updates, the LSTM meta-learner performs better than MAML because of overcoming the stated issues with differentiation through a long inner loop? The experiments done in the paper involve very few inner loop steps and so I don’t believe the claim is supported. Lastly, the experimental results are not very convincing. Though the authors say they modify the method proposed in Sun et al, the results from Sun et al. are not shown in the paper. Sun et al actually seem to achieve better results than the submission with the same 1-shot and better 5-shot accuracy. Was there a reason these results are not shown in the submission? Additionally, for the Omniglot experiment, is there any reason why it was not performed with the typical convolutional network architecture? Since the original MAML results on 20-way Omniglot are with a convolutional network, using the convolutional network would make the results more meaningful relative to previous work and show that the method is more broadly applicable to convolutional networks. I believe there are several issues with the paper as stated above. Because of these issues, it is hard to evaluate whether the idea proposed is of significant benefit. References Andrychowicz et al. Learning to learn by gradient descent by gradient descent. NIPS 2016. Ravi & Larochelle. Optimization as a Model for Few-Shot Learning. ICLR 2017. Sun et al. Meta-transfer learning for few-shot learning."}
{"id": "iclr2020_772", "title": "Ternary MobileNets via Per-Layer Hybrid Filter Banks | OpenReview", "abstract": "Abstract:###MobileNets family of computer vision neural networks have fueled tremendous progress in the design and organization of resource-efficient architectures in recent years. New applications with stringent real-time requirements in highly constrained devices require further compression of MobileNets-like already computeefficient networks. Model quantization is a widely used technique to compress and accelerate neural network inference and prior works have quantized MobileNets to 4 ? 6 bits albeit with a modest to significant drop in accuracy. While quantization to sub-byte values (i.e. precision ? 8 bits) has been valuable, even further quantization of MobileNets to binary or ternary values is necessary to realize significant energy savings and possibly runtime speedups on specialized hardware, such as ASICs and FPGAs. Under the key observation that convolutional filters at each layer of a deep neural network may respond differently to ternary quantization, we propose a novel quantization method that generates per-layer hybrid filter banks consisting of full-precision and ternary weight filters for MobileNets. The layer-wise hybrid filter banks essentially combine the strengths of full-precision and ternary weight filters to derive a compact, energy-efficient architecture for MobileNets. Using this proposed quantization method, we quantized a substantial portion of weight filters of MobileNets to ternary values resulting in 27.98% savings in energy, and a 51.07% reduction in the model size, while achieving comparable accuracy and no degradation in throughput on specialized hardware in comparison to the baseline full-precision MobileNets.", "review": "Review:###The authors focus on quantizing the MobileNets architecture to ternary values, resulting in less space and compute. The space of making neural networks more energy efficient is vital towards their deployment in the real world. I think the authors over-state their claims of no loss in accuracy, in Table 2 we see a clear loss in accuracy from MobileNets to MobileNets + Hybrid Filter Banks. I think this research is quite incremental over MobileNets and is unlikely to spur further research strains. I think a better venue for this research may be a more systems-focused conference or journal. There is a significant amount of compute and training complexity required to reduce the model size, e.g. versus model pruning or tensor decomposition. It seems this research would be incredibly difficult to reproduce."}
{"id": "iclr2020_773", "title": "End to End Trainable Active Contours via Differentiable Rendering | OpenReview", "abstract": "Abstract:###We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images.", "review": " EDIT: The rating changed from *1: Reject* to *6: Weak accept* after the rebuttal. See below for my reasoning. The submission considers two-class image segmentation problems, where a closed-contour image region is to be specified as the *object*/region of interest, vs. *no-object*/background. The approach taken here is end-to-end learning with an active-contour type approach. The main loss, in contrast to other active contour approaches, contains a direct difference of the estimated polygon area vs. ground truth polygon area. The applied method seems conceptually quite simple (as admitted by the authors in Section 5), and the neural rendering approach seems quite neat, but both method presentation (Section 3) and evaluation (Section 4) seem incomplete and leave significant open questions. One of my main concerns is related to the fact that the displacement field is static and, according to Figure 1 and Algorithm 1, is evaluated only once per image. If the displacement field J is not conditioned on the current polygon shape (and this does not seem to be the case), then I am wondering why T iterations in the sampling/rendering part are necessary at all. When only considering L_seg, the optimal solution should be found within one iteration, since the displacement field will be able to provide the optimal answer. So maybe these iterations are only necessary when L_B and L_K are incorporated? In any case, it is unclear why even L_seg is accumulated (using unweighted mean) over all T iterations before being backpropagated. Does this mean that these iterations are not meant to yield shape improvements? Why is ||M^t-M|| not evaluated per iteration, for the purpose of minimization? It is also not sufficiently clear whether M^t in Equation 4 is a filled polygon mask, or if the mask is just related to the boundary (with a certain width). In absence of explanatory image material, I am assuming the former. Overall the method description remains weak, since obvious questions/concerns such as the above are not addressed. The experimental results look good from a quantitative point of view, and indeed, the strongest baselines, e.g. DARNet, are outperformed significantly in many cases. Section 4 mostly focuses on quantitative evaluation and lots of picture examples, but fails to give insight into particular behaviors, failure cases, etc. The evaluation procedure is cast a bit into doubt by two things: 1) In Figure 4, the initializations (blue circles) between the DARNet method and the proposed method are very different in size. I am wondering if this then still constitutes a fair comparison, and I have some doubts there. 2) In Figure 6, the proposed method consistently looks much worse than the DARNet baseline (and, in contrast to the baseline, completely fails for 4 vertices), unless the colors were swapped in the description. Overall, I do not think the submission is in a good enough shape for acceptance. Minor remarks: - The values for lambda_1 and lambda_2 seem to come out of thin air, and they also seem quite small. It needs to be mentioned how they were determined. - Data augmentation by rotation seems to be missing several values (between 270 and 260 degrees) and also not evenly spaced. Is this a typo or on purpose? In the latter case, an explanation is needed, since this seems weird. - Section 4.3: There is no *Figure 4.2*, I assume you mean Figure 6, which otherwise remains unreferenced. - Section 4.3, Ablation Study: Don*t use the word *derivatives* when you*re talking about variations. - Section 4.3, Ablation Study: *even without no auxiliary loss* -> remove *no* or change *without* -> *with* ------------- Post-rebuttal comments: I have read the revised version, as well as the other reviews and all authors* comments. The inclusion of an evaluation on a larger-size data set is highly appreciated, and seems to indeed validate the robustness of the method. Typos were fixed, including the switched color descriptions in Figure 7 (which should not have passed initial submission in the first place, if the text had been proofread properly). Several of the open questions (e.g. *Why is L_seg accumulated before backpropagation?*, *Why is the algorithm iterative if the displacement map is computed only once, if not for the other loss terms?*, *Choice of values for lambda_1, lambda_2*, Initial diameter of initialization*) have been somewhat addressed by the authors in the rebuttal comment, though not in great detail. Based on the quality of the results across data sets, and because I believe that the timely publication of this rather simple method can benefit further research in this area, I have adjusted my score to a *Weak accept*. That said, I still do not think it is a good manuscript, and my score should be seen as a massive benefit of the doubt toward the authors. Most importantly, above questions have NOT been adequately addressed in the actual revised text. The authors claim they have *improved the manuscript considerably*, but yet I see more reasoning for certain choices described in the comment here than in the actual manuscript. Most of the changes are in Section 2 and the new Section 4.3, but not much relevant to my comments changed in Section 3. For example, balloon and curvature losses aside, it is still not clear why an iterative approach would be helpful past the first iteration. An ideal displacement map that is not conditioned on the polygon should point, for each pixel, straight to the closest contour pixel. It is clear to me that this may not be what is being learned when multiple iterations are forced, yet it is not addressed why multiple iterations should be beneficial. (I could see why they could be beneficial if the approach was conditioned on the polygon vertices, to avoid vertex collapsing, but it*s not.) A good submission preempts these kinds of questions by addressing them carefully. What seems crystal clear to the authors will not be crystal clear to every reader. The authors should be more careful to include their reasoning in the actual text, which I believe this is essential for proper, easy understanding of the paper."}
{"id": "iclr2020_774", "title": "Analyzing Privacy Loss in Updates of Natural Language Models | OpenReview", "abstract": "Abstract:###To continuously improve quality and reflect changes in data, machine learning-based services have to regularly re-train and update their core models. In the setting of language models, we show that a comparative analysis of model snapshots before and after an update can reveal a surprising amount of detailed information about the changes in the data used for training before and after the update. We discuss the privacy implications of our findings, propose mitigation strategies and evaluate their effect.", "review": "Review:###Summary: This paper looks at privacy concerns regarding data for a specific model before and after a single update. It discusses the privacy concerns thoroughly and look at language modeling as a representative task. They find that there are plenty of cases namely when the composition of the sequences involve low frequency words, that a lot of information leak occurs. Positives: The ideas and style of research is nice. This is an important problem and I think this paper does a good job investigating this in the context of language modeling. I do hope the community (and I think the community is) moving towards being aware of these sorts of privacy issues. Concerns: I don*t know how generalizable these results would be on really well-trained language models (rnn, convolution-based, or transformer-based). The related work section doesn*t seem particularly well put together, so its difficult to place the work in appropriate context and gauge its impact. Other Thoughts: I*d like more thorough error analysis looking at exactly what kinds of strings/more nuanced properties of sequences that get a high differential score. Overall I think this work is interesting and I would encourage the authors to try and add as much quantitative evaluation as possible, but also try and include qualitative information regarding specific sequences after prodding the models. Those could go a long way in strengthening the paper."}
{"id": "iclr2020_775", "title": "Removing the Representation Error of GAN Image Priors Using the Deep Decoder | OpenReview", "abstract": "Abstract:###Generative models, such as GANs, have demonstrated impressive performance as natural image priors for solving inverse problems such as image restoration and compressive sensing. Despite this performance, they can exhibit substantial representation error for both in-distribution and out-of-distribution images, because they maintain explicit low-dimensional learned representations of a natural signal class. In this paper, we demonstrate a method for removing the representation error of a GAN when used as a prior in inverse problems by modeling images as the linear combination of a GAN with a Deep Decoder. The deep decoder is an underparameterized and most importantly unlearned natural signal model similar to the Deep Image Prior. No knowledge of the specific inverse problem is needed in the training of the GAN underlying our method. For compressive sensing and image superresolution, our hybrid model exhibits consistently higher PSNRs than both the GAN priors and Deep Decoder separately, both on in-distribution and out-of-distribution images. This model provides a method for extensibly and cheaply leveraging both the benefits of learned and unlearned image recovery priors in inverse problems.", "review": "Review:###This paper presents a method for reducing the representation error generative convolutional neural networks by combining them with untrained deep decoder. The method is evaluated on compressive sensing and super-resolution, where a better performance than the isolated use of Deep Decoders and GAN priors. The main contribution of the paper is not the performance, but the simplicity of this approach. For the title, I would suggest to replace the word Removing with Reducing. Furthermore, the clarification of *GAN prior* is very nice in the introduction, maybe you could already clarify it in the abstract. You should perform a critical grammar check. There are too many commas, for example: *At sufficiently difficult superresolution problems, the Hybrid model outperforms, the Deep Decoder, Bicubic upsampling, the BEGAN prior, and the BEGAN as DIP prior.* -> there should be no comma after *outperforms* The sentence from Page 3 to 4 reads strangely, probably a word is missing after *For our GAN prior, we use the BEGAN architecture, and we demonstrate similar results* *Philosophically, they hybrid* -> *Philosophically, the hybrid* Fig 6 caption - shouldn*t it be 49152 instead of 49512? You perform various very good analysis experiments, which is well appreciated. Still, it would be good to think about some more experiments (and include at least one of them in the paper): 1. You compare to IGAN and show that you achieve similar performance. You describe that a state-of-the-art approach are invertible generative models and that they are very time consuming (e.g., 15 minutes for a 64x64 image). How good would the invertible models be in terms of performance? Could you perform tests as well? 2. It would be great if you report the runtime of all experiments as well - maybe also the memory usage."}
{"id": "iclr2020_776", "title": "Meta-learning curiosity algorithms | OpenReview", "abstract": "Abstract:###Exploration is a key component of successful reinforcement learning, but optimal approaches are computationally intractable, so researchers have focused on hand-designing mechanisms based on exploration bonuses and intrinsic reward, some inspired by curious behavior in natural systems. In this work, we propose a strategy for encoding curiosity algorithms as programs in a domain-specific language and searching, during a meta-learning phase, for algorithms that enable RL agents to perform well in new domains. Our rich language of programs, which can combine neural networks with other building blocks including nearest-neighbor modules and can choose its own loss functions, enables the expression of highly generalizable programs that perform well in domains as disparate as grid navigation with image input, acrobot, lunar lander, ant and hopper. To make this approach feasible, we develop several pruning techniques, including learning to predict a program*s success based on its syntactic properties. We demonstrate the effectiveness of the approach empirically, finding curiosity strategies that are similar to those in published literature, as well as novel strategies that are competitive with them and generalize well.", "review": "Review:###This paper proposes to meta-learn a curiosity module via neural architecture search. The curiosity module, which outputs a meta-reward derived from the agent’s history of transitions, is optimized via black box search in order to optimize the agent’s lifetime reward over a (very) long horizon. The agent in contrast is trained to maximize the episodic meta-reward and acts greedily wrt. this intrinsic reward function. Optimization of the curiosity module takes the form of an epsilon-greedy search, guided by a nearest-neighbor regressor which learns to predict the performance of a given curiosity program based on hand-crafted program features. The program space itself composes standard building blocks such as neural networks, non-differentiable memory modules, nearest neighbor regresses, losses, etc. The method is evaluated by learning a curiosity module on the MiniGrid environment (with the true reward being linked to discovering new states in the environment) and evaluating it on Lunar Lander and Acrobot. A reward combination module (which combines intrinsic and extrinsic rewards) is further evaluated on continuous-control tasks (Ant, Hopper) after having been meta-trained on Lunar Lander. The resulting agents are shown to match the performance of some recent published work based on curiosity and outperforms simple baselines. This is an interesting, clear and well written paper which covers an important area of research, namely how to find tractable solutions to the exploration-exploitation trade-off. In particular, I appreciated that the method was clearly positioned with respect to recent work on neural architecture search, meta-learning approaches to curiosity as well as forthcoming about the method’s limitations (outlining many hand-designed curiosity objectives which fall outside of their search space). There are also some interesting results in the appendix which show the efficacy of their predictive approach to program performance. My main reservation is with respect to the empirical validation. Very few existing approaches to meta-learning curiosity scale to long temporal horizons and “extreme” transfer (where meta-training and validation environments are completely different). As such, there is very little in the way of baselines. The paper would greatly benefit from scaled down experiments, which would allow them to compare their architecture search approach to recent approaches [R1, R2], black-box optimization methods in the family of evolution strategies (ES, NES, CMA-ES), Thompson Sampling [R3] or even bandits tasks for which Bayes-optimal policies are tractable (Gittins indices). These may very well represent optimistic baselines but would help better interpret the pros and cons of using neural architecture search for meta-learning reward functions versus other existing methods. Conversely, the paper claims to “search over algorithms which [...] generalize more broadly and to consider the effect of exploration on up to 10^5, 10^6 timesteps” but at the same time does not attempt to show this was required in achieving the reported result. Pushing e.g. RL2 or Learning to RL baselines to their limits would help make this claim. Along the same line, it is regrettable that the authors chose not to employ or adapt an off-the-shelf architecture search algorithm such as NAS [R4] or DARTS [R5]. I believe the main point of the paper is to validate the use of program search for meta-learning curiosity, and not the details of the proposed search procedure (which shares many components with recent architecture search / black-box optimization algorithms). Using a state-of-the-art architecture search algorithm would have made this point more readily. Another important point I would like to see discussed in the rebuttal, is the potential for cherry-picking result. How were the “lunar lander” and “acrobot” environments (same question for “ant” and “hopper”) selected? From my understanding, it is cheap to evaluate learnt curiosity programs on downstream / validation tasks. A more comprehensive evaluation across environments from the OpenAI gym would help dispel this doubt. Another important note: top-16 results reported in Figure 4 and Table 1 are biased estimates of generalization performance (as they serve to pick the optimal pre-trained curiosity program). Could the authors provide some estimate of test performance, by e.g. evaluating the performance of the top-1 program (on say lunar lander) on a held-out test environment? Alternatively, could you comment on the degree of overlap between the top 16 programs for acrobot vs lunar lander? Thanks in advance. [R1] Learning to reinforcement learn. JX Wang et al.. [R2] RL2: Fast Reinforcement Learning via Slow Reinforcement Learning. Yan Duan et al. [R3] Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search. Guez et al. [R4] Neural Architecture Search with Reinforcement Learning. Barrett et al. [R5] DARTS: Differentiable Architecture Search. Liu et al."}
{"id": "iclr2020_777", "title": "On Generalization Error Bounds of Noisy Gradient Methods for Non-Convex Learning | OpenReview", "abstract": "Abstract:###Generalization error (also known as the out-of-sample error) measures how well the hypothesis learned from training data generalizes to previously unseen data. Proving tight generalization error bounds is a central question in statistical learning theory. In this paper, we obtain generalization error bounds for learning general non-convex objectives, which has attracted significant attention in recent years. We develop a new framework, termed Bayes-Stability, for proving algorithm-dependent generalization error bounds. The new framework combines ideas from both the PAC-Bayesian theory and the notion of algorithmic stability. Applying the Bayes-Stability method, we obtain new data-dependent generalization bounds for stochastic gradient Langevin dynamics (SGLD) and several other noisy gradient methods (e.g., with momentum, mini-batch and acceleration, Entropy-SGD). Our result recovers (and is typically tighter than) a recent result in Mou et al. (2018) and improves upon the results in Pensia et al. (2018). Our experiments demonstrate that our data-dependent bounds can distinguish randomly labelled data from normal data, which provides an explanation to the intriguing phenomena observed in Zhang et al. (2017a). We also study the setting where the total loss is the sum of a bounded loss and an additiona l`2 regularization term. We obtain new generalization bounds for the continuous Langevin dynamic in this setting by developing a new Log-Sobolev inequality for the parameter distribution at any time. Our new bounds are more desirable when the noise level of the processis not very small, and do not become vacuous even when T tends to infinity.", "review": "Review:###This paper studies the generalization error bounds of stochastic gradient Langevin dynamics. The convexity of the loss function is not assumed. The author proposed *Bayes-stability* to derive generalization bound while taking the randomness of the algorithm into account. The generalization bound proposed in this paper applies to some existing problem setups. Also, the authors proposed the generalization bound of the continuous Langevin dynamics. This is an interesting paper. Overall, the readability is high. The Bayes-stability is a significant contribution of this paper, and the theoretical analysis of the SGLD with non-Gaussian noise distribution will have a practical impact. Some comments below: - What is the function f of f(w,0)=0 above the equation (5)? Besides, the role of zero data point, i.e., f(w,0)=0, was not very clear. - In the numerical results (b) and (c) of Figure 1, the scale in the y-axis was very different. What made the generalization bound so loose? - In this paper, the developed theory was a general-purpose methodology. For deep neural networks, however, is there a meaningful insight obtained from the method developed in this paper?"}
{"id": "iclr2020_778", "title": "Continuous Control with Contexts, Provably | OpenReview", "abstract": "Abstract:###A fundamental challenge in artificially intelligence is to build an agent that generalizes and adapts to unseen environments. A common strategy is to build a decoder that takes a context of the unseen new environment and generates a policy. The current paper studies how to build a decoder for the fundamental continuous control environment, linear quadratic regulator (LQR), which can model a wide range of real world physical environments. We present a simple algorithm for this problem, which uses upper confidence bound (UCB) to refine the estimate of the decoder and balance the exploration-exploitation trade-off. Theoretically, our algorithm enjoys a regret bound in the online setting where is the number of environments the agent played. This also implies after playing environments, the agent is able to transfer the learned knowledge to obtain an -suboptimal policy for an unseen environment. To our knowledge, this is first provably efficient algorithm to build a decoder in the continuous control setting. While our main focus is theoretical, we also present experiments that demonstrate the effectiveness of our algorithm.", "review": "Review:###This paper considers the problem of changing environments for LQR. The authors model this through the use of a decoder that maps an incoming context (C,D) to the LQR matrices (A,B). They provide an algorithm for this setting based on a UCB strategy, prove sample complexity and regret bounds, and experimental results. Overall the paper was well written but I had several concerns. 1. The results of this paper were not contrasted with other papers in this area. For example, if C,D are constant, and Theta_* is a Block diagonal matrix with A,B on the diagonal - then the contextual case reduces to the standard LQR problem. It*s unclear how the results given compare to past results in this setting, for example those of Abbasi-Yadkori/Szepesvari 2011. 2. I did not fully understand the UCB nature of the algorithm. In each round Theta^(k) (the least squares estimator) seems to be used to compute the optimal policy (line 10 of the algorithm) instead \tilde{Theta}^(k) -the optimistic estimate. The optimistic estimate is only used in line 16 - a randomized procedure that is unmotivated. 3. Building on (1), it is hard to understand the results as given since there are no lower bounds given nor is there a discussion of the problem dependent parameters that arise. For example, in Theorem 1, is dp^2 suspected to be tight? Since the number of parameters in Theta^{ast} is d(p+p*), perhaps this is off by a factor of p? 3. I struggled to understand the setup of the experiments - as described the algorithm given was not used at all, rather Theta^(k) was approximated and beta^k was set to be a constant. This does not seem like a fair evaluation of the method. In summary, I would reject this submission unless the authors couch it better in past work, explain their results better, and improve the experiment setup. Finally, a typo: I think the indexing variable in the equation on the top of page 4 is h* not h."}
{"id": "iclr2020_779", "title": "Understanding Generalization in Recurrent Neural Networks | OpenReview", "abstract": "Abstract:###In this work, we develop the theory for analyzing the generalization performance of recurrent neural networks. We first present a new generalization bound for recurrent neural networks based on matrix-1 norm and Fisher-Rao norm. The definition of Fisher-Rao norm relies on a structural lemma about the gradient of RNNs. This new generalization bound assumes that the covariance matrix of the input data is positive definite, which might limit its use in practice. To address this issue, we propose to add random noise to the input data and prove a generalization bound for training with random noise, which is an extension of the former one. Compared with existing results, our generalization bounds have no explicit dependency on the size of networks. We also discover that Fisher-Rao norm for RNNs can be interpreted as a measure of gradient, and incorporating this gradient measure not only can tighten the bound, but allows us to build a relationship between generalization and trainability. Based on the bound, we analyze the effect of covariance of features on generalization of RNNs theoretically and discuss how weight decay and gradient clipping in the training can help improve generalization.", "review": "Review:###In this paper, the authors investigate the topic of theoretical generalizability in recurrent networks. Specifically, they extend a generalization bound using matrix 1-norm and the Fisher-Rao norm, proving the effectiveness of adding noise to input data for generalizability. These bounds have no dependence on the size of networks being investigated. The authors also propose a technique for representing RNNs as a decomposition into a sum of linear networks with a difference term, which allows for easier estimation of Rademacher complexity. The authors claim this is a representation that can be extended to other neural network architectures such as convolutional networks. This work has the potential to be of interest for the learning theory community on theoretical properties of recurrent neural networks. One question I have for the authors: in the experiments on the IMDB dataset, the authors claim that the generalization error is worst at sigma_{epsilon} = 0, but it appears that the error is actually larger for sigma_{epsilon} = 0.4?"}
{"id": "iclr2020_780", "title": "Benchmarking Adversarial Robustness | OpenReview", "abstract": "Abstract:###Deep neural networks are vulnerable to adversarial examples, which becomes one of the most important problems in the development of deep learning. While a lot of efforts have been made in recent years, it is of great significance to perform correct and complete evaluations of the adversarial attack and defense algorithms. In this paper, we establish a comprehensive, rigorous, and coherent benchmark to evaluate adversarial robustness on image classification tasks. After briefly reviewing plenty of representative attack and defense methods, we perform large-scale experiments with two robustness curves as the fair-minded evaluation criteria to fully understand the performance of these methods. Based on the evaluation results, we draw several important findings and provide insights for future research.", "review": "Review:###The paper performs a large-scale empirical study by comparing different adversarial attack and defense techniques. From these experiments, they conclude that the robustness of different techniques varies based on perturbation budgets and attack iterations. They also found that adversarially trained models have, in general, the most successful defenses and the randomization based defenses are generally more robust for black-box attacks. + The author presented a comprehensive background of adversarial attacks, which itself is a good contribution to the literature. + I think the main contribution of the paper is the used of two curves: accuracy vs. perturbation budget and accuracy vs. attack strength curves to evaluate the attacks and defenses. I agree with the authors that such curves show a complete spectrum of weakness and strength of a technique, and future researches should adapt such evaluation. + I appreciate the authors’ effort to compare 16 states of the art attacks and defense techniques. - However, the study findings are not that novel. For example, it is intuitive that the robustness of a technique will vary with the perturbation budget and iteration steps. Also, PGD-based adversarial training is known to be state of the art defenses, and I am not surprised at all with their findings. - Since the authors used different defense models for CIFAR-10 and ImageNet, it becomes difficult to compare the model’s performances across the dataset. - The title is somewhat misleading---there is no benchmark tool or dataset that the paper contributed. Their main contribution is a thorough empirical study."}
{"id": "iclr2020_781", "title": "DUAL ADVERSARIAL MODEL FOR GENERATING 3D POINT CLOUD | OpenReview", "abstract": "Abstract:###Three-dimensional data, such as point clouds, are often composed of three coordinates with few featrues. In view of this, it is hard for common neural networks to learn and represent the characteristics directly. In this paper, we focus on latent space’s representation of data characteristics, introduce a novel generative framework based on AutoEncoder(AE) and Generative Adversarial Network(GAN) with extra well-designed loss. We embed this framework directly into the raw 3D-GAN, and experiments demonstrate the potential of the framework in regard of improving the performance on the public dataset compared with other point cloud generation models proposed in recent years. It even achieves state of-the-art performance. We also perform experiments on MNIST and exhibit an excellent result on 2D dataset.", "review": "Review:###Summary: The paper focuses on designing a generative framework for 3-D point data clouds. These point clouds correspond to objects shapes in 3-dimensions. According to the paper, previous approaches for generating such 3-D point clouds involved autoencoder and GANs used separately. The authors propose a framework combining both autoencoder and GAN in a single network. The authors claim that part of the network learns effective latent space embedding for 3-d point clouds corresponding to different objects and thus the entire network is more effective in generating 3-d point cloud. Experimental results are presented to support the claims for efficient embeddings, and better generation of 3-d point clouds. Decision The paper has some lacking in explanation. I am particularly interested in the answers to the following questions: 1. The encoder module (denoted as E in the paper) uses a loss function that apparently does not involve the encoder output (z in the paper). How the module weights can be updated this way is unclear. 2. G1 is updated twice in each iteration, according to algorithm 1 in paper (once during update of ??G1 and ??G). How the generator G1’s output manages to converge to output z from encoder E is unclear. 3. The authors claim that the proposed model has both better performance and efficiency. Although experimental results are provided to support claim for better performance, none have been offered for efficiency claim. (calculation of EMD distance seems to be very expensive computationally) Feedback For section 5.5, a quantitative analysis might make the claim for portability of the framework stronger. In paragraph 1 of introduction, disadvantage of 3d point cloud data can be better explained."}
{"id": "iclr2020_782", "title": "Batch Normalization is a Cause of Adversarial Vulnerability | OpenReview", "abstract": "Abstract:###Batch normalization (BN) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and common corruptions by double-digit percentages, as we show on five standard datasets. Furthermore, we find that substituting weight decay for BN is sufficient to nullify a relationship between adversarial vulnerability and the input dimension. A recent mean-field analysis found that BN induces gradient explosion when used on multiple layers, but this cannot fully explain the vulnerability we observe, given that it occurs already for a single BN layer. We argue that the actual cause is the tilting of the decision boundary with respect to the nearest-centroid classifier along input dimensions of low variance. As a result, the constant introduced for numerical stability in the BN step acts as an important hyperparameter that can be tuned to recover some robustness at the cost of standard test accuracy. We explain this mechanism explicitly on a linear ``toy model and show in experiments that it still holds for nonlinear ``real-world models.", "review": "Review:###Summary: In this empirical study, the authors identify that batch normalization -- a common technique for accelerating training -- leads to brittle representations that exhibit a lack of robustness and are more susceptible to adversarial attacks. The authors demonstrate their results on SVHN, CIFAR-10, CIFAR-100 CIFAR-10.1 using a variety of network architectures including VGG, BagNet, WideResNet, AlexNet, etc. Major Concerns: 1. As presented, the experiments are not convincing. I do not know how much of the changes in adversarial vulnerability are due to batch normalization as opposed to other facets of the training procedure that may have changed in their BN vs no-BN experiments. For instance, batch normalization usually accommodates higher learning rates and it is not clear if the authors adjusted the initial learning rate, learning rate schedule or training schedule accordingly. If so, it would be important to run a set of experiments with these parameters fixed as per the baseline no-BN models. That said, even if the authors did run these experiments, it is still not clear if the cause of adversarial vulnerability is due to BN. Consider that what is truly important in model training is not the learning rate (i.e. step size), but rather the magnitude of the changes in each weight (or the ratio of weight change to the weight). By swapping in batch normalization, the authors may just be altering the norm of the weight change in the (re-parameterized) weights. In this scenario, the gains of removing batch normalization could just as well be explained by the effective change in the learning rate, and not about batch normalization itself, c.f. Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks Yuanzhi Li, Colin Wei, Tengyu Ma https://arxiv.org/abs/1907.04595 If the differences in the adversarial vulnerability could be ascribed to effective changes in gradient updates, then this would change the interpretation of these results notably. 2. The underlying hypothesis is specious. I have several reservations about the underlying hypothesis that requires stronger evidence to overcome. In particular, I have reservations in believing that BN itself is a cause of adversarial vulnerability because BN is just a factorization of a network*s weights. That is, there is nothing *special* nor unique about BN-networks; instead, the BN factorization merely permits accelerated training efficiency. Consider the fact that a BN model may be re-expressed by merely folding in the parameters (i.e. applying the matrix multiplications) into the MLP weights or CNN filters. Thus, the numerical function approximated by the BN and the *folded* non-BN model is identical. What would it mean to say that the BN is *causing* adversarial vulnerability in the BN model given that both the BN and non-BN model perform the identical function? Another way to say this is to pretend we train a non-BN MLP or CNN model. After training the model, we could apply a BN factorization of the weights. Thus, the non-BN model may be factorized into a BN model. If the resulting BN model were adversarial vulnerable (which I suspect is the case), it would seem very hard to believe that BN was the cause of the vulnerability given it was a post-hoc factorization of the weights. That said, I could definitely imagine that the training procedure itself could lead to adversarial vulnerability (e.g. citation above) and by employing a BN factorization, one may be encouraged to use a training procedure which leads to increased vulnerability. I would encourage the authors to consider this line of attack and thus, re-orient their analysis and discussion accordingly. 3. The title is poorly worded. Not withstanding the point above, adversarial vulnerability predates BN. Likewise, non-BN models exhibit adversarial vulnerability. Thus, this title is not a great reflection of the findings of the paper. I would strongly suggest replacing *is a cause* with *increases* or *exacerbates*."}
{"id": "iclr2020_783", "title": "Superbloom: Bloom filter meets Transformer | OpenReview", "abstract": "Abstract:###We extend the idea of word pieces in natural language models to machine learning tasks on opaque ids. This is achieved by applying hash functions to map each id to multiple hash tokens in a much smaller space, similarly to a Bloom filter. We show that by applying a multi-layer Transformer to these Bloom filter digests, we are able to obtain models with high accuracy. They outperform models of a similar size without hashing and, to a large degree, models of a much larger size trained using sampled softmax with the same computational budget. Our key observation is that it is important to use a multi-layer Transformer for Bloom filter digests to remove ambiguity in the hashed input. We believe this provides an alternative method to solving problems with large vocabulary size.", "review": "Review:###This work presents Superbloom, which applies the bloom filter to the Transformer learning to deal with large opaque ids. Quantitative results demonstrate it can be efficiently trained and outperforms non-hashed models of a similar size. The authors also highlight an important distinction from learning using the entire vocabulary: the depth is crucial to get good performance. The size of the vocabulary could be largely reduced through hashing, which makes a larger embedding dimension and more complex internal transformation eligible and thus better performance. To make the story complete, it is good to have the results with the same embedding dimension and internal complexity as the baseline model to see the limitations. In Section 4, it is equally interesting to compare the performance between the bloom filter reduction and the data-driven word piece reduction. That is, models learned with the bloom filter applied to the whole vocabulary and models learned with the word pieces only. I think the empirical contribution is above the bar, but I do not think the authors gave enough credit to (Serra & Karatzoglou, 2017). The adoption of bloom filter on large opaque ids has already been proposed in (Serra & Karatzoglou, 2017) as bloom embeddings to deal with sparse high-dimensional binary-coded instances. It seems that the technical part of Superbloom, including the hashing and the inference, is the same as those in (Serra & Karatzoglou, 2017). I would appreciate a lot if the authors could revise the presentation to either emphasizing the adoption of the work, or highlighting the differences."}
{"id": "iclr2020_784", "title": "Sharing Knowledge in Multi-Task Deep Reinforcement Learning | OpenReview", "abstract": "Abstract:###We study the benefit of sharing representations among tasks to enable the effective use of deep neural networks in Multi-Task Reinforcement Learning. We leverage the assumption that learning from different tasks, sharing common properties, is helpful to generalize the knowledge of them resulting in a more effective feature extraction compared to learning a single task. Intuitively, the resulting set of features offers performance benefits when used by Reinforcement Learning algorithms. We prove this by providing theoretical guarantees that highlight the conditions for which is convenient to share representations among tasks, extending the well-known finite-time bounds of Approximate Value-Iteration to the multi-task setting. In addition, we complement our analysis by proposing multi-task extensions of three Reinforcement Learning algorithms that we empirically evaluate on widely used Reinforcement Learning benchmarks showing significant improvements over the single-task counterparts in terms of sample efficiency and performance.", "review": "Review:###This paper provides a theoretical justification for the benefit of multi-task deep RL (MTRL) with shared representations. By extending prior work (Farahmand (2011) and Maurer et al. (2016)), the authors demonstrate that the bound of MTRL can be improved if the cost of learning the shared representation at each AVI iteration can be reduced, which is mitigated as we increase the number of tasks. The author also empirically verify their theoretical results in a tabular Q-Fitted Iteration domain and also in challenging RL domains such as Mujoco. The results show that MTRL with shared representation can outperform their single task counterparts to some degree. Overall this paper adapts the theory shown in Farahmand (2011) and Maurer et al. (2016) to the setting of MTRL and demonstrates the effectiveness of using shared layers, which seems intuitive. While the theory seems a bit incremental, it’s the first paper that theoretically validates the benefits of sharing knowledge, which is a contribution to the MTRL field. I would recommend a weak accept, though I have a few concerns on experimental results, and hope that the authors can clarify them during rebuttal. Specifically, as the authors have noted, there is a wide range of prior works [1,2,3] that have empirically demonstrated the effectiveness of utilizing shared representations in MTRL. While the authors claim that the goal of the experiments is to show that MTRL with shared layers can outperform its sing task counterparts and thus they ignore other MTRL approaches. I believe that is not the main argument of the paper. The authors should provide empirical evidence on the claim that with an increasing number of tasks in MTRL, the error bound should improve and the performance of MTRL should also boost. Besides, I find the comparison where single-task training is initialized with shared representation a bit confusing. Training would definitely be improved when it’s initialized with some related pretrained features. Maybe the authors should compare this to some other methods such as initializing with single-task representation or even representation learned from training different tasks. [1] M. Hessel, H. Soyer, L. Espeholt, W. Czarnecki,S. Schmitt, and H. van Hasselt. Multi-task deep reinforcement learning with popart.arXiv preprintarXiv:1809.04474, 2018. [2] Teh, Y.W., Bapst, V., Czarnecki, W.M., Quan, J., Kirkpatrick, J., Hadsell, R.,Heess, N., Pascanu, R.: Distral: Robust multitask reinforcement learning. In: Ad-vances in Neural Information Processing Systems 30: Annual Conference on Neu-ral Information Processing Systems 2017 (2017) [3] Wulfmeier, M., Abdolmaleki, A., Hafner, R., Springenberg, J. T., Neunert, M., Hertweck, T., ... & Riedmiller, M. (2019). Regularized Hierarchical Policies for Compositional Transfer in Robotics. arXiv preprint arXiv:1906.11228."}
{"id": "iclr2020_785", "title": "Training Neural Networks for and by Interpolation | OpenReview", "abstract": "Abstract:###In modern supervised learning, many deep neural networks are able to interpolate the data: the empirical loss can be driven to near zero on all samples simultaneously. In this work, we explicitly exploit this interpolation property for the design of a new optimization algorithm for deep learning. Specifically, we use it to compute an adaptive learning-rate in closed form at each iteration. This results in the Adaptive Learning-rates for Interpolation with Gradients (ALI-G) algorithm. ALI-G retains the main advantage of SGD which is a low computational cost per iteration. But unlike SGD, the learning-rate of ALI-G uses a single constant hyper-parameter and does not require a decay schedule, which makes it considerably easier to tune. We provide convergence guarantees of ALI-G in the stochastic convex setting. Notably, all our convergence results tackle the realistic case where the interpolation property is satisfied up to some tolerance. We provide experiments on a variety of architectures and tasks: (i) learning a differentiable neural computer; (ii) training a wide residual network on the SVHN data set; (iii) training a Bi-LSTM on the SNLI data set; and (iv) training wide residual networks and densely connected networks on the CIFAR data sets. ALI-G produces state-of-the-art results among adaptive methods, and even yields comparable performance with SGD, which requires manually tuned learning-rate schedules. Furthermore, ALI-G is simple to implement in any standard deep learning framework and can be used as a drop-in replacement in existing code.", "review": "Review:###Thanks for the responses and my concerns seem to be addressed. But since I do not know much about this area, I would like to stick to my initial rating 6. ================================================================================================== This work designs a new optimization SGD algorithm named ALI-G for deep neural network with interpolation property. This algorithm only has a single hyper-parameter and doesn’t have a decay schedule. The authors provide the convergence guarantees of ALI-G in the stochastic convex setting as well as the experiment results on four tasks.This paper shows state-of-the-art results but I still have have two concerns. My main concern is that the performances of other SGD algorithms may be potentially better than the results showed in section 5 because it is not easy to tune the parameter. It would be better if the authors can tune the hyper-parameter more carefully. Take figure 3 as an example. The settings where step size bigger than 1e+1 can hardly shows something, because the step-size is too big for SGD to converge. The settings where step size smaller 1e-2 can also hardly shows something, because the step-size is too small and the experiments only runs 10k steps. It would be better if authors can do more experiments in the settings where step size is from 1e-3 to 1e+0. Moreover, the optimal step size of different optimization algorithms may differ a lot. It would be much more fair if the authors can compare the best performance of different algorithms. Another concern is that the authors only give the convergence rate of ALI-G in section 3 but haven’t make any comparisons. For example, it would be better if the authors can show that ALI-G has better convergence result than vanilla SGD without decay schedule."}
{"id": "iclr2020_786", "title": "Conditional Flow Variational Autoencoders for Structured Sequence Prediction | OpenReview", "abstract": "Abstract:###Prediction of future states of the environment and interacting agents is a key competence required for autonomous agents to operate successfully in the real world. Prior work for structured sequence prediction based on latent variable models imposes a uni-modal standard Gaussian prior on the latent variables. This induces a strong model bias which makes it challenging to fully capture the multi-modality of the distribution of the future states. In this work, we introduce Conditional Flow Variational Autoencoders (CF-VAE) using our novel conditional normalizing flow based prior to capture complex multi-modal conditional distributions for effective structured sequence prediction. Moreover, we propose two novel regularization schemes which stabilizes training and deals with posterior collapse for stable training and better match to the data distribution. Our experiments on three multi-modal structured sequence prediction datasets -- MNIST Sequences, Stanford Drone and HighD -- show that the proposed method obtains state of art results across different evaluation metrics.", "review": " The paper demonstrates how normalising flows can be conditioned. The method is then demonstrated on a set of sequential experiments which show improvements over the considered base lines. I recommend rejection of the paper, but I can see me changing that assessment if certain improvements are made. The central points are: - the paper has errors, - the paper does not respect some related work and has been published previously in parts, - the paper has a claim that is unsupported in my view, - the paper is overcrowded with annoying marketing language; the word *novel* appears 16 times according to my pdf viewer. In general I like the idea, and the presentation seems solid to a large degree. However, the above points are a show stopper for me personally. For one, the statements - p(y|x) = p(y|x, z) p(z | x) and - p(y|x) = p(y|z) p(z|x), are problematic. I would like the authors to clarify how they arrive at these. The paper starts with the claim that *prior work [...] imposes a uni-modal standard Gaussian prior on the lagent variables*. This is just wrong. The whole literature of stochastic recurrent models does not do this. See [1, 2] for starting points. Since the authors place their work in the setup of sequential prediction, this is what has to be respected. Further, the authors do not seem to be aware of a recently published work [3] that adresses *exactly* this problem. To quote from their abstract: *To this end, we modify the latent variable model by defining the likelihood as a function of the latent vari- able only and [sic] introduce an expressive multimodal prior to enable the model for capturing semantically meaningful features of the data.* I have two more questions with respect to the proposed regularisations. First, I would ask the authors to comment on the relationship of cR and the method proposed in [3]. To me, it appears as if cR is not novel, but has instead been proposed in [3] previously. Second, pR fixes the variance of q. The authors claim that the normalising flow of the conditional can undo this fixing by adequately scaling the prior. Hence, so the claim, the expressivity of the model is not reduced. This prohibits the posteriors of two distinct data points to share the same mean but not share the same variance. I request the authors to make a more formal analysis of this, as I do am not convinced how the expressivity of the model is maintained and what influence this has on the ELBO. References [1] Bayer, Justin, and Christian Osendorfer. *Learning stochastic recurrent networks.* arXiv preprint arXiv:1411.7610 (2014). [2] Chung, Junyoung, et al. *A recurrent latent variable model for sequential data.* Advances in neural information processing systems. 2015. [3] Klushyn, Alexej, et al. *Increasing the Generalisaton Capacity of Conditional VAEs.* International Conference on Artificial Neural Networks. Springer, Cham, 2019."}
{"id": "iclr2020_787", "title": "Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring | OpenReview", "abstract": "Abstract:###The use of deep pre-trained transformers has led to remarkable progress in a number of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common: Cross-encoders performing full self-attention over the pair and Bi-encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-encoder, that learns global rather than token level self-attention features. We perform a detailed comparison of all three approaches, including what pre-training and fine-tuning strategies work best. We show our models achieve state-of-the-art results on four tasks; that Poly-encoders are faster than Cross-encoders and more accurate than Bi-encoders; and that the best results are obtained by pre-training on large datasets similar to the downstream tasks.", "review": "Review:###This paper presents a new neural network architecture based on transformers called poly-encoders. These are compared against many state-of-the-art approaches including cross-encoders and bi-encoders on many large-scale datasets. Bi-encoders > Poly-encoders > Cross-encoders in terms of speed and Cross-encoders > Poly-encoders > Bi-encoders in terms of accuracy. I am not an expert in this area. However, to the best of my knowledge I don*t see anything immediately wrong with this. The experiments are also comprehensive. Therefore I recommend acceptance."}
{"id": "iclr2020_788", "title": "CRNet: Image Super-Resolution Using A Convolutional Sparse Coding Inspired Network | OpenReview", "abstract": "Abstract:###Convolutional Sparse Coding (CSC) has been attracting more and more attention in recent years, for making full use of image global correlation to improve performance on various computer vision applications. However, very few studies focus on solving CSC based image Super-Resolution (SR) problem. As a consequence, there is no significant progress in this area over a period of time. In this paper, we exploit the natural connection between CSC and Convolutional Neural Networks (CNN) to address CSC based image SR. Specifically, Convolutional Iterative Soft Thresholding Algorithm (CISTA) is introduced to solve CSC problem and it can be implemented using CNN architectures. Then we develop a novel CSC based SR framework analogy to the traditional SC based SR methods. Two models inspired by this framework are proposed for pre-/post-upsampling SR, respectively. Compared with recent state-of-the-art SR methods, both of our proposed models show superior performance in terms of both quantitative and qualitative measurements.", "review": " This work exploits the natural connection between CSC and Convolutional Neural Networks (CNN) to address CSC based image SR. Specifically, Convolutional Iterative Soft Thresholding Algorithm (CISTA) is introduced to solve CSC problem and state-of-the-art performance is achieved on popular benchmarks. [Strengths] - This paper is well-written and easy to follow. [Weaknesses] - My main concern on this work is its novelty seems to be limited. In Introduction, the authors raised 4 kinds of issues, including framework/optimization/memory/multi-scale. However, optimization and memory issues are mitigated by the CNN architectures; multi-scale issue is addressed with the help of work from Kim et al.. It seems the proposed questions are mostly addressed by the previous methods. - It seems CISTA may be the main novelty of this work. However, no comparasions with previous ISTA methods (e.g., [R1-3]) are conducted and discussed. - From Tabs. 2&3, the improvement is very limited. Besides, time complexity needs to be compared with the competitors, including flops or inference times. [R1] A Fast Proximal Method for Convolutional Sparse Coding, IJCNN 2013. [R2] Convolutional Neural Networks Analyzed via Convolutional Sparse Coding, JMLR 2017. [R3] On Multi-Layer Basis Pursuit, Efficient Algorithms and Convolutional Neural Networks, TPAMI 2018."}
{"id": "iclr2020_789", "title": "Frequency Analysis for Graph Convolution Network | OpenReview", "abstract": "Abstract:###In this work, we develop quantitative results to the learnablity of a two-layers Graph Convolutional Network (GCN). Instead of analyzing GCN under some classes of functions, our approach provides a quantitative gap between a two-layers GCN and a two-layers MLP model. Our analysis is based on the graph signal processing (GSP) approach, which can provide much more useful insights than the message-passing computational model. Interestingly, based on our analysis, we have been able to empirically demonstrate a few case when GCN and other state-of-the-art models cannot learn even when true vertex features are extremely low-dimensional. To demonstrate our theoretical findings and propose a solution to the aforementioned adversarial cases, we build a proof of concept graph neural network model with stacked filters named Graph Filters Neural Network (gfNN).", "review": "Review:###The paper shows the graph signal convolution in GCN-based models is typically a low-pass filter. Besides, the authors propose a simplified GCN-framework named gfNN, which is demonstrated on various benchmark datasets. To be honest, the paper is well-motivated, well-written. And, I enjoyed reading through the paper. However, I have some concerns regarding the technical contributions of this paper. (1) *GNN is a low-pass filter* is not a new observation. Various works (SGC, [Shuman, et al., 2013]) have studied this observation before. (2) If the frequency of the label vector is higher than the threshold of the low-pass filter (e.g., gfNN), will the model be problematic? (3) It seems the only difference between SGC and gfNN is that gfNN is equipped with one more hidden layer. The authors claim that gfNN perform better than SGC is because *the true features are non-linearly separable*. If it is the case, then the technical contribution of gfNN is trivial. Overall, I still like this paper due to its interesting point of the presentation. I vote for weak accept, but I am fine if it is rejected."}
{"id": "iclr2020_790", "title": "$\textrm{D}^2$GAN: A Few-Shot Learning Approach with Diverse and Discriminative Feature Synthesis | OpenReview", "abstract": "Abstract:###The rich and accessible labeled data fuel the revolutionary success of deep learning. Nonetheless, massive supervision remains a luxury for many real applications, boosting great interest in label-scarce techniques such as few-shot learning (FSL). An intuitively feasible approach to FSL is to conduct data augmentation via synthesizing additional training samples. The key to this approach is how to guarantee both discriminability and diversity of the synthesized samples. In this paper, we propose a novel FSL model, called GAN, which synthesizes Diverse and Discriminative features based on Generative Adversarial Networks (GAN). GAN secures discriminability of the synthesized features by constraining them to have high correlation with real features of the same classes while low correlation with those of different classes. Based on the observation that noise vectors that are closer in the latent code space are more likely to be collapsed into the same mode when mapped to feature space, GAN incorporates a novel anti-collapse regularization term, which encourages feature diversity by penalizing the ratio of the logarithmic similarity of two synthesized features and the logarithmic similarity of the latent codes generating them. Experiments on three common benchmark datasets verify the effectiveness of GAN by comparing with the state-of-the-art.", "review": " This paper presents a meta-learning method that learns a generative model that can augment the support set of a few-shot learner. The model is inspired from GANs. It optimizes a combination of losses, that includes the WGAN loss, a loss that measures the ability of a prototypical network to classify a query set and a loss that promotes diversity of samples. Positive results are demonstrated on various few-shot learning benchmark. I*m afraid the submission has some important flaws, that keep me from recommending acceptance. First, the proposed method is not particularly novel. It is a slight variant on *Low-shot learning from imaginary data* (Wang et al. 2018), where the main difference seems to be the addition of a WGAN loss and the proposed diversity-promoting loss. There are also some differences in the architecture of the generative model, which here conditions on a prototype of a class (instead of an example) and is trained to samples that can*t be distinguished from the prototypes. Also, here the prototypical network that classifies the query set is fed the samples from the generative model, and not the original support set examples (which I find to be an odd design choice), unlike in Wang et al. (2018). Overall, I find that the paper doesn*t demonstrate that the proposed method is superior from (Wang et al. 2018). Crucially, it doesn*t actually compare performance with that work. A first step in this direction would be to add to Table 1 an experiment where *CR* only is used, which would correspond to something close (but not identical) to Wang et al. (2018). But what would be preferable is to compare with Wang et al. (2018) as it is described in that paper. And generally, I don*t find that the submission makes a good case for why the WGAN and AR losses are necessary. Indeed, the objective of Equation 4 is trivially solved by having G ignore the noise vector z_i and copy s at its output (which would bring no actual data augmentation). This solution also doesn*t correspond to a mode collapse situation either, even though this is the motivation behind the diversity-promoting loss of Equation 7. So, even at a conceptual level, the motivation for the proposed method is not strong. Second, the results on miniImageNet are actually not better than the current SOTA. Indeed, for example, paper *Meta-Learning with Differentiable Convex Optimization* by Lee et al. (CVPR 2019) reports 64.09% and 80.00% for 1 and 5 shot respectively (and 62.64%, 78.63% when not retraining on train+valid, which is probably a fairer comparison). Personally, I could still be fine with this if a proper comparison with Wang et al. (2018) was made and showed better performance. However, ideally the proposed method would be shown to also improve over the state of the art. I also note that the experiments used image sizes of 224x224, while other papers have used the 84x84 size. This may actually give an unfair advantage to the propose method, which has nothing to do with its core contribution. Third, the paper is not particularly well written. I*ve found many phrases in the text that are not grammatically correct. I*d also be willing to be lenient on this point, given that it*s still possible to understand the technical contribution, but I*d strongly recommend that they authors better proofread the writing. For me to consider increasing my rating, I would like: - A proper, convincing and fair comparison with Wang et al. (2018) be presented. - A better justification/motivation be given for the WGAN and anti-collapse regularization losses. Finally, here are some minor issues I*ve found which the authors should consider: - *susceptible for mode collapse* => susceptible to mode collapse - *penalty on the case* => penalty for the case - *suspect to visual similarity* => rephase - *Regarding to the* => Regarding the - *the details how* => the details of how - *for in a typical* => in a typical - in Equation 5, in the denominator, F(q) should be replace by q - in Equation 7, the loss is not defined when the cosine similarity is negative (which can happen, unless both vectors have strictly positive values) - why train a softmax classifier at evaluation time, instead of using a prototypical network predictor as in Equation 5? - similarly, why use an SVM for the baseline? - in Table 1, why is the check mark for cGAN, in the second column, different from other check marks? - the CUB dataset wasn*t used in Ravi & Larochelle (2017), so it*s not the right reference for the splits of that dataset - similarly, where do the CUB results come from for Meta-LSTM, given that it wasn*t used in Ravi & Larochelle (2017)?"}
{"id": "iclr2020_791", "title": "Multiagent Reinforcement Learning in Games with an Iterated Dominance Solution | OpenReview", "abstract": "Abstract:###Multiagent reinforcement learning (MARL) attempts to optimize policies of intelligent agents interacting in the same environment. However, it may fail to converge to a Nash equilibrium in some games. We study independent MARL under the more demanding solution concept of iterated elimination of strictly dominated strategies. In dominance solvable games, if players iteratively eliminate strictly dominated strategies until no further strategies can be eliminated, we obtain a single strategy profile. We show that convergence to the iterated dominance solution is guaranteed for several reinforcement learning algorithms (for multiple independent learners). We illustrate an application of our results by studying mechanism design for principal-agent problems, where a principal wishes to incentivize agents to exert costly effort in a joint project when it can only observe whether the project succeeded, but not whether agents actually exerted effort. We show that MARL converges to the desired outcome if the rewards are designed so that exerting effort is the iterated dominance solution, but fails if it is merely a Nash equilibrium.", "review": "Review:###This work studies learning under independent MARL, and shows theoretically and experimentally that two independent MARL algorithms converge for games that can be solved by iterated dominance. This work is clear and well-written, but I do not understand what the contribution of this work is to the literature. The fact that standard MARL learning rules (e.g. independent Q learning) converge in games with iterated dominance solutions is a very well-known result in Learning in Games (see [1], [2]). The authors examined slightly different learning rules (REINFORCE and MCPI), but I would expect that almost any reasonable learning rule would converge in iterated-dominance-solvable games; if anything, it would be surprising if this were *not* the case. The applications of the convergence result result to *noisy effort* games is pretty standard and the results expected based on the theory. Question to the authors: - How does this work differ from the known results about convergence of naive learners in iterated-dominance-solvable games? [1] Michael Bowling, *Convergence Problems of General-Sum Multiagent Reinforcement Learning*, Sec. 5.2 [2] Fudenberg & Levine, 1999"}
{"id": "iclr2020_792", "title": "Deep Mining: Detecting Anomalous Patterns in Neural Network Activations with Subset Scanning | OpenReview", "abstract": "Abstract:###This work views neural networks as data generating systems and applies anomalous pattern detection techniques on that data in order to detect when a network is processing a group of anomalous inputs. Detecting anomalies is a critical component for multiple machine learning problems including detecting the presence of adversarial noise added to inputs. More broadly, this work is a step towards giving neural networks the ability to detect groups of out-of-distribution samples. This work introduces ``Subset Scanning methods from the anomalous pattern detection domain to the task of detecting anomalous inputs to neural networks. Subset Scanning allows us to answer the question: *``Which subset of inputs have larger-than-expected activations at which subset of nodes?* Framing the adversarial detection problem this way allows us to identify systematic patterns in the activation space that span multiple adversarially noised images. Such images are ``*weird together*. Leveraging this common anomalous pattern, we show increased detection power as the proportion of noised images increases in a test set. Detection power and accuracy results are provided for targeted adversarial noise added to CIFAR-10 images on a 20-layer ResNet using the Basic Iterative Method attack.", "review": "Review:###The authors apply linear time subset scanning to find groups of anomalous inputs and network activations. They further use this method to detect effects of the same adversarial perturbation algorithm over a set of images. Major comments: Overall this is quite interesting work, and seems like a promising method to detect groups of anomalies. However, this work is not complete without comparison to existing methods, the lack of which makes it impossible to evaluate its usefulness. See [1,2]. Minor questions/comments: The results seem to be applied to ReLU activation networks and anomalous signals are described as having higher activation that the background or the clean images. Could you clear up whether this method works for all activations or just ReLU networks? I’m envisioning a case where the anomalous images are all anomalous because they are all very similar. If I put 100 of the same (or very similar images) would this be something that this method could detect? Your results seem to suggest that adversarial perturbation methods produce adversarial images with activations that are more extreme than natural images. This doesn’t seem immediately obvious to me and I would be interested in further exploration of this direction. A value of epsilon=0.02 was used in experiments for BIM. While I agree that smaller values of epsilon are harder to detect it would be useful to have an evaluation of performance over smaller values of epsilon even if these did not perform as well. [1] Xiong, L., P ?oczos, B., Schneider, J., Connolly, A., VanderPlas, J.: Hierarchical probabilistic models for group anomaly detection. In: AISTATS 2011 (2011) [2] Chalapathy R., Toth E., Chawla S. (2019) Group Anomaly Detection Using Deep Generative Models. In: ECML PKDD 2018. Lecture Notes in Computer Science, vol 11051. Springer, Cham"}
{"id": "iclr2020_793", "title": "Leveraging inductive bias of neural networks for learning without explicit human annotations | OpenReview", "abstract": "Abstract:###Classification problems today are typically solved by first collecting examples along with candidate labels, second obtaining clean labels from workers, and third training a large, overparameterized deep neural network on the clean examples. The second, labeling step is often the most expensive one as it requires manually going through all examples. In this paper we skip the labeling step entirely and propose to directly train the deep neural network on the noisy raw labels and early stop the training to avoid overfitting. With this procedure we exploit an intriguing property of large overparameterized neural networks: While they are capable of perfectly fitting the noisy data, gradient descent fits clean labels much faster than the noisy ones, thus early stopping resembles training on the clean labels. Our results show that early stopping the training of standard deep networks such as ResNet-18 on part of the Tiny Images dataset, which does not involve any human labeled data, and of which only about half of the labels are correct, gives a significantly higher test performance than when trained on the clean CIFAR-10 training dataset, which is a labeled version of the Tiny Images dataset, for the same classification problem. In addition, our results show that the noise generated through the label collection process is not nearly as adversarial for learning as the noise generated by randomly flipping labels, which is the noise most prevalent in works demonstrating noise robustness of neural networks.", "review": "Review:###Summary The paper exploits the training dynamics of deep nets: gradient descent fits clean labels much faster than the noisy ones, therefore early stopping resembles training on the clean labels. The paper shows that early stopping the training on datasets with noisy labels can achieve classification performance higher than when training on clean labels (on condition that the total number of clean labels in the noisy training set is sufficiently large). The paper also makes the point that noise introduced during data collection are different from artificially generated noise through randomly flipping the labels of a clean dataset. The latter is often done in the literature. The “real” noise is more structured and therefore it is easier to fit and less harmful to the classification performance. Strengths The paper is well written. The results are very interesting even though they are very intuitive and simple. Weaknesses The idea seems to be known. For example, Better Generalization with On-the-fly Dataset Denoising https://openreview.net/forum?id=HyGDdsCcFQ The paper talks about early stopping. As shown by the above paper, it is also a function of the learning rate. Please comment on what happens in the case of small learning rate and early stopping. It would have been great to prove the theorem for deep learning. The result is limited to linear models with large number of random features. The paper does not make it clear under what conditions early stopping prevents the model from memorizing bad labels. The paper focuses on classification. Will the claims hold for other task types such as object detection, segmentation, etc?"}
{"id": "iclr2020_794", "title": "Fully Quantized Transformer for Improved Translation | OpenReview", "abstract": "Abstract:###State-of-the-art neural machine translation methods employ massive amounts of parameters. Drastically reducing computational costs of such methods without affecting performance has been up to this point unsolved. In this work, we propose a quantization strategy tailored to the Transformer architecture. We evaluate our method on the WMT14 EN-FR and WMT14 EN-DE translation tasks and achieve state-of-the-art quantization results for the Transformer, obtaining no loss in BLEU scores compared to the non-quantized baseline. We further compress the Transformer by showing that, once the model is trained, a good portion of the nodes in the encoder can be removed without causing any loss in BLEU.", "review": "Review:###This paper proposes an 8-bit quantization method to quantize the Transformer, which is a popular machine translation model. Uniform min-max quantization is used for computational expensive operations during the inference. The authors also propose to bucket the weights before quantization to reduce quantization error. Experiments are performed on WMT translation tasks. The paper is overall easy to follow. One of my main concerns is about the novelty since both the min-max quantization and bucketing the weights before quantization are not new. Another concern is that one important goal of quantization is to speed up inference, however, no inference time is reported in the paper. Can the authors provide the inference time results on some typical cpu/gpus? From Table 4, the proposed quantization has much larger BLEU gain than the base transformer after 100k training steps, than that after full training (Table 2). Does this mean the proposed quantization converges slower at the later stage of training? How many training steps are run to reach the result of the proposed method in Table 1? Can the authors compare the training curve of the base transformer and the quantized one? From Table 4, the earlier the quantization starts, the better the final performance. What about the performance of training from scratch? Will this give better performance? ------------post rebuttal comment----------- I thank the authors for their detailed response. However, the concerns on the novelty of the quantization technique and the inference efficiency of the proposed method still remain. Thus I keep my score unchanged. -------------------------------------------------------"}
{"id": "iclr2020_795", "title": "Learning Out-of-distribution Detection without Out-of-distribution Data | OpenReview", "abstract": "Abstract:###Deep neural networks have attained remarkable performance when applied to data that comes from the same distribution as that of the training set, but can significantly degrade otherwise. Therefore, detecting whether an example is out-of-distribution (OOD) is crucial to enable a system that can reject such samples or alert users. Recent works have made significant progress on OOD benchmarks consisting of small image datasets. However, such methods rely on training or tuning with both in-distribution and out-of-distribution data. The latter is generally hard to define \textit{a-priori}, and its selection can easily bias the learning. In this work, we focus on the feasibility of learning OOD detection without OOD data, proposing two strategies for the problem. We specifically propose to decompose confidence scoring as well as a modified input pre-processing method. We show that both of these significantly help detection performance, all without tuning to any out-of-distribution data during training. Our further analysis on a larger scale image dataset shows that the two types of distribution shifts, specifically semantic shift and non-semantic shift, present a significant difference in the difficulty of the problem, providing an analysis of when the proposed strategies do or do not work.", "review": " - Summary: This paper proposes to train an OOD detection model without OOD data, because OOD is hard to define a-priori, such that OOD observed during training is not guaranteed to be matched with OOD at test time. They propose a decomposed confidence strategy and modification on input preprocessing. They show the superiority of their method over the state-of-the-art methods, when OOD is not available. - Decision and supporting arguments: Weak reject. 1. The analysis around `g` is confusing. After equation 5, they state that *`g(x)` is shared among all classes like p(d_in|x).* At this point, I expect that `g` is a confidence score, but at last they used `max_i h_i` for OOD detection, and the role of `g` turned out to be just a learnable temperature in softmax. Also, even the learning objective for `g` is not introduced. Also, it seems the authors tried to make some relationship between equation 4 and their method, but I couldn*t find their correlation. 2. Contribution is limited. To my understand, the main novelty in this paper is on the learnable temperature `g`, and the others are already explored in the literature. The proposed three similarity measures are used in prior works, i.e., inner-product in Hendrycks & Gimpel (2017), Euclidean distance in Lee et al. (2018b) (though they claimed that Mahalanobis distance is better), and Cosine similarity in Techapanurak & Okatani (2019). The essential modification on input preprocessing strategy is simply removing OOD data from the validation set, and this is a natural setting as their proposal is to train an OOD detection model without OOD data. - Comments: 1. How is `g` trained? What is the learning objective and how the ground truth value is assigned? Since the activation of `g` is sigmoid, a natural conjecture is that in-distribution and OOD would be labeled as 1 and 0 (or reversed). However, they said they do not train their model with any OOD, so it is hard to figure out the learning objective and the ground truth value for `g`. 2. Why don*t you compare with the GAN-based OOD generation (Lee et al., 2018a) and the adversarial attack-based validation (Lee et al., 2018b)? They do not require any OOD as well. In section 2.1, the authors set the hyperparameter `alpha_l` in the Mahalanobis method to be uniform because no OOD for validation is available, but actually Lee et al. (2018b) proposed the adversarial attack-based validation method to avoid the dependency on OOD. 3. Notation is confusing. `f` is used for multiple parts: `f_i = h_i/g` in eq 5 and `h` is a function of `f^p`. Some notations like `w_g` and `b_g` are not defined."}
{"id": "iclr2020_796", "title": "Compositional Visual Generation with Energy Based Models | OpenReview", "abstract": "Abstract:###Humans are able to both learn quickly and rapidly adapt their knowledge. One major component is the ability to incrementally combine many simple concepts to accelerates the learning process. We show that energy based models are a promising class of models towards exhibiting these properties by directly combining probability distributions. This allows us to combine an arbitrary number of different distributions in a globally coherent manner. We show this compositionality property allows us to define three basic operators, logical conjunction, disjunction, and negation, on different concepts to generate plausible naturalistic images. Furthermore, by applying these abilities, we show that we are able to extrapolate concept combinations, continually combine previously learned concepts, and infer concept properties in a compositional manner.", "review": "Review:###The paper talks about training multiple energy-based models for each concept and then combining them in different ways to construct a composite model. For each composition of concepts, the samples from the composite energy-based model follow the logic of composition. The main novelty of the paper is to define an energy-based model for each logical composition of the concepts based on the energy-based model of each concept. Regarding the usage of energy-based models, the idea is interesting. However, I am not sure about the existing works using alternative approaches. My main question: the experiments only combine the energy models trained for the same domain. What does happen if you combine the energy models from different domains, for example, Celebs and color from the objects? What if you combine models with different architectures? -- The qualitative images show the method is working, but the outputs are not great!!!! -- I really want to see the outputs for old + male + smiling + heavy hair in contrast to Figure 3? -- The *concept inference is not clear from the text. The goal is to infer a concept from several given images of the same concept. It is not clear from the text, but I assume you compute an approximate likelihood (by estimating the partition function with a single sample using Langevin dynamics) of images for each concept and pick the concept with the highest value. So what do you mean by this: *We can then obtain maximum a posteriori (MAP) estimates of concepts by minimizing the energy of the above expression.* --I had a hard time following the experiments of sections 3.4 and 3.5, mostly because of unclear writing. --Some sentences are not clear, for example: *To test this, we construct a sphere dataset consisting of sphere of all sizes at a specified percentage of the rightmost positions and large spheres remaining positions, with size/position annotations.* --Equations 4 and 10 need negation for the energy term to be consistent with Equation 1. --The citation for Langevin dynamics should be Welling and Teh, 2011."}
{"id": "iclr2020_797", "title": "Counterfactual Regularization for Model-Based Reinforcement Learning | OpenReview", "abstract": "Abstract:###In sequential tasks, planning-based agents have a number of advantages over model-free agents, including sample efficiency and interpretability. Recurrent action-conditional latent dynamics models trained from pixel-level observations have been shown to predict future observations conditioned on agent actions accurately enough for planning in some pixel-based control tasks. Typically, models of this type are trained to reconstruct sequences of ground-truth observations, given ground-truth actions. However, an action-conditional model can take input actions and states other than the ground truth, to generate predictions of unobserved counterfactual states. Because counterfactual state predictions are generated by differentiable networks, relationships among counterfactual states can be included in a training objective. We explore the possibilities of counterfactual regularization terms applicable during training of action-conditional sequence models. We evaluate their effect on pixel-level prediction accuracy and model-based agent performance, and we show that counterfactual regularization improves the performance of model-based agents in test-time environments that differ from training.", "review": "Review:###This paper considers regularization based on *counterfactual* trajectories. Namely, it suggests two losses, action-control and disentanglement regularization. It experimentally evaluates the benefits of such regularization in the StarIntruders environment. The paper is well written and explained. Issues: 1) Authors evaluated the two suggested regularizations in separate. I would like to also see numbers from a combination of these. 2) I think the related work is missing a large line of work on *auxiliary tasks*. It seems to me that this paper would exactly fit within that scope? 3) My main issue is the evaluation. The evaluation is done on a in-house game and compares to very few methods. For a paper that has very little theory and thus most of the value is in the empirical evaluation, I think that is a problem. If authors opted for example for Space Invaders (they do say it is similar) or simply more games, one would have many more existing numbers to compare against. Minor issues: 1) The first regularization - action control regularization is motivated by the idea that there is always an action that changes the state. While true for most environments, this does not hold in general. Summary: Overall, this paper has potential but I don not believe is good enough - I suggest a reject. The main problem is that the idea is relatively simple, there is no theory and thus the crucial piece of the paper has to be the empirical evaluation. And the evaluation only compares to a single method with no regularization, no auxiliary tasks and reports only experiments on a single game."}
{"id": "iclr2020_798", "title": "On the Tunability of Optimizers in Deep Learning | OpenReview", "abstract": "Abstract:###There is no consensus yet on the question whether adaptive gradient methods like Adam are easier to use than non-adaptive optimization methods like SGD. In this work, we fill in the important, yet ambiguous concept of ‘ease-of-use’ by defining an optimizer’s tunability: How easy is it to find good hyperparameter configurations using automatic random hyperparameter search? We propose a practical and universal quantitative measure for optimizer tunability that can form the basis for a fair optimizer benchmark. Evaluating a variety of optimizers on an extensive set of standard datasets and architectures, we find that Adam is the most tunable for the majority of problems, especially with a low budget for hyperparameter tuning.", "review": "Review:###The main contributions of the submission are: 1. A comprehensive empirical comparison of deep learning optimizers, with their performance compared under different amount of hyper-parameter tuning (they perform hyper-parameter tuning using random search). 2. The introduction of a novel metric that tries to capture the *tunability* of an optimizer. This metric attempts to trade off the performance of an optimizer when tuned only with a small number of hyper-parameter trials, and its performance when carefully tuned. The metric is defined as a weighted average of the performance after tuning with i random trials, with i that goes from 1 to K. The weights of this weighted average and K are *hyper-parameters* of the metric itself. They use K=100 and suggest 3 possible choices of weights. The paper appears to treat 2. as the main contribution. However, I do not think the metric they introduce is good enough to be recommended in future work, when comparing tunability of optimizers (or other algorithms with hyperparameters). The reason is that simpler methods provide just as much information, and do not rely on the need of interpreting the choice of the weights and K. This point is proven in the paper itself, where for example Figure 2 provides a more concrete and easier to interpret information than the tunability metric, similar graphs could be easily provided per dataset. Similarly, figure 3 as well as figures 5-7 and 8 in the appendix provide very good information about the tunability of the various optimizers without using the introduced metrics. Information similar (although not identical) to that summarized in table 5 could be captured by substituting the 3 metrics with the best performance after tuning for 4, 16 and 64 iterations respectively (just as examples). A stronger contribution is 1., which however is somewhat incremental compared to similar comparisons made in the past. Comparisons which, while mentioned, should perhaps have been discussed and compared more in detail in this work. Overall, I do not feel the comparisons dramatically change the qualitative understanding the field has of the different optimizers and their tunability. They also suggest that when the tuning budget is low, using Adam but tuning only the learning rate is beneficial, which could be a valuable and practical suggestion. I enjoyed reading the submission, which is very clearly written, but due to the relatively limited value of the contributions, and excessive focus on the tunability metric which I do not feel is giustified, I slightly lean against acceptance here at ICLR. I do think, however, that it would make a great submission to a smaller venue or workshop. Other comments/notes: * One aspects that is mostly left out of the discussion (except from one side comment) is the wallclock time, as some optimizers might be on average quicker to train (for example due to quicker convergence), this can easily lead it to be quicker to tune even though it requires a higher budget of trials. I think it would be worth discussing this more. * minor: in figure 8 in the appendix, the results after 100 iterations is, as far as I understand, over a single replication, so is not particularly reliable (and will always be 100% of a single optimizer) * similarly to the above, if the configurations are always sampled from the same 100, confidence intervals in the graphs become less reliable as the budget increases."}
{"id": "iclr2020_799", "title": "Capsules with Inverted Dot-Product Attention Routing | OpenReview", "abstract": "Abstract:###We introduce a new routing algorithm for capsule networks, in which a child capsule is routed to a parent based only on agreement between the parent*s state and the child*s vote. Unlike previously proposed routing algorithms, the parent*s ability to reconstruct the child is not explicitly taken into account to update the routing probabilities. This simplifies the routing procedure and improves performance on benchmark datasets such as CIFAR-10 and CIFAR-100. The new mechanism 1) designs routing via inverted dot-product attention; 2) imposes Layer Normalization as normalization; and 3) replaces sequential iterative routing with concurrent iterative routing. Besides outperforming existing capsule networks, our model performs at-par with a powerful CNN (ResNet-18), using less than 25% of the parameters. On a different task of recognizing digits from overlayed digit images, the proposed capsule model performs favorably against CNNs given the same number of layers and neurons per layer. We believe that our work raises the possibility of applying capsule networks to complex real-world tasks.", "review": " Authors improve upon dynamic routing between capsules by removing the squash function (norm normalization) and apply a layerNorm normalization instead. Furthermore, they experiment with concurrent routing rather than sequential routing (route all caps layers once, then all layers concurrently again and again). This is an interesting development since provides better gradient in conjunction with layerNorm. They report results on Cifar10 and Cifar100 and achieve similar to CNN (resnet) performance. First, I want to point out that inverted attention is exactly what happens in dynamic routing (sabour et al 2017), proc. 1 line 4,5, and 7. In dynamic routing the dot product with the next layer capsule is calculated and then normalized over all next layer capsules. The only difference that I notice between alg. 1 here and proc. 1 there is replacement of squash with layer norm. There is no *reconstructing the layer bellow* in Dynamic routing as authors suggest in intro. Second, the Capsules are promised to have better viewpoint generalizability than CNNs while having comparable performance. Replacing the 1 convolution layer with a ResNet backbone and replacing the activation with a classifier on top seems reducing the proposed CapsNet to the level of CNNs in terms of Viewpoint Generalization. Why should someone use this network rather than the ResNet itself? Fewer number of parameters by itself is not interesting, the reason it is reported usually is that it indicates lower memory consumption or fewer flops. Is that the case when comparing the baseline ResNet with the proposed CapsNet? Otherwise, a set of experiments showcasing the viewpoint generalizability of proposed CapsuleNetworks might only justify the switch between resnets to the proposed capsnets. Thirdly, Fig. 4 top images seems to indicate all 3 routing procedures are following the same Learning Rate schedule. In the text it is said that optimization hyperparameters are tuned individually. Did authors tune learning rate schedule individually? Forth, the proper baseline for the current study is the dynamic routing CapsNet. Why the multiMNIST experiment lacks comparison with dynamic routing capsnet? For the reasons above, the manuscript in its current format is not ready for publication. ------------------------------------------------------rebuttal Thank you for your response. I acknowledged the novel contributions of this work. My comment was that some claims in the paper are not right. i.e. *inverted dot-product attention* is not new and *reconstructing the layer bellow* does not happen in Sabour et al . Parallel execution + layer norm definitely is novel and significant. Regarding the LR-schedule, I am not sure how fair it is to use same hyper-params tuned for the proposed method on the baselines. Regarding the viewpoint, the diverseMultiMNIST is two over lapping MNIST digits shifted 6 pixels. There is no rotation or scale in this dataset. An example experiment verifying the viewpoint generalizability of the proposed model is training on MNIST testing on AFFNIST."}
{"id": "iclr2020_800", "title": "Frequency-based Search-control in Dyna | OpenReview", "abstract": "Abstract:###Model-based reinforcement learning has been empirically demonstrated as a successful strategy to improve sample efficiency. Particularly, Dyna architecture, as an elegant model-based architecture integrating learning and planning, provides huge flexibility of using a model. One of the most important components in Dyna is called search-control, which refers to the process of generating state or state-action pairs from which we query the model to acquire simulated experiences. Search-control is critical to improve learning efficiency. In this work, we propose a simple and novel search-control strategy by searching high frequency region on value function. Our main intuition is built on Shannon sampling theorem from signal processing, which indicates that a high frequency signal requires more samples to reconstruct. We empirically show that a high frequency function is more difficult to approximate. This suggests a search-control strategy: we should use states in high frequency region of the value function to query the model to acquire more samples. We develop a simple strategy to locally measure the frequency of a function by gradient norm, and provide theoretical justification for this approach. We then apply our strategy to search-control in Dyna, and conduct experiments to show its property and effectiveness on benchmark domains.", "review": "Review:###Summary: This paper basically built upon [1]. The authors propose to do sampling in the high-frequency domain to increase the sample efficiency. They first argue that the high-frequency part of the function is hard to approximate (i.e., needs more sample points) in section 3.1. They argue that the gradient and Hessian can be used to identify the high-frequency region. And then they propose to use g(x)=||gradient||+||Hessian || as the sampling metric as illustrated in Algorithm 1. To be noticed that, they actually hybrid the proposed metric (6) and the value-based metric (7, proposed in [1]) in their algorithm. Strength: Compared to [1], their experiment environment seems more complicated (MazeGridWorld vs. GridWorld). Figure 3 shows that their method converges faster than Dyna-Value. Figure 5 is very interesting. It shows that their method concentrates more on the important region of the function. Weakness: In footnote 3: I don*t see why such an extension is natural. In theorem 1, why the radius of the local region has to be? Theorem1 only justifies the average (expectation) of gradient norm is related to the frequency. The proposed metric , however, is evaluated on a single sample point. So I think if adding some perturbations to (and then take the average) when evaluating will be helpful. The authors only evaluate their algorithm in one environment, MazeGridWorld. I would like to see the experiment results of using only (6) as the sampling rule. What kind of norm are you using? (||gradient||, ||hessian||) Why is the combination of gradient norm and hessian norm? What will be the performance of using only gradient or hessian? Figure 4(b), DQN -> Dyna Reference: [1] Hill Climbing on Value Estimates for Search-control in Dyna"}
{"id": "iclr2020_801", "title": "Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model | OpenReview", "abstract": "Abstract:###Deep reinforcement learning (RL) algorithms can use high-capacity deep networks to learn directly from image observations. However, these kinds of observation spaces present a number of challenges in practice, since the policy must now solve two problems: a representation learning problem, and a task learning problem. In this paper, we aim to explicitly learn representations that can accelerate reinforcement learning from images. We propose the stochastic latent actor-critic (SLAC) algorithm: a sample-efficient and high-performing RL algorithm for learning policies for complex continuous control tasks directly from high-dimensional image inputs. SLAC learns a compact latent representation space using a stochastic sequential latent variable model, and then learns a critic model within this latent space. By learning a critic within a compact state space, SLAC can learn much more efficiently than standard RL methods. The proposed model improves performance substantially over alternative representations as well, such as variational autoencoders. In fact, our experimental evaluation demonstrates that the sample efficiency of our resulting method is comparable to that of model-based RL methods that directly use a similar type of model for control. Furthermore, our method outperforms both model-free and model-based alternatives in terms of final performance and sample efficiency, on a range of difficult image-based control tasks. Our code and videos of our results are available at our website.", "review": "Review:###This paper proposes an actor-critic method that tries to aid learning good policies via learning a good representation of the state space (via a latent variable model). In actor-critic methods, the critic is learnt to evaluate policies in the latent space, which further helps with efficient policy optimization. The proposed method is evaluated on image-based control tasks, with baseline evaluations against both model-based and model-free methods in terms of sample efficiency. _x000b_ - The key argument is that learning policies in the latent space is more efficient, as it is possible to learn good representations in the latent space. There are quite a few recent works (e.g DeepMDP, Gelada et al., 2019; Dadashi et al., 2019) that talks about representation learning in RL, and yet the paper makes no relations or references to previous works. I find it surprising that none of the past related works are mentioned in the paper. _x000b_ - I find the arguments on solving a POMDP instead of a MDP a bit vague in this context. I understand that the goal is to solve image based control tasks - for which learning good representations via a latent variable model might be useful, but it does not explicitly require references to a POMDP? In most ALE tasks, we have pixel based observations too, which makes the ALE environments a POMDP in some sense, but we use approximations to it to make it equivalent to a MDP with sufficient history. The arguments on POMDP seems rather an additional mention, with no necessary significance to it?_x000b_ - The paper mentions solving RL in the learned latent space, which is empirically proposed to be a good approach without theoretical justifications. There are several recent works that tries to understand the representation learning in RL problem from a theoretical perspective too - it would be useful to see where this approach stands in light of those theoretical results? Otherwise, the contribution seems rather limited : solving RL in latent space is useful, but there are no justifications to it? Why should this approach even be adapted or what is the significance of it?_x000b_ - The proposed actor-critic method in the latent space is built on top of Soft Actor-Critic (SAC). I understand this is a design/implementation approach building from previous works - but it would have been useful to add more context as to what it means to learn a critic in the latent space. If the critic evaluates a policy in the latent space - then is this a good policy evaluation for actor-critic itself? Why or why not? I do not understand why the critic evaluation in the latent space is even a good approach? _x000b_ - My first impression was that the paper proposes a separate auxilliary objective for learning good representations based on which actor-critic algorithms can be made more efficient. However, this does not seem to be the case directly? Following on previous point - I find the argument of solving a critic in the latent space rather vague. _x000b_ - The sequential latent variable model proposed is based on existing literature. This can be any latent variable model (e.g VAEs), but I understand, as mentioned in the paper, the design choice of using sequential models to capture the temporal aspect. _x000b_ - The proposed algorithm is in fact a combination of SAC and sequential latent variable models, both of which are well-known in the literature. The SLAC algorithm combines these to solve image-based control tasks. As per equation 10, which is the regular policy optimization objective with max entropy - the only difference is that the critic is evaluated in the latent space. This appears to me as more of an engineeering choice, and experimentally one that perhaps give good results - but the lack of justifications of why equation 10 is even the right objective to solve makes the paper rather less appealing. _x000b_ - I think overall the contribution of the paper is rather limited. It is more of an experimental design and engineering approach that combines previous known techniques. The paper mentions learning good representations for RL, without any references or justifications - and it appears that overall there are bold claims made in the paper but it lacks significant scientific contribution. _x000b_ - Experimental evaluations are made on image based control tasks. Experimental results are compared to few baselines - but it is not clear whether these are even the right baselines. For example, it would have been good to include analysis of the proposed model with different latent variable models (including VAE) to perhaps justify the choice of the latent variable model. Results in figure 5 appear a bit concerning to me - these are mostly the standard Mujoco tasks from the OpenAI suite. Are these all image based benchmarks too, or the standard baselines? It is not clear from the text. Assuming they are standard baselines, the comparisons made are rather unfair (for example : SAC and MDP performs much better on tasks like HalfCheetah-v2). Why are the baselines performing so poorly in the results?_x000b_ - Overall, I think the paper needs more work in terms of writing and justifying the choice of the approach. There are significant references missing in the paper. Most importantly, there are quite a few claims made in the paper which are not properly justified, that makes the overall contribution and novelty of the paper rather limited. I would tend for a rejection of this paper, as it requires more work - both in terms of theoretical justifications (including references) and experimetnal ablation studies and more simpler benchmarks explaining the choice of the approach."}
{"id": "iclr2020_802", "title": "One-Shot Neural Architecture Search via Compressive Sensing | OpenReview", "abstract": "Abstract:###Neural architecture search (NAS), or automated design of neural network models, remains a very challenging meta-learning problem. Several recent works (called *one-shot* approaches) have focused on dramatically reducing NAS running time by leveraging proxy models that still provide architectures with competitive performance. In our work, we propose a new meta-learning algorithm that we call CoNAS, or Compressive sensing-based Neural Architecture Search. Our approach merges ideas from one-shot NAS approaches with iterative techniques for learning low-degree sparse Boolean polynomial functions. We validate our approach on several standard test datasets, discover novel architectures hitherto unreported, and achieve competitive (or better) results in both performance and search time compared to existing NAS approaches. Further, we provide theoretical analysis via upper bounds on the number of validation error measurements needed to perform reliable meta-learning; to our knowledge, these analysis tools are novel to the NAS literature and may be of independent interest.", "review": "Review:###This paper proposes a new algorithm for one-shot neural architecture search (NAS) via compressive sensing. The authors propose a new search strategy, as well as a slightly different search space compared to DARTS [1], ProxylessNAS [2], etc. They use architecture samples from the one-shot model evaluated with the search parameters as a surrogate of the true objective in order to speed-up the search. Afterwards, these surrogate function evaluations are used to compute Fourier coefficients which are eventually used to optimize the vector of binary parameters encoding the architecture. Overall, I think the proposed algorithm is interesting and of practical usefulness. However, in terms of novelty, this work seems more to be an application of Harmonica [6] to the NAS problem (with small modifications in order to make it applicable). In page 10 you state some of the differences of your method with Harmonica. I agree that the number of function evaluations you use (coming from the one-shot model) is larger and computationally less expensive to obtain, however this does not guarantee that these are a good surrogate of the true objective that NAS aims to minimize, i.e. the validation/test accuracy of final (stand-alone) architectures . The empirical evaluations of their algorithm seem to outperform/be competitive compared to other NAS methods on all benchmarks used in the paper, however only DARTS is evaluated on their search space and the other results are taken from the corresponding papers. The paper is well-written and -structured with the caveat of being more than the recommended 8 pages. I will adjust my score depending on the authors responses concerning the following questions/issues: 1. The correlation between the architectures evaluated using the one-shot weights and retrained from scratch, seems to be of crucial importance in your method, since you directly use the one-shot weights to collect the measurements, similarly to Random Search with weight sharing [3], ENAS [4] or Bender et al. [5]. What is the correlation of these measurements with the stand-alone architectures trained from scratch using the final evaluation settings? How did you tune the p in the Bernoulli distribution during the one-shot weight updates. According to Bender et al. [5] the ScheduledDropPath probability is an important hyperparameter affecting the aforementioned correlation. 2. What is the main motivation for using 5 operations in the operation set and not 8 as in DARTS [1] for example? Does the main contribution in the competitive results come from the different search space or the search method? 3. Is there any reference or proof for the correctness of Theorem 3.2? 4. I think there are some parts that can be moved in the Supplementary, such as the pseudocode for the proposed algorithm or Figure 3, and some other parts that can be compressed, such as the Related Work section. References [1] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. In ICLR, 2019. [2] Han Cai, Ligeng Zhu, Song Han. ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware. In ICLR, 2019. [3] LIAM LI, AMEET TALWALKAR. Random Search and Reproducibility for Neural Architecture Search. [4] Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, Jeff Dean. Efficient Neural Architecture Search via Parameter Sharing. In ICML, 2018 [5] Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, Quoc Le. Understanding and Simplifying One-Shot Architecture Search. In ICML, 2018 [6] Elad Hazan, Adam Klivans, Yang Yuan. Hyperparameter Optimization: A Spectral Approach"}
{"id": "iclr2020_803", "title": "On the Linguistic Capacity of Real-time Counter Automata | OpenReview", "abstract": "Abstract:###While counter machines have received little attention in theoretical computer science since the 1960s, they have recently achieved a newfound relevance to the field of natural language processing (NLP). Recent work has suggested that some strong-performing recurrent neural networks utilize their memory as counters. Thus, one potential way to understand the sucess of these networks is to revisit the theory of counter computation. Therefore, we choose to study the abilities of real-time counter machines as formal grammars. We first show that several variants of the counter machine converge to express the same class of formal languages. We also prove that counter languages are closed under complement, union, intersection, and many other common set operations. Next, we show that counter machines cannot evaluate boolean expressions, even though they can weakly validate their syntax. This has implications for the interpretability and evaluation of neural network systems: successfully matching syntactic patterns does not guarantee that a counter-like model accurately represents underlying semantic structures. Finally, we consider the question of whether counter languages are semilinear. This work makes general contributions to the theory of formal languages that are of particular interest for the interpretability of recurrent neural networks.", "review": "Review:###Summary ------- The authors investigate (subclasses of) generalized counter machines with respect to their weak generative capacity, their ability to represent structure, and several closure properties. This is motivated by recent indications that LSTMs have comparable expressivity to counter machines, so that the formal properties of these machines might provide indirect insights into the linguistic suitability of LSTMs. Evaluation ---------- I also reviewed this paper for SCiL a few months ago. While I had major reservations back then, I am happy to provide a more positive evaluation this time as the authors have done some revisions that clear up many points of confusion. I have to add two caveats, though. First, I am a bit disheartened that the authors chose not to adopt many of the excellent changes suggested by another SCiL reviewer (who went way beyond the call of duty with their multi-page review). Second, I did not have sufficient time to check all proofs for their correctness. In many cases the strategies strike me as intuitively sound, but my intuition tends to miss edge cases. Nonetheless, I think that this paper, albeit a bit of a gamble, would make for an interesting addition to the program. 1) Weakness: Link to neural networks still unclear The central weakness of the paper is still the link between neural networks and counter automata. Based on what is said in the paper, this is merely a conjecture at this point, not a well-established fact. Without this link, the value of the paper is unclear. If, however, this conjecture should turn out to be true, the paper would mark a very strong starting point for further exploration. This makes it a gamble worth taking. 2) Strong results, but lack of examples The results are not trivial and provide deep insights into the inner workings of counter machines. In particular the fact that counter machines cannot correctly represent Boolean expressions reveals key limitations on their representational power. The semilinearity result is less impressive because of how limited the machines are that it applies to, and I*m not sure that the proof provides a good basis for generalization to more complex machines. The authors might consider removing this part to clear some space for examples, which are sorely needed. The formalism is abstract and unfamiliar to most readers, and a few concrete examples would greatly strengthen the readers* intuition. 3) No investigation of linguistically important string languages As the authors make claims about linguistic adequacy, it is surprising that there is no discussion of TALs, MCFLs or PMCLFs. The grammar formalism of GPSG was abandoned because it was limited to context-free languages and could not handle those more complex language classes. So if counter machines fail here, the issue of their linguistic adequacy is already decided without further probing semilinearity or representational power. As far as I can tell, real-time counter machines cannot generate the PMCLF a^{2^n}, which is an abstract model of unbounded copying constructions in natural language (see Radzisnky on Chinese number names, Michaelis & Kracht on Old Georgian case stacking, and Kobele on Yoruba). Nor is it obvious to me that counter machines can handle the copy language {ww | w in Sigma^*}, a model of crossing dependencies, although they can handle a^n b^n c^n (a TAL). It should also be possible to generate the linguistically undesirable MIX language, which is a 2-MCFL but not a TAL. Minor comments -------------- - As noted in my SCiL review, your definitions still differ from those of Fischer et al. 1968. What is the reason for this? - Theorem 3.1: subsetneq would be clearer than subset - p4, typo: the the - Proof of Theorem 3.2: Unless I misunderstand your modulo construction, your ICL only has resolution up to mod n. For instance, with mod 2 it can distinguish 2 from 3, but not 2 from 4. The CL can do that. Don*t you need a second counter c_i* for each c_i, then, to keep track of how often you have wrapped around modulo n in c_i? That would still be incremental as you can never wrap around by more than 1 in any given update. - Sec 6.1: in all those definitions, if should be iff References ---------- @ARTICLE{Radzinski91, author = {Radzinski, Daniel}, title = {Chinese Number Names, Tree Adjoining Languages, and Mild Context Sensitivity}, year = {1991}, journal = {Computational Linguistics}, volume = {17}, pages = {277--300}, url = {http://ucrel.lancs.ac.uk/acl/J/J91/J91-3002.pdf} } @INPROCEEDINGS{MichaelisKracht97, author = {Michaelis, Jens and Kracht, Marcus}, title = {Semilinearity as a Syntactic Invariant}, year = {1997}, booktitle = {Logical Aspects of Computational Linguistics}, pages = {329--345}, editor = {Retor{*e}, Christian}, volume = {1328}, series = {Lecture Notes in Artifical Intelligence}, publisher = {Springer}, doi = {10.1007/BFb0052165}, url = {http://dx.doi.org/10.1007/BFb0052165} } @PHDTHESIS{Kobele06, author = {Kobele, Gregory M.}, title = {Generating Copies: {A}n Investigation into Structural Identity in Language and Grammar}, year = {2006}, school = {UCLA}, url = {http://home.uchicago.edu/~gkobele/files/Kobele06GeneratingCopies.pdf} }"}
{"id": "iclr2020_804", "title": "Contextualized Sparse Representation with Rectified N-Gram Attention for Open-Domain Question Answering | OpenReview", "abstract": "Abstract:###A sparse representation is known to be an effective means to encode precise lexical cues in information retrieval tasks by associating each dimension with a unique n-gram-based feature. However, it has often relied on term frequency (such as tf-idf and BM25) or hand-engineered features that are coarse-grained (document-level) and often task-specific, hence not easily generalizable and not appropriate for fine-grained (word or phrase-level) retrieval. In this work, we propose an effective method for learning a highly contextualized, word-level sparse representation by utilizing rectified self-attention weights on the neighboring n-grams. We kernelize the inner product space during training for memory efficiency without the explicit mapping of the large sparse vectors. We particularly focus on the application of our model to phrase retrieval problem, which has recently shown to be a promising direction for open-domain question answering (QA) and requires lexically sensitive phrase encoding. We demonstrate the effectiveness of the learned sparse representations by not only drastically improving the phrase retrieval accuracy (by more than 4%), but also outperforming all other (pipeline-based) open-domain QA methods with up to 97x faster inference in SQuADopen and CuratedTrec.", "review": "Review:###Paper Summary: This paper proposes a contextualized sparse vectors, called CoSPR, for encoding phrase with in open-domain question answering. It is different from existing static sparse vectors such as tf-idf in that CoSPR dynamically computes the weight of each n-gram that depends on the context. The authors argument the baseline model DenSPI (Seo et al., 2019) that uses tf-idf with their contextualized sparse representations (DenSPI+CoSPR). Experiments with SQuADOpen and CuratedTREC show the effectiveness of CoSPR. Strengths: —The model that uses contextualized encoding by BERT and the training strategy that leverages a kernelization is simple but effective. —DenSPI+CoSPR achieves 97x speedup in inference compared to a state-of-the-art pipeline approach, BERTserini (Yang et al., 2019). Also, the inference speed of DenSPI+CoSPR is comparable to that of the original DenSPI. —The paper is well written and well organized. Weaknesses: —This study is based on DenSPI (Seo et al., 2019). There is no novelty in the dense representation part, since the focus of this study is on improvement of sparse representation. —Multi-passage BERT (Wang et al., 2019) clearly outperforms DenSPI+CoSPR in terms of question answering accuracy. However, it is quite slower than DenSPI+CoSPR. Questions: —Can we use CoSPR as a passage ranker to find the passages that contain answer-phrase candidates? How well the pipeline method of CoSPR (as a passage ranker) and BERT (as a passage reader) work? Review Summary: The paper is well motivated, and the proposed model is simple but effective. I think this paper can be accepted."}
{"id": "iclr2020_805", "title": "ILS-SUMM: Iterated Local Search for Unsupervised Video Summarization | OpenReview", "abstract": "Abstract:###In recent years, there has been an increasing interest in building video summarization tools, where the goal is to automatically create a short summary of an input video that properly represents the original content. We consider shot-based video summarization where the summary consists of a subset of the video shots which can be of various lengths. A straightforward approach to maximize the representativeness of a subset of shots is by minimizing the total distance between shots and their nearest selected shots. We formulate the task of video summarization as an optimization problem with a knapsack-like constraint on the total summary duration. Previous studies have proposed greedy algorithms to solve this problem approximately, but no experiments were presented to measure the ability of these methods to obtain solutions with low total distance. Indeed, our experiments on video summarization datasets show that the success of current methods in obtaining results with low total distance still has much room for improvement. In this paper, we develop ILS-SUMM, a novel video summarization algorithm to solve the subset selection problem under the knapsack constraint. Our algorithm is based on the well-known metaheuristic optimization framework -- Iterated Local Search (ILS), known for its ability to avoid weak local minima and obtain a good near-global minimum. Extensive experiments show that our method finds solutions with significantly better total distance than previous methods. Moreover, to indicate the high scalability of ILS-SUMM, we introduce a new dataset consisting of videos of various lengths.", "review": " ILS-SUMM is applying ILS (iterated local search) to video summarization problem. The paper is well written and explains the method succinctly. The authors also introduce a new dataset for video summarization which consists of 18 movies of duration 10-104 minutes and summaries of <4 minutes. Authors claim that the application of ILS to video summarization is first, which if true (i verified and could not find other references that negate the claim) seems like valuable work and connection. But, I think the authors can strengthen the paper further by addressing improvements along the axis below: * Authors just apply ILS to the problem of video summarization, is there any change to ILS that we can do to make it work better for video summarization. ANy insights of it*s limitations? * Both the datasets are very small (25, 50 videos) to do real comparisons of hand crafted vs deep features * Authors don*t talk about how they can incorporate semantics (specific people, actions etc.) in to the framework which is still important after doing shot boundary"}
{"id": "iclr2020_806", "title": "DeepSFM: Structure From Motion Via Deep Bundle Adjustment | OpenReview", "abstract": "Abstract:###Structure from motion (SfM) is an essential computer vision problem which has not been well handled by deep learning. One of the promising trends is to apply explicit structural constraint, e.g. 3D cost volume, into the network. In this work, we design a physical driven architecture, namely DeepSFM, inspired by traditional Bundle Adjustment (BA), which consists of two cost volume based architectures for depth and pose estimation respectively, iteratively running to improve both. In each cost volume, we encode not only photo-metric consistency across multiple input images, but also geometric consistency to ensure that depths from multiple views agree with each other. The explicit constraints on both depth (structure) and pose (motion), when combined with the learning components, bring the merit from both traditional BA and emerging deep learning technology. Extensive experiments on various datasets show that our model achieves the state-of-the-art performance on both depth and pose estimation with superior robustness against less number of inputs and the noise in initialization.", "review": "Review:###The paper tackles Structure from Motion, one of the canonical problems in computer vision, and proposes an approach that brings together geometry and physics on one hand and deep networks on the other hand. Camera unprojection and warping (of depth maps and features) are used to build a cost volume onto hypothetical planes perpendicular to the camera axis. Similarly, various camera poses are sampled around an initial guess. A deep network regresses form the cost volume to a camera pose and a depth map. The method can be applied iteratively, using the outputs of the current stage as the initial guess of the next one. Training is supervised, and the the results are evaluated on multiple datasets. I am inclined to recommend accepting the paper for publication, because it addresses a canonical problem, outperforms the state of the art on multiple datasets and brings together geometry / physics and deep learning, which is IMO very a promising and underexplored direction. I found the method section a bit difficult to read though, and even after several readings I cannot get my head around it. Specifically, here are some issues that I hope the Authors could clarify. 1. In Sec. 3 the Authors write *We then sample the solution space for depth and pose respectively around their initialization*. However in Sec 3.2 they write *we uniformly sample a set of L virtual planes {dl} Ll=1 in the inverse-depth space*. In what way are the planes *around their initialization*? If the initial depth map spans over multiple orders of magnitude, will the planes be uniformly sampled between the minimum and maximum disparity of the initial map? If yes, it seems that the initial depth map is not really needed, just its minimum and maximum value is needed, but then how come the method can be applied iteratively with respect to depth? 2. The Authors mention that depth maps are warped onto the virtual planes using differentiable bilinear interpolation. Is there a mechanism to protect from interpolating across discontinuities? If no, were bleeding edge artifacts observed? 3. In the introduction, the Authors point that prior methods have trouble dealing with textureless, reflective or transparent approaches, but it*s not clear form the paper where it addresses these cases, and if yes, what is the mechanism for that. Lastly, if the authors are not planning to release the code, the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author*s technique. For example, *our network learns a cost volume of size L × W × H using several 3D convolutional layers with kernel size 3 × 3 × 3* - more details about this network are needed, as well as the others in the paper."}
{"id": "iclr2020_807", "title": "Relation-based Generalized Zero-shot Classification with the Domain Discriminator on the shared representation | OpenReview", "abstract": "Abstract:###Generalized zero-shot learning (GZSL) is the task of predicting a test image from seen or unseen classes using pre-defined class-attributes and images from the seen classes. Typical ZSL models assign the class corresponding to the most relevant attribute as the predicted label of the test image based on the learned relation between the attribute and the image. However, this relation-based approach presents a difficulty: many of the test images are predicted as biased to the seen domain, i.e., the emph{domain bias problem}. Recently, many methods have addressed this difficulty using a synthesis-based approach that, however, requires generation of large amounts of high-quality unseen images after training and the additional training of classifier given them. Therefore, for this study, we aim at alleviating this difficulty in the manner of the relation-based approach. First, we consider the requirements for good performance in a ZSL setting and introduce a new model based on a variational autoencoder that learns to embed attributes and images into the shared representation space which satisfies those requirements. Next, we assume that the domain bias problem in GZSL derives from a situation in which embedding of the unseen domain overlaps that of the seen one. We introduce a discriminator that distinguishes domains in a shared space and learns jointly with the above embedding model to prevent this situation. After training, we can obtain prior knowledge from the discriminator of which domain is more likely to be embedded anywhere in the shared space. We propose combination of this knowledge and the relation-based classification on the embedded shared space as a mixture model to compensate class prediction. Experimentally obtained results confirm that the proposed method significantly improves the domain bias problem in relation-based settings and achieves almost equal accuracy to that of high-cost synthesis-based methods.", "review": "Review:###The paper presents a novel approach for (generalized) Zero-shot learning (GZSL). As showing in the numerical experiments on some real data, the method demonstrates the significant improvement on the accuracy of prediction comparing to some state-of-the-art methods. The main key of the method is using Variational Inference, variational autoencoders. The authors have taken into account the modality of the data through reparametrize the distributions, especially the inside class invariant modality and class separability. Moreover, the authors also propose to take into account a kind of biasness domain into the learning procedure, which details in adding a regularization of the domain discriminator into the objective function. The paper is nicely written, espcially with a clear formal introduction to the problem of GZSL. However, I have some questions: 1) Does the test set has some labels? How do you know your method works well? I can not find where you have defined a kind of loss so that we can compare the predicted labels hat{y}_j ? (In Section 2.) 2) How do you learn the *replaced prior* in equation (4) ? 3) It is not enough detail on how do you optimize the objective (8) ? a detail explain algorithm would make the paper significant, indeed. 4) In Table1, would MCMVAE in the last row be MCMVAE-D ? Final, I expect the authors will make their codes available for the readers."}
{"id": "iclr2020_808", "title": "Harnessing Structures for Value-Based Planning and Reinforcement Learning | OpenReview", "abstract": "Abstract:###Value-based methods constitute a fundamental methodology in planning and deep reinforcement learning (RL). In this paper, we propose to exploit the underlying structures of the state-action value function, i.e., Q function, for both planning and deep RL. In particular, if the underlying system dynamics lead to some global structures of the Q function, one should be capable of inferring the function better by leveraging such structures. Specifically, we investigate the low-rank structure, which widely exists for big data matrices. We verify empirically the existence of low-rank Q functions in the context of control and deep RL tasks (Atari games). As our key contribution, by leveraging Matrix Estimation (ME) techniques, we propose a general framework to exploit the underlying low-rank structure in Q functions, leading to a more efficient planning procedure for classical control, and additionally, a simple scheme that can be applied to any value-based RL techniques to consistently achieve better performance on **low-rank** tasks. Extensive experiments on control tasks and Atari games confirm the efficacy of our approach.", "review": "Review:###The study is motivated by the observation that the Q-value matrix in reinforcement learning problems often has a low-rank structure. The paper proposes an approach called structured value-based planning or learning, where the Q matrix or the Q function is estimated from incomplete observations based on the prior that it is low-rank. The proposed strategy is demonstrated in stochastic control tasks and reinforcement learning applications. The paper is clearly written and the experimental results show that the proposed strategy leads to performance gains especially in problems where the Q matrix indeed conforms to a low-rank model. A few comments and questions: - The assumption that the Q matrix should be low-rank is demonstrated with several experiments. Is there any theoretical motivation or guarantee for this assumption as well? - The experimental results show that the proposed strategy performs well in problems that are low-rank, while the performance may degrade in problems where the low-rank assumption is not met. Would it be possible to detect the rank of the problem in a dynamical manner (i.e., during the learning), so that the number of incomplete observations of Q can be increased to improve the performance, or the solution strategy (e.g. whether to use the low-rank assumption or not) can be adapted to the nature of the problem? - The Q-value matrices and functions considered in the problem have a special structure as they result from Markov Decision Processes. Would it be possible to go beyond the low-rank assumption and propose and use a more elaborate type of prior that employs the special structure of MDPs? - Please clearly define the notation used in Section 4.2."}
{"id": "iclr2020_809", "title": "High-Frequency guided Curriculum Learning for Class-specific Object Boundary Detection | OpenReview", "abstract": "Abstract:###This work addresses class-specific object boundary extraction, i.e., retrieving boundary pixels that belong to a class of objects in the given image. Although recent ConvNet-based approaches demonstrate impressive results, we notice that they produce several false-alarms and misdetections when used in real-world applications. We hypothesize that although boundary detection is simple at some pixels that are rooted in identifiable high-frequency locations, other pixels pose a higher level of difficulties, for instance, region pixels with an appearance similar to the boundaries; or boundary pixels with insignificant edge strengths. Therefore, the training process needs to account for different levels of learning complexity in different regions to overcome false alarms. In this work, we devise a curriculum-learning-based training process for object boundary detection. This multi-stage training process first trains the network at simpler pixels (with sufficient edge strengths) and then at harder pixels in the later stages of the curriculum. We also propose a novel system for object boundary detection that relies on a fully convolutional neural network (FCN) and wavelet decomposition of image frequencies. This system uses high-frequency bands from the wavelet pyramid and augments them to conv features from different layers of FCN. Our ablation studies with contourMNIST dataset, a simulated digit contours from MNIST, demonstrate that this explicit high-frequency augmentation helps the model to converge faster. Our model trained by the proposed curriculum scheme outperforms a state-of-the-art object boundary detection method by a significant margin on a challenging aerial image dataset.", "review": " The main idea of the paper is adding a curriculum learning-based extension to CASEnet, a boundary detection method from 2017. In the first phase, the loss emphasizes easier examples with high gradient in the image, and in the second phase, the method is trained on all boundary pixels. This change seems to improve edge detection performance on a toy MNIST and an aerial dataset. A second innovation claimed by the authors is adding Wavelet decomposition-based processing into the net. Unfortunately, this mostly only speeds up learning, as the ablation does not show meaningful improvements relative to the error bounds in later stages of training. Furthermore, the paper lacks a discussion of related work on incorporating wavelet ideas into neural networks. For example: -- Generic Deep Networks with Wavelet Scattering, by Ouyallon et al. -- Invariant scattering convolution networks, by Bruna et al and multiple more recent ones. Without either clear performance gains or more in-depth discussion of this novelty, it is not clear how to take it into account. When reading the paper, it appears that *boundary detection* for the cases that the authors are exploring is very directly related to 2-class semantic segmentation (road / non-road), the only difference being that the edge boundaries are weighted much higher in the cross-entropy loss. As such, there is a lot more recent net architecture work for semantic segmentation that should be directly applicable, and should perform much better than CaseNet when adapted to the task. As a result, the experiments and the significance of this paper are rather marginal. In experimental results, the authors threshold prediction with 0.5, which is suboptimal. The resulting metric, which is just *accuracy* is called incorrectly *average precision*. Instead, true definition of average precision should be used, that is not dependent on potentially suboptimal fixed thresholding but on the area under precision-recall curve instead. Finally, it would be helpful to do ablation and confidence bounds also on the main aerial road results, as the 15% gain is significantly more than the gain that appears in the toy dataset."}
{"id": "iclr2020_810", "title": "Higher-order Weighted Graph Convolutional Networks | OpenReview", "abstract": "Abstract:###Graph Convolution Network (GCN) has been recognized as one of the most effective graph models for semi-supervised learning, but it extracts merely the first-order or few-order neighborhood information through information propagation, which suffers performance drop-off for deeper structure. Existing approaches that deal with the higher-order neighbors tend to take advantage of adjacency matrix power. In this paper, we assume a seemly trivial condition that the higher-order neighborhood information may be similar to that of the first-order neighbors. Accordingly, we present an unsupervised approach to describe such similarities and learn the weight matrices of higher-order neighbors automatically through Lasso that minimizes the feature loss between the first-order and higher-order neighbors, based on which we formulate the new convolutional filter for GCN to learn the better node representations. Our model, called higher-order weighted GCN (HWGCN), has achieved the state-of-the-art results on a number of node classification tasks over Cora, Citeseer and Pubmed datasets.", "review": " This paper studied the problem of aggregating high-order neighbor in GCN model. It first showed that stacking multiple layers with one-hop message propagation does not yield an advantage to learn node embedding from higher-order neighbors. Then it presents a solution based on learnable kth-order weight matrix, which can aggregate high-order neighbors in a more flexible and reasonable way. The approach itself is interesting. However, there are the following concerns: 1) The computational complexity. Although authors didn’t provide complexity analysis, it is definitely costly to run the proposed model on large graph. That’s also the reason why evaluation on Pubmed dataset was done on a sampled subset. 2) No significant improvement over other baselines on graph embedding results, as shown inn Table 4 and 5. 3) Authors are suggested to read “Simplifying graph convolutional networks” in ICML 2019, which also studied the neighborhood aggregation issue and presents a simplified GCN model."}
{"id": "iclr2020_811", "title": "Learning to Reason: Distilling Hierarchy via Self-Supervision and Reinforcement Learning | OpenReview", "abstract": "Abstract:###We present a hierarchical planning and control framework that enables an agent to perform various tasks and adapt to a new task flexibly. Rather than learning an individual policy for each particular task, the proposed framework, DISH, distills a hierarchical policy from a set of tasks by self-supervision and reinforcement learning. The framework is based on the idea of latent variable models that represent high-dimensional observations using low-dimensional latent variables. The resulting policy consists of two levels of hierarchy: (i) a planning module that reasons a sequence of latent intentions that would lead to optimistic future and (ii) a feedback control policy, shared across the tasks, that executes the inferred intention. Because the reasoning is performed in low-dimensional latent space, the learned policy can immediately be used to solve or adapt to new tasks without additional training. We demonstrate the proposed framework can learn compact representations (3-dimensional latent states for a 90-dimensional humanoid system) while solving a small number of imitation tasks, and the resulting policy is directly applicable to other types of tasks, i.e., navigation in cluttered environments.", "review": "Review:###This paper presents a framework for learning hierarchical policies using a latent variable conditioned policy operating at the low level, with model based planning at the high level. Unlike prior work which does hierarchical reinforcement learning, the key technical contribution of this work is that they use planning with a latent dynamics model as their high level policy. They demonstrate the method on a humanoid walking task in the DeepMimic [1] environment. While the idea is well motivated, this paper should be rejected, primarily due to a lack of experimental results. In particular, the experiments (1) are missing several critical details about the experimental setup, (2) the experimental setup differs significantly from the claims of self-supervision and multi-task RL made in the introduction/method, and (3) there is no comparison to any prior work. Without these, it is impossible to determine if any of the claims made about the proposed method are empirically true. First, no information is provided about the reward function used, the horizon of the tasks, or about the planning parameters or policy learning parameters. As currently stated, I don*t think any of the results in the paper could be reproduced. Furthermore, the reward function and task horizon used are necessary to determine the difficulty of the proposed tasks. Second, the title and method section would imply that the method is self-supervised, specifically in how the latent dynamics model is learned. While the samples used to train the latent dynamics model are taken from the agent*s experience, the latent conditioned policy is trained (1) with ground truth task reward, and (2) is actually pre-trained on demonstrations of the tasks with ground truth skill labels. This suggests that much of the actual skill learning is done offline in this pre-training stage - with full supervision. As a result, this would make the learning of the latent dynamics model much easier, since the sub-policies have already converged to different behaviors for different values of h. Without this pre-training, learning the low level policies and the LVM jointly would be much more challenging. Hence it seems that the *self-supervised* learning of the LVM is actually heavily dependent on the full supervision used in the pre-training stage. Additionally, the demonstrations used for the pre-training correspond to the same 3 tasks that the agent is later evaluated on (moving forward, left, right). So the method receives full supervision on the test tasks, so the experiments do not actually reflect generalization in multi-task RL as claimed. Lastly, and most importantly, there are no comparisons to prior work. The only result shown is the trajectory error against 3 ablations of the proposed method. The reported numbers are error between the reference trajectory and ground truth, predicted plan and reference, and predicted plan and ground truth. First, it seems like the most important number here is the difference between the predicted plan and ground truth, for which numbers are missing for 2/3 ablations. Why is task success or reward not the reported number, and why is performance for 2/3 ablations missing? Additionally, there should be comparisons to existing work both in terms of hierarchical model free RL (for example Nachum et al [2]) and model based RL with latent dynamics models (Hafner et al [3]). The results as presented do not actually support that the proposed method performs better than existing work. The final result of the video of the agent doing a long horizon task also has no quantitative numbers, so again does not support that the proposed method is better. Some other less significant points: - there are typos throughout (for example *Figure 3: (a) Leanred latent model.*). - the tables and figures have very limited or no captions. - The method section is difficult to follow and could use some figures which demonstrate the key technical contribution. - Also from the method section it seems like the novelty is combining the latent conditioned policy learning from Haarnoja et al [4] and the latent dynamics learning/planning from Ha et al [5]. Is there an additional technical contribution beyond combining these two existing works? If so the method section should more clearly show it. - There is a recent work (Sharma et al [6]), which also learns skill conditioned low level policies, and does model based planning in the space of skills to reach previously unseen goals. In this work the skill discovery is also totally unsupervised. The authors should add citation to this paper and clarify the differences between their work and this work. [1] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. DeepMimic: Example guided deep reinforcement learning of physics-based character skills. [2] Ofir Nachum, Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical reinforcement learning [3] Hafner, D., Lillicrap, T. P., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. Learning latent dynamics for planning from pixels [4] Tuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, and Sergey Levine. Latent space policies for hierarchical reinforcement learning. [5] Jung-Su Ha, Young-Jin Park, Hyeok-Joo Chae, Soon-Seo Park, and Han-Lim Choi. Adaptive path integral autoencoders: Representation learning and planning for dynamical systems. [6] Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamicsaware unsupervised discovery of skills."}
{"id": "iclr2020_812", "title": "Smooth Kernels Improve Adversarial Robustness and Perceptually-Aligned Gradients | OpenReview", "abstract": "Abstract:###Recent research has shown that CNNs are often overly sensitive to high-frequency textural patterns. Inspired by the intuition that humans are more sensitive to the lower-frequency (larger-scale) patterns we design a regularization scheme that penalizes large differences between adjacent components within each convolutional kernel. We apply our regularization onto several popular training methods, demonstrating that the models with the proposed smooth kernels enjoy improved adversarial robustness. Further, building on recent work establishing connections between adversarial robustness and interpretability, we show that our method appears to give more perceptually-aligned gradients.", "review": " Paper summary: This paper argues that reducing the reliance of neural networks on high-frequency components of images could help robustness against adversarial examples. To attain this goal, the authors propose a new regularization scheme that encourages convolutional kernels to be smoother. The authors augment standard loss functions with the proposed regularization scheme and study the effect on adversarial robustness, as well as perceptual-alignment of model gradients. Comments: I will first discuss some high-level concerns I have with the paper, followed by more specific comments. Motivation and general idea: The idea of improving model robustness by eliminating high-frequency components in the data is not new. This has in fact been the motivation behind several (later shown to be unsuccessful) defenses such as JPEG compression. In recent work, Yin et al. explore this phenomenon in depth, and importantly discuss how adversarial examples are by no means entirely a high-frequency phenomenon. Specifically, they show that while adversarial examples for natural models do tend to be biased towards higher-frequency components, this is not true for robust models. They argue that it is in general always possible to find adversarial examples in the lower end of the frequency spectrum (for instance, transfer attacks from robust models). Thus, based on evidence from prior work, it is unlikely that a defense based entirely on removing high-frequency components from the input data can be successful. Empirical evaluation: There are significant issues with the empirical evaluation of the proposed defense. In particular, in Table 1 (also Figures 7-9 in the Appendix): 1. The numbers suggest that FGSM is more successful as an attack than PGD. The later is a multi-step version of the former, and hence should be strictly better (more successful in lowering model accuracy). This clearly highlights that the PGD attack is not being used correctly/not run with enough steps, and thus the numbers are not an accurate reflection of model robustness. 2. At a higher-level, the numbers that the authors highlight in the table are the best performance over attacks. This is not the correct way to evaluate robustness, which must always be reported as the *worst-case performance* of the model and hence the lowest accuracy over attacks. If one takes this into consideration, it is clear that the proposed regularization does not really improve robustness. 3. Furthermore, the authors state they use default parameters from Foolbox to evaluate their models. The issue with this is evident in the MNIST/FashionMNIST results where even with arbitrary eps, strong attacks like PGD are not able to break the model. This does not mean that the model is robust, it just means that the default hyperparameters from Foolbox cannot break the model. The authors need to re-run the attacks with different steps sizes/step counts for various attacks. The goal in evaluating model robustness should not be finding one attack (or one set of hyperparameters) which is not able to fool the model, but to show that *no attack* (for the given perturbation set) can lower model accuracy. 4. The eps that are used for the CIFAR-10 and ImageNet experiments are extremely large, and not standard in the literature. In fact, based on my experience, it is probably not possible to be (too) robust to eps as large as 0.1 on CIFAR as this is large enough to visibly change the class even for a human (cf. Tsipras et al.). Thus, I think the robustness evaluation in Table 1 is incorrect and violates some basic sanity checks (such as PGD being stronger than FGSM), and thus does not accurately reflect the model’s true performance. Other comments: i. It is unclear why the sensitivity of the proposed regularization scheme to the scale of w can be fixed by subtracting the norm of w (the exp loss will just be raised to the power alpha if you scale weights by alpha). Dividing by the norm would be the more correct way to fix the scale invariance and the authors should include results with this regularization, at least in the Appendix. ii. In Figure 1, the kernels and activations that are visualized are after the first convolutional layer. The authors should perform a similar visualization after the second layer (or later layers in general) because it is somewhat obvious that you will get these properties at the first layer with the proposed regularization scheme (if the lambda is correctly tuned). The part that is unclear is how effective the regularization is over repeated applications of convolution coupled with non-linearities such as ReLU/pooling. iii. While the visualizations in Figures 2-6 are interesting, it seems that many of the cases in which +R looks semantically meaningful, it also looks meaningful with the base loss itself. It thus is unclear whether the perceptual alignment is coming from the base loss, or from the added regularization. Moreover, the perceptual alignment of models should improve when their dependance on “human-meaningless” features reduces. Since high-frequency components are likely “human-meaningless” features, it is plausible that the proposed regularization scheme makes activation maximization/gradients more perceptually alignment. However, I think this is somewhat orthogonal to (and is not enough to tell us anything about) the robustness of the model itself, which is affected by its reliance on many kinds of “non-robust” features (Ilyas et al., 2019) of which high-frequency components might just be one example. iv. In recent work by Wang et al., the authors conduct an interesting experiment to see how dependent the prediction of a given classifier is to high-frequency components in the data (vs low frequency components) (cf. Figure 1 in their paper). It would be interesting to see this experiment replicated with the proposed regularization scheme. Overall, I think there are significant issues with the paper, especially in the empirical evaluation section. The authors need to re-evaluate their models with the strongest possible form of the attack and demonstrate an improvement in worst-case performance to actually establish the merits of the proposed regularization scheme. Thus, I recommend rejection. References: Yin, Dong, et al. *A fourier perspective on model robustness in computer vision.* arXiv preprint arXiv:1906.08988 (2019). Tsipras, Dimitris, et al. *Robustness may be at odds with accuracy.* arXiv preprint arXiv:1805.12152 (2018). Ilyas, Andrew, et al. *Adversarial examples are not bugs, they are features.* arXiv preprint arXiv:1905.02175 (2019). Wang, Haohan, et al. *High Frequency Component Helps Explain the Generalization of Convolutional Neural Networks.* arXiv preprint arXiv:1905.13545 (2019)."}
{"id": "iclr2020_813", "title": "Synthesizing Programmatic Policies that Inductively Generalize | OpenReview", "abstract": "Abstract:###Deep reinforcement learning has successfully solved a number of challenging control tasks. However, learned policies typically have difficulty generalizing to novel environments. We propose an algorithm for learning programmatic state machine policies that can capture repeating behaviors. By doing so, they have the ability to generalize to instances requiring an arbitrary number of repetitions, a property we call inductive generalization. However, state machine policies are hard to learn since they consist of a combination of continuous and discrete structure. We propose a learning framework called adaptive teaching, which learns a state machine policy by imitating a teacher; in contrast to traditional imitation learning, our teacher adaptively updates itself based on the structure of the student. We show how our algorithm can be used to learn policies that inductively generalize to novel environments, whereas traditional neural network policies fail to do so.", "review": "Review:#### Summary This paper proposes a technique for synthesis of state machine policies for a simple continuous agent, with a goal of them being generalizable to out-of-distribution test conditions. The agent is modeled as a state machine with constant or proportional actions applied in each node (regime), and switching triggers between the regimes represented as length-2 boolean conditions on the observations. The technique is evaluated on 7 classic control environments, and found to outperform pure-RL baselines under *test* conditions in most of them. # Review I am not an expert in RL-based control, although I*m familiar with the recent literature that applies formal methods to these domains. I find the studied settings valuable albeit fairly limited, but the paper*s method undeniably shows noticeable improvement on these settings. Inductive generalization is an important problem, and the authors* approach of limiting the agent structure to a particular class of state-machine policies is a reasonable solution strategy. That said, the complexity of synthesizing a state machine policy clearly caused the authors to limit their supported action and condition spaces considerably (Figure 6). That, I*m assuming, limits the set of applicable control environments where optimization is still feasible. The authors don*t provide any analysis of complexity or empirical runtime of the optimization process. Breaking it down for each benchmark would allow me to appreciate the optimization framework in Section 4 much more. As it stands, Section 4 describes a complex optimization process with many moving parts, some of which are approximated (q* and p(?|?,x?)) or computed via EM iteration until convergence (?*). It is hard to appreciate all this complexity without knowing where the challenges manifest on specific examples. Section 4.2 needs an example, to link it to the introductory example in Figure 1. The *loop-free* policies of the teacher are, in programmatic terms, _traces_ of the desired state machine execution (if I understand correctly), but this is not obvious from just the formal definition. The EM optimization for the student policy makes significant assumptions on the action/condition grammars. Namely, the algorithm iterates over every possible discrete *sketch* of every program, and then numerically optimizes its continuous parameters (Appendix A). When the action/condition grammars grow, the number of possible programs there increases combinatorially. Is there a way to adapt the optimization process to handle more complex grammars, possibly with decomposition of the problem following the program structure? Section 5 needs a bit more details on the Direct-Opt baseline. It*s unclear how the whole state machine policy (which includes both discrete and continuous parts) is learned end-to-end via numerical optimization. Granted, the baseline performs terribly, but would be great to understand how it models the learning problem in order to appreciate why it*s terrible. Why were the *Acrobot* and *Mountain car* benchmarks removed from the main presentation of results?"}
{"id": "iclr2020_814", "title": "TabFact: A Large-scale Dataset for Table-based Fact Verification | OpenReview", "abstract": "Abstract:###The problem of verifying whether a textual hypothesis holds based on the given evidence, also known as fact verification, plays an important role in the study of natural language understanding and semantic representation. However, existing studies are mainly restricted to dealing with unstructured evidence (e.g., natural language sentences and documents, news, etc), while verification under structured evidence, such as tables, graphs, and databases, remains unexplored. This paper specifically aims to study the fact verification given semi-structured data as evidence. To this end, we construct a large-scale dataset called TabFact with 16k Wikipedia tables as the evidence for 118k human-annotated natural language statements, which are labeled as either ENTAILED or REFUTED. TabFact is challenging since it involves both soft linguistic reasoning and hard symbolic reasoning. To address these reasoning challenges, we design two different models: Table-BERT and Latent Program Algorithm (LPA). Table-BERT leverages the state-of-the-art pre-trained language model to encode the linearized tables and statements into continuous vectors for verification. LPA parses statements into LISP-like programs and executes them against the tables to obtain the returned binary value for verification. Both methods achieve similar accuracy but still lag far behind human performance. We also perform a comprehensive analysis to demonstrate great future opportunities.", "review": "Review:###Updated review: Thank you for addressing the comments and making relevant edits. Additionally, the HIT section provides a lot more insights into the data collection process. I*ve updated my score based on the responses/edits made. ------ This paper is about a dataset (TABFACT) aimed at promoting research for fact-verification using semi-structured data as evidence. The paper highlights how the existing fact-verification studies have been restricted to work with unstructured evidence, and hence lack generalization to use-cases where the evidence is in a structured format (eg. databases). The paper also highlights how fact-verification with semi-structured evidence is challenging, since it involves both linguistic reasoning (for paraphrasing, entailment etc.) and symbolic reasoning (for operations like count, min, max etc.). To tackle this, the authors suggest two approaches as baselines on the dataset - one uses off-the-shelf BERT model for NLI; the other one focuses on symbolic reasoning and is based on program execution - which primarily uses lexical matching and a set of predefined operations (like count/max/min) to construct a program. Apart from a few issues (mentioned below), the paper is well written. The authors have provided a detailed overview of their data collection/verification pipeline and related model/experiments. Overall, it seems like an interesting dataset and I*m inclined towards accepting the paper. A few remarks/concerns are: 1. Usefulness of the dataset: It seems limiting for a fact-verification dataset to restrict itself to a binary space i.e. entailed vs refuted. It is often the case, that statements are not completely true or false. For example, the 3rd refuted statement in Figure 1 is partially true (‘there are five candidates in total’). With a binary space for supervision, we don’t really know if the system is actually able to capture the linguistic and symbolic nuances present in the task. It is entirely possible for the system to “do well” without “learning well”, if the learning/output space is this coarse (as opposed to a dataset like Vlachos and Riedel, 2014). 2. Related work: The paper talks about introducing a new ‘format’ of evidence (structured text) and talks about ‘unstructured text’ as the only ‘other’ format of evidence. It misses out on a highly related task that uses image as evidence (notable datasets being: CLEVR-Humans, NLVR/2, GQA). Either these should be included in the related work, or the authors should make it explicit that this work only deals with ‘textual’ evidence. 3. The dataset statistics in Table 1 don’t seem to add up (train+dev+test = (92,283 + 12,792 + 12,779) = 117,854 != 118,275 (=Total #Sentence)). 4. Page 3, Section 2.3: “we further perform quality control” -> a line or two to explain quality control? 5. Appendix C: No data/statistics have been provided to support the conclusion of the ablation study. Minor remarks: - Page 2: Section 2: 1. overtly -> overly 2. huge tables(e.g. -> huge tables (e.g. - Page 3: Section 2.3 1. to filter 18% entailed of entailed statements -> to filter 18% entailed statements - Page 4: 1. candidate) . we need to -> candidate), we need to"}
{"id": "iclr2020_815", "title": "Target-directed Atomic Importance Estimation via Reverse Self-attention | OpenReview", "abstract": "Abstract:###Estimating the importance of each atom in a molecule is one of the most appealing and challenging problems in chemistry, physics, and material engineering. The most common way to estimate the atomic importance is to compute the electronic structure using density-functional theory (DFT), and then to interpret it using domain knowledge of human experts. However, this conventional approach is impractical to the large molecular database because DFT calculation requires huge computation, specifically, O(n^4) time complexity w.r.t. the number of electrons in a molecule. Furthermore, the calculation results should be interpreted by the human experts to estimate the atomic importance in terms of the target molecular property. To tackle this problem, we first exploit machine learning-based approach for the atomic importance estimation. To this end, we propose reverse self-attention on graph neural networks and integrate it with graph-based molecular description. Our method provides an efficiently-automated and target-directed way to estimate the atomic importance without any domain knowledge on chemistry and physics.", "review": "Review:###~The authors propose a modification to the attention mechanism to understand the importance of molecular moieties on DFT calculations.~ To me, it is unclear what the inverse attention mechanism is truly capturing. How does the importance feature correspond to the predicted DFT value? Does the sum of the importances correspond to the predicted DFT value? Can I compare DFTs across different molecules? What is the distribution of importances look like across a given molecule? Generally, is there any bias for the valency of the atom and the importance? From “Step 3” on page 5, which layer from the graph neural network do to take the importance features from? Are they aggregated across layers at some point? For Section 4.4, please show a plot relating atomic importance to whatever metric you are comparing to. In Figures 7 and 8, the selected submolecules are not the same as those in the larger structure. You have added hydrogens and removed double bonds that were present. Please just highlight the molecule as you did in Figures 4-6. For section 4.5, I’m not sure what I’m looking for at all for the qualitative analysis, and I have no idea if your figures are representative of the data. There are much easier, simpler ways to determine the importance of molecular moieties on predicted outcome WITHOUT using a graph. Please compare to making a fixed length vector via fragments and performing regression or using a random forest. How does your model compare? What about an attention mechanism directly on the SMILES itself? It seems like it is hard to get “ground-truth” data to compare to. Does your dataset have molecules that differ by only one, or a few, atoms? If so, how do the importance features compare between those molecules compared to the output prediction? How well does your method work out of distribution? What if molecules are held out by Tanimoto distance? Finally, this work should not be limited in scope to DFT calculations. Determining the importance of features in a graph is of importance in a wide variety of fields, including social networks and pharmaceutical applications."}
{"id": "iclr2020_816", "title": "Deep Graph Matching Consensus | OpenReview", "abstract": "Abstract:###This work presents a two-stage neural architecture for learning and refining structural correspondences between graphs. First, we use localized node embeddings computed by a graph neural network to obtain an initial ranking of soft correspondences between nodes. Secondly, we employ synchronous message passing networks to iteratively re-rank the soft correspondences to reach a matching consensus in local neighborhoods between graphs. We show, theoretically and empirically, that our message passing scheme computes a well-founded measure of consensus for corresponding neighborhoods, which is then used to guide the iterative re-ranking process. Our purely local and sparsity-aware architecture scales well to large, real-world inputs while still being able to recover global correspondences consistently. We demonstrate the practical effectiveness of our method on real-world tasks from the fields of computer vision and entity alignment between knowledge graphs, on which we improve upon the current state-of-the-art.", "review": " This paper proposes a two-stage GNN-based architecture to establish correspondences between two graphs. The first step is to learn node embeddings using a GNN to obtain soft node correspondences between two graphs. The second step is to iteratively refine them using the constraints of matching consensus in local neighborhoods between graphs. The overall refining process resembles the classic graph matching algorithm of graduated assignment (Gold & Rangarajan, 1996), but generalizes it using deep neural representation. Experiments show that the proposed algorithm performs well on real-world tasks of image matching and knowledge graph entity alignment. The paper is interesting and has some good potential but lacks some important evaluations and analyses. My main concerns are as follows. 1) The consensus in the second stage is crucial? As the title shows, the main technical contribution lies in the second stage of consensus inducing. But, for the real tasks, in the experiments, the gain by the second stage is not significant or often negligible (L=0 vs. L=10 or 20 in Table 1,2,3). The results of the first stage (L=0) already give better results than all the baselines in many cases, so that most gains appear to come from the usage of GNNs for representation. This makes the major contribution of this work less significant. I hope the authors justify this. And, I guess that*s maybe because the consensus information may also be induced in the first stage by matching nodes with relational presentations learned using a GNN. To see this, the authors may run the second stage only without the first stage. 2) Comparison to the graduated assignment (GA) process As discussed in 3.3, the proposed neighborhood consensus can be viewed as a generalization of GA of Eq.6 with trainable neural modules. But, it*s not actually shown what is the gain by this generalization. This needs to be shown experimentally by substituting the second stage by GA process. 3) Robustness to node addition or removal. All the experiments look assuming only edges are varied. Is this algorithm robust to node addition or removal, occurring in many practical graph matching problems? This needs to be also discussed. ====================================== The rebuttal succeeds in addressing most of my concerns so that I upgrade my initial rating to weak accept. I hope all the points in the rebuttal are included in the final manuscript."}
{"id": "iclr2020_817", "title": "Fast Bilinear Matrix Normalization via Rank-1 Update | OpenReview", "abstract": "Abstract:###Bilinear pooling has achieved an impressive improvement over classical average and max pooling in many computer vision tasks. Recent studies discover that matrix normalization is vital for improving the performance of bilinear pooling since it effectively suppresses the burstiness. Nevertheless, exiting matrix normalization methods such as matrix square-root and matrix logarithm are based on singular value decomposition (SVD), which is not supported well in the GPU platform, limiting its efficiency in training and inference. To boost the efficiency in the GPU platform, recent methods rely on Newton-Schulz (NS) iteration which approximates the matrix square-root through several times of matrix-matrix multiplications. Despite that Newton-Schulz iteration is well supported by GPU, it takes computation complexity where is dimension of local features and is the number of iterations, which is still costly. Meanwhile, NS iteration is applicable only to full bilinear matrix. In contrast, a compact bilinear feature obtained from tensor sketch or random projection has broken the matrix structure, cannot be normalized by NS iteration. To overcome these limitations, we propose a rank-1 update normalization (RUN), which reduces the computational cost from to where is the number of local feature per image. More importantly, it supports the normalization on compact bilinear features. Meanwhile, the proposed RUN is differentiable, and thus it is feasible to plug it in a convolutional neural network as a layer to support an end-to-end training. Comprehensive experiments on four public benchmarks show that, for full bilinear pooling, the proposed RUN achieves comparable accuracies with a speedup over NS iteration. For compact bilinear pooling, our RUN achieves comparable accuracies with a speedup over the SVD-based normalization.", "review": "Review:###This paper proposes a rank-1 update normalization (RUN), which supports the normalization on compact bilinear features and feasible to be plugged into end-to-end training. RUN uses power method to estimate the bilinear matrix, which is more friendly to GPU. The idea of RUN is well motivated but not surprising. The experimental results show RUN achieves comparable accuracies with much lower running time than NS iteration and SVD-based normalization. The presentation of this paper should be improved. It is necessary to summarize the procedure of RUN like Algorithm 1 and 2. The main results in Section 3 are desired to be presented by Theorem or Proposition. Some detailed derivation could be deferred into appendix. Some questions: 1. It is unclear how to obtain (16) from previous derivation. Where does epsilon come from? 2. Can you provide non-asymptotic results of (18) and (21) (which may be rely on the eigengap)? 3. It would like to plot how the accuracy of each algorithms varying with epochs and running time. Minor comments: 1. Should v_i in section 3 be standard normal distribution? 2. In the sentence between (11) and (12): normalization distribution -> normal distribution."}
{"id": "iclr2020_818", "title": "The Implicit Bias of Depth: How Incremental Learning Drives Generalization | OpenReview", "abstract": "Abstract:###A leading hypothesis for the surprising generalization of neural networks is that the dynamics of gradient descent bias the model towards simple solutions, by searching through the solution space in an incremental order of complexity. We formally define the notion of incremental learning dynamics and derive the conditions on depth and initialization for which this phenomenon arises in deep linear models. Our main theoretical contribution is a dynamical depth separation result, proving that while shallow models can exhibit incremental learning dynamics, they require the initialization to be exponentially small for these dynamics to present themselves. However, once the model becomes deeper, the dependence becomes polynomial and incremental learning can arise in more natural settings. We complement our theoretical findings by experimenting with deep matrix sensing, quadratic neural networks and with binary classification using diagonal and convolutional linear networks, showing all of these models exhibit incremental learning.", "review": "Review:###*Contributions* This paper deals with the theoretical study of the gradient dynamics in deep neural networks. More precisely, this paper define a notion of incremental learning for a particular learning dynamics and study how the depth of the network influence it. Then, the authors show two cases where it applies: matrix sensing, quadratic neural networks and provide intuitions on how it could also apply to linear convolutional networks. This work leverage the framework and the proofs of Gidel et al. 2019 and Saxe et al. 2014 [1] to study the impact of depth in that sequential learning (that is the novelty of this work). I really like the idea of studying the impact of the depth in the training dynamics. And the results (Thm2 and 3) are really interesting (but a bit hard to interpret in my opinion, see my questions) Also, this work should make clearer that the results stated in Thm1 were already substantially presented in Saxe et al. 2013 ( eq. 17 and eq. 12 in [1]) Note that this paper is borderline regarding anonymity since it contains an acknowledgement section revealing the fundings of the authors (that could give enough information to identify the authors of this paper). *Decision* Weak accept: The key results in this work is Theorem 2 and it’s extension to discrete case Theorem 3. they seems really interesting: when in order to observe sequential learning, the dependence in the eigengap for the initialization goes from polynomial ( ) to polynomial. Showing that result is interesting (even in this limited setting) because it theoretically shows (at least for these simple classes of problem) that deeper network perform a notion of incremental learning of components with non-prohibitively small initialization. However, these results are very hard to read. They could be interpreted and simplified in that purpose. I think it would greatly improve the quality of this work. I develop these points in the *questions* section of my review. *Questions* - The definition 1 is hard to interpret. For instance why do you need and since the functions are increasing ? (note: the fact that these function are increasing is never mentioned in the paper but is key to talk about “incremental learning” and for your Definition 1 to make sense, since otherwise could be “forgotten” without violating Definition 1) thus for any we have . Using only one time would make the definition easier to understand. - The result presented in theorem 2, (and 3) are hard to interpret because of the many parameters that distract the reader to the main point. The dependence in s and f would be interesting if we would like to compute these bound in practice but I think that the interest of this work is in the distinction exponential versus polynomial (in ). Thus even though I think that a version with s and f is worth being in the appendix, a version with ½ would make the result statement and the discussion way clearer. (other question why restrict yourself to ½ and ¾ ?) - In the theorem 3 who is ? who is (the largest eigenvalue?) ? - In theorem 3, I am very surprised that there is no notion of eigengap that restrict the size of . for instance let us consider the three eigenvalues with epsilon very small. I think that this condition is implicitly appearing in A and B. Actually for a fixed , if we do then we got thus one of them is smaller than 1. - You restrict yourself to a uniform initialization. Could you extend your results (and definitions) to non-uniform initialization (particularly initialization where - Very small initialization is a big issue in practice because it induces very small gradient at the beginning of the training - Figure 2, why is the time to learn those components increasing ? (i guess it is because of the that get smaller as increases. Isn’t it an issue in practice ? What is the sensitivity to the step size? in the discrete size (i.e. can we increase the step size to compensate the slower learning)? *Minor remark* - Saxe et al. 2013 as been accepted to ICLR in 2014 (would be better to cite the ICLR proceedings) see [1] [1] Saxe et al. 2014 in ICLR url: https://openreview.net/forum?id=_wzZwKpTDF_9C === After rebuttal === I have read the authors* response. I think the authors could have discussed more the interpretation of Theorem 3. in the revision. But, I really like the main takeaway which is that there is a huge discrepancy between 2 and 3 layers in terms of dynamics. I maintain my weak accept"}
{"id": "iclr2020_819", "title": "Gaussian Conditional Random Fields for Classification | OpenReview", "abstract": "Abstract:###In this paper, a Gaussian conditional random field model for structured binary classification (GCRFBC) is proposed. The model is applicable to classification problems with undirected graphs, intractable for standard classification CRFs. The model representation of GCRFBC is extended by latent variables which yield some appealing properties. Thanks to the GCRF latent structure, the model becomes tractable, efficient, and open to improvements previously applied to GCRF regression. Two different forms of the algorithm are presented: GCRFBCb (GCRGBC - Bayesian) and GCRFBCnb (GCRFBC - non-Bayesian). The extended method of local variational approximation of sigmoid function is used for solving empirical Bayes in GCRFBCb variant, whereas MAP value of latent variables is the basis for learning and inference in the GCRFBCnb variant. The inference in GCRFBCb is solved by Newton-Cotes formulas for one-dimensional integration. Both models are evaluated on synthetic data and real-world data. It was shown that both models achieve better prediction performance than relevant baselines. Advantages and disadvantages of the proposed models are discussed.", "review": "Review:###TITLE Gaussian Conditional Random Fields for Classification REVIEW SUMMARY A well justified approach to structured classification with demonstrated good performance. PAPER SUMMARY The paper presents methods for structured classification based on a Gaussian conditional random field combined with a softmax Bernoulli likelihood. Methods for inference and parameter learning are presented both for a *Bayesian* and maximum likelihood version. The method is demonstrated on several data sets. QUALITY In general, the technical quality of the paper is good. Except for minor typos, derivations appear to be correct, although I did not check everything in detail. CLARITY The paper could be improved by a careful revision with focus on improving grammar, but as it stands the paper is easy to follow. It is not clear to me exactly how the numbers in Table 1 were computed. Is this based on 10-fold crossvalidation as in the following tables? ORIGINALITY I am not familiar enough with the field to assess the novelty of the contribution. It would be great if the paper provided a better overview of competing structured classification methods. FURTHER COMMENTS *structured classification* ? *It was shown* -> We show *for given* Is the second sum over k=1 to K in eq. 1 a mistake? *We void* -> We avoid"}
{"id": "iclr2020_820", "title": "VIMPNN: A physics informed neural network for estimating potential energies of out-of-equilibrium systems | OpenReview", "abstract": "Abstract:###Simulation of molecular and crystal systems enables insight into interesting chemical properties that benefit processes ranging from drug discovery to material synthesis. However these simulations can be computationally expensive and time consuming despite the approximations through Density Functional Theory (DFT). We propose the Valence Interaction Message Passing Neural Network (VIMPNN) to approximate DFT*s ground-state energy calculations. VIMPNN integrates physics prior knowledge such as the existence of different interatomic bounds to estimate more accurate energies. Furthermore, while many previous machine learning methods consider only stable systems, our proposed method is demonstrated on unstable systems at different atomic distances. VIMPNN predictions can be used to determine the stable configurations of systems, i.e. stable distance for atoms -- a necessary step for the future simulation of crystal growth for example. Our method is extensively evaluated on a augmented version of the QM9 dataset that includes unstable molecules, as well as a new dataset of infinite- and finite-size crystals, and is compared with the Message Passing Neural Network (MPNN). VIMPNN has comparable accuracy with DFT, while allowing for 5 orders of magnitude in computational speed up compared to DFT simulations, and produces more accurate and informative potential energy curves than MPNN for estimating stable configurations.", "review": "Review:###This paper presents a number of new / extended datasets for the evaluation of ML-based prediction of energies of unstable systems, as well as a network (VIMPNN) that includes a new and better way of including bond-type information. It is also proposed to use auxiliary losses (predicting other chemical properties). Although I am not an expert in chemistry, the new datasets seem fairly well thought out and their utility is well motivated. The proposed change to the MPNN network architecture is rather simple and hardly physics inspired, but the empirical improvement seems substantial, so this too is a nice contribution. So I have decided to give the “weak accept” rating. In 4.2 it is explained how different ways of incorporating bond information were evaluated, and it is stated that “best results were obtained in the case a.ii). However, no results are presented to support this claim, leaving the reader to wonder how rigorous this exploration was. I would suggest systematically evaluating the different options and including the results in an appendix. In table 1 results are shown for the MPNN baseline, baseline with specialised node updates a.ii, and with auxiliary estimates. However, combinations of these are not evaluated. Nevertheless, if I understood correctly, the VIMPNN method tested later includes all of the separate improvements. It would be good to include experimental results to motivate this. As acknowledged in the paper, the idea of using bond-type information was already in Gilmer et al. Also, I think the different ways of including bond-type explored in this paper are not really informed by physics. The choice for method a.ii is made based on empirical results. This is not a problem in itself, but I would suggest that the authors change the wording to not over-promise on the physics-inspiredness. E.g. the abstract says “VIMPNN integrates prior knowledge such as the existence of different interatomic bonds”, suggesting that there is more prior knowledge being exploited than just bonds. Comments: “It produces comparable accuracy to that of DFT while also improving computation time by 5 orders of magnitude”. I assume this speedup is relative to DFT. It would be good to be explicit about that, and also discuss the speed relative to the MPNN baseline (I suppose MPNN and VIMPNN are similar). “The change of atomic distances are performed isomorphically” - I would say “isometrically”."}
{"id": "iclr2020_821", "title": "Scaling Up Neural Architecture Search with Big Single-Stage Models | OpenReview", "abstract": "Abstract:###Neural architecture search (NAS) methods have shown promising results discovering models that are both accurate and fast. For NAS, training a one-shot model has became a popular strategy to approximate the quality of multiple architectures (child models) using a single set of shared weights. To avoid performance degradation due to parameter sharing, most existing methods have a two-stage workflow where the best child model induced from the one-shot model has to be retrained or finetuned. In this work, we propose BigNAS, an approach that simplifies this workflow and scales up neural architecture search to target a wide range of model sizes simultaneously. We propose several techniques to bridge the gap between the distinct initialization and learning dynamics across small and big models with shared parameters, which enable us to train a single-stage model: a single model from which we can directly slice high-quality child models without retraining or finetuning. With BigNAS we are able to train a single set of shared weights on ImageNet and use these weights to obtain child models whose sizes range from 200 to 1000 MFLOPs. Our discovered model family, BigNASModels, achieve top-1 accuracies ranging from 76.5% to 80.9%, surpassing all state-of-the-art models in this range including EfficientNets.", "review": "Review:###Parameter sharing (PS) is an important approach to speed up neural network search (NAS), which further allows the development of differential architecture search (e.g., DARTS) methods. However, PS also deteriorates the performance of learning models on the validation/testing set. This paper first changes the search space from a DAG (micro+marco) in e.g., DARTS to a stacked one based on MBConv; and then, propose to use several tricks to train the super-net well. Finally, a search method is constructed for the supernet to find the desired architectures. Overall, the paper is too experimental. The method is an ensemble of existing approaches, i.e., every single component in the paper has been visited in the literature. Expect for experimental results, I do not see many general lessons we can learn from the paper. Finally, why the proposed method can be better than others is not well-explained and clarified. Please see the questions below: Q1. Is NAS a method only for ImageNet? Can the method generalize to more applications/datasets? - While ImageNet is a good dataset for CV experiments, I think NAS should be a method for deriving architectures with certain requirements. - So, with so many tricks proposed in the paper, I wish authors can carry on experiments on other data sets as well, e.g., CIFAR and MNIST, which can still be preferred. Q2. On motivation, could authors explain more about the difficulties of combining all these techniques? - Each method is brought from some other paper, what motivate authors to combine them together? What makes them believe this is possible? Q3. On presentation, could authors draw a figure of the search space in the main text and give an overall algorithm for Section *3.2 COARSE-TO-FINE ARCHITECTURE SELECTION*. It is hard for a reader to see novelties there. Q4. *We also use the swish activation (Ramachandran et al., 2017) and fixed AutoAugment V0 policy* - are all other compared methods using swish activation and AutoAugment V0 policy? Q5. How about the search efficiency of the proposed method? Only the accuracy is reported in the paper. Q6. Could authors give STD (i.e., gray area to represent STD) in Figure 4, 5 and 7? Some curves are too close, I am not sure they are statistically different. Q7. How is the performance of the super-net? Q8. Could the authors add an ablation study on this point? - *The motivation is to improve all child models in our search space simultaneously, by pushing up both the performance lower bound (the smallest child model) and the performance upper bound (the biggest child model) across all child models.* - It is important to avoid fine-tune - From the paper, I am not sure whether the problem is solved by changing the space or the proposed training method (See Q1)."}
{"id": "iclr2020_822", "title": "Blending Diverse Physical Priors with Neural Networks | OpenReview", "abstract": "Abstract:###Rethinking physics in the era of deep learning is an increasingly important topic. This topic is special because, in addition to data, one can leverage a vast library of physical prior models (e.g. kinematics, fluid flow, etc) to perform more robust inference. The nascent sub-field of physics-based learning (PBL) studies this problem of blending neural networks with physical priors. While previous PBL algorithms have been applied successfully to specific tasks, it is hard to generalize existing PBL methods to a wide range of physics-based problems. Such generalization would require an architecture that can adapt to variations in the correctness of the physics, or in the quality of training data. No such architecture exists. In this paper, we aim to generalize PBL, by making a first attempt to bring neural architecture search (NAS) to the realm of PBL. We introduce a new method known as physics-based neural architecture search (PhysicsNAS) that is a top-performer across a diverse range of quality in the physical model and the dataset.", "review": "Review:###The paper proposes PhysicsNAS, which proposes a method to automatically design architectures that incorporate domain expertise from phyiscs-based models while also accounting for potential mismatch between the model and real world due to unaccountable factors. While existing work seem to incorporate such information via one of 4 standard ways (given on page 2), the proposed work attempts to meld them so as to find the optimal combination for the problem at hand and the data available. While I don*t see anything fundamentally wrong with the paper, I do not feel that the technical contributions are substantial enough to warrant acceptance at ICLR. More specifically, the methodological novelty is limited and the experimental evaluation only evaluates the method on two fairly simple problems. On a positive note, the authors have done a good job of illustrating the idea and have compared it to most natural baselines. I also thought that the illustrations of the architectures found for different sample sizes (in the Appendix) quite insightful. I encourage the authors to pursue this line of work, but test this on more complex prediction tasks where entirely model-based approaches are unreliable, and entirely black-box estimators are sample inefficient. It also seems that the approach need not be confined to physics per se - in many problems in chemistry, materials science etc. scientists are looking for ways to incorporate domain expertise while accounting for model-mismatch."}
{"id": "iclr2020_823", "title": "Attacking Graph Convolutional Networks via Rewiring | OpenReview", "abstract": "Abstract:###Graph Neural Networks (GNNs) have boosted the performance of many graph related tasks such as node classification and graph classification. Recent researches show that graph neural networks are vulnerable to adversarial attacks, which deliberately add carefully created unnoticeable perturbation to the graph structure. The perturbation is usually created by adding/deleting a few edges, which might be noticeable even when the number of edges modified is small. In this paper, we propose a graph rewiring operation which affects the graph in a less noticeable way compared to adding/deleting edges. We then use reinforcement learning to learn the attack strategy based on the proposed rewiring operation. Experiments on real world graphs demonstrate the effectiveness of the proposed framework. To understand the proposed framework, we further analyze how its generated perturbation to the graph structure affects the output of the target model.", "review": "Review:###In this paper, the authors studied the adversarial attack problem for graph classification problem with graph convolutional networks. After observing that traditional attack by adding or deleting edges can change graph eigenvalues, the author proposed to attack by adding rewiring operation which make less effects. Rewiring does not change the graph edge number and the average degree. Further, the authors propose an RL based learning method to learn the policy of doing rewiring operation. Experiments show that the proposed method can make more successful attack on social network data than baselines and previous methods. The idea of using rewiring to make graph attack is interesting and sensible. The proposed RL-based method where the search space is constraint also can solve the problem. However, I have a few concerns on the experiments. 1. In figure 3, the authors also show that the proposed method can make less noticeable changes on eigenvalue. But are these changes still noticeable compared to original one? Please also show these information. 2. 2% data for testing is too few for me. The authors should increase these number. In addition, how many replication of experiments did the author do? The author should give the variance of the results and make significant test if needed. 3. What is the prediction accuracy of the target classifier? Did the attacker flip more correct predictions?"}
{"id": "iclr2020_824", "title": "UWGAN: UNDERWATER GAN FOR REAL-WORLD UNDERWATER COLOR RESTORATION AND DEHAZING | OpenReview", "abstract": "Abstract:###In real-world underwater environment, exploration of seabed resources, underwater archaeology, and underwater fishing rely on a variety of sensors, vision sensor is the most important one due to its high information content, non-intrusive, and passive nature. However, wavelength-dependent light attenuation and back-scattering result in color distortion and haze effect, which degrade the visibility of images. To address this problem, firstly, we proposed an unsupervised generative adversarial network (GAN) for generating realistic underwater images (color distortion and haze effect simulation) from in-air image and depth map pairs. Secondly, U-Net, which is trained efficiently using synthetic underwater dataset, is adopted for color restoration and de-hazing. Our model directly reconstructs underwater clear images using end-to-end autoencoder networks, while maintaining scene content structural similarity. The results obtained by our method were compared with existing methods qualitatively and quantitatively. Experimental results on open real-world underwater datasets demonstrate that the presented method performs well on different actual underwater scenes, and the processing speed can reach up to 125FPS on images running on one NVIDIA 1060 GPU.", "review": "Review:###This paper uses U-net for underwater image restoration and enhancement. But, it is difficult to obtain realistic underwater images, thus this paper introduces a GAN-based method to generate realistic underwater images from in-air image and depth map pairs. - Although this paper points out that the previous work (i.e. WaterGAN) generates color noise and the camera model is not suitable, how does this proposed method overcome these points? Please make it clear. - The figures in this paper are too blurry to see them. To evaluate the effectiveness of the proposed method, the figures are important, thus, it would be better to make them clear. - The technical contribution of the proposed method is not clear. The proposed method seems to be just using the existing techniques."}
{"id": "iclr2020_825", "title": "Transfer Active Learning For Graph Neural Networks | OpenReview", "abstract": "Abstract:###Graph neural networks have been proved very effective for a variety of prediction tasks on graphs such as node classification. Generally, a large number of labeled data are required to train these networks. However, in reality it could be very expensive to obtain a large number of labeled data on large-scale graphs. In this paper, we studied active learning for graph neural networks, i.e., how to effectively label the nodes on a graph for training graph neural networks. We formulated the problem as a sequential decision process, which sequentially label informative nodes, and trained a policy network to maximize the performance of graph neural networks for a specific task. Moreover, we also studied how to learn a universal policy for labeling nodes on graphs with multiple training graphs and then transfer the learned policy to unseen graphs. Experimental results on both settings of a single graph and multiple training graphs (transfer learning setting) prove the effectiveness of our proposed approaches over many competitive baselines.", "review": "Review:###In this paper, the authors proposed a new method for active learning on node classification with GCN. RL based framework is used. The labeled graph is treated as state and the action is labeling the nodes. Validation accuracy on the hold out set is used as reward. Further transfer learning framework is also proposed, where graph-specific policy and master policy are jointly learned. Experiments on benchmark dataset show the effectiveness of proposed method compared to several baselines. The idea of applying RL on active learning with GCN seems to be new and it sounds natural and technically. Also the idea of transferring the learned policy to new graphs make sense for similar graphs. However, the empirical results are a bit weak and not convincing enough for me. Please find the detailed comments below. 1. Is the state defined on node level or graph level? Eq (1) is defined on node level, but I believe it should be a global policy on graph. 2. All the results have a rather high variance. To compare such results, the authors should make a significant test. Otherwise, one cannot say that the performance from one method is better than the other. Especially, for Table 3 and 4, DAG-distill performs not better than DAG-Joint. 3. What do *0-4, 5-9,...* mean in Table 3? 4. Can the authors show curve as in figure 2 for table 2 and 3. It is important to see the progress for active learning. 5. Why the results differ so much in Figure 2 for only 1 query? I believe the first one should be randomly picked. Thus all the methods should perform equally. 6. *Graphs encode the relations between different objects and are ubiquitous in real-world.* Typo in first sentence. 7. homologous or homogenous?"}
{"id": "iclr2020_826", "title": "Closed loop deep Bayesian inversion: Uncertainty driven acquisition for fast MRI | OpenReview", "abstract": "Abstract:###This work proposes a closed-loop, uncertainty-driven adaptive sampling frame- work (CLUDAS) for accelerating magnetic resonance imaging (MRI) via deep Bayesian inversion. By closed-loop, we mean that our samples adapt in real- time to the incoming data. To our knowledge, we demonstrate the first genera- tive adversarial network (GAN) based framework for posterior estimation over a continuum sampling rates of an inverse problem. We use this estimator to drive the sampling for accelerated MRI. Our numerical evidence demonstrates that the variance estimate strongly correlates with the expected MSE improvement for dif- ferent acceleration rates even with few posterior samples. Moreover, the resulting masks bring improvements to the state-of-the-art fixed and active mask designing approaches across MSE, posterior variance and SSIM on real undersampled MRI scans.", "review": " The paper is quite well written and the idea is novel. However, the results are rather weak. The authors present a method to perform adaptive MR compressed sensing, i.e. decide online which readout to sample next. They compare it to an offline learning method where one sampling pattern is optimized for a whole training set, then applied to test data. The offline method performs better in terms of MSE, which is the loss it was trained for, meaning that the authors have not demonstrated a gain in adapting the sampling pattern to individual scans. The primary concern with the paper is not with the author’s contribution, but with serious flaws in [Adler 2018] that unfortunately snowball into this one. While the authors in [Adler 2018] do acknowledge issues with learning a variance, they misdiagnose the problem as mode collapse. Mode collapse is an optimization problem, where the training set contains variability but the generator fails to learn it due to the lack of an encoder. That is not the case here: all the variability of the training set is encapsulated in y, and for each y the target empirical posterior distribution is a Dirac. This is very similar to the calibration problem in classification [1], where classifiers become overconfident because they are trained to always output 0 or 1. If the generator does not learn a Dirac, it can only be because of regularization (either explicit or implicit in the model architecture) or optimization failure (either involuntary or voluntary with early stopping.) Tweaking the loss as advocated in [Adler 2018] does not fundamentally change the problem as long as the loss is minimal at the target empirical posterior. It may change the dynamic behavior and result in posteriors with more variance when combined with early stopping, but those variances are not calibrated, i.e. they have not been trained to match the variances of the true continuous posteriors. In order to learn the variances, one would have to either provide multiple posterior samples for each y during training (not practical in this case,) or perform some kind of calibration on the validation dataset as in [1], i.e. learn the mean and variance from different data, which effectively uses the network’s interpolation properties as a proxy for true random sampling. However this flaw does not invalidate the practical approach developed in the paper, but it seriously undermines its qualifications as a rigorous, principled, Bayesian approach. It also makes the reporting of posterior variances as final quality metrics pretty much useless since they are not interpretable: does lower variance mean that the generator got better at estimating the missing information, or that it got worse at estimating the true posterior variance? I would suggest to at least remove the variances highlights from Table 1 and Table 4, and maybe scrap the data altogether. The paragraph on posterior estimation should also be updated to represent the whole scope of the problem. Section 2: Theory: suggest to remove “Without loss of generality”. Due to the known issues with variance estimation, having p(y | x) as a density instead of a Dirac could very well change the behavior of the generator. Section 2.1 Adaptive masks. The whole first paragraph is somewhat misleading and should be revised. Real-time reconstruction is indeed possible without deep learning, see [2] for example. Furthermore, real-time reconstruction is not nearly fast enough for adaptive sampling. Real-time reconstruction means that reconstructing an image is at least as fast as scanning the whole image, i.e. in the order of 0.1 to 1s., but for adaptive sampling one must reconstruct at least as fast as the time between two successive readouts, i.e. in the order of 1 to 10 ms. Both [Jin 2019] and [Zhang 2019] only showed single-coil offline simulations with no indications of the reconstruction time and so do the authors. Figure 1: I must be missing something here. How can the image-domain and Fourier-domain figures be different? The Fourier transform being orthogonal, norms and variances should be the same in both domains. [1] C. Guo, G. Pleiss, Y. Sun and K. Q. Weinberger, “On Calibration of Modern Neural Networks”, ICML 2017 70:1321-1330. [2] M. Uecker, S. Zhang, and J. Frahm, “Nonlinear Inverse Reconstruction for Real-Time MRI of the Human Heart Using Undersampled Radial FLASH”, MRM 63:1456-1462 (2010)."}
{"id": "iclr2020_827", "title": "To Relieve Your Headache of Training an MRF, Take AdVIL | OpenReview", "abstract": "Abstract:###We propose a black-box algorithm called {it Adversarial Variational Inference and Learning} (AdVIL) to perform inference and learning on a general Markov random field (MRF). AdVIL employs two variational distributions to approximately infer the latent variables and estimate the partition function of an MRF, respectively. The two variational distributions provide an estimate of the negative log-likelihood of the MRF as a minimax optimization problem, which is solved by stochastic gradient descent. AdVIL is proven convergent under certain conditions. On one hand, compared with contrastive divergence, AdVIL requires a minimal assumption about the model structure and can deal with a broader family of MRFs. On the other hand, compared with existing black-box methods, AdVIL provides a tighter estimate of the log partition function and achieves much better empirical results.", "review": "Review:###The work proposes using variational distributions to model the model the inference of latent variables and model the partition function building on NVIL, thereby providing an algorithm that would work on general MRFs for both inference and learning. Since the two terms in the NLL are opposite in sign, it is a minimax operation and GAN like adversial training can be used. The paper shows providing tighter results to estimate the log partition function and comparisons on the digits dataset and Anneal importance sampling. The paper builds on NVIL by using two variational distributions for the NLL and how to solve the parameter estimation problem. I think this strategy can be tested more extensively on more types of general MRFs and more rigourous experimentation and that the community will benefit from reading from these ideas. The paper contains some theory behind the work and experimental analysis of Advil including the sensitivity to parameters. Advil shows promise compared to the competing methods in some of the problems. Questions and suggestions for improving the paper 1. Is this applicable to multivalued nodes or just binary problems? Or are any modifications needed? 2. Inference can be done in general using approximate methods like variational message passing, QBPO among others that don*t depend on graph structures either, how does this work compared when those algorithms are used with simple gradient descent while training the parameters? 3. The paper states that the algorithm is convergent if the variational encoder approximates the model well. How would you define good approximation? 4. It would be good to add more details of how the GAN framework/adversial training is used. 5. Would it be possible to compare Advil to ALI in some of the experiments? 6. Fig. 2 d was not clear to me as why to expect the plot we see. The NLL flattens out with no progress. 7. It would be helpful to the reader to understand the comparison using persistent contrastive divergence. Why is it not used in other experiments. The paper says the main comparison point is NVIL but different experiments either mention ALI, VCD or PCD which is confusing. 8. It would be nice to see the time comparison between this learning parameters and other methods."}
{"id": "iclr2020_828", "title": "Inductive Matrix Completion Based on Graph Neural Networks | OpenReview", "abstract": "Abstract:###We propose an inductive matrix completion model without using side information. By factorizing the (rating) matrix into the product of low-dimensional latent embeddings of rows (users) and columns (items), a majority of existing matrix completion methods are transductive, since the learned embeddings cannot generalize to unseen rows/columns or to new matrices. To make matrix completion inductive, content (side information), such as user*s age or movie*s genre, has to be used previously. However, high-quality content is not always available, and can be hard to extract. Under the extreme setting where not any side information is available other than the matrix to complete, can we still learn an inductive matrix completion model? In this paper, we investigate this seemingly impossible problem and propose an Inductive Graph-based Matrix Completion (IGMC) model without using any side information. It trains a graph neural network (GNN) based purely on local subgraphs around (user, item) pairs generated from the rating matrix and maps these subgraphs to their corresponding ratings. Our model achieves highly competitive performance with state-of-the-art transductive baselines. In addition, since our model is inductive, it can generalize to users/items unseen during the training (given that their ratings exist), and can even transfer to new tasks. Our transfer learning experiments show that a model trained out of the MovieLens dataset can be directly used to predict Douban movie ratings and works surprisingly well. Our work demonstrates that: 1) it is possible to train inductive matrix completion models without using any side information while achieving state-of-the-art performance; 2) local graph patterns around a (user, item) pair are effective predictors of the rating this user gives to the item; and 3) we can transfer models trained on existing recommendation tasks to new tasks without any retraining.", "review": "Review:###This paper presents an inductive matrix completion model using graph neural networks. The proposed method assumes the rating between a user-item pair is determined by this pair*s surrounding sub-graph via a neural network. It claims to be an inductive model and don*t need any side information as it only uses the surrounding sub-graph structure to give predictions. The experiments also show the promising results on matrix completion tasks, sparse rating matrix cases and transfer learning tasks The paper is well written and easy to follow. I think the *cold-start* setting this method is mainly focusing on is worth exploring. In this setting, new users have given a few ratings but the quality of their side information is still low. It seems to me that this setting is common in real case. For example, newly registered Netflix users may quickly watch some videos without completing their profile information. However, when the *new* users have given some ratings, we can re-train a traditional MC model including the new ratings. But I believe the proposed method of this paper will be useful when re-training cannot be done very frequently. I have the following concerns or questions. 1. The paper claims the proposed method doesn*t need any side information. However, it seems that the side information can be integrated into the model such as concatenating with the one-hot encoding. Have you tried to use the side information for IGMC? 2. The claims in Section 4 don*t make much sense to me. For example, it doesn*t seem to me that GC-MC can*t distinguish Figure (a) and (b) because GC-MC will take the whole graph into consideration. Also, why will node-based approaches push all nodes to have similar embeddings with multiple layers? Could you give some experimental verifications? 3. It*s unclear to me how Eq. (6) helps the performance. It will be better to compare a version without using L_{ARR}. Moreover, I believe Eq. (6) can be designed better. 4. GC-MC uses bilinear decoder to give probability predictions for different ratings. Is there any particular reason why you choose squared loss as in Eq. (5) instead of a bilinear decoder? 5. I have a big concern for the scalability. If we want to make recommendations for a user from a million movies, the proposed method seems to need to compute Eq. (4) for one million times. Is there any approximate way to speedup it? Overall, I think the proposed method is interesting and the experimental results are impressive."}
{"id": "iclr2020_829", "title": "Multi-agent Reinforcement Learning for Networked System Control | OpenReview", "abstract": "Abstract:###This paper considers multi-agent reinforcement learning (MARL) in networked system control. Specifically, each agent learns a decentralized control policy based on local observations and messages from connected neighbors. We formulate such a networked MARL (NMARL) problem as a spatiotemporal Markov decision process and introduce a spatial discount factor to stabilize the training of each local agent. Further, we propose a new differentiable communication protocol, called NeurComm, to reduce information loss and non-stationarity in NMARL. Based on experiments in realistic NMARL scenarios of adaptive traffic signal control and cooperative adaptive cruise control, an appropriate spatial discount factor effectively enhances the learning curves of non-communicative MARL algorithms, while NeurComm outperforms existing communication protocols in both learning efficiency and control performance.", "review": "Review:###1. Summary The authors use decentralized MARL for networked system control. Each agent might control a traffic light (exp 1) or a car in traffic (exp 2). Some features of their approach are a spatial Markov assumption (only neighborhood matters), a spatial discount factor, and NeurComm: a general message passing scheme between agent policies. The authors compare their method with CommNet (averages messages before broadcast), DIAL (small-scale direct communication), etc. 2. Decision (accept or reject) with one or two key reasons for this choice. Weak accept. 3. Supporting arguments The experiments and analysis of the more general communication scheme are nice and the assumptions used make sense for the environments considered (spatial interactions and dynamics). The 2 environments used are nice, but it would be nice to see non-traffic applications as well. The methodological contributions make sense in the spatial domain, but it would be interesting to see how neighborhood-based assumptions can be used in non-spatial / non-Euclidean domains. 4. Additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment. It would be nice to explain why / what communication strategy enables FPrint to do better than NeurComm. 5. Questions"}
{"id": "iclr2020_830", "title": "Weight-space symmetry in neural network loss landscapes revisited | OpenReview", "abstract": "Abstract:###Neural network training depends on the structure of the underlying loss landscape, i.e. local minima, saddle points, flat plateaus, and loss barriers. In relation to the structure of the landscape, we study the permutation symmetry of neurons in each layer of a deep neural network, which gives rise not only to multiple equivalent global minima of the loss function but also to critical points in between partner minima. In a network of hidden layers with neurons in layers , we construct continuous paths between equivalent global minima that lead through a `permutation point* where the input and output weight vectors of two neurons in the same hidden layer collide and interchange. We show that such permutation points are critical points which lie inside high-dimensional subspaces of equal loss, contributing to the global flatness of the landscape. We also find that a permutation point for the exchange of neurons and transits into a flat high-dimensional plateau that enables all permutations of neurons in a given layer at the same loss value. Moreover, we introduce higher-order permutation points by exploiting the hierarchical structure in the loss landscapes of neural networks, and find that the number of -th order permutation points is much larger than the (already huge) number of equivalent global minima -- at least by a polynomial factor of order . In two tasks, we demonstrate numerically with our path finding method that continuous paths between partner minima exist: first, in a toy network with a single hidden layer on a function approximation task and, second, in a multilayer network on the MNIST task. Our geometric approach yields a lower bound on the number of critical points generated by weight-space symmetries and provides a simple intuitive link between previous theoretical results and numerical observations.", "review": "Review:###This paper studies the permutation symmetry of deep neural networks. It was known that by reordering neurons and their connections in each layer, the input -> output map the neural network represents can be preserved. This corresponded to a set of unconnected equivalent points in the weight space. The authors study the weight-space connectivity of these points through midpoints they call “permutation points”. They demonstrate that such points are members of high-dimensional manifolds of equivalent points. After that, they look at empirical experiments and explicitly construct a path between two equivalent weight-space points on a toy task and MNIST. I generally like the paper and its geometrical lens on the problem. I find the figures very helpful in understanding what is going on. However, there are a few points that I wasn’t entirely clear on. I will detail them below. -- Point 1 -- Connecting equivalent minima vs connecting SGD-found minima. If I understood the paper correctly, the derivation connects two weight space points A and B whose weights and biases, once loaded to the neural network, would have the exact same answers on all inputs X i.e. f_A(X) == f_B(X), i.e. they are a pair of equivalent points. I understand that those are the ones we obtain by using the permutation symmetries. However, some of the papers cited look at the low-loss paths between pairs of optima found by training with SGD from independent initializations, which in turn represent different functions. I.e. for two such optima C and D, the predictions on the val/test set are (sometimes) different, showing that the functions are not the same. I found the initial evidence in: Large Scale Structure of Neural Network Loss Landscapes. Stanislav Fort and Stanislaw Jastrzebski. NeurIPS 2019. (https://arxiv.org/abs/1906.04724) And also in much more detail in another OpenReview submission: Deep Ensembles: A Loss Landscape Perspective. (https://openreview.net/forum?id=r1xZAkrFPr) I found your results very compelling, however, the two problems seem to be quite different -- on one hand you are connecting a pair of minima that are in fact *identical* by construction. On the other hand the empirical work in literature (especially in the two papers I provided above) deals with pairs of minima that in fact do differ on the test set (at least). Would you mind commenting on how the two approaches relate to each other? -- Point 2 -- Higher order connectivity In Large Scale Structure of Neural Network Loss Landscapes. Stanislav Fort and Stanislaw Jastrzebski. NeurIPS 2019. (https://arxiv.org/abs/1906.04724), the authors look at higher-order connectivity between SGD-found optima (e.g. connecting 3 optima on a 2-manifold, 4 optima on a 3-manifold etc.). They also have a particularly simple path-finding algorithm. This seems relevant to the approach you are presenting, although the points in Point 1 still stand. -- Point 3 -- Previous work on connecting two optima using layer-wise weight merging Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets. Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Zhiyuan Li, Wei Hu, Sanjeev Arora, Rong Ge. (https://arxiv.org/abs/1906.06247) They prove that a low-loss path between 2 optima exists provided you can apply a p_keep = 0.5 dropout on each of the optima without incurring a significant loss punishment for it. This paper seems very related to your approach. Would you mind commenting on the differences? -- Conclusion -- I like the paper and the idea in general. I appreciate the geometrical lens the authors took. My main point of confusion relates to the connection between this work and the low-loss connectivity between inequivalent optima found in literature, which (at least to me) seems to be the more interesting of the two connectivities."}
{"id": "iclr2020_831", "title": "On the Unintended Social Bias of Training Language Generation Models with News Articles | OpenReview", "abstract": "Abstract:###There are concerns that neural language models may preserve some of the stereotypes of the underlying societies that generate the large corpora needed to train these models. For example, gender bias is a significant problem when generating text, and its unintended memorization could impact the user experience of many applications (e.g., the smart-compose feature in Gmail). In this paper, we introduce a novel architecture that decouples the representation learning of a neural model from its memory management role. This architecture allows us to update a memory module with an equal ratio across gender types addressing biased correlations directly in the latent space. We experimentally show that our approach can mitigate the gender bias amplification in the automatic generation of articles news while providing similar perplexity values when extending the Sequence2Sequence architecture.", "review": "Review:###This paper focuses on the timely and important topic of unintended social bias in language models and proposes an approach to mitigate gender bias in the automatic generation of news articles. The automatic text generation is done by a sequence-to-sequence-model using word embeddings. The pattern of these embeddings in an multi-dimensional space can be associated with the stereotypes found in the corpora the models are trained on. To mitigate this bias the authors combine the network with a memory module storing additional gender information. This approach is inspired by the memory module used by Kaiser et al. (2017) to remember rare events, replacing the vector to track the age of items with a key representing the gender. The proposed network architecture (*Seq2Seq+FairRegion*) is evaluated using the bias score presented by Zhao et al. (2017) and compared to the sequence-to-sequende-models *Seq2Seq2* by Sutskever, Vinyals and Lee (2014) and *Seq2Seq+Attention* by Bahdanau, Cho and Bengio (2015). Besides some typographical errors (*work embedding*) the paper is overall well written but unfortunately the flow of the text could be improved to make it easier to follow. The authors should perhaps consider to explain the structure of the paper in the introduction. There are some errors according the brackets of the in-text citations which should be corrected. The two figures both illustrate the memory module and could be easily combined to one because the second figure just adds the encoding stage of the architecture. The caption of figure 1 could be used to further explain the *Fair Region* in the text (section 2). In section 4 the authors explain how to compute the bias score. Because these calculations are used to evaluate their methods, this section should perhaps be a subsection to section 5 (*experiments*). To evaluate the proposed method the authors use newspaper articles from Chile, Peru and Mexiko and compare the bias amplification metric for the trained models. The results are shown for the datasets from Peru and Mexico (Chile is missing) with a larger bias amplification for the Peru dataset. It could be justified more clearly why the experiments compare the bias amplification scores of different countries while the main focus of the paper is to mitigate gender bias in general. While this is an important topic, the contribution is rather minor and not well presented. Specifically the experiements should be extended."}
{"id": "iclr2020_832", "title": "PLEX: PLanner and EXecutor for Embodied Learning in Navigation | OpenReview", "abstract": "Abstract:###We present a method for policy learning to navigate indoor environments. We adopt a hierarchical policy approach, where two agents are trained to work in cohesion with one another to perform a complex navigation task. A Planner agent operates at a higher level and proposes sub-goals for an Executor agent. The Executor reports an embedding summary back to the Planner as additional side information at the end of its series of operations for the Planner*s next sub-goal proposal. The end goal is generated by the environment and exposed to the Planner which then decides which set of sub-goals to propose to the Executor. We show that this Planner-Executor setup drastically increases the sample efficiency of our method over traditional single agent approaches, effectively mitigating the difficulty accompanying long series of actions with a sparse reward signal. On the challenging Habitat environment which requires navigating various realistic indoor environments, we demonstrate that our approach offers a significant improvement over prior work for navigation.", "review": " This paper proposes a hierarchical policy consisting of a planner and an executor policy for solving navigation tasks. A planner policy consumes the desired target goal and a ELI context from the executor to propose subgoals for the executor. The executor consumes these subgoals to output actions that the agent should execute, and returns the ELI context to the planner policy. Strengths: 1. Decomposing the problem hierarchically makes a lot of sense for the given task. 2. The design of the ELI vector that passes information seen by the executor during the intermediate steps back to the planner seems novel, and has been shown to experimentally help with performance. 3. Paper conducts experiments in visually realistic environments and presents comparisons against methods from recent papers. Shortcomings: 1. Limited novelty: Hierarchical decomposition is extremely specific to the task of pointgoal navigation. The proposed decomposition employs use of pointgoal targets for the executor. While this works for navigation, it is unclear how this can be made to work for other robotic tasks such as manipulation, or for other long-horizon problems like montezuma revenge. In fact, such a decomposition of the problem have been investigated in past works for navigation [A,B]. 2. Experimental evaluations: a) Lack of use of standard metrics. Paper uses tasks in Habitat for evaluation, but doesn*t use the metrics that come along with it. It is fine to point out problems with standard metrics, but it will really help if the original metrics were included (possibly in addition to other metrics that will address the shortcomings like #collisions, etc as mentioned). b) Additionally, there was a challenge https://aihabitat.org/challenge/2019/ at CVPR 2019, and there is a leaderboard with results: https://evalai.cloudcv.org/web/challenges/challenge-page/254/leaderboard/839 The paper should include these previous results on this task on the dataset. While I agree that details for some of these methods may not be available, making meaningful comparisons and discussions difficult, details about some of these methods are in fact available, and these methods should be included in the paper. c) Following up on point b), the paper is tackling the RGBD version of this problem and details about a very strong baseline `Map and Plan Baseline` is available along with code. I am not sure if success rates presented in Table 1 are directly comparable to these publicly available numbers, but it seems like this baseline outperforms the methods proposed in this paper by a healthy margin (81 vs SPL of 89, which means success rate would be even higher). This is striking as this baseline does not need any training at all. This being said, I can imagine that the proposed method can work with RGB images alone (while such a baseline may not), but this should be experimentally demonstrated. Even then, paper should compare to other methods for this task as available on the leaderboard. Thus, while I like the hierarchical decomposition and the exchange of information between the executor and the planner via the ELI. However, I believe the contribution is limited, in that it is not clear how generally applicable the framework is, and the experimental validation for the problem where it is indeed applicable is weak. [A] Combining Optimal Control and Learning for Visual Navigation in Novel Environments Somil Bansal, Varun Tolani, Saurabh Gupta, Jitendra Malik, Claire Tomlin [B] M. Müller, A. Dosovitskiy, B. Ghanem, and V. Koltun. Driving policy transfer via modularity and abstraction. arXiv preprint arXiv:1804.09364, 2018."}
{"id": "iclr2020_833", "title": "ROBUST SINGLE-STEP ADVERSARIAL TRAINING | OpenReview", "abstract": "Abstract:###Deep learning models have shown impressive performance across a spectrum of computer vision applications including medical diagnosis and autonomous driving. One of the major concerns that these models face is their susceptibility to adversarial attacks. Realizing the importance of this issue, more researchers are working towards developing robust models that are less affected by adversarial attacks. Adversarial training method shows promising results in this direction. In adversarial training regime, models are trained with mini-batches augmented with adversarial samples. In order to scale adversarial training to large networks and datasets, fast and simple methods (e.g., single-step gradient ascent) are used for generating adversarial samples. It is shown that models trained using single-step adversarial training method (adversarial samples are generated using non-iterative method) are pseudo robust. Further, this pseudo robustness of models is attributed to the gradient masking effect. However, existing works fail to explain when and why gradient masking effect occurs during single-step adversarial training. In this work, (i) we show that models trained using single-step adversarial training method learns to prevent the generation of single-step adversaries, and this is due to over-fitting of the model during the initial stages of training, and (ii) to mitigate this effect, we propose a single-step adversarial training method with dropout scheduling to learn robust models. Unlike models trained using single-step adversarial training method, models trained using the proposed single-step adversarial training method are robust against both single-step and multi-step adversarial attacks, and achieve on-par results compared to the computationally expensive state-of-the-art multi-step adversarial training method, in white-box and black-box settings.", "review": " This paper first analyzes the gradient mask effect caused by naive single-step adversarial training. Then it proposes the SADS method to improve the robustness of the single-step adversarial training by adding the dropout mechanism. The evaluation in the experiments is appropriate and reasonable. However, the critical point that prevents me from accepting this paper is that there is a severe degradation on clean accuracy when applying SADS. For example, in Table 3, the clean accuracy of SADS on CIFAR-10 is only 75%. So I doubt that if SADS can scale to a larger dataset like ImageNet, since the degradation on the clean accuracy may be more serious."}
{"id": "iclr2020_834", "title": "Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks | OpenReview", "abstract": "Abstract:###We study the training process of Deep Neural Networks (DNNs) from the Fourier analysis perspective. We demonstrate a very universal Frequency Principle (F-Principle) --- DNNs often fit target functions from low to high frequencies --- on high-dimensional benchmark datasets, such as MNIST/CIFAR10, and deep networks, such as VGG16. This F-Principle of DNNs is opposite to the learning behavior of most conventional iterative numerical schemes (e.g., Jacobi method), which exhibits faster convergence for higher frequencies, for various scientific computing problems. With a naive theory, we illustrate that this F-Principle results from the regularity of the commonly used activation functions. The F-Principle implies an implicit bias that DNNs tend to fit training data by a low-frequency function. This understanding provides an explanation of good generalization of DNNs on most real datasets and bad generalization of DNNs on parity function or randomized dataset.", "review": "Review:###The paper studies the training process of NNs through the lens of Fourier analysis. The authors argue that during the training process, NNs will first learn low frequencies part of the function first and then the high frequency part. To verify this claim empirically, the author propose two methods: 1. examine the convergence of different frequencies in a pre-selected direction in the frequency space during training; 2. examine the convergence rate of the 2-norm of low v.s. high frequencies during training. Through the experimental results of these two methods, the authors conclude that NNs learn the low frequency components before the high frequency components. The authors also discuss a potential application of this observation to solving high dimensional PDEs: coupling DNNs training (good at learning low frequency components) with the Jacobi method (good at learning high frequency components). Finally, the authors also provide some theoretical intuition (Thm 1., 2.) why low frequency components are learned faster and an explanation why NNs could generalize well on images but perform poorly on tasks like learning parity functions. Other comments: 1. It seems the filtering method is a better (might be a sufficient) way to justify the F-Principle than the projection method, given the projection method examines only one direction (also appointed out in the paper). 2. When talking about Fourier transform, would you specify what is the domain of the functions and how the functions are defined (section 3.1) The notation there is somewhat confusing (which makes the rest of the paper difficult to follow) since you are mentioning the Fourier transform of the set {(x_i, y_i)}. It will be helpful to define the function before defining its Fourier transform. Please also mention what is the domain of the function, {x_i}_i or R^d? 3. According to equation (4), it seems the domain of the functions is {x_i}_i, otherwise equation (4) should be a function of xin R^d, not x_i. 4. Could you elaborate why (4) is a good approximation of the low frequency energy rather than the L2 norm (over xin R^d) of (4) with x_i replaced by xin R^d. 5. It might be useful to refine the related work section. It is not clear what are the previous contributions prior to this paper, and it seems [1] shares some similar results/observation with the this paper. Overall, I lean to a weak rejection. The key findings (and similar results, e.g. NNs learn simple functions first), i.e. F-Principle seems to have already appeared in previous works, e.g. [1] and the theoretical results of this paper are limited to an idealized setting (results of more general setting appear in another work, mentioned in the paper.) [1]Nasim Rahaman, Devansh Arpit, Aristide Baratin, Felix Draxler, Min Lin, Fred A Hamprecht, Yoshua Bengio, and Aaron Courville. On the spectral bias of deep neural networks. arXiv preprint arXiv:1806.08734, 2018. 1, 8, A"}
{"id": "iclr2020_835", "title": "Sparse Networks from Scratch: Faster Training without Losing Performance | OpenReview", "abstract": "Abstract:###We demonstrate the possibility of what we call sparse learning: accelerated training of deep neural networks that maintain sparse weights throughout training while achieving dense performance levels. We accomplish this by developing sparse momentum, an algorithm which uses exponentially smoothed gradients (momentum) to identify layers and weights which reduce the error efficiently. Sparse momentum redistributes pruned weights across layers according to the mean momentum magnitude of each layer. Within a layer, sparse momentum grows weights according to the momentum magnitude of zero-valued weights. We demonstrate state-of-the-art sparse performance on MNIST, CIFAR-10, and ImageNet, decreasing the mean error by a relative 8%, 15%, and 6% compared to other sparse algorithms. Furthermore, we show that sparse momentum reliably reproduces dense performance levels while providing up to 5.61x faster training. In our analysis, ablations show that the benefits of momentum redistribution and growth increase with the depth and size of the network.", "review": "Review:###This paper proposes an algorithm called Sparse Momentum for learning sparse neural networks. They claim to maintain sparse weights throughout training while achieving dense network performance levels. They also show their method improves training speed up to 5.61x faster training. The provides a decent motivation for why sparse networks can be helpful. The related work section is well summarized and they emphasize that the current work*s primary motivation is to reduce training time while maintaining performance. They compare their method with other methods that also maintain sparse neural networks throughout training and involve single training phase, which is fair. Their method consists of primarily 3 phases - i) pruning weights ii) redistribution of weights iii) regrowing of weights based on the exponentially smoothed momentum term for each layer. The method is well explained and motivation is clear. Edge case was also explained for more clarity. However, there is something that needs more clarification in Pg-3, 3rd line, they say the 3 components of the algorithm i) ii) and iii) can be tackled independently with a divide and conquer strategy to gain some computational benefits. In my understanding, the method performs these 3 steps sequentially after each epoch. Not sure how divide and conquer strategy can be used here ? The results were shown for MNIST & CIFAR-10 using AlexNet, VGG16 and LeNet-5 models. They show that the current algorithm is better than other proposed methods in the literature in most of the cases. The claims made in some cases that this method reaches dense network performance is not completely true though. For example, in Table 1, the only case where the proposed method reaches dense network*s performance is for VGG16-D. In all the rest of the cases, the current method and dense network error differ by at least 5%. The authors compare speed up results for training in two ways: theoretical speedups which are proportional to reduction in number of FLOPS and practical speedups using dense convolutional algorithms corresponding to completely empty channels. It*s good that the authors have mentioned due to the current lack of optimal sparse matrix multiplication implementations these speedups cannot be garnered practically yet. The estimates based on FLOPs reduction or Empty Channel based look promising. They also show ablation study of how the redistribution and weight regrowth based on momentum is better than doing in a random fashion. Overall, I think the paper proposes an interesting idea of using momentum with promising results to learn sparse neural networks."}
{"id": "iclr2020_836", "title": "WHAT ILLNESS OF LANDSCAPE CAN OVER-PARAMETERIZATION ALONE CURE? | OpenReview", "abstract": "Abstract:###Over-parameterized networks are widely believed to have nice landscape, but what rigorous results can we prove? In this work, we prove that: (i) from under-parameterized to over-parameterized networks, there is a phase transition from having sub-optimal basins to no sub-optimal basins; (ii) over-parameterization alone cannot eliminate bad non-strict local minima. Specifically, we prove that for any continuous activation functions, the loss surface of a class of over-parameterized networks has no sub-optimal basin, where “basin” is defined as the setwise strict local minimum. Furthermore, for under-parameterized network, we construct loss landscape with strict local minimum that is not global. We then show that it is impossible to prove “all over-parameterized networks have no sub-optimal local minima”, by giving counter-examples for 1-hidden-layer networks with a class of neurons. Viewing various bad patterns of landscape as illnesses (bad basins, flat regions, etc.), our results indicate that over-parameterization is not a panacea for every “illness” of the landscape, but it can cure one practically annoying illness (bad basins).", "review": "Review:###This paper theoretically studies the benefit of overparameterization for the loss landscape in fully connected deep neural networks. Here the definition of overparameterization is the number of training samples larger than the number of neurons in a hidden layer. This paper shows that if the last hidden layer is overparameterized (with some more mild assumptions) than loss function is weakly global. This implies there are no bad strict local minima. Authors also show by example that overparameterization networks could have bad non-strict local minima could exist. Due to last minute request, I have not been able to digest the proof so I won*t be able to say anything regarding the correctness of the paper. While there are interesting observations one could find on overparamerized network*s loss landscape, with current form the paper*s presentation is unclear and I wan not convinced on its relevance to realistic settings. Pros: The paper is tackling an important question regarding the benefits of overparameterization. As authors note isolating benefits coming from overparameterization is important study. Assumptions used to prove the theorems are general enough except for the overparameterized assumption. Cons: There are few problems I encountered reviewing. I found paper unclear to read and understand. Maybe due to last minute submission, the paper does not appear to be polished and needs more work making the presentation clear. Beyond various typos, broken reference (unnecessary Yu et al. infront of Yu & Chen (1995)), I did not find section 2 from 1-dimensional case especially helpful for understanding impact of the paper. Also since various previous works have similar claims to this paper, it would be clear to distinguish how current results are distinguished in the contribution section. For example as authors say Nguyen et al (2018)/ Venturi et al (2018) also show that non-existence of bad strict local minima. The contribution section may become clear if authors could describe specific contributions beyond what already have been described in literature. Is the relaxation to continuous activation function that is significant for example? I have concerns about the applicability of the results. For example, in realistic data would A3 or B1 hold? For example with 50k MNIST, one would need 50k hidden layer and the benefit of overparameterization in practice appears for much smaller networks. Also for Theorem 2 assumption B2 is quite strict which doesn’t include ReLU/TanH/Sigmoid activation functions. For applicability, I wonder if there would be a practical guidance based on findings in the paper which would make the results more impactful. While it is important to understand what type of minima overparameterization network brings in, wouldn’t more relevant true phenomenology of deep networks would be captured through statistical arguments in terms of how likely our initialization and optimization algorithm would fall into a certain type of minima. I was not convinced some of the particular examples in low dimensional settings reflects what realistic over/under parameterized neural networks are showing. Questions and Comments: How is counter example in section 6.1 an example of overparameterized case? I only see one hidden unit with 1 data point which is not overparameterized by paper’s definition. Please remove unnecessary “Yu et al.” Typo p1 last paragraph “has -> have”"}
{"id": "iclr2020_837", "title": "Model-based Saliency for the Detection of Adversarial Examples | OpenReview", "abstract": "Abstract:###Adversarial perturbations cause a shift in the salient features of an image, which may result in a misclassification. We demonstrate that gradient-based saliency approaches are unable to capture this shift, and develop a new defense which detects adversarial examples based on learnt saliency models instead. We study two approaches: a CNN trained to distinguish between natural and adversarial images using the saliency masks produced by our learnt saliency model, and a CNN trained on the salient pixels themselves as its input. On MNIST, CIFAR-10 and ASSIRA, our defenses are able to detect various adversarial attacks, including strong attacks such as C&W and DeepFool, contrary to gradient-based saliency and detectors which rely on the input image. The latter are unable to detect adversarial images when the L_2- and L_infinity- norms of the perturbations are too small. Lastly, we find that the salient pixel based detector improves on saliency map based detectors as it is more robust to white-box attacks.", "review": "Review:###This paper proposes an adversarial defense method that is a saliency-based adversarial example detector. The method is motivated by the well-known fact that saliency maps and adversarial perturbations are having similar mathematical formulations and derivations. By using model-based saliency maps rather than gradient-based ones, it seems to detect hard attacks with smaller perturbation size as well. As far as the authors mentioned, the proposed method is simply using different techniques to derive saliency maps compared to the previous methods. Overall, the intuition and motivation of this paper are from the previous works and the main contribution is to use another (powerful) saliency map extractor for learning an adversarial detector. Although the overall results are improved from the previous methods, the proposed method is lack of novelty. - For SMD (Saliency Map Defense), what is the reason that the input image is not used together? computational issue? performance degradation? - Is it possible to train a single detector that can handle all different adversarial attacks? - Would the distance between saliency maps from different attacks be small? How does the saliency map change under different attacks? - Have you tried any other powerful saliency maps other than Dabkowski & Gal (2017)?"}
{"id": "iclr2020_838", "title": "Conditional Flow Variational Autoencoders for Structured Sequence Prediction | OpenReview", "abstract": "Abstract:###Prediction of future states of the environment and interacting agents is a key competence required for autonomous agents to operate successfully in the real world. Prior work for structured sequence prediction based on latent variable models imposes a uni-modal standard Gaussian prior on the latent variables. This induces a strong model bias which makes it challenging to fully capture the multi-modality of the distribution of the future states. In this work, we introduce Conditional Flow Variational Autoencoders (CF-VAE) using our novel conditional normalizing flow based prior to capture complex multi-modal conditional distributions for effective structured sequence prediction. Moreover, we propose two novel regularization schemes which stabilizes training and deals with posterior collapse for stable training and better match to the data distribution. Our experiments on three multi-modal structured sequence prediction datasets -- MNIST Sequences, Stanford Drone and HighD -- show that the proposed method obtains state of art results across different evaluation metrics.", "review": "Review:###The work proposes a method to improve conditional VAE with a learnable prior distribution using normalizing flow. The authors also design two regularization methods for the CF-VAE to improve training stability and avoid posterior collapse. The paper is clearly motivated and easy to follow. Experiment results on MNIST, Stanford Drone and HighD datasets show the proposed that the model achieves better results than previous state-of-the-art models by significant margins. However, the reviewer has the following comments on improving the paper: The motivation of the conditional normalizing flow design could be made more clear. The posterior regularization originates from the problem that the log Jacobian term encourages contraction of the base distribution. The log Jacobian term would be zero and would not encourage the contraction of the base distribution if the normalizing flow was volume-preserving, like NICE (http://proceedings.mlr.press/v37/rezende15.pdf, https://arxiv.org/pdf/1410.8516.pdf), which could be to convert into a conditional normalizing flow. On the MNIST results, the CF-VAE model with the proposed conditional normalizing flow even has worse performance than the affine flow model without the regularization. Therefore, clarifying the motivation behind this design choice is important. The work claims the two regularization methods are used to avoid a low-entropy prior and posterior collapse. But the claims are not fully substantiated in the experimental results. It would be better if the paper explicitly compares the CF-VAE models with and without regularizations in terms of the entropy of prior distribution and KL divergence."}
{"id": "iclr2020_839", "title": "Overcoming Catastrophic Forgetting via Hessian-free Curvature Estimates | OpenReview", "abstract": "Abstract:###Learning neural networks with gradient descent over a long sequence of tasks is problematic as their fine-tuning to new tasks overwrites the network weights that are important for previous tasks. This leads to a poor performance on old tasks – a phenomenon framed as catastrophic forgetting. While early approaches use task rehearsal and growing networks that both limit the scalability of the task sequence orthogonal approaches build on regularization. Based on the Fisher information matrix (FIM) changes to parameters that are relevant to old tasks are penalized, which forces the task to be mapped into the available remaining capacity of the network. This requires to calculate the Hessian around a mode, which makes learning tractable. In this paper, we introduce Hessian-free curvature estimates as an alternative method to actually calculating the Hessian. In contrast to previous work, we exploit the fact that most regions in the loss surface are flat and hence only calculate a Hessian-vector-product around the surface that is relevant for the current task. Our experiments show that on a variety of well-known task sequences we either significantly outperform or are en par with previous work.", "review": "Review:###This paper proposes a method for tackling catastrophic forgetting. Similar to previous methods such as EWC (Kirkpatrick et al., 2017), they penalize parameter updates that align with the Fisher information matrix of the previous tasks. This will prevent the model from changing the previously useful parameters. They try to match the result of previous fisher-based methods but at a lower computational cost. They propose using a low-rank approximation to the Hessian using Hessian-vector-product with two types of vectors: the momentum velocity vector and the largest eigen-vector of the hessian. Then they build a diagonal approximation to the Hessian. Cons: - Eq 11, there is no justification for forming a curvature matrix by putting the absolute value of the hessian-vector-product with the proposed vectors on the diagonal. Particularly considering the largest eigen-value, Hv will be a vector of zeros with exactly one 1. This does not seem to be a good estimate of the hessian. - Fig 1, the proposed method seem to perform poorly compared to the kfac-based method on permuted mnist. - Figure 2 mainly compares to EWC as a baseline. In Farquhar & Gal (2019), other methods such as VGR perform significantly better. The proposed method is not competitive with state-of-the-art."}
{"id": "iclr2020_840", "title": "DIVA: Domain Invariant Variational Autoencoder | OpenReview", "abstract": "Abstract:###We consider the problem of domain generalization, namely, how to learn representations given data from a set of domains that generalize to data from a previously unseen domain. We propose the Domain Invariant Variational Autoencoder (DIVA), a generative model that tackles this problem by learning three independent latent subspaces, one for the domain, one for the class, and one for any residual variations. We highlight that due to the generative nature of our model we can also incorporate unlabeled data from known or previously unseen domains. To the best of our knowledge this has not been done before in a domain generalization setting. This property is highly desirable in fields like medical imaging where labeled data is scarce. We experimentally evaluate our model on the rotated MNIST benchmark and a malaria cell images dataset where we show that (i) the learned subspaces are indeed complementary to each other, (ii) we improve upon recent works on this task and (iii) incorporating unlabelled data can boost the performance even further.", "review": " In this work, the authors propose a domain invariant variational autoencoder for domain generalization problem. Specifically, the data is assumed to be constructed from three independent variables, one for the domain, one for the class and one for the residual variations. The method can be used for both unsupervised and semi-supervised cases. Experimental studies on rotated MNIST dataset and a malaria cell images dataset verify the effectiveness of the proposed method. The paper is well-written and easy to follow. The proposed generative model is simple and technically sound. However, I have the following concerns. (1) It is not clear how the problem setting, i.e., domain generalization, matters in DIVA. In another words, the proposed DIVA is not specific to domain generalization problem, but can be used for domain adaptation, multiple source transfer learning etc. Actually, I find that the authors compare with DA, which is a conventional domain adaptation method, in the experiment. Moreover, the experimental setups in section 4.1.3 is a multi-source transfer setting, and DIVA can be well be applied. In this sense, I am not very convinced on the claim of the contribution that DIVA is proposed for domain generalization. For me, DIVA is a more general method. (2) With point 1, more related works on VAE for domain adaptation need to be discussed. (3) The idea of constructing data from disentangled latent variables is not new, see a latest work [ref1]. The main difference of DIVA from [ref1] is the residual variation variable. Two baselines are necessary for comparisons: (1) [ref1], and (2) DIVA without the residual variation variable. Actually, the semantic meaning of z_x is not well discussed. Even in the experimental studies figures 2 and 3, it is hard to tell what Z_x actually represents. (4) Regarding the 4.1.2, why DA is selected as a baseline? How does DA deal with the multiple domains? Can any other domain adaptation methods, e.g., [ref2], or multiple source transfer methods, e.g. [ref3], be compared? (5) In the right side of Table 1, the improvements seem to be very marginal considering the variance. This makes the ability of DIVA to use unlabelled data less convincing. (6) Regarding 4.1.3, it seems that the domain similarity plays an important role in the performance, comparing the results of M_{30} with M_{60}. Without the labelled M_{60}, which is very similar to the target M_{75}, the performance degenerates dramatically. The current DIVA treats all the domains equally, is it possible to have a weighted form of DIVA that distinguishes the contributions of different domains? (7) What is the task of the malaria cell images experiments? Is it to classify the parasitized and uninfected cells? For a given patient, it makes more sense that all the cells belongs to one category, either infected or healthy. How is the class distribution for a person (in this case a domain)? Is it very unbalanced? (8) For figure 3, It is hard to judge the cell images parasitized or uninfected without domain knowledge, can you give the label for each image? Again, the semantic meaning of Z-x is hard to tell. I am not convinced by the shape of the cell for Z-x. (9) For 4.2.2, why and how DA is compared? As far as I know, it is for unsupervised domain adaptation. Moreover, the improvements are quite marginal. Some minor comments: (1) Page1, first para, 3rd line, “present” -> “presented”. (2) Page2, first para, 2nd line, “Y” - > “Y denotes” (3) Some references lack of page information The paper should be self-contained. I would suggest the authors move some paragraphs in appendix to the paper, for instance, 5.1.1, 5.2.2, and 5.2.3. Overall, the paper is presented with extensive empirical evaluations, but less theoretical justification. The significance of the paper is moderate as the key idea of learning disentangled latent variables has been studied, and the paper lacks of evidence to show the pure benefits of introducing Z_x as well as the comparison with the related work [ref1]. [ref1] Learning Disentangled Semantic Representation for Domain Adaptation [ref2] Conditional Adversarial Domain Adaptation [ref3] Multiple Source Domain Adaptation with Adversarial Learning"}
{"id": "iclr2020_841", "title": "Inductive representation learning on temporal graphs | OpenReview", "abstract": "Abstract:###Inductive representation learning on temporal graphs is an important step toward salable machine learning on real-world dynamic networks. The evolving nature of temporal dynamic graphs requires handling new nodes while learning temporal patterns. The node embeddings, which become functions of time under the temporal setting, should capture both static node features and evolving topological structures. Moreover, node and topological features may exhibit temporal patterns that are informative for prediction, of which the temporal node embeddings should also be aware. We propose the temporal graph attention (TGAT) layer to effectively aggregate temporal-topological neighborhood features as well as learning time-feature interactions. For TGAT, we use the self-attention mechanism as the building block and develop the novel functional time encoding technique based on the classical Bochner*s theorem from harmonic alaysis. By stacking TGAT layers, the network learns node embeddings as functions of time and can inductively infer embeddings for both new and observed nodes whenever the graph evolves. The proposed approach handles both node classification and link prediction task, and can be naturally extended to aggregate edge features. We evaluate our method with transductive and inductive tasks under temporal setting with two benchmark and one industrial dataset. Our TGAT model compares favorably to state-of-the-art baselines and prior temporal graph embedding approaches.", "review": " The major contribution of this paper is the use of random Fourier features as temporal (positional) encoding for dynamic graphs. These encodings are concatenated with standard node embeddings in transformer-like attention calculations for graph message passing. The reader finds that the proposed approach is interesting. Experimental results are also favorable. Concern: Whereas the use of random Fourier features (RFF) is well justified, a limitation is that it is based on a stationarity assumption. Thus, it may be less applicable to nonstationary structural changes. To cope with nonstationarity, a straightforward idea is to parameterize the temporal encoding by using neural networks rather than RFF. In the authors* approach, the RFF is in a sense parameterized, because the frequencies omega are learned. Nevertheless, the stationarity limitation persists. Question: In the ablation study, what exactly is *the original positional encoding*? Are they learned embedding vectors? Since the authors consider continuous time rather than discrete time, how many embedding vectors are there?"}
{"id": "iclr2020_842", "title": "Noise Regularization for Conditional Density Estimation | OpenReview", "abstract": "Abstract:###Modelling statistical relationships beyond the conditional mean is crucial in many settings. Conditional density estimation (CDE) aims to learn the full conditional probability density from data. Though highly expressive, neural network based CDE models can suffer from severe over-fitting when trained with the maximum likelihood objective. Due to the inherent structure of such models, classical regularization approaches in the parameter space are rendered ineffective. To address this issue, we develop a model-agnostic noise regularization method for CDE that adds random perturbations to the data during training. We demonstrate that the proposed approach corresponds to a smoothness regularization and prove its asymptotic consistency. In our experiments, noise regularization significantly and consistently outperforms other regularization methods across seven data sets and three CDE models. The effectiveness of noise regularization makes neural network based CDE the preferable method over previous non- and semi-parametric approaches, even when training data is scarce.", "review": "Review:###This paper proposes a noise regularization method which adds noise on both x and y for conditional density estimation problem (e.g., regression and classification). The writing is good and the whole paper is easy to follow. However, I vote for reject, since the novelty is somehow limited, the claims made in the paper is not well supported and experiments are not very convincing. 1. Adding noise on x (e.g., [1]), y (e.g., [2]) is not new. Though it is claimed that this paper extends previous results on classification/regression to conditional density estimation which is a more general case. This claim is not well supported. Experiments are still evaluated in classification/regression tasks. 2. Theorem 1 & 2 in Sec 4.2 only show the asymptotic case, which are quite obvious and seems helpless in understanding the advantage of adding noise regularization in conditional density estimation. 3. Sec 4.1. The explanation that Page 5, ```*The second term in (6) penalizes large negative second derivatives of the conditional log density estimate...*. It is hard for me to understand. Large positive second derivatives also lead to poor smoothness. [1] Learning with Marginalized Corrupted Features, ICML 2013 [2] Learning with Noisy Labels, NIPS 2013"}
{"id": "iclr2020_843", "title": "A Base Model Selection Methodology for Efficient Fine-Tuning | OpenReview", "abstract": "Abstract:###While the accuracy of image classification achieves significant improvement with deep Convolutional Neural Networks (CNN), training a deep CNN is a time-consuming task because it requires a large amount of labeled data and takes a long time to converge even with high performance computing resources. Fine-tuning, one of the transfer learning methods, is effective in decreasing time and the amount of data necessary for CNN training. It is known that fine-tuning can be performed efficiently if the source and the target tasks have high relativity. However, the technique to evaluate the relativity or transferability of trained models quantitatively from their parameters has not been established. In this paper, we propose and evaluate several metrics to estimate the transferability of pre-trained CNN models for a given target task by featuremaps of the last convolutional layer. We found that some of the proposed metrics are good predictors of fine-tuned accuracy, but their effectiveness depends on the structure of the network. Therefore, we also propose to combine two metrics to get a generally applicable indicator. The experimental results reveal that one of the combined metrics is well correlated with fine-tuned accuracy in a variety of network structure and our method has a good potential to reduce the burden of CNN training.", "review": "Review:###The authors propose several metrics to evaluate the transferability of pretrained CNN models for a target task without actually fine-tuning the networks to accelerate fine-tuning. The paper has done some interesting empirical studies on how to predict the transferability of a neural network. The motivation of performing model selection without actual finetuning has many benefits for practical applications and I believe this is the right direction. In this perspective, the paper is novel. However, my major concern is a lack of in-depth analysis and thorough verification of the proposed metrics. First, there is no clear relationship between the six evaluation metrics and the fine-tuning performance. Figure 1 depicts some correlation, however, it is not clear enough. The difference in ResNet18 is not further investigated. Given the dominant usage of ResNets and its variations in practice, it is important to provide analysis for the reasons behind. Second, some of the proposed metrics (S1/S2) are actually closely related with weight norms. The definition of high value elements in S6 is also based on the absolute difference with the maximum feature map values. Does the author apply weight decay during training? Normally the weight norm will becomes smaller and so could the featuremap values. The S1/S2/S6 metrics could therefore be influenced. I would like to see figures showing the relationship between the weight/feature norms and the metric during training. Third, it turns out that the only useful metrics are the combination of feature sparsity and feature steepness (i.e., C_56). The authors should make it more specific and clear for their contributions. Why only two combination of the metrics is considered? Does the best selected coefficient generalize to other models and datasets? Finally, according to https://arxiv.org/abs/1805.08974, better pretrained ImageNet model also generalize better. I would expect a deeper ResNets should outperform AlexNet and VGG16. What is the relationship between the proposed metrics and the ImageNet performance? Minor: The S3 metric is actually “a ratio of inter-class variance to intra-class variance” rather than “ ratio of intra-class variance to inter-class variance”."}
{"id": "iclr2020_844", "title": "Meta-learning curiosity algorithms | OpenReview", "abstract": "Abstract:###Exploration is a key component of successful reinforcement learning, but optimal approaches are computationally intractable, so researchers have focused on hand-designing mechanisms based on exploration bonuses and intrinsic reward, some inspired by curious behavior in natural systems. In this work, we propose a strategy for encoding curiosity algorithms as programs in a domain-specific language and searching, during a meta-learning phase, for algorithms that enable RL agents to perform well in new domains. Our rich language of programs, which can combine neural networks with other building blocks including nearest-neighbor modules and can choose its own loss functions, enables the expression of highly generalizable programs that perform well in domains as disparate as grid navigation with image input, acrobot, lunar lander, ant and hopper. To make this approach feasible, we develop several pruning techniques, including learning to predict a program*s success based on its syntactic properties. We demonstrate the effectiveness of the approach empirically, finding curiosity strategies that are similar to those in published literature, as well as novel strategies that are competitive with them and generalize well.", "review": "Review:###This paper presents an algorithm to generate curiosity modules for reinforcement learning. The authors define a program language which can represent many possible curiosity modules that include training neural networks, replay buffers, etc. It also presents an approach to searching for the best curiosity module in this set, which is some various ways to do pruning and to determine which methods to try. The paper is very novel - the idea of developing a domain specific language full of building blocks to represent various curiosity modules is unique and interesting. The search over curiosity modules is a bit mis-represented I think. In the introduction, it gives the impression that part of the algorithm is to search over these curiosity modules, and also that it*s to find the best one that works across a wide set of tasks. Instead the search method is a separate procedure outside of the algorithm and most of the search steps are performed on individual tasks instead of over a set of tasks. In Sec 3.3, you say that *perhaps surprisingly, we find that we can predict performance directly from program structure,* but you never provide any evidence of doing so. The simple environment that you used is a bit contrived, rather than taking a normal task, the goal itself is to do complete exploration (maximize the total number of pixels visited). It seems like the combination of the intrinsic curiosity program here with the reward combiner is that the intrinsic curiosity program should be only about complete exploration, and then the combiner is responsible for balancing that with task rewards. You should be more explicit that in this first part of the search you*re only looking at the intrinsic curiosity program, without the combiner, and therefore do not want a task with extrinsic rewards. This breakdown of searching for the intrinsic curiosity program first and the combiner later seems like another important aspect of making your search efficient. The main drawback of this paper is that there are little comparisons to related work. The only methods compared to are ones where the curiosity method is expressible in the language of the method."}
{"id": "iclr2020_845", "title": "Emergent Communication in Networked Multi-Agent Reinforcement Learning | OpenReview", "abstract": "Abstract:###With the ever increasing demand and the resultant reduced quality of services, the focus has shifted towards easing network congestion to enable more efficient flow in systems like traffic, supply chains and electrical grids. A step in this direction is to re-imagine the traditional heuristics based training of systems as this approach is incapable of modelling the involved dynamics. While one can apply Multi-Agent Reinforcement Learning (MARL) to such problems by considering each vertex in the network as an agent, most MARL-based models assume the agents to be independent. In many real-world tasks, agents need to behave as a group, rather than as a collection of individuals. In this paper, we propose a framework that induces cooperation and coordination amongst agents, connected via an underlying network, using emergent communication in a MARL-based setup. We formulate the problem in a general network setting and demonstrate the utility of communication in networks with the help of a case study on traffic systems. Furthermore, we study the emergent communication protocol and show the formation of distinct communities with grounded vocabulary. To the best of our knowledge, this is the only work that studies emergent language in a networked MARL setting.", "review": "Review:###The paper proposes a framework for communication in networks by formulating the problem as MARL-based setup. Agents in the network can communicate with their neighbors to share their local observations which leads to overall better performance in partially observable scenarios. The paper then shows results on SUMO traffic simulator which suggest that networks with communication outperform the ones without it. The paper further shows that in these networks individual groups emerge which coordinate with each other to achieve a common goal. The paper hasn’t been situated properly with respect to prior work. Similar concepts have been tested in Graph Attention Networks [1], Interaction Networks [6] and VAIN [2] papers. In [1], the value of each network is attended sum of neighbors’ hidden state. The networks based on continuous communication can be easily converted to discrete ones through Gumbel Softmax. So, due to concerns around novelty and limited experiments only on traffic junction with some critical details missing, I would give initial rating of reject. I post my other concerns and feedback below: Paper claims Sukhbaatar et. al. 2017 [3] as concurrent work which was published in 2017. [1] has further two derivative works Singh et. al. 2018 [4] and Das et. al. 2017 [5] which need to be compared against as well. On the difference from [3] as mentioned in the paper, in [3, 4, 5], (i) agents can be easily converted to be traffic lights instead of cars. (ii) continuous communication can be converted into discrete using Gumbel Softmax (iii) every agent can communicate to every other agent. The paper essentially contributes results on what happens when the agents are in network topology instead of open communication so (iii) point needs to be framed better. It would be interesting and informative to see how the paper’s model compare against [3, 4, 5] in open communication setting (it may be an upper bound). As mentioned above, proper differences from [1] and [2] also need to be mentioned. Paper is also missing experiments on scalability of this approach. Does the approach scale well as agents in the network increase? It would be helpful to know (i) the generalization of this approach by testing on different configuration at test time (ii) if the approach works well on task other than traffic control like network packet routing (iii) How well the approach scales as the number of agents increase. What is the value of beta used for reward function? And paper doesn’t comment on how does the performance get affected by value beta as it goes from 0 (individual rewards) [4] to 1 (global rewards) [3]. Also, what is the composition of the factors mentioned in reward structure at page 5. Having the exact equation in paper would be helpful. If the paper can provide the following details, it would help on its readability and my understanding. (i) For easily comparing different baseline, a table should be added with the final rewards. (ii) What are the architectural details of the CNN used in image encoder and the RNN used in accumulator. What are the values of other hyperparameters? (iii) How exactly the model has been trained? (iv) Why does 1 blind agent network converges faster and higher initially even though there is a missing observation? (v) The paper claims at many points that global rewards help but have they tried with individual rewards? As [4] suggest, individual rewards converge faster and higher than global rewards it might be worth exploring this. [1] Veli?kovi?, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. *Graph attention networks.* arXiv preprint arXiv:1710.10903 (2017). [2] Hoshen, Yedid. *Vain: Attentional multi-agent predictive modeling.* In Advances in Neural Information Processing Systems, pp. 2701-2711. 2017. [3] Sukhbaatar, Sainbayar, and Rob Fergus. *Learning multiagent communication with backpropagation.* In Advances in Neural Information Processing Systems, pp. 2244-2252. 2016. [4] Singh, Amanpreet, Tushar Jain, and Sainbayar Sukhbaatar. *Learning when to communicate at scale in multiagent cooperative and competitive tasks.* arXiv preprint arXiv:1812.09755 (2018). [5] Das, Abhishek, Théophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Michael Rabbat, and Joelle Pineau. *Tarmac: Targeted multi-agent communication.* arXiv preprint arXiv:1810.11187 (2018). [6] Battaglia, Peter, Razvan Pascanu, Matthew Lai, and Danilo Jimenez Rezende. *Interaction networks for learning about objects, relations and physics.* In Advances in neural information processing systems, pp. 4502-4510. 2016."}
{"id": "iclr2020_846", "title": "A Group-Theoretic Framework for Knowledge Graph Embedding | OpenReview", "abstract": "Abstract:###We have rigorously proved the existence of a group algebraic structure hidden in relational knowledge embedding problems, which suggests that a group-based embedding framework is essential for model design. Our theoretical analysis explores merely the intrinsic property of the embedding problem itself without introducing extra designs. Using the proposed framework, one could construct embedding models that naturally accommodate all possible local graph patterns, which are necessary for reproducing a complete graph from atomic knowledge triplets. We reconstruct many state-of-the-art models from the framework and re-interpret them as embeddings with different groups. Moreover, we also propose new instantiation models using simple continuous non-abelian groups.", "review": "Review:###This paper start merely by studying the graph reconstruction problem and prove that the intrinsic structure of this task itself automatically produces the complete definition of groups. it seems to be a novel result. Based on this result, one could construct embedding models that naturally accommodate all possible local graph patterns, and the paper also shows a few simulations. My main concern is that, while the focus on this work is the theoretical finding, there is no rigorous statement of it as a theorem. As a result, I am not exactly sure what the proofs in the appendix is trying to show. In addition, the proofs seems to be very trivial. For the algorithm section, I feel that it is also lacking in the sense that there is still no automatic way to choose which group to embed. It is also unclear what is the purpose of the simulation section. While it says *As theoretically analyzed in Section 3.2, and empirically shown above, continuous nonabelian groups are more reasonable choices for general tasks*, the advantage of continuous nonabelian groups are not so significant in the tables."}
{"id": "iclr2020_847", "title": "Putting Machine Translation in Context with the Noisy Channel Model | OpenReview", "abstract": "Abstract:###We show that Bayes* rule provides a compelling mechanism for controlling unconditional document language models, using the long-standing challenge of effectively leveraging document context in machine translation. In our formulation, we estimate the probability of a candidate translation as the product of the unconditional probability of the candidate output document and the ``reverse translation probability** of translating the candidate output back into the input source language document---the so-called ``noisy channel** decomposition. A particular advantage of our model is that it requires only parallel sentences to train, rather than parallel documents, which are not always available. Using a new beam search reranking approximation to solve the decoding problem, we find that document language models outperform language models that assume independence between sentences, and that using either a document or sentence language model outperform comparable models that directly estimate the translation probability. We obtain the best-published results on the NIST Chinese--English translation task, a standard task for evaluating document translation. Our model also outperforms the benchmark Transformer model by approximately 2.5 BLEU on the WMT19 Chinese--English translation task.", "review": " ** Paper summary ** In this paper, the authors propose a new re-ranking mechanism leveraging document-level information. Let X and Y denote two languages for ease of reference. The authors focus on X->Y translation and Y->X is a model used for re-ranking. Specifically, (1) Two translation models X->Y and Y->X are trained, where X->Y is a document Transformer and Y->X is a sentence transformer. (2) Train a language model P(Y) on document-level corpus (rather than sentence-level LM). (3) Given a document with sentences (x^1, …, x^I), translate each source sentence to K candidates. (4) Using beam search guided by Eqn.(4) to find optimal translation paths, which is a combination of X->Y translation, Y->X translation, document-level language model and the number of words. The authors work on NIST Chinese-to-English translation and WMT’19 Zh->En translation to verify the proposed algorithm. ** Novelty ** The novelty is limited. Compared to the paper “the Neural Noisy Channel” (Yu et. al, 2017), the authors use document Transformer and document-level LM for re-ranking, which is of limited novelty. ** Details ** 1. Some baselines are missing from this paper: (A) dual inference baseline [ref1]; (B) X->Y is sentence-level transformer and LM is sentence-level LM, i.e., (Yu et. al, 2017), where P(Y|X) and P(X|Y) are sentence-level translation models. 2. In Table 1, the improvement of doc-reranker is not very significant compared to sent-reranker, ranging from 0.21 to 0.66. 3. In Table 4, “Channel + LM” and *Proposal + Channel + LM* achieved almost the same results. Does it mean that the *proposal* component do not work? 4. Many models are used in this framework. I am not sure whether simple re-ranking or ensemble can outperform this baseline, e.g., 2 Zh->En + 1 En->Zh [ref1] Dual Inference for Machine Learning, Yingce Xia, Jiang Bian, Tao Qin, Nenghai Yu, Tie-Yan Liu, IJCAI’17"}
{"id": "iclr2020_848", "title": "Unsupervised Learning of Node Embeddings by Detecting Communities | OpenReview", "abstract": "Abstract:###We present Deep MinCut (DMC), an unsupervised approach to learn node embeddings for graph-structured data. It derives node representations based on their membership in communities. As such, the embeddings directly provide interesting insights into the graph structure, so that the separate node clustering step of existing methods is no longer needed. DMC learns both, node embeddings and communities, simultaneously by minimizing the mincut loss, which captures the number of connections between communities. Striving for high scalability, we also propose a training process for DMC based on minibatches. We provide empirical evidence that the communities learned by DMC are meaningful and that the node embeddings are competitive in different node classification benchmarks.", "review": "Review:###This paper is reporting an unsupervised approach to learn node embeddings and communities simultaneously by minimizing the mincut loss function. This approach programs the data through encoder to generate membership likehood matrix H, and then generates corresponding membership matrix P using node selection. The matrix P and adjacency matrix are coupled to generate community adjacency matrix to minimize the cutting. However, despite such attractive points, the novelty and strength of this study is not outstanding enough for publication in ICLR. Details comments are as follows, 1. Similar work such as the methods of transferring matrix E to H then to P have already been published , which reduce the novelty of this study. 2. Though there is a schematic, the learning embedding process is still not described clearly. More details of the algorithms need to be discussed to such as how to get the node embedding matrix E."}
{"id": "iclr2020_849", "title": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation | OpenReview", "abstract": "Abstract:###We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters.", "review": "Review:###Summarize what the paper claims to do/contribute. * The paper proposes a new image-to-image GAN-based translator that uses attention and a new normalization that learns a proper ratio between instance and layer normalization. Experiments benchmark the new method against multiple prior ones, and on a number of dataset pairs. Clearly state your decision (accept or reject) with one or two key reasons for this choice. Weak Accept * The paper was well-written and the method and contributions are clearly explained. * There is clear novelty in this paper, even if slightly limited. However, the newly proposed normalization seems to work quite well. * The results look good, however it is hard to compare methods quantitatively with only few samples. (Nothing that the authors could have done: there are many samples in the supplementary material and results seem consistent.) Qualitative measures like FID and KID should be taken with a grain of salt also. It is a big plus that a user study was conducted! (However, details of how these subjects were selected would be useful)"}
{"id": "iclr2020_850", "title": "Robust Graph Representation Learning via Neural Sparsification | OpenReview", "abstract": "Abstract:###Graph representation learning serves as the core of many important prediction tasks, ranging from product recommendation in online marketing to fraud detection in financial domain. Real-life graphs are usually large with complex local neighborhood, where each node is described by a rich set of features and easily connects to dozens or even hundreds of neighbors. Most existing graph learning techniques rely on neighborhood aggregation, however, the complexity on real-life graphs is usually high, posing non-trivial overfitting risk during model training. In this paper, we present Neural Sparsification (NeuralSparse), a supervised graph sparsification technique that mitigates the overfitting risk by reducing the complexity of input graphs. Our method takes both structural and non-structural information as input, utilizes deep neural networks to parameterize the sparsification process, and optimizes the parameters by feedback signals from downstream tasks. Under the NeuralSparse framework, supervised graph sparsification could seamlessly connect with existing graph neural networks for more robust performance on testing data. Experimental results on both benchmark and private datasets show that NeuralSparse can effectively improve testing accuracy and bring up to 7.4% improvement when working with existing graph neural networks on node classification tasks.", "review": "Review:###The paper proposes a trainable graph sparsification mechanism that can be used in conjunction with GNNs. This process is parameterized using a neural network and can be trained end-to-end (by using the Gumbel softmax reparameterization trick) with the loss given by the task at hand. Experimental results on node classification tasks are presented. The paper is well written and easy to follow. I think that overall the method is sound and well executed. Empirical results are convincing as the method consistently improves performance (compared to not applying the sparsification) over several GNNs. I feel that the claim of the improvement could be softened a bit. Please correct me if I*m reading Table 2 wrongly, with the exception of the Transaction dataset, most differences are below 3%. I am not familiar with these datasets so I cannot judge the significance of the improvement. In any case, the reduction in computation with the improved performance is a strong result. The baselines that include unsupervised graph sparsification as a pre-processing make the results worse (with respect to not applying it) in all cases. This shows that for this problem, a task driven specification is crucial for maintaining performance. Neural Sparse model has more parameters than the version that does not use a sparsifier. Do you think that this could influence the performance? Would it be possible to compare using similar number of trainable parameters? I assume that using more parameters would not imply better performance for the baseline, but it would be good to clarify. I can understand that the performance of the model depends critically on k, and that k might vary significantly over datasets. In my view, it would be informative to include (maybe in the supplementary material) the performance variations on the corresponding validation sets as one changes k for the different datasets (as done in Figure 3 (c)). In Algorithm 2 it would be better to use a different letter for the edge set (as currently looks like the real numbers)."}
{"id": "iclr2020_851", "title": "Matrix Multilayer Perceptron | OpenReview", "abstract": "Abstract:###Models that output a vector of responses given some inputs, in the form of a conditional mean vector, are at the core of machine learning. This includes neural networks such as the multilayer perceptron (MLP). However, models that output a symmetric positive definite (SPD) matrix of responses given inputs, in the form of a conditional covariance function, are far less studied, especially within the context of neural networks. Here, we introduce a new variant of the MLP, referred to as the matrix MLP, that is specialized at learning SPD matrices. Our construction not only respects the SPD constraint, but also makes explicit use of it. This translates into a model which effectively performs the task of SPD matrix learning even in scenarios where data are scarce. We present an application of the model in heteroscedastic multivariate regression, including convincing performance on six real-world datasets.", "review": "Review:###This paper generalized neural networks into case where a semidefinite positive matrix is learned at the output. The paper presents theoretical derivations that look sound, and validating experiments on synthetic and real data. I must say my expertise does not really correspond to what is done in this paper, but I do not see any obvious flaws and the results look solid. I appreciated the discussion of limitations in section 6. I vote for acceptance with the weakest possible confidence level since it is likely I missed many important points."}
{"id": "iclr2020_852", "title": "Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks | OpenReview", "abstract": "Abstract:###The selection of initial parameter values for gradient-based optimization of deep neural networks is one of the most impactful hyperparameter choices in deep learning systems, affecting both convergence times and model performance. Yet despite significant empirical and theoretical analysis, relatively little has been proved about the concrete effects of different initialization schemes. In this work, we analyze the effect of initialization in deep linear networks, and provide for the first time a rigorous proof that drawing the initial weights from the orthogonal group speeds up convergence relative to the standard Gaussian initialization with iid weights. We show that for deep networks, the width needed for efficient convergence to a global minimum with orthogonal initializations is independent of the depth, whereas the width needed for efficient convergence with Gaussian initializations scales linearly in the depth. Our results demonstrate how the benefits of a good initialization can persist throughout learning, suggesting an explanation for the recent empirical successes found by initializing very deep non-linear networks according to the principle of dynamical isometry.", "review": "Review:###This paper rigorously proves that if a deep linear network is initialized with random orthogonal weights and trained with gradient descent, its width required for convergence does not depend on its depth. To compare, when weights in deep linear networks are initialized with Gaussian initialization, the minimal width required for convergence will depend on the depth of the network. This proof explains why orthogonal weight initialization can help to train networks efficiently, especially for those very deep ones. The theoretical contribution of this paper is very important. Orthogonal initialization is found to be useful in deep network training. Although the theory in this paper is developed for linear networks, it still has important guidance meaning in practices in more areas of deep learning. The derivations are correct to my best knowledge. And the paper is well-written and easy to read. Minor points: - typo in the last equation in (4) ======================= Update: Despite the similarity with a previous paper, I still think the theoretical results and empirical observations important and thus I will keep my score."}
{"id": "iclr2020_853", "title": "Entropy Penalty: Towards Generalization Beyond the IID Assumption | OpenReview", "abstract": "Abstract:###It has been shown that instead of learning actual object features, deep networks tend to exploit non-robust (spurious) discriminative features that are shared between training and test sets. Therefore, while they achieve state of the art performance on such test sets, they achieve poor generalization on out of distribution (OOD) samples where the IID (independent, identical distribution) assumption breaks and the distribution of non-robust features shifts. Through theoretical and empirical analysis, we show that this happens because maximum likelihood training (without appropriate regularization) leads the model to depend on all the correlations (including spurious ones) present between inputs and targets in the dataset. We then show evidence that the information bottleneck (IB) principle can address this problem. To do so, we propose a regularization approach based on IB called Entropy Penalty, that reduces the model*s dependence on spurious features-- features corresponding to such spurious correlations. This allows deep networks trained with Entropy Penalty to generalize well even under distribution shift of spurious features. As a controlled test-bed for evaluating our claim, we train deep networks with Entropy Penalty on a colored MNIST (C-MNIST) dataset and show that it is able to generalize well on vanilla MNIST, MNIST-M and SVHN datasets in addition to an OOD version of C-MNIST itself. The baseline regularization methods we compare against fail to generalize on this test-bed.", "review": "Review:###This paper proposed Entropy Penalty (EP) training, based on Information Bottleneck (IB) to make the trained model generalize beyond the IID assumption that is satisfied by usual training and testing datasets. First, a loss function derived from IB is provided Prop. 1. Then they argue that minimizing this loss over lower layers is better Eq. (5) and (6). They further assume the hidden layer is Gaussian, and use squared l2 loss as entropy penalty. They theoretically analyze a different loss function Eq. (7) under two simple cases, where optimal solutions have closed forms, and find that the optimal solutions contain smaller weights for non-discriminative features (p_i there). Experiments on coloured versions of MNIST are conducted to show that the proposed Entropy Penalty achieves better results than several baselines. 1. There is no definition of a robust or non-robust feature, but just a vague description by a simple example (camel appearance). I suggest before presenting the method, providing examples like p_i the synthetic cases, to give a sense of what kind of features are (non-)robust. 2. The Gaussian assumption of the hidden layer is quite restricted as mentioned. And why only apply EP on the first layer rather than all the hidden layers? 3. The objective in analysis Eq. (8) looks different from Eq.(7). Why not using the original objective? 4. The synthetic examples are not convincing enough. For example, what are the solutions of other methods, and how can we see the EP solution is better than others for these examples. More comparisons are needed to give a sense of the advantage of EP. 5. Why is randomly assigning colours considered as a distribution shift? It is not clear to connect what the synthetic examples try to deliver and the coloured version of MNIST here. This paper shows that EP based on IB, together with the Gaussian assumption of the hidden layer can learn robust features as shown in synthetic case analyses and coloured MNIST experiments, comparing with several baselines. However, there are several gaps, 1) what exactly is a non-robust feature, in synthetic examples, these are p_i and k, however, in experiments, it seems to colour. There is no definition and thus it fails to show what exact kinds of features the proposed EP actually can capture (p_i, k, colour, or something more general). In this sense, I have the impression that the presentation is not very clear (EP can learn something, but what it is?). 2) The claim is that EP learns robust features for deep learning methods, but both the analyses and experiments are not enough to show that. First, several restricted assumptions are made as mentioned, learn the first layer only, Gaussian assumption, and the examples are too simple, and lack of calculation for other methods and comparisons. Second, just coloured version of MNIST is not convincing to show that EP captures many/most robust features, even though the proposed EP significantly outperforms the other baselines here because of there probably (must) have many different kinds of other features. More experiments are needed to support the efficacity of EP."}
{"id": "iclr2020_854", "title": "A Closer Look at the Optimization Landscapes of Generative Adversarial Networks | OpenReview", "abstract": "Abstract:###Generative adversarial networks have been very successful in generative modeling, however they remain relatively challenging to train compared to standard deep neural networks. In this paper, we propose new visualization techniques for the optimization landscapes of GANs that enable us to study the game vector field resulting from the concatenation of the gradient of both players. Using these visualization techniques we try to bridge the gap between theory and practice by showing empirically that the training of GANs exhibits significant rotations around LSSP, similar to the one predicted by theory on toy examples. Moreover, we provide empirical evidence that GAN training seems to converge to a stable stationary point which is a saddle point for the generator loss, not a minimum, while still achieving excellent performance.", "review": "Review:###The authors present a study of GAN dynamics in training with the goal of understanding whether rotational behavior occurs when training GANs on real world datasets, and whether training methods find local Nash equilibria. The authors start by motivating the study of a game vector field with a toy example in which we can see all relevant behavior in the relevant directions. The work investigates the game vector field with a visualization technique called “path-angle” that attempts to alleviate the problem of high dimensionality by only looking at the cosine similarity (between the linear interpolation between the two points versus the true gradient at the point) along a path between two (concatenated) weight vectors at a time. The work also investigates by looking at the gradient norms of weights in these optimization trajectories. The work uses these techniques to visualize the dynamics of GANs trained on standard datasets. The authors find that GANs do not converge to local Nash equilibria, that each player ends at a saddle point, and state evidence for “rotational behavior” in GAN dynamics. The finding that GAN training methods do not find local Nash equilibria is interesting. However, it is unclear to me what the experiments presented show about GAN training dynamics (in particular, it is unclear to me what rotational dynamics are in the context of GANs and what consequences they have for training). I have listed more detailed feedback below. Detailed feedback: * Section 3.1: What is the formulation of Mescheder et al 2017? This should be explicitly stated in the paper. * The authors never explicitly, formally define what it means for there to be rotational behavior. However, this terminology is used frequently throughout the paper, particularly in the empirical section (5.1). What does rotational component mean in this Section? * What is the motivation for completing the path based landscape visualization methods between a random initialization and the final weight vector? Is there a reason why the actual iterates were not investigated with this method? * Why does the bump in Figure 3 imply that there is a non zero rotational component? * It would be good to complete a more thorough understanding of the spectra of hessians at convergence; the extent of the experiments in Figure 5 and 6 appears to just be 3 training runs; a larger sample size would be good to establish trends. * What is the motivation for visualizing via the path based methods? Why does interpolating between initialization and the final learned weight vector tell us about the rotational dynamics in high dimensions? This line may not be representative of the optimization trajectory followed by the actual iterates."}
{"id": "iclr2020_855", "title": "Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP | OpenReview", "abstract": "Abstract:###A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently, Jin et al. (2018) proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards emph{without} accessing a generative model. We show that the \textit{sample complexity of exploration} of our algorithm is bounded by . This improves the previously best known result of in this setting achieved by delayed Q-learning (Strehlet al., 2006),, and matches the lower bound in terms of as well as and up to logarithmic factors.", "review": "Review:###Summary: In this paper, the authors extend the UCB Q-learning algorithm by Jin et al. (2018) to infinite horizon discounted MDPs, and prove a PAC bound of \tilde{O}(SA/epsilon^2 (1-gamma)^7) for the resulting algorithm. This bound improves the one for delayed Q-learning by Strehl et al. (2006) and matches the lower-bound in terms of epsilon, S, and A. Comments: - Overall, the paper is well-written and well-structured. Although most of the results are in the appendix, the authors have done a good job in what they reported in the paper and what they left for the appendix. - I personally think ICLR is not a good venue for this kind of papers. Places like COLT and journals are better because they allow the authors to report most of the results in the main paper, and give more time to the reviewers to go over all the proofs. - I did not have time to go over all the proofs in the appendix but I checked those in the paper. I did not find any error, but the math can be written more rigorously. - The algorithm is quite similar to the one by Jin et al. (2018), which is for finite-horizon problems. The authors discuss on Page 4 that why despite the similarity of the algorithms, the techniques in Jin et al. (2018) cannot be directly applied to their case. It would be good if the authors have a discussion on the novelty of the techniques used in their analysis, not only w.r.t. Jin et al. (2018), but w.r.t. other analysis of model free algorithms in infinite-horizon discounted problems. - What makes this work different than Jin et al. (2018), and is its novelty, is obtaining a PAC bound. The resulting regret bound is similar to the one in Jin et al. (2018), but the PAC bound is new. It would be good if the authors mention this on Page 4. This makes it more clear in which sense this work is different than that by Jin et al. (2018). - It is important to write in the statement of Condition 2 that *i* changes between 0 and log_2 R."}
{"id": "iclr2020_856", "title": "DeepSimplex: Reinforcement Learning of Pivot Rules Improves the Efficiency of Simplex Algorithm in Solving Linear Programming Problems | OpenReview", "abstract": "Abstract:###Linear Programs (LPs) are a fundamental class of optimization problems with a wide variety of applications. Fast algorithms for solving LPs are the workhorse of many combinatorial optimization algorithms, especially those involving integer programming. One popular method to solve LPs is the simplex method which, at each iteration, traverses the surface of the polyhedron of feasible solutions. At each vertex of the polyhedron, one of several heuristics chooses the next neighboring vertex, and these vary in accuracy and computational cost. We use deep value-based reinforcement learning to learn a pivoting strategy that at each iteration chooses between two of the most popular pivot rules -- Dantzig and steepest edge. Because the latter is typically more accurate and computationally costly than the former, we assign a higher wall time-based cost to steepest edge iterations than Dantzig iterations. We optimize this weighted cost on a neural net architecture designed for the simplex algorithm. We obtain between 20% to 50% reduction in the gap between weighted iterations of the individual pivoting rules, and the best possible omniscient policies for LP relaxations of randomly generated instances of five-city Traveling Salesman Problem.", "review": "Review:###The authors present a learned method for speeding up optimization of LP, with an application to the TSP problem. In particular, they discuss choosing a pivoting strategy in the simplex algorithm that is set up as a 2-class classification problem, taking the simplex tableau and outputting one of two classes (Dantzig or steepest edge). They consider both the supervised learning approach where they first compute the optimal policy Q*, as well as the RL approach. The paper presents a quite interesting approach to solving the TSP, using NNs on top of the tableaus. I enjoyed the presented ideas, however the authors could have done a great job of clearly presenting their work. Notation is not the best, and the experiments are quite limited, indicating limited practical value of the current approach. Detailed comments follow: - Especially given that the authors have 2 more pages for writing, it would be beneficial to add more visualization and intuitive explanations of the presented ideas. - The notation should be improved, especially given that there isn*t that much notation anyway. E.g., eq (1e) and the following one are not quite clear, what is M? Is it a scalar, or a set? Reading closely it becomes clear it is a set, but the authors use upper-case letter both for scalars and sets, leading to confusion. This is very easy to fix, and would help readers quite a lot. - The first paragraph in Section 4 is literally a copy-paste of the last paragraph in Section 3. - Accompanying Section 4 discussion, would be good to add a figure showing an example tableau. Actually, tableau is not even mentioned here and is the main input to the network, it is only mentioned later. - Using just 5 cities does seem small, indicating limited practical value of the method. Please comment on the actual full size of the exact graph of the relaxation, and why is it infeasible to store. Currently the explanation is hand-wavy. - *The state space is the set of possible simplex tableau*, so you enumerate all the possible tableaus of extreme points prior to training? It seems yes, but would be good to explain that explicitly. - In *Reward function* section, *as the objective value ...*, please add a reference to eq (1a) here. It helps readers follow the text by referencing already introduced parts. - Where did the equations for rewards come from? They are just given, without much explanation or intuitive discussion. - Above Section 5.1, *where s_{t-1} -> s_t*, where is this notation even used? - Typo: *value of at* - Is it *tableau* or *tableaux*? Choose one. - Section 5.1, are both outputs updated at each iteration? It says that a random action is taken, but is that after Q-values for both actions are checked? It seems yes, but it is not clear. - In general, a number of small details are missing, and the authors should make sure to not skip them. This introduces confusion to the text which should clearly be avoided. - *Epsilon is decayed appropriately.* How? - Please define an epoch. In particular, what constitutes a 0.5 epoch, and how does the update actually work here? So you store examples into batches, keep them on the side, and once half an epoch passes only then use them to update the model? It seems yes, but again would be good to be more gentle in explanations. - Maybe add an algorithm illustrating the methods? In addition to visualization mentioned above. - Figures are not really super visible. Also, figures should have captions, so text go below the figure. - Figure 2 is referenced in Section 6.2, where it should be Figure 3. - *weighted* vs. *unweighted* iteration should be defined earlier. It is nowhere introduced, and while it may be clear what it means, it is not clearly defined and it should be. - *Adams optimizer*? Do you mean Adam? - Experiments are somewhat weak, and seem to indicate that the method does not have much practical value. Is the method useful beyond 5-city size? ===== AFTER THE REBUTTAL ====== Thank you for your rebuttal, and especially for the extra experiments! It definitely adds a lot of value to this effort. The writing could still be improved, as a number of details are missing as explained in the review and to some extent acknowledged by the authors. I liked the idea quite a lot and would not mind seeing it in the conference, but given the number of issues raised by myself and others it seems that the best route forward is rewriting the paper given the inputs by the reviewers and submitting to a future venue. I will remain at a *borderline* evaluation, and if the other reviewers change their recommendations to (weak) accept I would not mind accepting the paper (assuming all the changes and promised fixes are implemented)."}
{"id": "iclr2020_857", "title": "AE-OT: A NEW GENERATIVE MODEL BASED ON EXTENDED SEMI-DISCRETE OPTIMAL TRANSPORT | OpenReview", "abstract": "Abstract:###Generative adversarial networks (GANs) have attracted huge attention due to its capability to generate visual realistic images. However, most of the existing models suffer from the mode collapse or mode mixture problems. In this work, we give a theoretic explanation of the both problems by Figalli’s regularity theory of optimal transportation maps. Basically, the generator compute the transportation maps between the white noise distributions and the data distributions, which are in general discontinuous. However, DNNs can only represent continuous maps. This intrinsic conflict induces mode collapse and mode mixture. In order to tackle the both problems, we explicitly separate the manifold embedding and the optimal transportation; the first part is carried out using an autoencoder to map the images onto the latent space; the second part is accomplished using a GPU-based convex optimization to find the discontinuous transportation maps. Composing the extended OT map and the decoder, we can finally generate new images from the white noise. This AE-OT model avoids representing discontinuous maps by DNNs, therefore effectively prevents mode collapse and mode mixture.", "review": "Review:###This paper deals with an important problem of mode collapse and mode mixture. In order to tackle the both problems, the paper proposes to separate the manifold embedding and the optimal transportation problems; the first part being carried out using an autoencoder to map the images onto the latent space and the second part is accomplished using a GPU-based convex optimization to find the discontinuous transportation maps. I have some doubts about moving from the *semi-discrete OT map* to the piece-wise linear extension. The illustration in Fig. 3, and implicit in all the explanation charts is the fact that discontinuity can be found by a linear separation. This seems to be an extremely simplifying assumption, which leads to not so great visual results from the paper. Although the numerical results seems promising, I feel that fewer images, but larger in size, and analysis of mode collapse phenomenon in real images would have been much better. Singular set detection seems to be the most tricky part in this paper, which should have been explained further. The Simplex projection assumption, renders this part not that tricky, but that is where I feel the biggest doubt about this paper lies. The authors themselves mention the need for a high quality auto encoder model to encode celebA dataset, which has been improved upon by numerous other papers, the claims seems not too strong. Also, the method does not have any adversarial training and hence, it studies the GAN idea from only fixing the generator point of view."}
{"id": "iclr2020_858", "title": "Few-Shot One-Class Classification via Meta-Learning | OpenReview", "abstract": "Abstract:###Although few-shot learning and one-class classification have been separately well studied, their intersection remains rather unexplored. Our work addresses the few-shot one-class classification problem and presents a meta-learning approach that requires only few data examples from only one class to adapt to unseen tasks. The proposed method builds upon the model-agnostic meta-learning (MAML) algorithm (Finn et al., 2017) and explicitly trains for few-shot class-imbalance learning, aiming to learn a model initialization that is particularly suited for learning one-class classification tasks after observing only a few examples of one class. Experimental results on datasets from the image domain and the time-series domain show that our model substantially outperforms the baselines, including MAML, and demonstrate the ability to learn new tasks from only few majority class samples. Moreover, we successfully learn anomaly detectors for a real world application involving sensor readings recorded during industrial manufacturing of workpieces with a CNC milling machine using only few examples from the normal class.", "review": " This paper tackles an interesting problem, one-class classification or anomaly detection, using a meta-learning approach. The main contribution is to introduce a parameter such that the inner-loop of the meta-learning algorithm better reflects the imbalance which occurs during meta-testing. Results are shown comparing a few simple baselines to both MAML and the modified variant, on a few datasets such as image-based ones (MNIST, miniImageNet), a synthetic dataset, and a real-world time-series example from CNC milling machines. Overall, the paper presents an interesting problem and awareness that meta-learning might be general enough to solve it well, but provides no real novelty in the approach. The datasets and comparison to other state of art methods (including both other anomaly detection methods and out of distribution methods) is lacking. I suggest the authors perform more rigorous experimentation and focus the paper to be a paper about an understudied problem with rigorous experiments/findings, or improve their method beond the small modification made. Due to these weaknesses, I vote for rejection at this time. Detailed comments are below. Strengths - The problem is interesting and under-studied in the context of deep learning and transferable methods from similar ML problems (e.g. few-shot learning) - The method is simple and adapts a state of art in few-shot learning (meta-learning, and specifically MAML) Weaknesses - While I enjoyed reading the paper since it tackles an under-explored problem, it is hard to justify publishing the method/approach at a top machine learning conference. Changing the balance in meta-learning is a relatively obvious modification that one would do to better reflect the problem; I don*t think it results in general scientific/ML principles that can be used elsewhere. - The relationship to out-of-distribution detection (which some of the experiments, e.g. Multi-task MNIST and miniImagenet essentially test) is not discussed or compared to. How are anomalies defined and is it really different than just being out-of-distribution? - The datasets are limited. The MNIST dataset seems to choose a fixed two specific categories for meta-validation and meta-testing, as opposed to doing cross-validation. Results on just one meta-testing seems limited in this case with just one class. In terms of time-series, anomaly detection has been studied for a long time; is there a reason that the authors create a new synthetic dataset? For the milling example, how were anomalies provoked? - The baselines do not represent any state of art anomaly detection (e.g. density based, isolation forests, etc.) nor out of distribution detection; the latter especially would likely do extremely well for the simple image examples. - There is no analysis of what the difference is in representation (initialization) learning due to the differences between the OCC and FS setup. What are the characteristics of the improved initialization? One minor comment not reflecting the decision: - Exposition: Define the one-class classification problem; it*s not common so it would be good to define in the abstract, or mention anomaly detection which is a better-known term."}
{"id": "iclr2020_859", "title": "Pretraining boosts out-of-domain robustness for pose estimation | OpenReview", "abstract": "Abstract:###Deep neural networks are highly effective tools for human and animal pose estimation. However, robustness to out-of-domain data remains a challenge. Here, we probe the transfer and generalization ability for pose estimation with two architecture classes (MobileNetV2s and ResNets) pretrained on ImageNet. We generated a novel dataset of 30 horses that allowed for both within-domain and out-of-domain (unseen horse) testing. We find that pretraining on ImageNet strongly improves out-of-domain performance. Moreover, we show that for both pretrained and networks trained from scratch, better ImageNet-performing architectures perform better for pose estimation, with a substantial improvement on out-of-domain data when pretrained. Collectively, our results demonstrate that transfer learning is particularly beneficial for out-of-domain robustness.", "review": "Review:###The paper proposes a horse dataset to study transfer learning or so-called out-of-domain pose estimation. They also study which model is a better initialization model for pose estimation, and how to utilize transfer learning to get a better estimation model. There are several questions from the paper: 1. why are they not using human pose estimation datasets, as there are already lots of them and that will be easier to compare with other models: MPII, COCO, AI challenge, CrowdPose, etc. I am not fully convinced the horse dataset is better than human pose. In terms of out domain, authors can use pose pre-trained models to analysis horse pose prediction. 2. The analysis is good and with lots of experiments, however, the key part is that they do not provide a way to improve the overall performance for out-of-domain pose estimation."}
{"id": "iclr2020_860", "title": "Policy Optimization In the Face of Uncertainty | OpenReview", "abstract": "Abstract:###Model-based reinforcement learning has the potential to be more sample efficient than model-free approaches. However, existing model-based methods are vulnerable to model bias, which leads to poor generalization and asymptotic performance compared to model-free counterparts. In this paper, we propose a novel policy optimization framework using an uncertainty-aware objective function to handle those issues. In this framework, the agent simultaneously learns an uncertainty-aware dynamics model and optimizes the policy according to these learned models. Under this framework, the objective function can represented end-to-end as a single computational graph, which allows seamless policy gradient computation via backpropagation through the models. In addition to being theoretically sound, our approach shows promising results on challenging continuous control benchmarks with competitive asymptotic performance and sample complexity compared to state-of-the-art baselines.", "review": "Review:###Summary: The main contribution of this work is introducing the uncertainty-aware value function prediction into model-based RL, which can be used to balance the risk and return empirically. Methodology This work uses a linear combination of the mean and standard deviation of value function to capture the uncertainty in learning state value function. It is not clear how to convert the objective function from Eq 2 (expectation over the initial state) to Eq 5 (expectation over all states). Those two objectives are not equal. It is not clear how does the uncertainty in model prediction (dynamics and reward function) can be alleviated through the proposed method, as claimed in the introduction. It seems the novelty part lies in considering the uncertainty of value function estimation. How does this relate to solving the limitation of model predictive control? What is the objective function for learning reward function r_phi? Experimental results: The experiments are not sufficient to demonstrate the effectiveness of the proposed method. It would be more convincing to compare the proposed method with a few more model-based approaches on more tasks. The results of MBPO is better the proposed POUM in one of two tasks. The performance of MBPO on Reacher-v2 and Pusher-v2 is missing? Writing: This paper has many typos and the presentation is not very clear. - Section 4.1 *in Section 3.4,* - Last paragraph in P3: convergence convergence analysis - *shows that POUM has a sample efficiency compared*"}
{"id": "iclr2020_861", "title": "LocalGAN: Modeling Local Distributions for Adversarial Response Generation | OpenReview", "abstract": "Abstract:###This paper presents a new methodology for modeling the local semantic distribution of responses to a given query in the human-conversation corpus, and on this basis, explores a specified adversarial learning mechanism for training Neural Response Generation (NRG) models to build conversational agents. The proposed mechanism aims to address the training instability problem and improve the quality of generated results of Generative Adversarial Nets (GAN) in their utilizations in the response generation scenario. Our investigation begins with the thorough discussions upon the objective function brought by general GAN architectures to NRG models, and the training instability problem is proved to be ascribed to the special local distributions of conversational corpora. Consequently, an energy function is employed to estimate the status of a local area restricted by the query and its responses in the semantic space, and the mathematical approximation of this energy-based distribution is finally found. Building on this foundation, a local distribution oriented objective is proposed and combined with the original objective, working as a hybrid loss for the adversarial training of response generation models, named as LocalGAN. Our experimental results demonstrate that the reasonable local distribution modeling of the query-response corpus is of great importance to adversarial NRG, and our proposed LocalGAN is promising for improving both the training stability and the quality of generated results.", "review": " In this paper, the author proposed a model to address the training instability of a GAN model on the NRG task. The authors take advantage of an energy based model to measure the distance between a predicted response and the center of all qualified responses. The training process becomes a hybrid one with the original loss function in the beginning followed by the loss that pulls the response to the center of the response cluster later. In general the paper is well written, with experiments clearly showcased improved training stabilities. However, one major flaw in the experiments in that the authors almost only compared diversity measures such as Dist-1, Dist-2 and Ent4. These measures did not take into consideration the matches between the predicted one and the ground truth. The only relevance measure used in this paper is the Rel., which the authors defined as the average of embedding distances. Such a measure is by no means an objective measure and can*t really demonstrate the effectiveness of the model in terms of generating responses that are close to the ground truth. The authors would need to submit results on one of the widely adopted benchmark metrics (e.g., BLEU, ROGUE) or their equivalents in order to demonstrate the quality of the generated response. And this is the main reason of my rating recommendation."}
{"id": "iclr2020_862", "title": "Sparsity Learning in Deep Neural Networks | OpenReview", "abstract": "Abstract:###The main goal of network pruning is imposing sparsity on the neural network by increasing the number of parameters with zero value in order to reduce the architecture size and the computational speedup.", "review": "Review:###This paper proposed Guided Attention for Sparsity Learning (GASL) to learn sparse model in neural networks. The authors demonstrate through experiments that their methods achieve state-of-the-art sparsity level and speedup with competitive accuracy. Questions: 1) In the abstract, the authors claim “Our work is aimed at providing a framework based on an interpretable attention mechanisms...”. How is your method interpretable? I think the paper needs more justifications on this point. 2) The results of this paper would be more convincing if the authors could include experiments on ImageNet datasets, as in [1], [2]. 3) In section 4.2, why is “ the choice of V^r should be highly correlated with V”? 4) In section 3.2, why is variance loss related to attention mechanism? From my view, you simply add a loss function to penalize the variance of weights in a group. This part is confusing. 5) Also you said that the detailed mathematical proof for proposition 1 is put in the appendix. But where is the appendix? 6) For the experimental results, your method fall short of Sparse Variational Dropout in terms of clean accuracy. It seems to me that your method may not be so effective. 7) There seems to be some typos in the current version: Section 4.1 last line, “from now one” should be “from now on” Section 4.1 something is wrong with the last line in equation (5). In general, the paper is not clearly presented. Many details are needed to clarify the contributions. The paper writing needs to be improved, as there exist many typos in the current version and the presentation is confusing. Therefore I recommend weak reject for the current version of the paper. [1] Learning both Weights and Connections for Efficient Neural Networks. Song Han, Jeff Pool, John Tran, William J. Dally, NIPS 2015. [2] Learning Structured Sparsity in Deep Neural Networks. Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li. NIPS 2016."}
{"id": "iclr2020_863", "title": "Learning Deep-Latent Hierarchies by Stacking Wasserstein Autoencoders | OpenReview", "abstract": "Abstract:###Probabilistic models with hierarchical-latent-variable structures provide state-of-the-art results amongst non-autoregressive, unsupervised density-based models. However, the most common approach to training such models based on Variational Autoencoders often fails to leverage deep-latent hierarchies; successful approaches require complex inference and optimisation schemes. Optimal Transport is an alternative, non-likelihood-based framework for training generative models with appealing theoretical properties, in principle allowing easier training convergence between distributions. In this work we propose a novel approach to training models with deep-latent hierarchies based on Optimal Transport, without the need for highly bespoke models and inference networks. We show that our method enables the generative model to fully leverage its deep-latent hierarchy, and that in-so-doing, it is more effective than the original Wasserstein Autoencoder with Maximum Mean Discrepancy divergence.", "review": "Review:###The paper aims to develop a deep generative model, which -unlike VAEs or GANs- comprises a hierarchy of latent variables rather than a direct map from the stochastic latent manifold to the observation space. To this end, the paper builds a training objective based on nesting the Wasserstein distance between the data distribution and its estimation arbitrarily many times. The generated objective corresponds naturally to a deep hierarchical generative model. The principled approach followed to achieve the objective is solid and elegant. It is also intuitive and matches nicely with some valid observations highlighted in the paper such as insufficiency of by-passing intermediate latent variables (sentence above the Sec 2.3 title). One major weakness of the paper is that it lacks a sufficient argumentation about how it differentiates from earlier attempts to nest Wasserstein distances. For instance, Y. Dukler et al., *Wasserstein of Wasserstein Loss for Learning Generative Models*, ICML, 2019 Apart from the theoretical argumentation, the paper should also compare their solution to this prior work on a number of benchmarks. Another major weakness is that the paper lacks a quantitative evaluation scheme for its success. The experiments section starts with the claim that the proposed method *significantly* improves on the WAE, which I fail to see on the plots. Lastly, Having said that the proposed method is novel and elegant, it is still a straightforward extension of the existing and well-known Wasserstein Auto-Encoder (WAE) approach. It extends WAEs by repetitively applying the tricks proposed by this earlier work, putting aside some minor additional adjustments. Minor on style: The abstract does not give any single hint about the methodological novelty of the work. --- Post-rebuttal: Thanks to authors for their effort for clarifications. Yet, I*m afraid the author response does not touch at all to any of the concerns I have raised. There are well-known ways to compare the success of generative models, FID being one of them as the authors point out. Another could be the test log-likelihood of a synthetic data set the true distribution of which can be predesigned. I understand the issues the authors raise about the difficulties in comparing generative models, but I kindly disagree with the attitude that there are no ways to compare, so we are obliged to live with qualitative comparisons. If a one-score comparison is not enough, the right way to go is to provide multiple scores. If direct metrics are not feasible, one should go for indirect ones, but should still provide outcomes a reader can reproduce."}
{"id": "iclr2020_864", "title": "Graph convolutional networks for learning with few clean and many noisy labels | OpenReview", "abstract": "Abstract:###In this work we consider the problem of learning a classifier from noisy labels when a few clean labeled examples are given. The structure of clean and noisy data is modeled by a graph per class and Graph Convolutional Networks (GCN) are used to predict class relevance of noisy examples. For each class, the GCN is treated as a binary classifier learning to discriminate clean from noisy examples using a weighted binary cross-entropy loss function, and then the GCN-inferred *clean* probability is exploited as a relevance measure. Each noisy example is weighted by its relevance when learning a classifier for the end task. We evaluate our method on an extended version of a few-shot learning problem, where the few clean examples of novel classes are supplemented with additional noisy data. Experimental results show that our GCN-based cleaning process significantly improves the classification accuracy over not cleaning the noisy data and standard few-shot classification where only few clean examples are used. The proposed GCN-based method outperforms the transductive approach (Douze et al., 2018) that is using the same additional data without labels.", "review": "Review:###This paper studies the problem of learning from multiple tasks and additional noisy data. The proposed representation learning method first assigns each noisy data a relevance score using the topological information. Then the authors propose to minimize a combination of the loss of a class-prototype learning loss and a cosine classifier learning loss to learn a good representation generator g_theta. The empirical study validates the effectiveness of the proposed method. I have the following comments, 1. The studied problem that learning from few-shot data and large-scale noisy data is interesting. According to the experimental results, the proposed method seems to be promising. 2. The learning procedure is confusing. It is highly recommended to provide the pseudocode of the proposed method. 3. Since there are many tasks and each task has a large-scale data, I*m afraid that the running time will explode. How to deal with this issue?"}
{"id": "iclr2020_865", "title": "Global Concavity and Optimization in a Class of Dynamic Discrete Choice Models | OpenReview", "abstract": "Abstract:###Discrete choice models with unobserved heterogeneity are commonly used Econometric models for dynamic Economic behavior which have been adopted in practice to predict behavior of individuals and firms from schooling and job choices to strategic decisions in market competition. These models feature optimizing agents who choose among a finite set of options in a sequence of periods and receive choice-specific payoffs that depend on both variables that are observed by the agent and recorded in the data and variables that are only observed by the agent but not recorded in the data. Existing work in Econometrics assumes that optimizing agents are fully rational and requires finding a functional fixed point to find the optimal policy. We show that in an important class of discrete choice models the value function is globally concave in the policy. That means that simple algorithms that do not require fixed point computation, such as the policy gradient algorithm, globally converge to the optimal policy. This finding can both be used to relax behavioral assumption regarding the optimizing agents and to facilitate Econometric analysis of dynamic behavior. In particular, we demonstrate significant computational advantages in using a simple implementation policy gradient algorithm over existing *nested fixed point* algorithms used in Econometrics.", "review": "Review:###This paper deals with a certain class of models, known as discrete choice models. These models are popular in econometrics, and aim at modelling the complex behavioural patterns of individuals or firms. Entities in these models are typically modelled as rational agents, that behave optimally for reaching their goal of maximizing a certain objective function such as maximizing expected cumulative discounted payoff over a fixed period. This class of models, modelled as a MDP with choice-specific heterogeneity, is challenging as not all the payoffs received by the agents is externally observable. One solution in this case is finding a balance condition and a functional fixed point to find the optimal policy (closely related to value function iteration), and this is apparently the key idea behind ‘nested fixed point’ methods used in Econometrics. The paper proposes an alternative. First it identifies a subclass of discrete choice models (essentially MDP with stochastic rewards) where the value function is globally concave in the policy. The consequence of this observation is that a direct method, such as the policy gradient that circumvents explicitly estimating the value function, can (at least in principle) converge to the optimal policy without calculating a fixed point. The authors illustrate computational advantages of this direct approach. Moreover, the generality of the policy gradient method enables the relaxation of extra assumptions regarding the behaviour of agents while facilitating a wider applicability/econometric analysis. The key contribution claimed by the paper is the observation that in the class of dynamic discrete choice models with unobserved heterogeneity, the value function is globally concave in the policy. This enables using computationally efficient policy gradient algorithms with convergence guarantees for this class of problems. The authors also claim that the simplicity of policy gradient makes it also a viable model for understanding economic behaviour in econometric analysis, and more broadly for social sciences. The paper deals with Discrete choice models with unobserved heterogeneity as a special class of MDP’s and is relevant to ICLR. However, the writing style is quite technical and terse -- while I could appreciate the rigour, the authors develop the basic material until page 5 -- Notation is also somewhat non-standard at places (alternating using delta or sigma for a policy and pi for thresholds of an exponential softmax distribution) and makes it harder to see the additional structure from generic MDPs more familiar in RL. I suspect that a reader more familiar with the relevant econometric, marketing and finance literature could follow the model description more easily. There are a number of assumptions in the paper, especially 2.4 and 2.5 that relate to the monotonicity and ordering of the states. These assumptions seem to be important in subsequent developments for showing the concavity but they seem to be coming from out of the blue. Unfortunately the authors do not provide any intuition/discussion -- an example problem with these properties would make these assumptions more concrete. I was hoping to find such an example in the empirical application however this section does not make the necessary connections with the theoretical development. There are not even references to basic claims done in the abstract, for example, I am not able to find an illustration of ‘significant computational advantages in using a simple implementation policy gradient algorithm over existing “nested fixed point” algorithms used in Econometrics’. The lack of any conclusions makes it also hard for me to appreciate the contributions. Minor: Abstract: .. Existing work in Econometrics assumes that optimizing agents are fully rational and requires finding a functional fixed point to find the optimal policy. … Ambiguous sentence: Existing work in Econometrics [...] requires finding a functional fixed point to find the optimal policy. Theorem 3.1 and elsewhere Freéchet => Fréchet"}
{"id": "iclr2020_866", "title": "The Dual Information Bottleneck | OpenReview", "abstract": "Abstract:###The Information-Bottleneck (IB) framework suggests a general characterization of optimal representations in learning, and deep learning in particular. It is based on the optimal trade off between the representation complexity and accuracy, both of which are quantified by mutual information. The problem is solved by alternating projections between the encoder and decoder of the representation, which can be performed locally at each representation level. The framework, however, has practical drawbacks, in that mutual information is notoriously difficult to handle at high dimension, and only has closed form solutions in special cases. Further, because it aims to extract representations which are minimal sufficient statistics of the data with respect to the desired label, it does not necessarily optimize the actual prediction of unseen labels. Here we present a formal dual problem to the IB which has several interesting properties. By switching the order in the KL-divergence between the representation decoder and data, the optimal decoder becomes the geometric rather than the arithmetic mean of the input points. While providing a good approximation to the original IB, it also preserves the form of exponential families, and optimizes the mutual information on the predicted label rather than the desired one. We also analyze the critical points of the dualIB and discuss their importance for the quality of this approach.", "review": "Review:###Description: The information bottleneck (IB) is an information theoretic principle for optimizing a mapping (encoding, e.g. clustering) of an input, to trade off two kinds of mutual information: minimize mutual information between the original input and the mapped version (to compress the input), and maximize mutual information between the mapped input and an output variable. It is related to a minimization of an (expected) Kullback-Leibler divergence betwen conditional distributions of an output variable. In this paper, instead of the original IB, authors consider a previously presented dual problem of Felice and Ay, where the Kullback-Leibler divergence is minimized in the reverse direction: from the conditional distribution of output given encoding, p(y|hat x), to the conditional distribution given the original input, p(y|x). The *dual problem* itself has a more complicated form than the original IB, authors claim this is *a good approximation* of the original bottleneck formulation, and aim to prove various *interesting properties* of it. - An iterative algorithm (Algorithm 1) similar to the original IB algorithm but with a few more steps is provided. - A theorem about critical points where cardinality of the representation changes is given, similar to the IB critical points, and another theorem about difference of curves on an information plane between the IB and dual-IB solutions. - Authors also show that if the true conditional distribution of outputs given inputs has an exponential-family form, the dual-IB decoder also has a form in the same family, which is said to reduce computational complexity of the algorithm. Evaluation: This is an entirely theory-based paper; although an algorithm is given, it is not instantiated for any concrete representation learning task, and no experiments at all are demonstrated. Overall, I feel the motivation is not clear and strong enough. The abstract does not illustrate the importance of the mentioned *interesting properties* well enough for concrete tasks. Reading the paper, the clearest motivations seem to be improving computational complexity, and having a clearer connection to output prediction in cases where the predictor may be sub-optimal. However, authors do not quantify these well: - The computational complexity improvement is not made clear (quantified) in a concrete IB optimization task: it seems it is only for exponential families, and even for them only affects one part of the algorithm, reducing its complexity from dim(X) to d; the impact of this is not tried out in any experiment. - For output prediction, authors motivate that dual-IB could have a more direct connection e.g. *due to finite sample, in which it can be very different from the one obtained from the full distribution*). Authors further claim that the dual-IB formulation can *improve the generalization error when trained on small samples since the predicted label is the one used in practice*. However, this is not tested at all: no prediction experiments, no quantification of generalization error, and no comparisons are done, thus the impact of the clearer connection to output prediction is not tested at all, and no clear theorems are given about it either. The property that the algorithm *preserves exponential form of the original data distribution, if one exists* is interesting in principle, but it is unclear if any real data would anyway precisely have such a distribution; what happens if the data is not exactly in an exponential family? In its current state the paper, although based on an interesting direction, in my opinion does not make a sufficient impact to be accepted to ICLR. Other comments: *Application to deep learning* mentioned in Section 1.5 is only a sincle sentence in the conclusions. There have been some other suggested alternative IB formulations, for example the Deterministic IB of Strouse and Schwab (Neural Computation 2017) which also claim improved computational efficiency. How does the method compare to those? Section 1.5 claims the algorithm *preserves the low dimensional sufficient statistics of the data*: it is not clear what *preserves* means here, certainly it seems the decoder in Theorem 7 uses the same kinds of sufficient statistics as in the original data, but it is not clear that hat(x) would somehow preserve the same values of the sufficient statistics."}
{"id": "iclr2020_867", "title": "Fractional Graph Convolutional Networks (FGCN) for Semi-Supervised Learning | OpenReview", "abstract": "Abstract:###Due to high utility in many applications, from social networks to blockchain to power grids, deep learning on non-Euclidean objects such as graphs and manifolds continues to gain an ever increasing interest. Most currently available techniques are based on the idea of performing a convolution operation in the spectral domain with a suitably chosen nonlinear trainable filter and then approximating the filter with finite order polynomials. However, such polynomial approximation approaches tend to be both non-robust to changes in the graph structure and to capture primarily the global graph topology. In this paper we propose a new Fractional Generalized Graph Convolutional Networks (FGCN) method for semi-supervised learning, which casts the L*evy Fights into random walks on graphs and, as a result, allows to more accurately account for the intrinsic graph topology and to substantially improve classification performance, especially for heterogeneous graphs.", "review": "Review:###This paper proposes a fractional graph convolutional networks for semi-supervised learning. The proposed method used a classification function of a fractional graph semi-supervised learning (GSSL) [De Nigris et al., 2017] as a graph filter. In addition, the authors adopt a parallel system and weighted combination of max and average pool. Experimental results show that the proposed method (FGCN) shows the best accuracy compared to other recent graph-based neural networks for all datasets except one. The key approach of the proposed method is to apply a classification function (equation (3)) obtained by solving a GSSL problem to graph convolutional networks. However, this idea is too incremental and applying the classification function to graph filter is very trivial. This works also combines the fractional GSSL with a parallel system and weighted pool. But, it is not clear which contribution actually improves the results. Moreover, the intuition of the fractional approach is not clear too, e.g., how the optimization (equation (2)) is derived?, and some explanations are unnatural to demonstrate the methodology, e.g., equation (4). For these reasons, this paper is under the bar of acceptance. Main concerns: 1. What is the intuition of the optimization of GSSL? How is it obtained? And among all fractional methods (e.g., SL, NL, and PR), which one doe achieve the best performance? 2. The FGS filter in equation (7) is the sum of infinite terms. However, in practical, it is impossible to compute the infinite terms. Does this approximate the sum of finite terms? If does, what is the number of truncation? 3. The authors mention that they establish a theoretical guarantee of the parallel system. But, I could not find any theoretical results. It would be better to include the analysis in the paper. Minor concerns: 1. In page 3, please edit “forulation” -> “formulation” 2. In equation (8), I think “X+alpha \tilde{L} X” should change to “\tilde{L} X”"}
{"id": "iclr2020_868", "title": "AN EXPONENTIAL LEARNING RATE SCHEDULE FOR BATCH NORMALIZED NETWORKS | OpenReview", "abstract": "Abstract:###Intriguing empirical evidence exists that deep learning can work well with exotic schedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or BN(Ioffe & Szegedy, 2015), which is ubiq- uitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about BN with weight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone BN (Ioffe & Szegedy, 2015; Santurkar et al., 2018; Arora et al., 2018) • Training can be done using SGD with momentum and an exponentially in- creasing learning rate schedule, i.e., learning rate increases by some (1 + ?) factor in every epoch for some ? > 0. (Precise statement in the paper.) To the best of our knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As ex- pected, such training rapidly blows up network weights, but the net stays well-behaved due to normalization. • Mathematical explanation of the success of the above rate schedule: a rigor- ous proof that it is equivalent to the standard setting of BN + SGD + Standard Rate Tuning + Weight Decay + Momentum. This equivalence holds for other normalization layers as well, Group Normalization(Wu & He, 2018), Layer Normalization(Ba et al., 2016), Instance Norm(Ulyanov et al., 2016), etc. • A worked-out toy example illustrating the above linkage of hyper- parameters. Using either weight decay or BN alone reaches global minimum, but convergence fails when both are used.", "review": "Review:###This work makes an interesting observation that it is possible to use exponentially growing learning rate schedule when training with neural networks with batch normalization. This paper provides both theoretical insights and empirical demonstration of this remarkable property. In detail, the authors prove that for stochastic gradient descent (SGD) with momentum, this exponential learning rate schedule is equivalent to constant learning rate + weight decay, for any scale invariant networks, including networks with Batch Normalization and other normalization methods. This paper also contains an interesting toy example where gd converges when normalization or weight decay is used alone while not when normalization and weight decay are used together. Pros: 1. This paper gives new and important insight to the complex interplay between the tricks of network training, such as weight decay, normalization and momentum. The assumption and derivation are simple but the result is quite surprising. In classical optimization framework, it is common to keep the learning rate smaller than the 1/smoothness such that gd decreases the loss. However, the connection between exponential learning rate schedule and weight decay in common practice built by this paper suggests that the current neural net training recipe may be inherently non-smooth. 2. The experiment of this paper also suggests that in practice (with normalization layer), learning rate and weight decay coefficient can be packed into a single parameter, which reduces the effort needed for hyper-parameter tuning. Cons: 1. Though it*s obvious for the feedforward networks with normalization layers to be scale invariant, it*s not the case for ResNet ( and the authors use this for experiment). And this needs to be clarified. 2. The writing of the proofs should be imporved. Typos: 1. Definition 1.2 broke citation 2. Equation (1) eta_t should be eta_{t-1} 3. Some facts about Equation 4, incomplete sentence 4 In thm B.2,R_infty might be 0. So the authors can just delete the last equation on page 12 and use the equation above as the statement of the lemma. 5. In the first line of Equation (13), the appearance of ( _x0008_eta * e^{1-_x0008_eta} )^{ k/2 } seems to be a mistake"}
{"id": "iclr2020_869", "title": "Random Partition Relaxation for Training Binary and Ternary Weight Neural Network | OpenReview", "abstract": "Abstract:###We present Random Partition Relaxation (RPR), a method for strong quantization of the parameters of convolutional neural networks to binary (+1/-1) and ternary (+1/0/-1) values. Starting from a pretrained model, we first quantize the weights and then relax random partitions of them to their continuous values for retraining before quantizing them again and switching to another weight partition for further adaptation. We empirically evaluate the performance of RPR with ResNet-18, ResNet-50 and GoogLeNet on the ImageNet classification task for binary and ternary weight networks. We show accuracies beyond the state-of-the-art for binary- and ternary-weight GoogLeNet and competitive performance for ResNet-18 and ResNet-50 using a SGD-based training method that can easily be integrated into existing frameworks.", "review": " The authors propose a method for weight quantization in neural networks, RPR (Random Partition Relaxation) which extends previous approaches for weight quantization. The difference to previous approaches is that RPR iteratively selects a random fraction (the constrained weights) of continuous weights (called relaxed weights) to quantize during optimization and increases this fraction over time until all weights are constrained, while maintaining the ability to use SGD as the main workhorse for optimization. The authors demonstrate cases for constrained weights to binary and tertiary cases while keeping continuous weights for input and output layers. During experiments the authors show that (a) their method is competitive on most benchmarks with simpler methods but can be beaten in some conditions and (b) RPR yields strong results for the GoogleNet architecture. The paper overall is written concisely and well organized, though it is light on technical formalism and generalizable technical insights. Comments: 1. How is training evaluated here? Do the authors repeat training K times (over i.e. random seeds)? The results on testing contain no error-bars to evaluate if the experiments were lucky or are reliable. This is particularly important to show the value of an iterative optimization scheme as chosen here since *lucky* runs can dominate results. I would find this a necessary requirement here to trust the results and to understand how much variance the iterative optimization and the masking process introduce to the training. 2. How robust is the method to the scheme for setting FF (fraction of constrained weights) vs learning rate? How much damage would strange settings do to the model? Is there a reliable heuristic for picking the scheme? There is not nearly enough information on this. References: In ICLR last year, Frankle et al introduced the Lottery ticket hypothesis. In this paper, progressive masking and retraining of NNs is studied as a mechanism for sparsification and masking is heuristically performed by quantizing weights close to 0 (by some threshold) to the value 0. It would be worthwhile for the authors to connect their idea to these observations and to include a discussion of those results, as it seems to indicate that quantization/sparsification/masking over iterations of SGD training share useful dynamics here. Decision: As the paper stands I find it technically and experimentally not mature enough. The process is certainly interesting, but seems neither empirically nor theoretically studied well enough in the current form."}
{"id": "iclr2020_870", "title": "Learning Compositional Koopman Operators for Model-Based Control | OpenReview", "abstract": "Abstract:###Finding an embedding space for a linear approximation of a nonlinear dynamical system enables efficient system identification and control synthesis. The Koopman operator theory lays the foundation for identifying the nonlinear-to-linear coordinate transformations with data-driven methods. Recently, researchers have proposed to use deep neural networks as a more expressive class of basis functions for calculating the Koopman operators. These approaches, however, assume a fixed dimensional state space; they are therefore not applicable to scenarios with a variable number of objects. In this paper, we propose to learn compositional Koopman operators, using graph neural networks to encode the state into object-centric embeddings and using a block-wise linear transition matrix to regularize the shared structure across objects. The learned dynamics can quickly adapt to new environments of unknown physical parameters and produce control signals to achieve a specified goal. Our experiments on manipulating ropes and controlling soft robots show that the proposed method has better efficiency and generalization ability than existing baselines.", "review": "Review:###The paper is well written and the proposed idea is novel and builds on a sound theoretical framework of Koopman operator theory. A physical system is a represented as a graph; graph neural network is used to encode the current state to an object-centric embedding where the dynamics are assumed to be linear (Koopman operator theory) and modeled as a transition matrix. The key contribution is to recognize that similar physical interactions can be modeled using same parameters which constraints the transition matrix to be block-wise with shared parameters. Furthermore, the model is extended to add a control matrix to model external control. Experiments are conducted on simulations as well as control for 3 different settings - a a hanging rope/string anchored at the top, soft robot with an anchor, and soft robot in fluids. Strengths * Proposed method builds on a sound theoretical basis; although the linear dynamics model appear somewhat limited compared for the complex dynamics, thorough experiments are conducted to demonstrate the effectiveness of the method. The efficiency of the proposed algorithm compared to prior work makes is much practically useful. * Well written with examples and illustrative figures. * Quantitative analysis together with ablation studies on the structure are insightful. Weaknesses * My primary concern are with the evaluation. - Experiments are only conducted on synthetic datasets. While experiments on real datasets are understandably difficult especially for quantitative validation, it would help to map the experiments to real problems to gain a more intuitive understanding and thus cater to a broader community. - This paper and several related works are all evaluated on different problems; it would be useful to evaluate on similar tasks; for instance, strings [Battaglia 2016]. It would make it easier to draw comparisons. - It*s clear that the community would benefit significantly by having a benchmark or a web-based evaluation methodology (similar to OpenAI gym used actively reinforcement learning community). Unfortunately, this paper does not seem to offer a solution to this issue but continue to evaluate in ways similar to previous papers."}
{"id": "iclr2020_871", "title": "GQ-Net: Training Quantization-Friendly Deep Networks | OpenReview", "abstract": "Abstract:###Network quantization is a model compression and acceleration technique that has become essential to neural network deployment. Most quantization methods per- form fine-tuning on a pretrained network, but this sometimes results in a large loss in accuracy compared to the original network. We introduce a new technique to train quantization-friendly networks, which can be directly converted to an accurate quantized network without the need for additional fine-tuning. Our technique allows quantizing the weights and activations of all network layers down to 4 bits, achieving high efficiency and facilitating deployment in practical settings. Com- pared to other fully quantized networks operating at 4 bits, we show substantial improvements in accuracy, for example 66.68% top-1 accuracy on ImageNet using ResNet-18, compared to the previous state-of-the-art accuracy of 61.52% Louizos et al. (2019) and a full precision reference accuracy of 69.76%. We performed a thorough set of experiments to test the efficacy of our method and also conducted ablation studies on different aspects of the method and techniques to improve training stability and accuracy. Our codebase and trained models are available on GitHub.", "review": "Review:###The paper propose a new quantization-friendly network training algorithm called GQ (or DQ) net. I addresses the existing issues in the common paradigm, where a floating-point network is trained first, followed by a second-phase training step for the quantized version. It is a well-written paper. Concepts were clearly explained and easy to follow. Below I present my comments about some details in the paper that were not entirely clear for me. - The two loss terms conflict each other. If the training algorithm focuses too much on the first term, it will make the network less friendly to the quantization process. On the other hand, the second one is going to enforce too much emphasis on the accuracy from the quantized network. It is natural to involve some hyperparameter search to find the balance between the two blending parameters. The paper suggests a strategy as to how to handle this issue, but it is not comprehensive, and rather controversial. I think the paper will benefit from a more in-depth discussion and analysis on this regularization issue. - The schedule for the loss term blending parameters looks drastic to me. It’s more like “train the floating point net first, and then train the quantized one, and then revisit the floating point one, and so on.” I know I simplified, because the floating point network never stops getting updated as it’s omega_f is always 1. However, it seems to me that this drastic scheduling strategy sounds like very similar to the traditional approach that trains the floating point network first and then finetune the quantized one, except for the fact that this proposed algorithm repeats this process a few times. Hence, I think the authors’ argument about the supremacy of the proposed method to the two-step finetuning approach is not clearly supported. - The exponentially decaying learning rate scheduling looks like the one from ResNet. I’m wondering if it should be the best, especially with the drastic introduction and omission of the second loss. - In the ablation studies, it seems that some of the suggested training options are conflicting each other and the clear winner seems to be the multi-domain BN. I cannot conclude anything from this analysis as to which one is more important than the other one, except for the Alt{W,\theta} case. Some minor things: - What’s the name of the proposed network? Is it GQ or DQ?"}
{"id": "iclr2020_872", "title": "End-to-End Multi-Domain Task-Oriented Dialogue Systems with Multi-level Neural Belief Tracker | OpenReview", "abstract": "Abstract:###It has been an open research challenge for developing an end-to-end multi-domain task-oriented dialogue system, in which a human can converse with the dialogue agent to complete tasks in more than one domain. First, tracking belief states of multi-domain dialogues is difficult as the dialogue agent must obtain the complete belief states from all relevant domains, each of which can have shared slots common among domains as well as unique slots specifically for the domain only. Second, the dialogue agent must also process various types of information, including contextual information from dialogue context, decoded dialogue states of current dialogue turn, and queried results from a knowledge base, to semantically shape context-aware and task-specific responses to human. To address these challenges, we propose an end-to-end neural architecture for task-oriented dialogues in multiple domains. We propose a novel Multi-level Neural Belief Tracker which tracks the dialogue belief states by learning signals at both slot and domain level independently. The representations are combined in a Late Fusion approach to form joint feature vectors of (domain, slot) pairs. Following recent work in end-to-end dialogue systems, we incorporate the belief tracker with generation components to address end-to-end dialogue tasks. We achieve state-of-the-art performance on the MultiWOZ2.1 benchmark with 50.91% joint goal accuracy and competitive measures in task-completion and response generation.", "review": "Review:###[Contribution summary] Authors propose a new E2E model for the DST task that (1) implements a response generator that leverages DB query results as well as other dialog contexts with a late fusion approach for joining domain and slot pair representations, and (2) obtains the competitive results (50.91% for the joint task of DST & context-to-text generation, and 49.55% joint accuracy for the DST task, both on MultiWoZ 2.1). [Comments] The proposed model is reasonably structured. Empirical results show improvement over other baselines, with the main gain (among ablations) coming from incorporating the dialog states from previous turns, and the joint domain-slot module, etc. The empirical analysis section could improve and the novelty of the system over the previous literature be clarified -- some of the components used in this work are not entirely novel and previously utilized in other work, hence it is hard to discern where the main gain is really coming from, compared to the previous work (especially for the DST task). Empirical comparison of the joint DST & generation, which is one of the main results, is limited to Lei et al. There have been recent work on DST with new SOTA results (e.g. “Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset” by Rastogi et al.) -- please consider comparing the approaches."}
{"id": "iclr2020_873", "title": "Topological based classification using graph convolutional networks | OpenReview", "abstract": "Abstract:###In colored graphs, node classes are often associated with either their neighbors class or with information not incorporated in the graph associated with each node. We here propose that node classes are also associated with topological features of the nodes. We use this association to improve Graph machine learning in general and specifically, Graph Convolutional Networks (GCN). First, we show that even in the absence of any external information on nodes, a good accuracy can be obtained on the prediction of the node class using either topological features, or using the neighbors class as an input to a GCN. This accuracy is slightly less than the one that can be obtained using content based GCN. Secondly, we show that explicitly adding the topology as an input to the GCN does not improve the accuracy when combined with external information on nodes. However, adding an additional adjacency matrix with edges between distant nodes with similar topology to the GCN does significantly improve its accuracy, leading to results better than all state of the art methods in multiple datasets.", "review": "Review:###** Summary This paper propose to adding the topology of each node as auxiliary information to improve the node classification accuracy in the graph. Specifically, the authors show that using adjacency matrix as the topology representation is better than directly using the topology, like the degree or centrality. The results suggest that using the extra information in the form of adjacency matrix brings marginal improvement on some of the tested datasets. ** Weaknesses 1. This paper is poorly written and hard to follow. There are multiple un-defined terms which are important to understanding the proposed idea. For example, in the first line of Section 5.1, there are 2 terms “neighbor class” and “node self-topology”. There is no context about what they are. Is “neighbor class” the class labels of the neighbors of a given node or is it the class labels of a pair of nodes who are neighbors in the graph? What is “node self-topology”? This term only occurred once in the paper. In the first bullet of page 4, the authors mentioned that the nodes with similar topology are connected in the dual graph. How is the similarity on topology measured? What are the most important topology feature? They are core to the proposed idea and therefore should be in the main content. 2. The empirical results are not persuasive. Although the authors claim that the proposed method outperforms all the state of the art methods, it is hard to tell the difference on the accuracy according to Table. 1. For example, the gap between the authors’ re-implementation and the existing method (70.9 compared with 72.5) is larger than the gap between the re-implemented model with or without the proposed extra information (72.2 compared with 70.9). 3. The comparisons are not fair. For example, compared with GAT, the T-GAT has an extra dual graph which means extra parameters and computations during training and testing. This is not reflected in the comparison."}
{"id": "iclr2020_874", "title": "Contrastive Representation Distillation | OpenReview", "abstract": "Abstract:###Often we wish to transfer representational knowledge from one neural network to another. Examples include distilling a large network into a smaller one, transferring knowledge from one sensory modality to a second, or ensembling a collection of models into a single estimator. Knowledge distillation, the standard approach to these problems, minimizes the KL divergence between the probabilistic outputs of a teacher and student network. We demonstrate that this objective ignores important structural knowledge of the teacher network. This motivates an alternative objective by which we train a student to capture significantly more information in the teacher*s representation of the data. We formulate this objective as contrastive learning. Experiments demonstrate that our resulting new objective outperforms knowledge distillation on a variety of knowledge transfer tasks, including single model compression, ensemble distillation, and cross-modal transfer. When combined with knowledge distillation, our method sets a state of the art in many transfer tasks, sometimes even outperforming the teacher network.", "review": "Review:###This paper combines a contrastive objective measuring the mutual information between the representations learned by a teacher and a student networks for model distillation. The objective enforces correlations between the learned representations. When combined with the popular KL divergence between the predictions of the two networks, the proposed model shows consistently improvement over existing alternatives on three distillation tasks. This is a solid work – it is based on sound principles and provides both rigorous theoretical analysis and extensive empirical evidence. I only have two minor suggestions. 1, From Section 3.2 to Section 3.4, it is not clear to me that on the model compression task, are both the proposed contrastive loss and the loss in Eq. (10) used? 2, The “Deep mutual learning”, Zhang et al, CVPR’18 paper needs to be discussed. I’d also like to see some experiments on the effects of training the teacher and student networks jointly from scratch using the proposed loss."}
{"id": "iclr2020_875", "title": "Count-guided Weakly Supervised Localization Based on Density Map | OpenReview", "abstract": "Abstract:###Weakly supervised localization (WSL) aims at training a model to find the positions of objects by providing it with only abstract labels. For most of the existing WSL methods, the labels are the class of the main object in an image. In this paper, we generalize WSL to counting machines that apply convolutional neural networks (CNN) and density maps for counting. We show that given only ground-truth count numbers, the density map as a hidden layer can be trained for localizing objects and detecting features. Convolution and pooling are the two major building blocks of CNNs. This paper discusses their impacts on an end-to-end WSL network. The learned features in a density map present in the form of dots. In order to make these features interpretable for human beings, this paper proposes a Gini impurity penalty to regularize the density map. Furthermore, it will be shown that this regularization is similar to the variational term of the -variational autoencoder. The details of this algorithm are demonstrated through a simple bubble counting task. Finally, the proposed methods are applied to the widely used crowd counting dataset the Mall to learn discriminative features of human figures.", "review": " This article proposes a method for object counting which can be trained with weak supervision. Object counting methods are often trained with point annotations, i.e., one click-point per object. In this article, a weaker way of annotation is used: count-based annotation, i.e., the number of objects of each class present in the image are given as annotation but no precise location of the objects. This article is an extension of density-based object counting methods for weakly supervision. This article analyzes the effect of pooling layers on WSL. The findings indicate that the pooling layer constrained the objects* locations to a predefined grid and accordingly they remove it. When removing this effect is alleviated. Moreover, the authors realized that a sift on the predictions also happen. Accordingly, they learn rotated versions of the model for centering the predictions. As a countereffect, several points per object are detected, and then a gini impurity regularization is proposed to reduce the number of detected objects. Then, the authors connect this formulation with VAE. The experiments are conducted on a toy example, i.e., circle finding, and on the well established Mall dataset. Main concerns: The experimental section should make experiments on standard datasets (i.e., USCD, Trancos, Mall, PkLot, Shangai, Penguins) using standard evaluation protocols (MAE, GAME). The current evaluation only shows the qualitative results on one frame of the Mall dataset. The results of the proposed method should be compared with baselines. For instance, Glance method, which is only trained with count-level annotation, and with *C-WSL: Count-guided Weakly Supervised Localization*. It should be also compared with WSL methods based on image-level labels such as *Object Counting and Instance Segmentation with Image-level Supervision* or *Where are the Masks: Instance Segmentation with Image-level Supervision* and then with methods that use point supervision as an upper bound such as density method or Where are the blobs. Moreover, some ablation studies about the effect of each component of the model would be needed. Regarding the model I have some concerns: - Removing the pooling layers does not seem a very good idea. This will remove part of the ability to detect objects at multiple scales and moreover will increase its computational power. Maybe for easy cases as circles and heads of mall people where the resolution changes are small could work. But not for more challenging datasets. - The rotation of the model could be ok for circles or heads. But making the model to learn rotation-invariant features could be challenging in more difficult datasets. - Eq.5 and 8 which are used in the model seem to use point supervision and not just the count based. Which makes the method not a weakly supervised one. Are these equations the ones used or others?"}
{"id": "iclr2020_876", "title": "Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning | OpenReview", "abstract": "Abstract:###The field of Deep Reinforcement Learning (DRL) has recently seen a surge in the popularity of maximum entropy reinforcement learning algorithms. Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In this paper, we seek to understand the primary contribution of the entropy term to the performance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that the entropy term in Soft Actor Critic (SAC) principally addresses the bounded nature of the action spaces. With this insight, we propose a simple normalization scheme which allows a streamlined algorithm without entropy maximization match the performance of SAC. Our experimental results demonstrate a need to revisit the benefits of entropy regularization in DRL. We also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training. We further show that the streamlined algorithm with the simple non-uniform sampling scheme outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks.", "review": "Review:###The main contribution of this paper is a normalization scheme to avoid saturating the squashing function typically used to constrain actions within a bounded range in continuous control problems. It is argued that algorithms like DDPG and TD3 suffer from such saturation, which prevents proper exploration during training, while maximum entropy algorithms like Soft Actor-Critic (SAC) avoid it thanks to their entropy bonus. The main reason behind the success of SAC would thus be its ability to keep exploring throughout training, by avoiding saturation. A second contribution is a new experience replay sampling scheme, named Emphasizing Recent Experience (ERE), based on the idea that most recently added transitions should be given higher weights when sampling mini-batches from the replay bufffer. Combining both ideas leads to the SOP (Streamlined Off-Policy)+ERE algorithm, which is shown to consistently outperform SAC on Mujoco tasks. Although this paper presents interesting insights and very good empirical results, I am currently leaning towards rejection mostly due to missing some important empirical comparisons, which hopefully can be added in a revised version. The first key missing comparison (IMO) is to the Inverting Gradients approach from Hausknecht & Stone (2016), which the authors know about since it is cited in the related work section. Note that in that paper, the problem of saturating squashing functions preventing proper exploration was already mentioned, although not investigated in as much depth as in this submission («(…) squashing functions quickly became saturated. The resulting agents take the same discrete action with the same maximum/minimum parameters each timestep »). Their proposed Inverting Gradients technique was found to work significantly better than squashing functions, which is why I believe it should be an obvious baseline to compare to. The other important experiments which I think need to be added are simply to implement the proposed normalization scheme within DDPG & TD3 to demonstrate its usefulness as a standalone improvement over existing algorithms. This would strengthen the claim that « algorithms such as DDPG and TD3 based on the standard objective with additive noise exploration can be greatly impaired by squashing exploration ». Without this comparison on the same benchmark, it is difficult to fully grasp the impact of this normalization. Finally, regarding the ERE sampling scheme, I would appreciate to see SAC+ERE as well, to (hopefully) show that it can benefit SAC too (since this second contribution is orthogonal to the SOP algorithm). Minor points: • I would tone down a bit the claims for « the need to revisit the benefits of entropy maximization in DRL », since better exploration has always been put forward as a major benefit (« the maximum entropy formulation provides a substantial improvement in exploration and robustness », as written in « Soft Actor-Critic Algorithms and Applications »). To me, what this submission shows is essentially that naive implementation of additive noise exploration in e.g. DDPG is very bad for exploration, more than uncovering some novel properties of SAC. • Below eq. 1: « the optimal policy is deterministic » => should be replaced with « there exists an optimal policy that is deterministic » • « principle contribution » => principal • The normalization scheme does not appear in Alg. 1 • In Alg. 1 there are a Q_phi,i and a Q_phi,1 that should probably be Q_phi_i and Q_phi_1 • The results from section E in the Appendix should be mentioned in the main text • In Fig. 4f the y axis’ label is a bit clipped Update based on new revision: thank you for adding more results. From what I can see, it is difficult to conclude on the benefits of SOP over IG, which I find really problematic. It seems to me that the most impactful result is related to the improvements brought by the ERE sampling scheme, which could probably be worth a paper on its own (by showing its benefits over a wider range of algorithms, e.g. TD3 & DQN+variants), but this would be a different paper. As a result I am sticking to *Weak Reject*."}
{"id": "iclr2020_877", "title": "Inferring Dynamical Systems with Long-Range Dependencies through Line Attractor Regularization | OpenReview", "abstract": "Abstract:###Vanilla RNN with ReLU activation have a simple structure that is amenable to systematic dynamical systems analysis and interpretation, but they suffer from the exploding vs. vanishing gradients problem. Recent attempts to retain this simplicity while alleviating the gradient problem are based on proper initialization schemes or orthogonality/unitary constraints on the RNN’s recurrency matrix, which, however, comes with limitations to its expressive power with regards to dynamical systems phenomena like chaos or multi-stability. Here, we instead suggest a regularization scheme that pushes part of the RNN’s latent subspace toward a line attractor configuration that enables long short-term memory and arbitrarily slow time scales. We show that our approach excels on a number of benchmarks like the sequential MNIST or multiplication problems, and enables reconstruction of dynamical systems which harbor widely different time scales.", "review": "Review:###Overview This paper proposes a type of regularization for recurrent networks, with the goal of encouraging particular dynamical structures (in this case, line attractors) in the dynamics of the networks. The proposed regularization penalty is only applied to a subset of the recurrent units; the motivation for this is to allow neurons not contained in the subset to learn different structures. The paper applies this regularization method on three example machine learning sequence tasks: an addition task, a multiplication task, and sequential MNIST classification, as well as on learning a 2-D dynamical system model of a bursting neuron with two different timescales. Major comments I have a number of serious concerns about the paper*s motivation, logic, and experiments: - First, the paper motivates the proposed regularization as a way to encourage the network to have line attractor dynamics. In particular, the paper dismisses gated architectures as not being interpretable, stating that LSTMs and GRUs *are complicated and tedious to analyze from a DS perspective.* (pg 2). However, there is recent work both theoretical (https://arxiv.org/abs/1906.01005) and empirical (https://arxiv.org/abs/1907.08549) that analyzes these gated architectures from a DS perspective. In particular, these papers demonstrate that LSTMs and GRUs are perfectly capable of learning line attractors. Given that it is possible to analyze gated architectures as dynamical systems, the overall motivation of the paper is much weaker. - The paper proposes a squared penalty on subsets of the weights in the recurrent network as a way to encourage line attractor dynamics. However, it is not clear to me that this is sufficient. In particular, unless the subset of the network that implements the line/plane attractor is completely disconnected from the rest of the network, then the overall dynamics may not contain a line attractor (the units will interact with the unregularized units). Also, the proposed regularization penalty only penalizes the diagonal elements of the A matrix to be close to 1--but shouldn*t the off diagonal elements also be penalized to be close to zero? - Moreover, the paper makes no mention of the Jacobian of these recurrent networks. The eigenvalues of the Jacobian of the recurrent networks determine the behavior of the linearized system around fixed points--specifically, eigenvalues with real part close to 1 will exhibit slow dynamics (approximate line attractors along those dimensions). It seems to me that a much more natural way of encouraging line attractor dynamics is to place a regularization penalty on the Jacobian itself (which is analytically tricky, but numerically more plausible with modern autodifferentiation software). Regardless, the authors should compare the eigenvalues of the recurrent networks* Jacobian when using their regularization method vs without it. Does the proposed regularization encourage the Jacobian of the resulting networks to have eigenvalues close to 1? - The paper compares the proposed method with a number of vanilla RNNs with different initializations, and an LSTM. However, a critical missing baseline is simply an RNN with l2 regularization on the weights (standard regularization in the literature). This baseline is important to determine if the proposed regularization simply helps because it is an l2 penalty on the weights (note that none of the other baselines have regularization). - The paper motivates the method as trying to study line attractor dynamics, but then does not apply them to tasks where line attractors are required. For example, the addition and multiplication tasks require discrete memories, not line attractors. The bursting neuron approximation (2D dynamical system) also does not involve a line attractor. However, there definitely exist tasks both in neuroscience (e.g. sensory integration in decision making, path integration in navigation, etc.) and in machine learning (c.f. https://arxiv.org/abs/1906.10720) that use or require line attractors. The motivation of the paper would be much better tested on these tasks. Minor comments - The authors comment at the beginning of page 4 that by setting A=I, W=0, and h=0, the network contains a line attractor, but the more precise language would be to state that the network contains an N-dimensional plane attractor, where N is the number of units. Typically, *line attractor* refers to a 1-dimensional manifold of fixed points along which the system can integrate inputs, but perturbations off of the line attractor are not remembered (decay back to the line attractor)."}
{"id": "iclr2020_878", "title": "Learning Key Steps to Attack Deep Reinforcement Learning Agents | OpenReview", "abstract": "Abstract:###Deep reinforcement learning agents are known to be vulnerable to adversarial attacks. In particular, recent studies have shown that attacking a few key steps is effective for decreasing the agent*s cumulative reward. However, all existing attacking methods find those key steps with human-designed heuristics, and it is not clear how more effective key steps can be identified. This paper introduces a novel reinforcement learning framework that learns more effective key steps through interacting with the agent. The proposed framework does not require any human heuristics nor knowledge, and can be flexibly coupled with any white-box or black-box adversarial attack scenarios. Experiments on benchmark Atari games across different scenarios demonstrate that the proposed framework is superior to existing methods for identifying more effective key steps.", "review": " Learning Key Steps to Attack Deep Reinforcement Learning Agents This paper proposes an extension of existing discrete action image space adversarial attack algorithms. Instead of choosing the steps by heuristic, the authors propose to choose the key steps by augmenting the reward function with a penalty to decrease the ratio of attacks. I tend to vote rejection for this paper, given that the proposed algorithms seem incremental compared to the existing algorithms, and the experiments seem not sufficient enough to support the core claim proposed in the paper. Pros: - The paper is well written, with sufficient background and related work section for the paper to be self-contained. - The proposed framework is an interesting and practical framework for attacking RL agents. Cons: - The experiment section is insufficient. More specifically: 1) Results from Figure 5 and Figure 6 seem to disagree with what the authors claim in the paper. In many (the majority) of the environments, the proposed algorithm has only trivial improvement and even worse performance under the same attack rate. 2) It is not very convincing when only one result sample is plotted in Figure 5 and Figure 6. I think it is necessary to show the performance of the proposed algorithm under different attack rate. A wide range of candidate penalty parameter lambda should be tested, so that a curve can be fitted for the proposed algorithm similar to the baselines (similar to the one shown in Table 1, but with much more test values). 3) Related to 2), it seems the Lagrange relaxation makes it hard to control the attack rate in the proposed algorithms. How sensitive it is to control the attack rate? 3) Can the authors elaborate on why the algorithms is not too sensitive to the value of penalty in section 4.5? Table 1, where the performance is almost the same for different penalty parameter, does not necessarily show that the algorithms is not too sensitive to the choice of the penalty parameter. As mentioned by the author, -21 is the minimum reward (or random reward) an agent can get from Pong. In general, given the current status of the paper, where there is a lot of room for improvement of experiment section, I will vote for a rejection."}
{"id": "iclr2020_879", "title": "Implicit Rugosity Regularization via Data Augmentation | OpenReview", "abstract": "Abstract:###Deep (neural) networks have been applied productively in a wide range of supervised and unsupervised learning tasks. Unlike classical machine learning algorithms, deep networks typically operate in the overparameterized regime, where the number of parameters is larger than the number of training data points. Consequently, understanding the generalization properties and the role of (explicit or implicit) regularization in these networks is of great importance. In this work, we explore how the oft-used heuristic of data augmentation imposes an implicit regularization penalty of a novel measure of the rugosity or “roughness” based on the tangent Hessian of the function fit to the training data.", "review": "Review:###This paper shows (theorem 1) that data augmentation (DA) induces a reduction of rugsity on the loss function associated to the model. Here rugosity is defined as a measure of the curvature (2nd order) of the function. However, the two concepts seems to be different because the authors empirically show that directly reducing the rugosity of a network does not improve generalization (in contrast to DA). I lean to reject this paper because the contributions, even if interesting, do not lead to any new understanding of the topic. More in detail, data augmentation improves the generalization on deep learning models. This paper shows that DA induces rugosity (theorem 1), but rugosity does not improve generalization (empirically). Thus, rugosity is not responsible for generalization, which is the interesting property that we care about. The paper is well written and easy to follow, however I found the actual contribution limited because: - The definition of rugosity is an extension of (Donoho & Grimes (2003)) in which the extension is not really improving anything or used anywhere in the paper. - The Hessian-based rugosity analysis of DA is correct, but it does not help to understand the generalization performance or any other useful property of DA. Additional Comments: - In 3.4 second paragraph the authors suggest that reducing rugosity can improve generalization as DA, but later we see that this is not the case. - The entire paper seems written with the idea of using rugosity as a surrogate of DA, but at the end it does not work"}
{"id": "iclr2020_880", "title": "Mixing Up Real Samples and Adversarial Samples for Semi-Supervised Learning | OpenReview", "abstract": "Abstract:###Consistency regularization methods have shown great success in semi-supervised learning tasks. Most existing methods focus on either the local neighborhood or in-between neighborhood of training samples to enforce the consistency constraint. In this paper, we propose a novel generalized framework called Adversarial Mixup (AdvMixup), which unifies the local and in-between neighborhood approaches by defining a virtual data distribution along the paths between the training samples and adversarial samples. Experimental results on both synthetic data and benchmark datasets exhibit the benefits of AdvMixup on semi-supervised learning.", "review": "Review:###The paper proposes a consistency regularization method, called AdvMixup, for semi-supervised learning tasks. The approach enforces similar predictions between an unlabeled sample and its neighborhood samples. Each sample of the latter is generated by mixing an adversarial sample and a synthetic sample created by linearly interpolating a real sample pair. The paper is easy to follow, and the experiments on both the Cifar10 and SVHN datasets show promising results. Nevertheless, I consider the idea here is a minor modification of the approach introduced in Verma et al., 2019. Additional, the experimental section is weak in its current form. Main Remarks: 1. The proposed AdvMixup is very similar to ICT as introduced in Verma et al., 2019, where the neighborhood samples are created by interpolating a pair of real samples. In addition, the experimental results as shown in the last two columns of Table 2 for the SVHN data set indicate that the improvement of AdvMixup over the ICT method is less than 1%. Similar patterns can be seen in Table1 for the CIFAR10 dataset. In this sense, I consider the proposed AdvMixup approach has limited technical novelty and with minor improvement over the exist methods. 2. The proposed method is evaluated on two small, easy image tasks, namely CIFAER10 and SVHN, using a 13-layer CNN. Experiments on a variety of data sets such as CIFAR100, ImageNet, and MNIST and with some other network architectures such as DenseNet and wideResNet could significantly improve the quality of the paper. 3. The way the synthetic samples are created in the proposed AdvMixup approach seems a bit similar to that of the 3-fold Mixup as discussed in Guo et al. 2019 (Mixup as Locally Linear Out-Of-Manifold Regularization), where a synthetic sample is created by interpolating three real samples. Ideally, using a 3-fold Mixup as the comparison baseline for evaluating the AdvMixup makes more sense to me."}
{"id": "iclr2020_881", "title": "A Closer Look at Deep Policy Gradients | OpenReview", "abstract": "Abstract:###We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: surrogate rewards do not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the *true* gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods.", "review": " This is an interesting and important paper, it emphasizes and analyzes how policy gradient methods modify their objective functions and how this leads to training differences (and often errors w.r.t. the true objective). I have some minor comments on terminology used that I would like to see properly defined within the paper, but otherwise believe this should be accepted for its useful insights. Assorted Comments: + Maybe I simply have a difference of opinion or have misunderstood, but I am hesitant to agree that the work is comparing the surrogate *reward* function, but rather the surrogate objective. You*ll notice that in the TRPO paper, it is called a surrogate objective not a surrogate reward: https://arxiv.org/pdf/1502.05477.pdf . + I think better specification of what exactly is being plotted (pointing to an equation) or defining very concretely what is a surrogate reward or true reward (which I suspect is the objective) will make this paper much clearer. + In fact, it was a bit unclear whether the comparisons were of the sampled/observed reward function R(s,a) (provided by the environment and sampling regime) or the objective function often the advantage A(s,a) (or the surrogate objective, GAE, etc.) I assume it should be the latter, but the wording of the paper makes this a bit unclear. I suggest discussing things in terms of objectives not rewards -- unless in fact the paper does approximate reward functions in which case this should be specified in much more detail. + Also, in a lot of places it seems like there*s a mixup between rewards and returns. I think typically in the literature reward = r_t and return = V_t (sum of reward). Perhaps, in places the paper truly speaks of rewards, but from the context it seems as though it mainly refers to returns. Examples: * Evidently (since the agent attains a high reward) these estimates are sufficient to consistently improve reward* * This is in spite of the fact that our agents continually improve throughout training, and attain nowhere near the maximum reward possible on each task*"}
{"id": "iclr2020_882", "title": "Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models | OpenReview", "abstract": "Abstract:###In natural language processing, it has been observed recently that generalization could be greatly improved by finetuning a large-scale language model pretrained on a large unlabeled corpus. Despite its recent success and wide adoption, finetuning a large pretrained language model on a downstream task is prone to degenerate performance when there are only a small number of training instances available. In this paper, we introduce a new regularization technique, to which we refer as “mixout”, motivated by dropout. Mixout stochastically mixes the parameters of two models. We show that our mixout technique regularizes learning to minimize the deviation from one of the two models and that the strength of regularization adapts along the optimization trajectory. We empirically evaluate the proposed mixout and its variants on finetuning a pretrained language model on downstream tasks. More specifically, we demonstrate that the stability of finetuning and the average accuracy greatly increase when we use the proposed approach to regularize finetuning of BERT on downstream tasks in GLUE.", "review": "Review:###The authors introduce a new regularization technique for the specific task of finetuning models. It*s inspired by dropout and stochastically mixes source and target weights in order to avoid moving the parameters towards 0. The authors provide a theoretical justification as to why mixout would do useful things in the convex case and then demonstrate empirically that using it achieves good accuracies on some downstream, finetuned, non-convex-loss-utilizing tasks. Their experiments incorporate both small models with good analysis (ie, sec 4) as well as larger, real-world models (sect 5). The paper is in general well-written. I have a few concerns that I would like to see addressed: 1a. For starters, all the theoretical motivation describes a particular way in which mixout is supposed to aid in downstream tasks for the case of convex functions, but there are no experiments that match their assumptions and which demonstrate this is the actual behavior we see. It would be nice to see results which demonstrate the theory. 1b. In few of the prsented empirical experiments is it the case that the use of mixout by itself is useful. Why does mixout have to be coupled with other regularization techniques? There is little analysis given here, either empirical or theoretical. 2. Why are there only 4 GLUE tasks reported? Devlin 2018 reports on all but WNLI. 3. The choice of hyperparameters for GLUE in sect 5 is a bit misleading. Devlin 2018 chose those parameters to get the maximum scores on downstream tasks; the metric they use is max score. However, the authors want to instead discuss the average score against a set of random restarts, perhaps because the max scores using their method aren*t terribly different from the baselines. Therefore, a more extensive hyperparameter sweep should have been run for the baselines: they should have been re-tuned for the average score if that is the metric the authors wish to use. Instead, the authors only used one task, RTE, to find baseline hyperparameters."}
{"id": "iclr2020_883", "title": "Role-Wise Data Augmentation for Knowledge Distillation | OpenReview", "abstract": "Abstract:###Knowledge Distillation (KD) is a common method for transferring the ``knowledge** learned by one machine learning model (the teacher) into another model (the student), where typically, the teacher has a greater capacity (e.g., more parameters or higher bit-widths). To our knowledge, existing methods overlook the fact that although the student absorbs extra knowledge from the teacher, both models share the same input data -- and this data is the only medium by which the teacher*s knowledge can be demonstrated. Due to the difference in model capacities, the student may not benefit fully from the same data points on which the teacher is trained. On the other hand, a human teacher may demonstrate a piece of knowledge with individualized examples adapted to a particular student, for instance, in terms of her cultural background and interests. Inspired by this behavior, we design data augmentation agents with distinct roles to facilitate knowledge distillation. Our data augmentation agents generate distinct training data for the teacher and student, respectively. We focus specifically on KD when the teacher network has greater precision (bit-width) than the student network. We find empirically that specially tailored data points enable the teacher*s knowledge to be demonstrated more effectively to the student. We compare our approach with existing KD methods on training popular neural architectures and demonstrate that role-wise data augmentation improves the effectiveness of KD over strong prior approaches. The code for reproducing our results will be made publicly available.", "review": "Review:###This paper takes the idea of Population-Based Augmentation (PBA) and extends it to knowledge distillation (KD). The idea is that the ideal augmentation protocol for training-from-scratch (or in this context teacher training) may not be the best for student networks under a KD loss. I am borderline about this paper and had to pick one, so I landed on Weak Reject. On one hand, I think it’s a really neat idea to apply PBA in this context, and package it as a strage-alpha/stage-beta training procedure. However, the experimental results seem very incremental and I’m not convinced there is a genuine signal there. Experiments: Table 1 offers a great comparison of prior work, as well as the combination of prior work (II-KD). Table 2 tells me that PBA seems incremental both for the teacher and the student. Going from vanilla training to student with II-KD gets you most of the way there, and the primary contribution of this paper just gives you a slight benefit above this. It’s great that a more traditional full-precision comparison was also added in Table 4, but this table also confuses me. First, it was unclear if “vanilla training” referred to the teacher or the student. The number 74.31 is not the same as in Table 2 (74.85), so I assume this means it’s the student? If so, the student is already very close to the teacher, and this is not a great starting point for evaluating KD. The student after stage-beta also outperforms the teacher - something that was mentioned in the related work, but I would like more discussion around it specifically for Table 4. Another thing I was wondering was how important PBA is for the teacher’s ability to be a good teacher. It gives a modest boost in Table 2; what if we skip PBA in the teacher but still do it for the student. This would be interesting to add. Overall, there aren’t that many experiments. The dataset is never more challenging than CIFAR-100. There are also no error bars, which are particularly important when the improvements are small. As for Figure 3, I don’t know if there is anything intuitive we can glean from this. I may just be variation between experiments, as far as I can tell. I think this paper can be made stronger by making the experimental evidence broader, as well as the analysis of why this works stronger. Without these improvements, the reader is left wondering if there really is any significant benefit. We have to remember that PBA is not cheap (perhaps much cheaper than AutoAugment, but more expensive than fixed augmentation). For most practitioners, the complication and compute costs of PBA would probably not be worth adding on top of KD, if the benefits are too modest. Minor: In Table 3, it says *Ours* for AlexNet and *II-KD* for ResNet8. Should the both be the same?"}
{"id": "iclr2020_884", "title": "Restoration of Video Frames from a Single Blurred Image with Motion Understanding | OpenReview", "abstract": "Abstract:###We propose a novel framework to generate clean video frames from a single motion-blurred image. While a broad range of literature focuses on recovering a single image from a blurred image, in this work, we tackle a more challenging task i.e. video restoration from a blurred image. We formulate video restoration from a single blurred image as an inverse problem by setting clean image sequence and their respective motion as latent factors, and the blurred image as an observation. Our framework is based on an encoder-decoder structure with spatial transformer network modules to restore a video sequence and its underlying motion in an end-to-end manner. We design a loss function and regularizers with complementary properties to stabilize the training and analyze variant models of the proposed network. The effectiveness and transferability of our network are highlighted through a large set of experiments on two different types of datasets: camera rotation blurs generated from panorama scenes and dynamic motion blurs in high speed videos. Our code and models will be publicly available.", "review": "Review:###OVERVIEW: The authors present a framework to generate video frames from a single motion-blurred image. Their framework is based on an encoder-decoder architecture with Spatial Transformer and Image Warping layers. A main difference between their work and prior work is that they reduce dependency on predicted central frame and thus have lesser error accumulation which prior works have. They also predict the non-central frames using same decoder but with appropriate global and local transforms. They train their network using a combination of (i) photometric loss (predicted image is close to ground-truth), (ii) transformation consistency loss (transformations that lead to nearby frames are not very different) and (iii) penalty term (different predicted frames have different content). They demonstrate results on two datasets: (1) Synthetic dataset with rotation blur and (2) High-speed video dataset containing dynamic blur. COMMENTS: 1. I like the proposed approach of modeling motion via the STN and LW layers. However, I wonder if both are required simultaneously. Any arbitrary motion could in principle be modeled using the LW layers that estimate the motion flow with the caveat that predicted frames are close (to approximate a complex motion with linear estimates in delta time steps). Why is the STN then important and useful? Can it be avoided? Any empirical evidence to back up claim? 2. Table 1 seems to indicate that you get better center frame prediction also compared to prior work. Would it make sense to run their algorithm with your predicted center frame and add it as a baseline for comparison. It would emphasize to the reader that even with your predicted center frame, prior work fails in the non-center frames. 3. Will the synthetic images generated as part of the Rotation Blur dataset be made public for future evaluation? 4. Please discuss the limitations of the proposed approach specifically how much motion can be deblurred and using how many frames (maybe relative to magnitude of blur). How is the number of predicted frames determined? DECISION: I think the proposed framework to generate multiple de-blurred video frames from a single motion-blurred image is very interesting and the ability to handle arbitrary motion makes it very appealing. However, I do not know enough about the area to make a strong decision. Hence, I give WEAK ACCEPT (subject to change based on discussion)."}
{"id": "iclr2020_885", "title": "Energy-Aware Neural Architecture Optimization with Fast Splitting Steepest Descent | OpenReview", "abstract": "Abstract:###Designing energy-efficient networks is of critical importance for enabling state-of-the-art deep learning in mobile and edge settings where the computation and energy budgets are highly limited. Recently, Wu et al. (2019) framed the search of efficient neural architectures into a continuous splitting process: it iteratively splits existing neurons into multiple off-springs to achieve progressive loss minimization, thus finding novel architectures by gradually growing the neural network. However, this method was not specifically tailored for designing energy-efficient networks, and is computationally expensive on large-scale benchmarks. In this work, we substantially improve Wu et al. (2019) in two significant ways: 1) we incorporate the energy cost of splitting different neurons to better guide the splitting process, thereby discovering more energy-efficient network architectures; 2) we substantially speed up the splitting process of Wu et al. (2019), which requires expensive eigen-decomposition, by proposing a highly scalable Rayleigh-quotient stochastic gradient algorithm. Our fast algorithm allows us to reduce the computational cost of splitting to the same level of typical back-propagation updates and enables efficient implementation on GPU. Extensive empirical results show that our method can train highly accurate and energy-efficient networks on challenging datasets such as ImageNet, improving a variety of baselines, including the pruning-based methods and expert-designed architectures.", "review": "Review:###Summary: This paper builds on a recently proposed algorithm (*splitting steepest descent*, Wu et al 2019) for guiding the growth of a smaller network into a larger one in architecture search. The algorithm in Wu, et al. alternates between two steps, (i) optimization of parameters for a fixed model and (ii) modification of the architecture by identifying a subset of neurons to split into more neurons, based on the *splitting index* of each neuron (amounting to evaluating the smallest eigenvalue of a matrix). This work builds on that in two ways: (i) it incorporates an energy budget into the optimization procedure for choosing which subset of neurons to split, which it approximately solves by a continuous relaxation, and (ii) avoids doing exact eigendecomposition to extract the minimum eigenvalue (splitting index) but instead replaces it with a more efficient SGD on the Rayleigh quotient. Evaluation: --Evaluations are done on variants of MobileNet on CIFAR-100 and ImageNet (the latter would be infeasible without the approximation scheme). The proposed approach appears to get better tradeoff between accuracy and FLOPs in these cases. In practice the non-energy aware *vanilla* networks do tend towards models that are small in size (fewer parameters) but are not necessarily low in energy consumption. --There is new material here, although I find the novelty a bit limited (e.g. only an additional constraint compared to the original approach of Wu et al and addressing a clear scalability issue with the original work, i.e. eigendecomposition of a matrix, with what seem straightforward approximations, ). The empirical results in Table 1 and 2 seem solid, but I*m not familiar enough with past results in this area to evaluate their significance."}
{"id": "iclr2020_886", "title": "Branched Multi-Task Networks: Deciding What Layers To Share | OpenReview", "abstract": "Abstract:###In the context of multi-task learning, neural networks with branched architectures have often been employed to jointly tackle the tasks at hand. Such ramified networks typically start with a number of shared layers, after which different tasks branch out into their own sequence of layers. Understandably, as the number of possible network configurations is combinatorially large, deciding what layers to share and where to branch out becomes cumbersome. Prior works have either relied on ad hoc methods to determine the level of layer sharing, which is suboptimal, or utilized neural architecture search techniques to establish the network design, which is considerably expensive. In this paper, we go beyond these limitations and propose a principled approach to automatically construct branched multi-task networks, by leveraging the employed tasks* affinities. Given a specific budget, i.e. number of learnable parameters, the proposed approach generates architectures, in which shallow layers are task-agnostic, whereas deeper ones gradually grow more task-specific. Extensive experimental analysis across numerous, diverse multi-tasking datasets shows that, for a given budget, our method consistently yields networks with the highest performance, while for a certain performance threshold it requires the least amount of learnable parameters.", "review": "Review:###This paper proposes a novel soft parameter sharing Multi-task Learning framework based on a tree-like structure. The idea is interesting. However, the technique details, the experimental results and the analysis are not as attractive as the idea. The proposed method is a simple combination of existing works without any creative improvement. Furthermore, comparing with the MTL baseline, the experimental performance of proposed method does not get obvious improvement while the computation cost increasing significantly. Besides, there is not enough analysis about the idea this paper proposed. The intuition that more similar tasks share more parameters probably cannot always ensure the improvement of MTL."}
{"id": "iclr2020_887", "title": "Improving Dirichlet Prior Network for Out-of-Distribution Example Detection | OpenReview", "abstract": "Abstract:###Determining the source of uncertainties in the predictions of AI systems are important. It allows the users to act in an informative manner to improve the safety of such systems, applied to the real-world sensitive applications. Predictive uncertainties can originate from the uncertainty in model parameters, data uncertainty or due to distributional mismatch between training and test examples. While recently, significant progress has been made to improve the predictive uncertainty estimation of deep learning models, most of these approaches either conflate the distributional uncertainty with model uncertainty or data uncertainty. In contrast, the Dirichlet Prior Network (DPN) can model distributional uncertainty distinctly by parameterizing a prior Dirichlet over the predictive categorical distributions. However, their complex loss function by explicitly incorporating KL divergence between Dirichlet distributions often makes the error surface ill-suited to optimize for challenging datasets with multiple classes. In this paper, we present an improved DPN framework by proposing a novel loss function using the standard cross-entropy loss along with a regularization term to control the sharpness of the output Dirichlet distributions from the network. Our proposed loss function aims to improve the training efficiency of the DPN framework for challenging classification tasks with large number of classes. In our experiments using synthetic and real datasets, we demonstrate that our DPN models can distinguish the distributional uncertainty from other uncertainty types. Our proposed approach significantly improves DPN frameworks and outperform the existing OOD detectors on CIFAR-10 and CIFAR-100 dataset while also being able to recognize distributional uncertainty distinctly.", "review": "Review:###The paper proposes a novel loss function using the standard cross-entropy loss along with a regularization term on logits for training the Dirichlet prior network. The benefit of using Dirichlet prior network is that it can distinguish the in-domain noisy data and completely out-of-domain data. For in-domain noisy data, the Dirichlet distribution should be sharp but in the middle of the simplex, while for the OOD data, the Dirichlet distribution should be flat. The Dirichlet prior network is proposed by Malinin & Gales (2018), the new method in this paper overcomes the challenge of training the network based on the KL divergence which cannot work well for dataset with large number of classes. The paper is well written and easy to follow. Here are some comments and questions: - In the proposed loss function, could you explain what is the reason that you choose to use sigmoid(z_c(x)) instead of sum exp(z_c(x))? As you mentioned in the paper, sum exp(z_c(x)) suggests the sharpness of the distribution. Shouldn’t using sum exp(z_c(x)) be more direct than the sigmoid(z_c(x))? - The methods requires OOD dataset for training. The authors used the same OOD dataset for training and test. One concern is that what if the OOD dataset for test is not available at training. What is the alternative plan? How does that perform? - In Table 1, for Gaussian in-domain dataset, why is sum exp(z_c(x)) able to distinguish Gaussian in-domain from original in-domain images? If I understand correctly, Gusaain in-domain should have a sharp distribution which means the differential entropy (D. Ent) is small (as illustrated in Figure 2(d)), and the sum of the exponential of logits (sum exp(z_c(x))) should be large. For the original in-domain, it should also have a sharp distribution with small differential entropy and large sum of the exponential of logits. Then I don’t understand why in the table, the AUROC and AUPR for the measure sum exp(z_c(x) are very high values while that for the measure D. Ent are very low. - Could you also compute the sum of the exponential of logits for the synthetic data, since it is the only metric that is evaluated in the real data experiments but not in the synthetic data? - Could you clarify how you compute the differential entropy of the Dirichlet distribution given an input x? Did you use alpha_c = exp(z_c(x))?"}
{"id": "iclr2020_888", "title": "Towards Controllable and Interpretable Face Completion via Structure-Aware and Frequency-Oriented Attentive GANs | OpenReview", "abstract": "Abstract:###Face completion is a challenging conditional image synthesis task. This paper proposes controllable and interpretable high-resolution and fast face completion by learning generative adversarial networks (GANs) progressively from low resolution to high resolution. We present structure-aware and frequency-oriented attentive GANs. The proposed structure-aware component leverages off-the-shelf facial landmark detectors and proposes a simple yet effective method of integrating the detected landmarks in generative learning. It facilitates facial expression transfer together with facial attributes control, and helps regularize the structural consistency in progressive training. The proposed frequency-oriented attentive module (FOAM) encourages GANs to attend to only finer details in the coarse-to-fine progressive training, thus enabling progressive attention to face structures. The learned FOAMs show a strong pattern of switching its attention from low-frequency to high-frequency signals. In experiments, the proposed method is tested on the CelebA-HQ benchmark. Experiment results show that our approach outperforms state-of-the-art face completion methods. The proposed method is also fast with mean inference time of 0.54 seconds for images at 1024x1024 resolution (using a Titan Xp GPU).", "review": "Review:###This paper aims at the problem of face synthesis. The authors propose a progressive GAN with frequency-oriented attention modules for high resolution and fast controllable and interpretable face completion, which learns face structures from coarse to fine guided by the FOAM. Experiments are conducted to verify the effectiveness of the proposed method. This paper is well written and is easy to understand. 1. The most interesting idea is the frequency-oriented attention modules ,while the idea of structure-aware seems very common in the area. 2. The experiments are unconvincing. There are so many works about face synthesis in recent years. Why do the authors only compare with GL and CTX? Also, the authors do not study how each component affects the final performance, which is very important for the reader to understand why it works. 3. In general, the framework of the proposed method is very common. This paper only follow previous work and lacks new insights about the problem."}
{"id": "iclr2020_889", "title": "From Variational to Deterministic Autoencoders | OpenReview", "abstract": "Abstract:###Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules.", "review": "Review:###This paper propose an extension to deterministic autoencoders. Motivated from VAEs, the authors propose RAEs, which replace the noise injection in the encoders of VAEs with an explicit regularization term on the latent representations. As a result, the model becomes a deterministic autoencoder with a L_2 regularization on the latent representation z. To make the model generalize well, the authors also add a decoder regularization term L_REG. In addition, due to the encoder in RAE is deterministic, the authors propose several ex-post density estimation techniques for generating samples. The idea of transferring the variational to deterministic autoencoders is interesting. Also, this paper is well-written and easy to understand. However, in my opinion, this paper needs to consider more cases for autoencoders and needs more rigorous empirical and theoretical study before it can be accepted. Details are as follow: 1. The RAEs are motivated from VAEs, or actually CV-VAEs as in this paper. More precisely, the authors focus on VAEs with a constant covariance Gaussian distribution as the variational distribution and a Gaussian distribution with the identity matrix as the covariance matrix as the model likelihood. However, there might be many other settings for VAEs. For example, the model likelihood can be a Gaussian distribution with non-constant covariance, or even some other distributions (e.g. Multinomial, Bernoulli, etc). Similarly, the variational distribution can be a Gaussian distribution with non-constant covariance, or even some more complicated distributions that do not follow the mean-field assumption. Any of these more complex models may not be easily transferred to the RAE models that are mentioned in this paper. Perhaps it is better if the authors can consider RAEs for some more general VAE settings. 2. Perhaps the authors needs more empirical study, especially on the gain of RAE over CV-VAE and AE. a) As the motivated model (CV-VAE) and the most related model in the objective (AE), they are not appearing in the structured input experiment (Section 6.2). It will be great if they can be compared with in this experiment. b) The authors did not show us clearly whether the performance gain of RAE over VAE, AE and CV-VAE is due to the regularization on z (the term L_z^RAE) or the decoder regularization (the term L_REG) in the experiments. In table 1, the authors only compare the standard RAE with RAE without decoder regularization, but did not compare with RAE without the regularization on z (i.e. equivalent to AE + decoder regularization) and CV-VAE + decoder regularization. The authors would like to show that the explicit regularization on z is better than injecting the noise, hence the decoder regularization term should appear also in the baseline methods. It is totally possible that perhaps AE + decoder regularization or CV-VAE + decoder regularization perform better than RAE. c) The authors did not show how they tune the parameter sigma for CV-VAE. Since the parameter _x0008_eta in the objective of RAE is tunable, for fair comparison, the authors needs to find the best sigma for CV-VAE in order to get the conclusion that explicit regularization is better than CV-VAE. d) Although the authors mention that the 3 regularization techniques perform similarly, from Table 1, it is still hard to decide which one should we use in practice in order to get a performance at least not too much worse compared to the baseline methods. RAE-GP and RAE-L2 perform not well on CelebA while RAE-SN perform not well on MNIST, compared to the baseline methods. We know that the best performance over the 3 methods is always comparable to or better than the baselines, but not none of the single methods do. It is better if the authors can provide more suggestions on the choice for decoder regularization for different datasets. 3. The authors provided a theoretical derivation for the objective L_RAE (Equation 11), but this is only for the L_GP regularization. Besides, this derivation (in Appendix B) has multiple technique issues. For example, in the constraints in Equation 12, the authors wrote ||D_\theta(z1) - D_\theta(z2)|| < epsilon for all z1, z2 ~ q_phi(z | x), this is impossible for CV-VAE since this constraint requires D_theta() to be bounded while q_phi(z | x) in CV-VAE has an unbounded domain. Moreover, in the part (||D_\theta(z1) - D_\theta(z2)||_p=\nabla D_\theta(\tilde z)cdot ||z_1-z_2||_p) of Equation 13, \nabla D_\theta(\tilde z) is a vector well the other two terms are scalars, which does not make sense. There are many other issues as well. Please go through the proof again and solve these issues. Questions and additional feedback: 1. Can the authors provide more intuitions why do you think the explicit regularization works better compared to the noise injection? Can you provide a theoretical analysis on that? 2. Can the authors provide some additional experiments as mentioned above? Also, can the authors provide more details about how do they tune the parameters _x0008_eta and lambda? ======================================================================================================== After the rebuttal: Thanks the authors for the detailed response and the additional experiments. I agree that the additional experiment results help to support the claims from the authors, especially for the CV-VAE for the structured data experiments and the AE + L2 experiment. So I think now the authors have more facts to support that RAE is performing better compared to the baseline methods. Therefore, I agree that after the revision, the proposed method RAE is supported better empirically. So I am changing my score from *weak reject* to *weak accept*. But I still think the baseline CV-VAE + regularization is important for Table 1 and the technical issues in the theoretical analysis needs to be solved. Hope the authors can edit them in the later version."}
{"id": "iclr2020_890", "title": "Topology-Aware Pooling via Graph Attention | OpenReview", "abstract": "Abstract:###Pooling operations have shown to be effective on various tasks in computer vision and natural language processing. One challenge of performing pooling operations on graph data is the lack of locality that is not well-defined on graphs. Previous studies used global ranking methods to sample some of the important nodes, but most of them are not able to incorporate graph topology information in computing ranking scores. In this work, we propose the topology-aware pooling (TAP) layer that uses attention operators to generate ranking scores for each node by attending each node to its neighboring nodes. The ranking scores are generated locally while the selection is performed globally, which enables the pooling operation to consider topology information. To encourage better graph connectivity in the sampled graph, we propose to add a graph connectivity term to the computation of ranking scores in the TAP layer. Based on our TAP layer, we develop a network on graph data, known as the topology-aware pooling network. Experimental results on graph classification tasks demonstrate that our methods achieve consistently better performance than previous models.", "review": " This paper presented a new pooling method for learning graph-level embeddings. The key idea is to use the initial node attributes to compute all-pair attention scores for each node pair and then use these attention scores to formulate a new graph adjacency matrix beyond the original raw graph adjacency matrix. As the result, each node can average these attention score edges to compute the overall importance. Based on these scores, the method chooses top-k nodes to perform graph coarsening operation. In addition, a graph connectivity term is proposed to address the problems of isolated nodes. Experiments are performed to validate the effectiveness of the proposed method. Although I found this paper is generally well written and well motivated, there are several concerns about the novelty of paper, computational expenses, and the experimental results as listed below: 1) The idea of using attention score is simple yet seems effective. But using attention score to identify the importance of nodes has been first presented in GAT [Velic?kovic ? et al., 2019] and further studied by a lot of subsequent works. 2) In this paper, authors considered using raw node features to obtain good attention scores between each node pair. However, the effectiveness of this will heavily depend on how informative these original node features are. In the extreme case, the original node features are completely random, then the proposed method will definitely fail since there are no meaningful similarity scores that can be learnt from. Surprisingly, I did not see any discussion or any ablation study on this aspect. Also, why only using initial node attributes? How about the computed node embeddings to compute the similarity scores? 3) The pair-wise similarity scores (similarity or attention matrix) are very expensive to compute and score, which easily renders quadratic complexity in terms of the number of nodes O(N^2) for both computation and memory. This makes it scale to really large graph. In this sense, the proposed scheme is not promising in real applications. 4) The experiments are really problematic. There are no descriptions on how the graph data are split in train/val/test. Following the traditional graph kernel settings, it should be 9/1/1. Also, there are no information about how many runs are performed on each dataset. I noticed that authors basically collected all performance results from published related works except for several baselines. Different data split and number of runs could result quite different performance report, leading to apple-and-orange comparisons. 5) The experimental results are also problematic as well. For example, WL baseline has extremely large variance for all of datasets, which are completely different from the reported number from other literature [Zhang et al, neurIPS*18] and my personal experience. In general, the variance of WL could be 1/10 smaller than what are reported in this paper. Also, there is no clear explanations why the proposed method achieved quite large margin compared to G-UNet and GIN as shown in Table 1. RetGK: Graph Kernels based on Return Probabilities of Random Walks, [Zhang et al, neurIPS*18] 6) In Table 3, authors tried to show the performance difference between different pooling methods. I was so surprised about the results after I looked back to Table 1. For instance, Net_top-k (G-Unet) achieved 71.5 on PTC while GUnet achieved 64.7 on PTC in Table 2. It is really hard to believe this almost 7 points difference are just due to slight different model choices in other parts beyond the main contribution - new pooling component. I feel the whole experimental results are not convincing or at least no well explanations what*s going on here. Of course, based on very large variances shown in table 1 and 2, table 3 and table 4 are meaningless to look without seeing the variances for each ablation study."}
{"id": "iclr2020_891", "title": "Computation Reallocation for Object Detection | OpenReview", "abstract": "Abstract:###The allocation of computation resources across different feature resolutions in the backbone is a crucial issue in object detection. However, classification allocation pattern is usually adopted directly to object detection, which is proved to be sub-optimal. In order to reallocate the engaged computation resources in a more efficient way, we present CR-NAS (Computation Reallocation Neural Architecture Search) that can learn computation reallocation strategies on the target detection dataset. A two-level reallocation space is proposed for both stage and spatial reallocation. A novel hierarchical search procedure is adopted to cope with the complex search space. We apply CR-NAS to multiple backbones and achieve consistent improvements. Our CR-ResNet50 and CR-MobileNetV2 outperforms the baseline by 1.9\\% and 1.7\\% COCO AP respectively without any additional computation budget. The models discovered by CR-NAS can be easily transfered to other dataset, e.g. PASCAL VOC, and other vision tasks, e.g. instance segmentation. Our CR-NAS can be used as a plugin to improve the performance of various networks, which is demanding.", "review": "Review:###This paper describes a neural architecture search method for computation resources allocation across feature resolutions in object detection. A two level reallocation space is proposed for both stage and spatial reallocation. The experiment results have quite nice improvements on several standard data sets. This is a great, well written paper overall. The design and experiment settings are well described with details. In short, this is a perfect paper that I enjoy reading. I only have very small questions and suggestions to this paper. The paper claimed the approach is able to reallocate the engaged computation resources in a more efficient way. If I did not missing anything, the paper only shows related experiments in figure 5 and figure 6 with corresponding descriptions in 4.3.2. I hope the author could have more details in these two figures with more analysis. Personally, I think more analysis on computational effectiveness may make the paper more attractive. We all know that neural network training may not be very stable in some settings. One thing I am curious about in this paper is whether the output network architectures from different training are always the same. If they are not the same, can you compare the differences? I am also curious if the author could give some intuition of the network architecture of the final best network. In other words, we want to know why the final network is better than other networks. I read the Figure 4, Table 5 and Table 6, but I really cannot understand why those networks are that *good*. Maybe, we can find some clues by answering the last paragraph. Following the last question, we also find out that the same NAS algorithm produces different networks on different data sets. Is it because of the data set settings, or because of the content of the data sets or because of network randomness? What are your intuitions? A detailed question in 3.2.2. Why you only modify the second 3x3 conv in ResNest BasicBlock and only modify the center 3x3 conv in ResNet Bottleneck. Does the hyperparameter K in 3.3.2 mater a lot (like 4 or 5)? The 4.2.2 *transfer-ability verification* is a very nice section. Do you train NAS on VOC or only a fixed network architecture on VOC? If you did both, what is the performance difference?"}
{"id": "iclr2020_892", "title": "TriMap: Large-scale Dimensionality Reduction Using Triplets | OpenReview", "abstract": "Abstract:###We introduce ``TriMap**; a dimensionality reduction technique based on triplet constraints that preserves the global accuracy of the data better than the other commonly used methods such as t-SNE, LargeVis, and UMAP. To quantify the global accuracy, we introduce a score which roughly reflects the relative placement of the clusters rather than the individual points. We empirically show the excellent performance of TriMap on a large variety of datasets in terms of the quality of the embedding as well as the runtime. On our performance benchmarks, TriMap easily scales to millions of points without depleting the memory and clearly outperforms t-SNE, LargeVis, and UMAP in terms of runtime.", "review": "Review:###Authors introduce TriMap based on triplet constraints that preserves the global accuracy of the data. A measure of global accuracy is proposed to reflect the global accuracy of the embedding. Experiments on various datasets the better performance than baselines. Authors define the minimum reconstruction error from the embedding as the global measure in reflecting the global structure of the data similar to PCA. From the definition, this measure has preferences to the linear projection model such as PCA, so the score becomes lower for non-linear projection methods such as t-SNE and UMAP. The illustrated S-shape example in Figure 1 somehow demonstrate the difference of the proposed method with PCA, t-SNE and UMAP, but the usage of the embedding is not clear since Figure 1(d) looks like a 2-d visualizing of the original 3-d data visualized from certain angle. In addition, the initialization of the proposed method is the PCA method, which prefers the GS measure. It is interesting to see how the GS measure will change if the random initialization is used. Authors demonstrate GS and AUS for all the tested data. It might be interesting and more important to see how to get the better embedding with a balanced score for a given data since GS and AUC seems always opposite measures. The TriMap method defines the loss of triplet based on unnormalized weighting schema and the weights are adjusted by applying a non-linear transformation that emphasizes the small weights. These formulations are quite heuristic and constructive. It is better to have some formal explanation on the proposed method."}
{"id": "iclr2020_893", "title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers | OpenReview", "abstract": "Abstract:###We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly ?nd the optimal network parameters and sparse network structure in a uni?ed optimization process with trainable pruning thresholds. These thresholds can have ?ne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and ef?ciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.", "review": "Review:###This paper presents a novel network pruning algorithm -- Dynamic Sparse Training. It aims at jointly finding the optimal network parameters and sparse network structure in a unified optimization process with trainable pruning thresholds. The experiments on MNIST, and cifar-10 show that proposed model can find sparse neural network models, but unfortunately with little performance loss. The key limitation of the proposed model come from the experiments. (1) Nowadays, the nature and important question is that, one can not tolerate the degraded performance, even with sparse neural network. Thus, it is important to show that the proposed model can find sparse neural network models, and with increased performance. (2) Another weakness is that proposed model has to be tested on large scale dataset, e.g. ImageNet-2012.current two datasets are too small to support the conclusive results of this proposed model. (3) As for the model itself, I donot find very significant novelty. For example, Sec. 3.3 (TRAINABLE MASKED LAYERS) in general is quite following previous works’ designing principle. Thus, the novelty should be summarized, and highlighted in the paper."}
{"id": "iclr2020_894", "title": "Disentangled GANs for Controllable Generation of High-Resolution Images | OpenReview", "abstract": "Abstract:###Generative adversarial networks (GANs) have achieved great success at generating realistic samples. However, achieving disentangled and controllable generation still remains challenging for GANs, especially in the high-resolution image domain. Motivated by this, we introduce AC-StyleGAN, a combination of AC-GAN and StyleGAN, for demonstrating that the controllable generation of high-resolution images is possible with sufficient supervision. More importantly, only using 5% of the labelled data significantly improves the disentanglement quality. Inspired by the observed separation of fine and coarse styles in StyleGAN, we then extend AC-StyleGAN to a new image-to-image model called FC-StyleGAN for semantic manipulation of fine-grained factors in a high-resolution image. In experiments, we show that FC-StyleGAN performs well in only controlling fine-grained factors, with the use of instance normalization, and also demonstrate its good generalization ability to unseen images. Finally, we create two new datasets -- Falcor3D and Isaac3D with higher resolution, more photorealism, and richer variation, as compared to existing disentanglement datasets.", "review": " === A. Summary === This paper proposes to train a new conditional GAN model that allows for controllable image generation by changing the input factors of variations (e.g. object color). The supervised labels for the controllable attributes are obtained from a 3D renderer. That is, the work combines the recent StyleGAN (that learns to generate images with disentangle latent vectors in an unsupervised manner) with AC-GAN (a clas-conditional GAN but here class information is replaced by the attribute information that we want to control). The resultant AC-StyleGAN has essentially two latent vectors, one trained unsupervised and one trained with supervised labels. The proposed GANs were thoroughly tested with different factors of variations (lighting, camera, objects) and on two different datasets self-contructed via 3D renderer. The work is a solid demonstration that GANs can be used to synthesize images with fine-grained and coarse controllability if we have supervision signals! The authors also released the anonymous code (which is a plus!). === B. Decision === Weak Reject. I voted for Weak Reject because this paper presents a fairly incremental advance over what has been done (e.g. HoloGAN, StyleGAN, AC-GAN). The impact of the unsupervised work (e.g. HoloGAN or StyleGAN) is much higher since they are generally applicable to real data without labels. Here, although reasonable, the demonstration was done on a relatively small-scale 3D synthetic data (where there is only one scene and one object being manipulated). Therefore, the claim that only 5% of supervised labels is required may not carry over to larger-scaled datasets with larger scene variability. Plus, I don*t see any baseline whatsoever being compared with the proposed methods here. === C. Suggestions for Improvement === I have no problems with the novelty or idea of the work. For me, the key problem with this work is the low impact or significance. Some suggestions for showing the impact of this work: - Show how your pre-trained GANs can be fine-tuned or transferred to the real data where we don*t have labels. - You could also plug in your pre-trained GANs to a separate synthetic-to-real image translation model to show that we could indeed learn to control these factors of variations of the real images. Hopefully, would the above setups yield better results than HoloGAN or StyleGAN? - Clarify the main focus points of this paper and try to substantiate the result. The author wrote *Our work extends the above works by scaling up the disentanglement learning to high- resolution images, and emphasizing the importance of supervision in controllable generation.* <---- but (1) high-res images here are synthetic and limited in scene variability; (2) the second part is expected given previous work."}
{"id": "iclr2020_895", "title": "Unifying Graph Convolutional Neural Networks and Label Propagation | OpenReview", "abstract": "Abstract:###Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs. Both solve the task of node classification but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. However, while conceptually similar, theoretical relation between LPA and GCN has not yet been investigated. Here we study the relationship between LPA and GCN in terms of two aspects: (1) feature/label smoothing where we analyze how the feature/label of one node are spread over its neighbors; And, (2) feature/label influence of how much the initial feature/label of one node influences the final feature/label of another node. Based on our theoretical analysis, we propose an end-to-end model that unifies GCN and LPA for node classification. In our unified model, edge weights are learnable, and the LPA serves as regularization to assist the GCN in learning proper edge weights that lead to improved classification performance. Our model can also be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models. In a number of experiments on real-world graphs, our model shows superiority over state-of-the-art GCN-based methods in terms of node classification accuracy.", "review": "Review:###This paper introduces a unified model which combines label propagation algorithm (LPA) and graph convolutional networks (GCNs) for node classification. The motivation of this combination is supported by two analysis on the feature/label smoothing and feature/label influence. The proposed GCN-LPA framework utilizeds LPA to adjust the edge weight A* through the label information. Then, this edge weight A* is used to transfer the knowledge from label information to feature information for enhancing the representation learning in GCN. An end-to-end solution is proposed by treating the LPA process as regularization. Overall, the idea of unifying GCNs and LAP in an end-to-end fashion is interesting. One major concern is that from the experiment, it is unclear how much the LPA impacts the node classification. It will be more convincing if the performance comparison under different percentage of labeled samples (during LPA) is provided."}
{"id": "iclr2020_896", "title": "MANIFOLD FORESTS: CLOSING THE GAP ON NEURAL NETWORKS | OpenReview", "abstract": "Abstract:###Decision forests (DF), in particular random forests and gradient boosting trees, have demonstrated state-of-the-art accuracy compared to other methods in many supervised learning scenarios. In particular, DFs dominate other methods in tabular data, that is, when the feature space is unstructured, so that the signal is invariant to permuting feature indices. However, in structured data lying on a manifold---such as images, text, and speech---neural nets (NN) tend to outperform DFs. We conjecture that at least part of the reason for this is that the input to NN is not simply the feature magnitudes, but also their indices (for example, the convolution operation uses ``feature locality). In contrast, naive DF implementations fail to explicitly consider feature indices. A recently proposed DF approach demonstrates that DFs, for each node, implicitly sample a random matrix from some specific distribution. Here, we build on that to show that one can choose distributions in a manifold aware fashion. For example, for image classification, rather than randomly selecting pixels, one can randomly select contiguous patches. We demonstrate the empirical performance of data living on three different manifolds: images, time-series, and a torus. In all three cases, our Manifold Forest (Mf) algorithm empirically dominates other state-of-the-art approaches that ignore feature space structure, achieving a lower classification error on all sample sizes. This dominance extends to the MNIST data set as well. Moreover, both training and test time is significantly faster for manifold forests as compared to deep nets. This approach, therefore, has promise to enable DFs and other machine learning methods to close the gap with deep nets on manifold-valued data.", "review": " This paper proposed a new method called manifold forest to improve decision forest (DF) classification results. It is motivated by that, natural data is often in some manifold but not randomly distributed. It showed how to use the 2D spatial structures of natural images by constructing structured atoms. Results on 3 toy examples and MNIST showed the better performance than standard RF and SPORF. Overall, the paper is easy to follow and well written. The idea is intuitive: using structured 2D information to improve the classification results. But there are some issues with the implemetation. 1. In image classification, we definitely need 2D structure information. This is normally extracted by the descriptors such as SIFT, GIST, where the 2D information has been included. It is rare to use the pixel values as the features directly for classification. In this case, the benefit of the proposed method is very weak. This is the main issue of the paper. No results on real features. 2. The results are weak too. The real data is MNIST, which is also a very toy dataset. It would be good to include some real world image dataset, such as CIFAR, ImageNet etc. 3. The algorithm is somehow incremental compared with SPORF."}
{"id": "iclr2020_897", "title": "New Loss Functions for Fast Maximum Inner Product Search | OpenReview", "abstract": "Abstract:###Quantization based methods are popular for solving large scale maximum inner product search problems. However, in most traditional quantization works, the objective is to minimize the reconstruction error for datapoints to be searched. In this work, we focus directly on minimizing error in inner product approximation and derive a new class of quantization loss functions. One key aspect of the new loss functions is that we weight the error term based on the value of the inner product, giving more importance to pairs of queries and datapoints whose inner products are high. We provide theoretical grounding to the new quantization loss function, which is simple, intuitive and able to work with a variety of quantization techniques, including binary quantization and product quantization. We conduct experiments on public benchmarking datasets url{http://ann-benchmarks.com} to demonstrate that our method using the new objective outperforms other state-of-the-art methods. We are committed to release our source code.", "review": " The paper proposes new loss functions for quantization when the task of interest is maximum inner product search (MIPS). The paper is well written with clear descriptions, fairly comprehensive analysis and empirical exploration, and good results, and in general I agree that learning quantization so as to minimize quantization related errors on task at hand is a good strategy. Specific comments and suggestions for strengthening the paper are: a) The proposed loss function in (2) includes a weight function that serves as a proxy for the task objective of giving more emphasis to quantization errors on samples with larger inner product. Instead, why not use the true task objective which for the MIPS task is stated in the Introduction section? If this was considered please comment on reasons for not including / discussing this in the paper, otherwise perhaps this’ll be good to discuss. b) Did the authors consider using a task dependent training data set which will capture both ‘q’ and ‘x’ distributions and potentially lead to even further improved quantization? This has the disadvantage of making quantization dependent on query distribution, but in cases where such data is available it will be very valuable to know if incorporating data distributions in quantization process helps performance and to what extent. c) It will also be valuable to consider the closely related task of cosine distance based retrieval and comment on how that impacts the modifications of loss functions. d) The idea of learning quantization under objective of interest using observed data distribution has been studied earlier (e.g. see Marcheret et al., “Optimal quantization and bit allocation for compressing large discriminative feature space transforms,” ASRU 2009), perhaps worth citing as related work."}
{"id": "iclr2020_898", "title": "Capsule Networks without Routing Procedures | OpenReview", "abstract": "Abstract:###We propose Pure CapsNets (P-CapsNets) without routing procedures. Specifically, we make three modifications to CapsNets. First, we remove routing procedures from CapsNets based on the observation that the coupling coefficients can be learned implicitly. Second, we replace the convolutional layers in CapsNets to improve efficiency. Third, we package the capsules into rank-3 tensors to further improve efficiency. The experiment shows that P-CapsNets achieve better performance than CapsNets with varied routine procedures by using significantly fewer parameters on MNIST&CIFAR10. The high efficiency of P-CapsNets is even comparable to some deep compressing models. For example, we achieve more than 99% percent accuracy on MNIST by using only 3888 parameters. We visualize the capsules as well as the corresponding correlation matrix to show a possible way of initializing CapsNets in the future. We also explore the adversarial robustness of P-CapsNets compared to CNNs.", "review": " Authors argue that the routing mechanism is unnecessary in a capsnet and one can just simply aggregate. They scale the matrix product introduced in Hinton 2018 to rank 3 tensors to decrease the number of parameters. Furthermore, they eliminate the free-form initial convolution layers. The motivational derivation on equation 3 is simply wrong. c_ij is not a scalar or a learnt parameter. It is a function of c_ik at previous iteration for all other k. equation 3 would have been correct for a scalar or a learnt parameter but not a function of other parameters that just multiplication with W will not bring into the equation. Removing c_ij all together reduces capsule networks into simple CNNs, because there is no notion of agreement (cluster finding, attention) anymore. It is just a CNN with more weight sharing now. Also arguing that since number of parameters involved is jut 7 percent it is not important is just nonsense. The importance of a parameter is not based on the quantity. Several routing iterations may indeed be unnecessary, but a grouping mechanism need to still validate if the votes agree or not or it will be simple pattern matching like normal CNNs. CapsNets where proposed as a network that have comparable classification accuracy with CNNs with the extra benefit of the transformation generalization. If the classification accuracy was the primary goal one would have just used ResNets or etc. There is complete ignorance to the main point of CapsNets (viewpoint generalization) throughout the paper. No experiments showing any viewpoint generalizability of the current proposed network. As for Adversarial Robustness of EMCapsNet, they also have fewer parameters incompare to the baseline CNN. I would argue the white box rebustness exactly comes from the aggregation method (here essentially an xor function is replaced with addition)."}
{"id": "iclr2020_899", "title": "Explaining A Black-box By Using A Deep Variational Information Bottleneck Approach | OpenReview", "abstract": "Abstract:###Interpretable machine learning has gained much attention recently. Briefness and comprehensiveness are necessary in order to provide a large amount of information concisely when explaining a black-box decision system. However, existing interpretable machine learning methods fail to consider briefness and comprehensiveness simultaneously, leading to redundant explanations. We propose the variational information bottleneck for interpretation, VIBI, a system-agnostic interpretable method that provides a brief but comprehensive explanation. VIBI adopts an information theoretic principle, information bottleneck principle, as a criterion for finding such explanations. For each instance, VIBI selects key features that are maximally compressed about an input (briefness), and informative about a decision made by a black-box system on that input (comprehensive). We evaluate VIBI on three datasets and compare with state-of-the-art interpretable machine learning methods in terms of both interpretability and fidelity evaluated by human and quantitative metrics.", "review": "Review:###*an information theoretic principle, information bottleneck principle* in the abstract is quite redundant with the use of *principle* twice **great, great* and *great, thought provoking*. They have the same level of sparsity.* What kind of sparsity are you referring to with this example? Why can*t sparsity reduce semantic redundancy? Please explain further. *However, the first explanation has a large MI with the input document where *great* occurs a lot.* What example input document are you referring to? You should save the explanation of how your method in Equation 2 differs from the original information bottleneck of Equation 1 until after you have actually written out Equation 2. As it is now, you are referencing Equation 2 before it has been seen. I find Equation 2 confusing. Is it possible to make the dependence of the expression on z more explicit. It isn*t clear from the equation itself how p(z|x) influences either quantity in Equation 2. Perhaps you should wait to introduce this equation until you have first explained how z relates to x and t. As it is, z is not clearly defined. I can gather information about it from the figure, from how you describe the difference in your method from the original information bottleneck, but the relationship is not clear enough by reading only the text of the paper before Equation 2 is presented. Can you explain briefly how your *hierarchical LSTM* works in the main text of the paper? Its an unusual enough term that I would want to see a citation or brief explanation right away rather than having it deferred to the Appendix. Why not use a state-of-the-art model for IMDb? Are you not using the standard splits for IMDb? the In Appendix B.1 *output vector is averaged and followed by log-softmax calculation. The final layer is formed to return a log-probability indicating which cognitive chunks should be taken as an input to the approximator* The single output vector of the biLSTM is averaged and followed by log-softmax? the final layer is formed? What does this mean? I find the phrasing of *Negative Sentiment if any negative words* and the corresponding title for positive in Fig 2 confusing. What do you mean by *if any*? The phrasing makes it sound like the prediction of the model somehow depends on a logical step based on whether there are any negative/positive words found. I find the lack of a comparison to some kind of attentional method somewhat glaring in the IMDb example, since I would expect that many classifiers with attention would simply attend to the same words. What does your method give us that attention would not? The same can be said for the MNIST example regarding an attention map. *by the human intelligences* sounds quite robotic Can you provide some sense of inter annotator agreement for labeling the images and sentences? It does seem that there is key information like the definition of approximator fidelity in the appendices which is crucial to actually understanding the paper."}
{"id": "iclr2020_900", "title": "Is Deep Reinforcement Learning Really Superhuman on Atari? Leveling the playing field | OpenReview", "abstract": "Abstract:###Consistent and reproducible evaluation of Deep Reinforcement Learning (DRL) is not straightforward. In the Arcade Learning Environment (ALE), small changes in environment parameters such as stochasticity or the maximum allowed play time can lead to very different performance. In this work, we discuss the difficulties of comparing different agents trained on ALE. In order to take a step further towards reproducible and comparable DRL, we introduce SABER, a Standardized Atari BEnchmark for general Reinforcement learning algorithms. Our methodology extends previous recommendations and contains a complete set of environment parameters as well as train and test procedures. We then use SABER to evaluate the current state of the art, Rainbow. Furthermore, we introduce a human world records baseline, and argue that previous claims of expert or superhuman performance of DRL might not be accurate. Finally, we propose Rainbow-IQN by extending Rainbow with Implicit Quantile Networks (IQN) leading to new state-of-the-art performance. Source code is available for reproducibility.", "review": "Review:###The paper proposes an extension to the work of Machado et al. (2018) for standardizing training and evaluation procedures in the Arcade Learning Environment (ALE). It then introduces a collection of human world records for each Atari game to refute previous claims of superhuman performance, as well as recommend comparisons against these records. They proceed to evaluate Rainbow under their proposed evaluation procedures, as well as introduce a new algorithm, Rainbow-IQN, with similar evaluations made based on their proposal. I*m proposing a weak rejection as I feel some of the arguments made in the paper aren*t very strong. In particular, I*d like the authors to comment on the following: 1) The key difference between their evaluation benchmark and the recommendations in Machado et al. (2018) are that episodes should not have a time limit. The justification for this is that many algorithms might achieve practically optimal performance within this time limit, and so one wouldn*t be able to compare algorithms on certain games within significance. They further emphasize that human high scores were achieved without limiting to 30 minutes of play. That said, several algorithms performing similarly within said limit can be instead interpreted as shifting emphasis toward comparing performance on the harder games. As the paper acknowledged, removing the maximum episode length ended up introducing more issues, such as the emulator never ending an episode (due to a supposed bug), as well as increasing the likelihood of the score overflowing. The paper suggested a trick of limiting how long an agent can go without receiving a reward, but it*s unclear (1) if needing this fix is worth the proposed change, and (2) if the fix introduces additional game-specific nuances in evaluation; e.g., are there any situations where this can be detrimental to properly evaluating performance, or introduce biases based on a game*s reward distribution? 2) The paper gathered a list of human world records for the Atari games in the ALE. In my opinion, this is very valuable for the literature in terms of addressing prior work misrepresenting the competency of an algorithm relative to what humans are capable of; a professional game tester is supposed to be representative of the average game player, who is typically tasked with optimizing fun, whereas speedrunners and scorerunners of games are tasked with optimizing a comparable objective to an RL agent. Beyond this though, I think an alternative conclusion would be to use this information in support of not comparing results to human scores, and to focus on comparisons between algorithms. A considerable number of the human world records have reached the maximum allowable score, over drastically variable gameplay times to achieve these scores, that it might still not be that fair a comparison. Have the authors considered this possibility? 3) Were any other algorithms evaluated on this benchmark, beyond Rainbow? While there are computational considerations, it seems odd for a benchmarking-focused paper to only evaluate one standard algorithm and slight modification of it. Suggestions 1) The introduction of Rainbow-IQN in this paper feels a little random and out of place given the context created by the rest of the paper*s contributions- I feel it might be more appropriate for a benchmarking paper to focus on a representative set of *standard* or relatively simple/trivial algorithms (Like Machado et al. (2018) did) to give a frame of reference for comparing novel ones."}
{"id": "iclr2020_901", "title": "On Predictive Information Sub-optimality of RNNs | OpenReview", "abstract": "Abstract:###Certain biological neurons demonstrate a remarkable capability to optimally compress the history of sensory inputs while being maximally informative about the future. In this work, we investigate if the same can be said of artificial neurons in recurrent neural networks (RNNs) trained with maximum likelihood. In experiments on two datasets, restorative Brownian motion and a hand-drawn sketch dataset, we find that RNNs are sub-optimal in the information plane. Instead of optimally compressing past information, they extract additional information that is not relevant for predicting the future. Overcoming this limitation may require alternative training procedures and architectures, or objectives beyond maximum likelihood estimation.", "review": "Review:###This manuscript shows that good ability to compress past information in RNNs help them to predict the future, and that improving upon this ability leads to more useful RNNs. The manuscript first adapts modern mutual-information estimators to mini-batch settings in order to measure the information that an RNN has on the past. It then considers stochastic training, adding Gaussian noise to the hidden states during the training of the RNNs to limit past information. A significant section is dedicated to an empirical study that shows that classically-train MLE RNNs lead to internal representations with a suboptimal mutual-information to the past and the future. For LSTM and GRU architecture, stochastic training actually significantly helps. Experiments on applications such as synthetizing hand-drawn sketches suggest that stochastic training leads to more useful RNNs. This work has interesting observations and makes a credible case. The stochastic training does seem useful. However, I would like to understand better how it connects to the set of publications discussing dropout in RNNs. It is already known that stochastic perturbations during training help. In addition, the way stochastic training is introduced in this paper make it seem a bit contradictory with the fact that it actually helps generalization. I have the feeling that the benefit that is not understood and that intuitions that arise from the manuscript may not be as useful as we would like."}
{"id": "iclr2020_902", "title": "Adaptive Generation of Unrestricted Adversarial Inputs | OpenReview", "abstract": "Abstract:###Neural networks are vulnerable to adversarially-constructed perturbations of their inputs. Most research so far has considered perturbations of a fixed magnitude under some norm. Although studying these attacks is valuable, there has been increasing interest in the construction of—and robustness to—unrestricted attacks, which are not constrained to a small and rather artificial subset of all possible adversarial inputs. We introduce a novel algorithm for generating such unrestricted adversarial inputs which, unlike prior work, is adaptive: it is able to tune its attacks to the classifier being targeted. It also offers a 400–2,000× speedup over the existing state of the art. We demonstrate our approach by generating unrestricted adversarial inputs that fool classifiers robust to perturbation-based attacks. We also show that, by virtue of being adaptive and unrestricted, our attack is able to bypass adversarial training against it.", "review": "Review:###The paper proposes using GANs to generate unrestricted adversarial examples. They seek to generate examples that are adversarial for a specific classifier, and they do so by using class-conditional GANs and a fine-tuning loss. The fine-tuning loss consists of both the ordinary GAN loss (to fool the discriminator) as well as an adversarial loss (which rewards the GAN for generating examples misclassified by the specific classifier). The authors perform various experiments on their generated examples to check for realism and how adversarial the generated images are. I would reject this paper for two key reasons. First, I feel that the contributions are not significant enough (in comparison to the prior work of Song et. al). Second, I feel that some of the methods (and some of the writing) are not too principled. In my opinion, unrestricted adversarial examples are significant if they can be made to be realistic. If our current deep learning models often mislabeled very realistic images, that would properly expose a big failure mode of our current models. However, if our machine learning models perform poorly on images that look fake/generated 40% of the time (which is what the authors state) and don’t look too realistic to humans, it is less worrying. In comparison to Song et. al, the authors state that their methods result in very similar results in terms of realism and how adversarial their images are (arguably, Song et. al actually produces better results in terms of being adversarial). In my opinion, the authors’ claimed improvements are not significant enough, because I think realism should be the primary metric to evaluate this field. Improving speed of generation is nice, and being able to bypass a simple adversarial training procedure is interesting but not significant unless this insight is expanded upon. The results on MNIST in Fig. 5 and Fig. 6 are not too convincing, as simpler attacks that generate (arguably) more realistic images like translations and rotations [1] or L1/L2 attacks [2] (since the networks are trained for L_inf robustness) can also degrade accuracy. Finally, I can also think of another reasonable baseline that I would have liked to see the authors compare their method against. Because the authors want to attack a specific network, they could have (1) generated realistic images using a pre-trained GAN (2) used a norm-bounded attack on the specific classifier and the generated GAN images. These images could be even more realistic if the norm-bound of the attack is fairly small, and would still be able to attack specific classifiers. Finally, I am confused by the comparison to a not-fine-tuned GAN in Fig. 14/Fig. 15 and would appreciate a clarification so that I can understand the results. For example, what does it mean for intended true label = 9, target label = 0 to have 90% success in Fig. 15? Does this mean that when you try to generate a 9 with the GAN, the classifier misclassifies it as a 0 90% of the time? In particular, I’m struggling to understand what the target label is for the case of the not-fine-tuned GAN. Secondly, I feel that there are many instances in the paper where the methods used are not explained in a principled way. For example, one of the key parts of this work is the fine-tuning loss function. Why does the loss function involve multiplying the ordinary GAN loss (with some additional transformation applied to it which seems unnecessary) with the adversarial loss? It seems most reasonable add the adversarial loss and the ordinary GAN loss (without the additional transformation). Is the stochastic loss selection procedure necessary? If all these peculiarities of the method are necessary, it seems that the success of this method is quite brittle. Additional feedback: - In the intro, I think citing [3] in addition to Xu et. al is more appropriate. - You should refer to Figure 1 somewhere in the text of your work - In section 3.2, you can use “cosine similarity” to describe what you are doing faster. - When you talk about “global optima of realistic adversarial examples” and “local optimal of unrealistic adversarial examples,” it sounds weird. I would try to reword this because I don’t think you are trying to make a precise mathematical statement but it sounds like one when you write it this way. - In Table 1, I would format the numbers better to be vertically aligned - You should provide a citation for MixTrain on page 5 [1] https://arxiv.org/abs/1712.02779, [2] https://arxiv.org/abs/1905.01034 [3] https://arxiv.org/abs/1802.00420"}
{"id": "iclr2020_903", "title": "Black-Box Adversarial Attack with Transferable Model-based Embedding | OpenReview", "abstract": "Abstract:###We present a new method for black-box adversarial attack. Unlike previous methods that combined transfer-based and scored-based methods by using the gradient or initialization of a surrogate white-box model, this new method tries to learn a low-dimensional embedding using a pretrained model, and then performs efficient search within the embedding space to attack an unknown target network. The method produces adversarial perturbations with high level semantic patterns that are easily transferable. We show that this approach can greatly improve the query efficiency of black-box adversarial attack across different target network architectures. We evaluate our approach on MNIST, ImageNet and Google Cloud Vision API, resulting in a significant reduction on the number of queries. We also attack adversarially defended networks on CIFAR10 and ImageNet, where our method not only reduces the number of queries, but also improves the attack success rate.", "review": "Review:###This paper proposes a new black-box adversarial attack method called TREMBA, in which the search for the “adversary” is done in a reduced space z. Summary of its contributions: - A attack method that improves query efficiency of black-box attack - Produces perturbations that are effective across different networks - Improves attack success over SOTA defended networks In general, the paper is very well written, with clear mostly clear exposition and sufficient experimental verification. What follows are the itemized pros and cons (mostly just points that would be good to address): [pros]: - Well written - A good overview of previous methods and how the TREMBA fits within them - Sufficient experimental validation [points to address] - In Black-Box Attack method of related works, did you mean to say, “Targeted attack is much harder than un-targeted attack for transfer-based method.”? - You ought to explain what NES is - Natural Evolution Strategies – and the general description of the method, as it is a major part of your algorithm (Section 3.2). It took two papers to find what NES stands for. - In section 3.2 you write – “The sign function provides an approximation of the gradient, …” – is there a citation that should go with this claim? - Make sure to explain all of the variables in the paper, e.g. in Eq. (3) or in Eq. (4). - IS there a particular reason you chose the hinge loss to train the generator? Could you have used other losses instead? - “A higher value of laeds to higher transferability to other models” – maybe a citation required? Or else more intuition? - Adding specifics about ConvNet1, ConvNet2 - How did you set ? - Would changing the sample size for each method improve the performance for respective methods? - It would have been interesting to include a Future Works section - A more thorough discussion why the models works so well."}
{"id": "iclr2020_904", "title": "Compositional Transfer in Hierarchical Reinforcement Learning | OpenReview", "abstract": "Abstract:###The successful application of flexible, general learning algorithms to real-world robotics applications is often limited by their poor data-efficiency. To address the challenge, domains with more than one dominant task of interest encourage the sharing of information across tasks to limit required experiment time. To this end, we investigate compositional inductive biases in the form of hierarchical policies as a mechanism for knowledge transfer across tasks in reinforcement learning (RL). We demonstrate that this type of hierarchy enables positive transfer while mitigating negative interference. Furthermore, we demonstrate the benefits of additional incentives to efficiently decompose task solutions. Our experiments show that these incentives are naturally given in multitask learning and can be easily introduced for single objectives. We design an RL algorithm that enables stable and fast learning of structured policies and the effective reuse of both behavior components and transition data across tasks in an off-policy setting. Finally, we evaluate our algorithm in simulated environments as well as physical robot experiments and demonstrate substantial improvements in data data-efficiency over competitive baselines.", "review": "Review:###This paper introduces a hierarchical policy structure for use in both single task and multitask reinforcement learning. The authors then assess the usefulness of such a structure in both settings on complex robotic tasks. These tasks include the stacking and reaching of blocks using a robotic hand, as an example. In addition to carrying out these experiments on simulated robots, the authors have also carried out experiments on a real Sawyer robotic arm. The particular form of their hierarchical policy for the multitask case is as follows. The policy, which is conditioned on the current state and task index consists of a gaussian mixture, where the individual gaussian densities are conditioned on the state and a context variable. The weights of this mixture are then dependent on a density on this context variable, which is conditioned on the state and task index. The intuition behind this is that the weight portion, which is called the high level component identifies task specific information, while the low level policy learns general, shareable knowledge of the different problems. The authors adapt the Multitask Policy Optimisation algorithm for their use by introducing an intermediate non-parametric policy, which is derived by setting KL bounds on the policy w.r.t to a reference policy. Having derived a closed-form solution to this, they go on to learn the parametric policy of interest. The authors consider 3 settings of experiments. Firstly, they assess the benefits of the hierarchical structure for single task settings in a simulated environment. For the most part, they find that compared to a flat policy, the hierarchical structure shows benefits only if the initial means of the high-level components are sampled to be different. While the experimental results are shown to support this, further discussion of why this is the case would have been welcome. The main benefits of the hierarchical policy are shown in the multitask case, in both simulated and real situations. In fact, the authors have shown that the hierarchical case often shows major benefits in difficult, more complicated tasks (reach vs stacking for example). I think that the paper was very well written. It is nicely structured, with easy to read language, and without unnecessary jargon or clutter. Where necessary, the relevant extra details were provided in the Appendices. The following are some additional notes: 1) It would have been interesting to see how the hierarchal policy faired in new tasks that were not a part of the original training set, compared to a flat multitask policy. 2) Further details about how each task is differentiated from each other in the experiments. That is, what are their different goals, which are reflected by the reward functions. As such I recommend this paper to be weak accepted."}
{"id": "iclr2020_905", "title": "Newton Residual Learning | OpenReview", "abstract": "Abstract:###A plethora of computer vision tasks, such as optical flow and image alignment, can be formulated as non-linear optimization problems. Before the resurgence of deep learning, the dominant family for solving such optimization problems was numerical optimization, e.g, Gauss-Newton (GN). More recently, several attempts were made to formulate learnable GN steps as cascade regression architectures. In this paper, we investigate recent machine learning architectures, such as deep neural networks with residual connections, under the above perspective. To this end, we first demonstrate how residual blocks (when considered as discretization of ODEs) can be viewed as GN steps. Then, we go a step further and propose a new residual block, that is reminiscent of Newton*s method in numerical optimization and exhibits faster convergence. We thoroughly evaluate the proposed Newton-ResNet by conducting experiments on image and speech classification and image generation, using 4 datasets. All the experiments demonstrate that Newton-ResNet requires less parameters to achieve the same performance with the original ResNet.", "review": "Review:###This paper proposes a new network structure Newton-ResNet motivated by Newton’s numerical optimization method. The structure proposed by the author(s) shows less parameters to get comparable performance with ResNet. Questions: 1. Does last term in (2) is a scalar? What*s the dimension of , please specify the vector dimension since it is confusing. 2. Why compare with removing activation? We need nonlinear activation to improve the performance. What*s normalization you use and why use normalization? 3. Due to the limit motivation of your structure, maybe you need more comparation with other structures."}
{"id": "iclr2020_906", "title": "Pre-trained Contextual Embedding of Source Code | OpenReview", "abstract": "Abstract:###The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "review": "Review:#### Summary This paper presents a BERT-inspired pretraining/finetuning setup for source code tasks. It collects a corpus of unlabeled Python files for BERT pretraining, designs or adopts 5 tasks on established smaller-scale Python corpora, and adjusts the BERT model to tokenize and encode source code snippets appropriately. # Strengths * The idea of applying the pretraining/finetuning paradigm to program analysis tasks makes sense, and has been informally attempted by multiple groups in the community in 2019. This is the first high-quality submission to a top-tier ML conference I*ve seen on the subject, though. * The authors exercised commendable care and diligence in preparing the training data, adopting BERT to source code inputs, and ensuring correctness of the experimental setup. I appreciated all the provided details on tokenization (Section 3.3), deduplication (Sections 3.1-3.2), and task setup (Section 3.5). This should become a technical standard in the community. * The paper is written clearly and concisely, and is generally a pleasure to read. # Weaknesses I have a gripe with the authors* choice to ignore program structure (e.g. abstract syntax trees) or features (e.g. types) in their program representation. Without this extra information (easily available from a compiler/interpreter API) the pipeline is not substantially different from the original NLP pipeline of BERT et al. The main program-related representation insight comes in tokenization (Section 3.3) and the definition of *sentences*. To repeat, I appreciate the effort the authors put in making tokenization appropriate for BERT processing of source code, but this is a drop in the bucket compared to the all the other program-related features the work is leaving off the table. Programs are not natural language. The argument that source code analysis would *pass on the burden ... to downstream tasks* (Page 3) is odd. First, most downstream tasks of interest occur in the settings where this analysis is already available: IDEs, code review assistants, linters, etc. Second, one often needs program analysis to even define downstream tasks in the first place -- for example, determining whether function arguments are swapped required detecting a function call and boundaries of its arguments, thus parsing the program! This work obtains (and nicely analyzes) impressive results obtained by applying CuBERT. However, it does not put the results in context with prior work based on structured program representations. Without this, it is difficult to say whether the improvement comes from pretraining or from the language model simply learning a better *parsed* representation of an input program from all the unlabeled corpus. If it*s the latter, one might argue that supplying the model with structured program features explicitly might eliminate much of the need for the unlabeled corpus. I personally think that there will still be a gap between pretraining and finetuning even with structured program features simply due to the sheer volume of available data, which, as the authors showed, is crucial for good generalization of Transformer. However, this still needs to be shown empirically. The *Function-Docstring Mismatch* task, as presented, seems too easy. If the distractors (negative examples) are truly chosen at random, most of them are going to use obviously different vocabulary from the original function signature (as Figure 4 demonstrates). A well designed task would somehow bias the sampling toward subtle distractors such as `get` vs. `set` docstrings, but this seems challenging. This also explains why the task is not influenced as much by reduction of training data (Table 3). The Next Sentence Prediction pretraining task, as adapted for CuBERT, seems too difficult, in contrast. If the paired sentences (i.e. code lines) are chosen at random, the model would lack most of the context required to make a decision about the logical relationship between them, such as which variables are defined and available in context, which functionality is being implemented, etc. I wonder, can the authors experiment with pretraining CuBERT only with the Masked Language Model task? Will it worsen the results substantially or at all? # Questions Section 3.2: *similar files according to the same similarity metric...* What are these metrics? What is the fraction of positive/negative examples in the constructed finetuning datasets? What is the motivation for making Variable Misuse and Wrong Operator/Operand into a simple classification tasks instead of the original (more useful) correction task?"}
{"id": "iclr2020_907", "title": "Low Rank Training of Deep Neural Networks for Emerging Memory Technology | OpenReview", "abstract": "Abstract:###The recent success of neural networks for solving difficult decision tasks has incentivized incorporating smart decision making *at the edge.* However, this work has traditionally focused on neural network inference, rather than training, due to memory and compute limitations, especially in emerging non-volatile memory systems, where writes are energetically costly and reduce lifespan. Yet, the ability to train at the edge is becoming increasingly important as it enables applications such as real-time adaptability to device drift and environmental variation, user customization, and federated learning across devices. In this work, we address four key challenges for training on edge devices with non-volatile memory: low weight update density, weight quantization, low auxiliary memory, and online learning. We present a low-rank training scheme that addresses these four challenges while maintaining computational efficiency. We then demonstrate the technique on a representative convolutional neural network across several adaptation problems, where it out-performs standard SGD both in accuracy and in number of weight updates.", "review": "Review:###This paper proposes a low-rank training method targeting for edge devices. The main contribution is an algorithm called Streaming Kronecker-Sum Approximation. The authors claim that the proposed method addresses four key challenges of low weight update density, weight quantization, low auxiliary memory, and online learning. The paper should be rejected because of the following reasons: (1) The paper is a little hard to follow and the writing can be significantly improved. In particular, the authors introduce four main challenges in section 3. However, I found they are not that accessible and hard to understand. In section 4.4.2, the objective is to get a minimum variance rank-r approximation to the diagonal matrix Sigma, but I think the authors mix *m* up with *r*. (2) The novelty of the algorithm is limited. From section 4.1 to 4.5, most discussions are about previously proposed methods. The algorithm proposed by the author (i.e., SKS) only involves some basic manipulations of linear algebra. I don*t think it*s novel enough to be a new algorithm. (3) Experimental results are limited. The authors spent a lot of time discussing on-device computing, but all their experiments are just simulations on standard benchmarks. For such a paper concerning training on edge devices, I would expect to see some experiments on real edge devices. Overall, I think the paper needs further improvements to be qualified for being accepted. ----------------------------------------------------------------------------------------------------------------------------------------------------------------- post rebuttal: I*ve read the authors* responses and the updated paper. Though my concern on writing has been resolved to some extent, I*m still unsatisfied with the empirical experiments. I believe the authors need to do experiments on edge devices since they have emphasized a lot about on-device computing. That being said, I*m not an expert in hardware and have no idea how hard it is to conduct those experiments. I*ve increased the score to 3 but still vote for rejection."}
{"id": "iclr2020_908", "title": "Longitudinal Enrichment of Imaging Biomarker Representations for Improved Alzheimer*s Disease Diagnosis | OpenReview", "abstract": "Abstract:###Longitudinal data is often available inconsistently across individuals resulting in ignoring of additionally available data. Alzheimer*s Disease (AD) is a progressive disease that affects over 5 million patients in the US alone, and is the 6th leading cause of death. Early detection of AD can significantly improve or extend a patient*s life so it is critical to use all available information about patients. We propose an unsupervised method to learn a consistent representation by utilizing inconsistent data through minimizing the ratio of -Order Principal Components Analysis (PCA) and Locality Preserving Projections (LPP). Our method*s representation can outperform the use of consistent data alone and does not require the use of complex tensor-specific approaches. We run experiments on patient data from the Alzheimer’s Disease Neuroimaging Initiative (ADNI), which consists of inconsistent data, to predict patients* diagnosis.", "review": " This manuscript develops a metric-learning with non-Euclidean error terms for robustness and applies in to data reduction to learn diagnostic models of Alzheimer*s disease from brain images. The manuscript discusses a metric-learning formulation as a quotient for reconstruction-error terms, how to optimize the quotient based on results from Wang et 2014, an iterated reweighted approach to circumvent the non-smooth part of the l1 loss in zero, and experiments on brain images of Alzeimer*s disease. My two main issues with the manuscript is that the theoretical part is written very imprecisely and that the experiments are not convincing due to the lack of good baselines and of statistical power. With regards to the theoretical contributions, a fraction of the results in the present manuscript are trivial consequences or Wang2014, and yet it comes with errors in the statements. For instance, in equation (9), the present manuscript writes greater or equal, while I believe that it should be strictly greater, as in Wang. Theorem 1 and 4 seem almost the same thing, though with a contradiction between the two. Other statements are inaccurate: the authors claim some results on reaching global optima, while I believe that they can only claim that they reach stationary points. Theorem 2 and 5 seem to be the same thing. Concerning the iterated reweighted approach, I believe that this is non smooth only for g(x)=0, which is not covered by the theorems of Wang 2014. Is this algorithm needed? Note that Wang apply their algorithm with an l1 norm, ie non-smooth in zero, and do not report problems with out. The manuscript mentions that *to inverted matrices that divide 0s, which routinely lead to inferior learning performance.*. I am not exactly sure what that means and I would need to understand better the problem. Also, the theoretical contribution that with the added the delta the algortihm converges, seems quite trivial: it seems to me that it is the eta trick. Minor comments: in algorithm 3, it would be useful to write the full expression of the equations, rather than just reference the numbers. Also, the computational cost of the eigenvectors at each iteration seems quite prohibitive. How was the value r=3 selected? Figure 2 seem to choose that approaches have not converged: they final value is larger than intermediate values? How were the p-values between cross-validation assessment of estimators computed? If it was done using standard paired t-test, this is incorrect are the folds are not independent. With regards to the experiments, I worry that the model is not compared against simple baselines, such as a PCA."}
{"id": "iclr2020_909", "title": "LEX-GAN: Layered Explainable Rumor Detector Based on Generative Adversarial Networks | OpenReview", "abstract": "Abstract:###Social media have emerged to be increasingly popular and have been used as tools for gathering and propagating information. However, the vigorous growth of social media contributes to the fast-spreading and far-reaching rumors. Rumor detection has become a necessary defense. Traditional rumor detection methods based on hand-crafted feature selection are replaced by automatic approaches that are based on Artificial Intelligence (AI). AI decision making systems need to have the necessary means, such as explainability to assure users their trustworthiness. Inspired by the thriving development of Generative Adversarial Networks (GANs) on text applications, we propose LEX-GAN, a GAN-based layered explainable rumor detector to improve the detection quality and provide explainability. Unlike fake news detection that needs a previously collected verified news database, LEX-GAN realizes explainable rumor detection based on only tweet-level text. LEX-GAN is trained with generated non-rumor-looking rumors. The generators produce rumors by intelligently inserting controversial information in non-rumors, and force the discriminators to detect detailed glitches and deduce exactly which parts in the sentence are problematic. The layered structures in both generative and discriminative model contributes to the high performance. We show LEX-GAN*s mutation detection ability in textural sequences by performing a gene classification and mutation detection task.", "review": "Review:###In the paper, authors proposed a generative adversarial network-based rumor detection model that can label short text like Twitter posts as rumor or not. The model can further highlight the words that are responsible for the rumor accusation. Proposed model consists of 4 sub models: a G_Where model finds the word to replace so to create an artificial rumor; a G_replace model decides what the replacement word should be; a D_classify model detects if a sequence is a rumor; a final D_explain model pinpoints the word of concern. D_ models and G_ models are trained in an adversarial competing way. Experiments showed that the LEX-GAN model outperforms other non-GAN models by a large margin on a previously published rumor dataset (PHEME) and in a gene classification task. My questions: 1) The task modeled is essentially a word replacement detection problem. Is this equivalent to rumor detection? Even if it performs really well on a static dataset, it could be very vulnerable to attackers. Various previous works mentioned in the paper, including the PHEME paper by Kochkina et al, used supporting evidence for detection, which sounds like a more robust approach. 2) Authors didn*t explain the rationale behind the choice of model structure, e.g. GRU vs LSTM vs Conv. The different structures have been used in mix in the paper. Are those choices irrelevant or critical? 3) I would like to see more discussion on the nature of errors from those models, but it*s lacking in the paper. This could be critical to understand the model’s ability and limitation, esp given that it’s not looking at supporting evidences from other sequences. Small errors noticed: The citation for PHEME paper (Kochkina et al) points to a preprint version, while an ACL Anthology published version exists."}
{"id": "iclr2020_910", "title": "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty | OpenReview", "abstract": "Abstract:###Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.", "review": "Review:###The paper discusses a new data augmentation method which improves the accuracy of the network for several specific shifted domain scenarios. The main goal of the paper is to increase the robustness of the deep model trained on the augmented data to generalize well beyond the data corruption like the rotation, translation, noise,.... For each input, they apply different operation of image shift and make the weighted combination of them. The weight vector is generated randomly from Dirichlet distribution with the parameter . The weighted combined images would be added to the original image in convex combination. The convex weights are generated from distribution Beta with parameter . Later they train the network with adding the Jensen-Shannon divergence for the posterior distributions of augmented images as the consistency regularizer. They show this data augmentation will increase the accuracy of the model for shifted and non-shifted domains and also it leads to more calibrated model for domain shift problem. Pros: the paper is well-written with clear implementation details. The level of experiments are wide and cover different aspects. The experiments shows the significant improvement compared to several baselines. The authors conducted the experiments for a wide range of model-datasets to show the validity of their ideas. Cons: 1- The title of this work is a strong claim that is not supported in the paper. In this paper, it is mentioned that AugMix is a data augmentation method that generates data to add to the training set and after training with data augmentation, the method would be more robust to other distortions that can be added to the datasets. Generally, the definition of domain shift is wider than just adding perturbation to the dataset. To support the claim, the paper should also report the results for similar tasks datasets such as CIFAT10-STL10- or MINIST-SVHN for different models and with different domain adaptation methods. The claim about the improvement of uncertainty also is not supported well by the experiments. The method should be tested for many model-datasets specifically, to support improving the uncertainty under the domain shift idea like the paper [1]. 2- The novelty of the work is limited. The generating method of distorted images is the combination of previously proposed methods like [2] and [3]. The motivation of why the proposed method is working well is not clear. How this objective function can improve the robustness to the image perturbation but it does not lose the accuracy is not discussed. It would be better if the proposed method were supported by theory and also the intuition and explained why it should get better results than previous data augmentation methods such as AutoAugment [3]. 3- Fine-tuning the parameters like , and is not discussed at all. 4- To show the robustness of the proposed method to domain shift, the paper compares the proposed method to other data augmentation methods that are not designed for domain shift which seems unfair. References: [1] Ovadia, Yaniv, et al. *Can You Trust Your Model*s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift.* arXiv preprint arXiv:1906.02530 (2019). [2] Zhang, Hongyi, et al. *mixup: Beyond empirical risk minimization.* arXiv preprint arXiv:1710.09412 (2017). [3] Cubuk, Ekin D., et al. *Autoaugment: Learning augmentation policies from data.* arXiv preprint arXiv:1805.09501 (2018)."}
{"id": "iclr2020_911", "title": "Lyceum: An efficient and scalable ecosystem for robot learning | OpenReview", "abstract": "Abstract:###We introduce Lyceum, a high-performance computational ecosystem for robotlearning. Lyceum is built on top of the Julia programming language and theMuJoCo physics simulator, combining the ease-of-use of a high-level program-ming language with the performance of native C. Lyceum is up to 10-20Xfaster compared to other popular abstractions like OpenAI’sGymand Deep-Mind’sdm-control. This substantially reduces training time for various re-inforcement learning algorithms; and is also fast enough to support real-timemodel predictive control with physics simulators. Lyceum has a straightfor-ward API and supports parallel computation across multiple cores or machines.The code base, tutorials, and demonstration videos can be found at: https://sites.google.com/view/lyceum-anon.", "review": "Review:###The paper introduces a computational ecosystem for robot learning built on Julia. The authors claim to achieve faster learning for various RL algorithms and support real-time model predictive control. Overall, the paper is well written and the main contribution and the main structure of the ecosystem is clearly described. That this supports parallel computation can be of immense use for the training time is often a caveat in RL algorithms and parallel computation is crucial. The benchmark results reported in Fig. 3 show that sampling throughputs are better in the proposed ecosystems across reported environments and this ecosystem outperforms Gym and DMC in parallel scaling of sampling throughputs. While I am inclined to accept the paper as I believe that it has the potential to offer an origin and efficient ecosystem to work well in real-time MPC and RL algorithms, I think the paper lacks certain details. 1. Fig. 1 shows that the although the time taken by Lyceum is less than that by Gym, the performance of the latter is much better in Humanoid. Although time is a crucial parameter in training, it is the performance that is usually of utmost importance. A more detailed explanation /intuition than given is needed for one to understand the limitation of the proposed ecosystem, which may in turn reflect on possible success or failures in other environments (not reported in the paper). 2. There is hardly any code given in the paper! While I understand that this is beyond the scope due to the limited space, I checked the link given in the abstract and it was not working. It is very hard to draw a critical and analytical conclusion without having a look into the details of implementation."}
{"id": "iclr2020_912", "title": "Exploring Cellular Protein Localization Through Semantic Image Synthesis | OpenReview", "abstract": "Abstract:###Cell-cell interactions have an integral role in tumorigenesis as they are critical in governing immune responses. As such, investigating specific cell-cell interactions has the potential to not only expand upon the understanding of tumorigenesis, but also guide clinical management of patient responses to cancer immunotherapies. A recent imaging technique for exploring cell-cell interactions, multiplexed ion beam imaging by time-of-flight (MIBI-TOF), allows for cells to be quantified in 36 different protein markers at sub-cellular resolutions in situ as high resolution multiplexed images. To explore the MIBI images, we propose a GAN for multiplexed data with protein specific attention. By conditioning image generation on cell types, sizes, and neighborhoods through semantic segmentation maps, we are able to observe how these factors affect cell-cell interactions simultaneously in different protein channels. Furthermore, we design a set of metrics and offer the first insights towards cell spatial orientations, cell protein expressions, and cell neighborhoods. Our model, cell-cell interaction GAN (CCIGAN), outperforms or matches existing image synthesis methods on all conventional measures and significantly outperforms on biologically motivated metrics. To our knowledge, we are the first to systematically model multiple cellular protein behaviors and interactions under simulated conditions through image synthesis.", "review": "Review:###The authors present a GAN for multiplexed imaging (MIBI-TOF) data called CCIGAN. They propose an interesting architecture design with protein-specific attention to find association between cell types and neighboring pattens and cell-cell interactions. They also propose new and biologically interpretable metrics including a reconstruction metric, projected EMD and regressing of expression on neighbors. They present improved reconstruction of interactions compared to other models in the context of PD-1 and PD-L1 interactions. It would be great to extend the evaluation to other interactions and tissue types. Overall the paper is well written, the application and especially the focus on cell-cell interactions is novel. The model is properly justified and evaluated, and there is a high demand for this framework in the multiplexed imaging field."}
{"id": "iclr2020_913", "title": "Retrospection: Leveraging the Past for Efficient Training of Deep Neural Networks | OpenReview", "abstract": "Abstract:###Deep neural networks are powerful learning machines that have enabled breakthroughs in several domains. In this work, we introduce retrospection loss to improve the performance of neural networks by utilizing prior experiences during training. Minimizing the retrospection loss pushes the parameter state at the current training step towards the optimal parameter state while pulling it away from the parameter state at a previous training step. We conduct extensive experiments to show that the proposed retrospection loss results in improved performance across multiple tasks, input types and network architectures.", "review": "Review:###This paper presents the retrospective loss to optimize neural network training. The idea behind the retrospective loss is to add a penalization term between the current model to the model from a few iterations before. Extensive experimental results on a wide range of datasets are provided to show the effectiveness of the retrospective loss. The retrospective loss is additionally controlled by two hyperparameters, the strength parameter K and the update frequency T_p. This loss, measured in L-1 norm, is added to the training objective. The geometric intuition of the added loss term is that this pushes the model away from the model at iteration T_p. The paper argues that this shrinks the parameter space of the loss function. One of the concern regards the writing of the paper. - Algorithm 1 and Figure 6 look very blurry, which I think are both below the publication standard. - The introduction could be written to be more helpful, such as providing more context on why the obtained experimental results are important (e.g. getting state-of-the-art results on the datasets studied in the experiments) - The Related Work contrasts with previous work which is not clear because the precise contribution has not been stated at the point. More detailed questions: - What are the standard deviations for the experimental results (as you reported in Table 4 but not in other experiments)? - I*m curious whether the use of L-1 norm is critical or not in the retrospective loss."}
{"id": "iclr2020_914", "title": "Meta-Learning for Variational Inference | OpenReview", "abstract": "Abstract:###Variational inference (VI) plays an essential role in approximate Bayesian inference due to its computational efficiency and general applicability. Crucial to the performance of VI is the selection of the divergence measure in the optimization objective, as it affects the properties of the approximate posterior significantly. In this paper, we propose a meta-learning algorithm to learn (i) the divergence measure suited for the task of interest to automate the design of the VI method; and (ii) initialization of the variational parameters, which reduces the number of VI optimization steps drastically. We demonstrate the learned divergence outperforms the hand-designed divergence on Gaussian mixture distribution approximation, Bayesian neural network regression, and partial variational autoencoder based recommender systems.", "review": "Review:###SUMMARY OF THE PAPER: This paper proposes a way to automatically select a divergence to use in variational inference (VI) given a set of datasets (tasks). They consider searching within the alpha- divergence and f-divergence families. The proposed algorithm works as follows: do a few gradient descent steps on the variational parameters given a fixed divergence parameter, then update the divergence parameter by taking the gradient with respect to a meta loss which is a task-specific measure of goodness of the variational distribution (like test log marginal likelihood). The latter gradient is computed through the gradient descent computation in the inner loop. The second proposed algorithm does the same, but also learns a good initialization for the variational parameters. This initialization is good in the sense that taking one (or a few) gradient descent step on the variational parameters should give us good variational parameters (MAML style). This is done by taking a gradient with respect to this good initialization parameter through the inner loop gradient descent. STRUCTURE: The paper is well-written and easy to understand. NOVELTY: As far as I know, learning a divergence for VI using meta-learning is new. The related work is discussed well in section 5. EXPERIMENTS: There are experiments on three tasks of increasing complexity. In the simpler experiments, careful ablation studies are done. The results generally show that the proposed method is preferable to alternatives. CONCLUSION: I recommend acceptance."}
{"id": "iclr2020_915", "title": "Trajectory representation learning for Multi-Task NMRDPs planning | OpenReview", "abstract": "Abstract:###Expanding Non Markovian Reward Decision Processes (NMRDP) into Markov Decision Processes (MDP) enables the use of state of the art Reinforcement Learning (RL) techniques to identify optimal policies. In this paper an approach to exploring NMRDPs and expanding them into MDPs, without the prior knowledge of the reward structure, is proposed. The non Markovianity of the reward function is disentangled under the assumption that sets of similar and dissimilar trajectory batches can be sampled. More precisely, within the same batch, measuring the similarity between any couple of trajectories is permitted, although comparing trajectories from different batches is not possible. A modified version of the triplet loss is optimised to construct a representation of the trajectories under which rewards become Markovian.", "review": " ** Summary The paper studies a specific class of Non-Markovian Reward Decision Processes (NMRDPs). In general, in NMRDP the dynamics is Markovian but the reward function may depend on the entire trajectory. The authors consider a sub-case where the trajectory can be mapped to a specific *task* and the reward function can be formalized as a mapping between the state-task pair to the reward. This greatly simplifies the problem that can be mapped onto an augmented MDP and solved using standard RL tools. The authors focus on the representation learning problem of the mapping between trajectories to an embedding of the task itself. In particular, they consider contrastive learning to find a representation that discriminate between trajectories associated to the same task and trajectories coming from different tasks. The resulting LSTM-based architecture is then evaluated on a grid-world synthetic problem and on GPS trajectories from tourists in the city of Salzburg. ** Overall evaluation My main concerns about the paper is that the exact objective of the setting and the representation learning is not that clear and the empirical validation is limited to a qualitative assessment of the representation learning with no down-stream task. More in detail. 1- The assumption of task-dependent reward functions is very sensible. Yet I wonder to which extent it overlaps with literature in multi-task and meta-learning, or more in general with embedding of time-series (the trajectory in your case). It would be useful to frame/compare at the conceptual level the similarities and differences of the proposed setting with those fields. 2- At training time, the PK dataset is somehow supervised, as it is possible to know which trajectories are associated to the same task. At test time, the learned f_theta recursively maps trajectories to tasks. As such, it seems like it could effectively detect changes into the task itself by tracking how the trajectory evolves (when the task function is unknown). This aspect is never really evaluated in the empirical section, but it would be one of the most interesting uses of the learned representation. 3- If the task function T is known, it means that standard RL techniques can be used to solve MDP N. The actual advantage of using a specific function to embed the task to a space in Re^d is never really explained in the paper. Does it make solving N simpler or more effective than using a simple encoding for the task? No evaluation is available in this sense. 4- The empirical evaluation is limited to qualitative analysis of the representation learned in the problem. Yet, there is no clear support that the representation is good/useful in a more quantitative way (e.g., by actually solving the RL part or in identifying quickly the current task). Some more specific questions/comments 1- Some of the notation is a bit redundant. For instance, phi is mapping from H to Re^d, while f is a mapping from trajectories to Re^d, but in the end they are doing the same thing, as H itself can be obtained as a mapping from trajectories to tasks through T. 2- *Using RL techniques, finding an optimal policy pi* in the equivalent MDP hat M is possible and by extension pi* is optimal for N*. This passage is not fully clear, if phi is introducing some form of approximation, then pi* may no longer be optimal for the original MDP N. On the other hand, if phi is not approximating but *just* changing the representation from H to Re^d, then it is not clear what is the interest of it. 3- The assumption that similar trajectories can be identified is somehow strong. It would be good to have a more thorough support for it. 4- It is not fully clear what L_BH^local is indeed a local loss. It seems like you are simply using the mapping from the whole trajectory. Is this why it is called local? 5- While I appreciate that the introduction is sketching many different scenarios to support the models studied in the paper, in the end they mostly lack of depth and they rather give a confusing impression instead of clarifying in a compelling way what is the problem studied in the paper. I suggest you rather pick one single scenario with a good level of detail to provide a more solid support to the paper. ** Minor comments 1- You often use *he* to refer to the agent. It would be better to use *it* or *she*."}
{"id": "iclr2020_916", "title": "Graph Neural Networks for Reasoning 2-Quantified Boolean Formulas | OpenReview", "abstract": "Abstract:###It is valuable yet remains challenging to apply neural networks in logical reasoning tasks. Despite some successes witnessed in learning SAT (Boolean Satisfiability) solvers for propositional logic via Graph Neural Networks (GNN), there haven*t been any successes in learning solvers for more complex predicate logic. In this paper, we target the QBF (Quantified Boolean Formula) satisfiability problem, the complexity of which is in-between propositional logic and predicate logic, and investigate the feasibility of learning GNN-based solvers and GNN-based heuristics for the cases with a universal-existential quantifier alternation (so-called 2QBF problems). We conjecture, with empirical support, that GNNs have certain limitations in learning 2QBF solvers, primarily due to the inability to reason about a set of assignments. Then we show the potential of GNN-based heuristics in CEGAR-based solvers and explore the interesting challenges to generalize them to larger problem instances. In summary, this paper provides a comprehensive surveying view of applying GNN-based embeddings to 2QBF problems and aims to offer insights in applying machine learning tools to more complicated symbolic reasoning problems.", "review": "Review:###This paper investigated the GNN-based solvers for the 2-Quantified Boolean Formula satisfiability problem. This paper points out that GNN has limitations in reasoning about unsatisfiability of SAT problems possibly due to the simple message-passing scheme. To extend the GNN-based SAT solvers to 2-QBF solvers, this paper then turns to learn GNN-based heuristics that work with traditional decision procedure, and proposes a CEGAR-based 2QBF algorithm. Overall, the topic of combining logic reasoning and graph neural networks is interesting. But it is not clear how important is the targeted 2-QBF problem, except for testing and finding the limitations of GNN. In other words, this paper picks up a specific class of model for a very specific class of problem, which lacks sufficient and convincing motivations. Although GNN achieves success in solving SAT problems, it is not necessary that GNN is a must for solving the 2-QBF problems. Also, when analyzing the limitations of GNN, the paper makes conjecture only based on empirical results. It would be much more insightful to provide some theoretical analysis so that the paper can inspire other researchers working on different problems. Based on the above reasons, I would like to recommend a weak reject for this paper."}
{"id": "iclr2020_917", "title": "Instant Quantization of Neural Networks using Monte Carlo Methods | OpenReview", "abstract": "Abstract:###Low bit-width integer weights and activations are very important for efficient inference, especially with respect to lower power consumption. We propose to apply Monte Carlo methods and importance sampling to sparsify and quantize pre-trained neural networks without any retraining. We obtain sparse, low bit-width integer representations that approximate the full precision weights and activations. The precision, sparsity, and complexity are easily configurable by the amount of sampling performed. Our approach, called Monte Carlo Quantization (MCQ), is linear in both time and space, while the resulting quantized sparse networks show minimal accuracy loss compared to the original full-precision networks. Our method either outperforms or achieves results competitive with methods that do require additional training on a variety of challenging tasks.", "review": "Review:###Summary The paper introduces a novel method for quantizing neural network weights and activations that does not require re-training or fine-tuning of the network. To this end the paper notes that weights and activations in a ReLU network can be scaled by a multiplicative factor, in particular such that weight-magnitudes of a layer sum to one, thus defining a probability mass function. The main idea of the paper is to cleverly sample from this PMF such that a histogram of the sampled values produces an approximate integer-representation of the original weights. The quality of this approximation can be refined arbitrarily by taking more samples, however, the value of the bin with the highest count defines the maximally required bit-precision, such that higher approximation accuracy comes at the cost of requiring larger bit-precision. The method is evaluated on several networks on CIFAR-10, SVHN and ImageNet, as well as some language models. On the image classification tasks, the paper also reports previously published relevant results from competitor methods for comparison. Quality, Clarity, Novelty, Impact The paper is well written, the main ideas are presented clearly and concisely and necessary experimental details are given in the appendix. The main idea of the paper is neat and I have not seen its application to neural network quantization before. Unfortunately I am not sure how the paper exactly fits within existing methods: while the method could in principle be used to train low bit-width networks (<4bit), the results suggest that in this domain the method is outperformed by other methods. Particularly <=2-bit weights and activations are interesting since they can be used to construct networks that replace multipliers with much faster and more energy-efficient bit-wise operations (XNOR bit-counting). But this regime seems not to be the strong suit of the proposed method. On the other hand models in the >=8-bit regime can be readily obtained with most major deep learning frameworks (e.g. Tensorflow lite and Pytorch), such that having to train or fine-tune a model is rarely a problem, unless one does not have access to training data. But even for the latter, many papers have proposed fine-tuning on dummy-data, or not fine-tuning at all (using linear or non-linear quantization schemes). If this is the targeted regime, then the literature review is missing quite a few important references that should also be compared against in the experiments section. So if the method is specifically targeted at the regime between 4- and 8-bit when re-training or fine-tuning is not possible (for some reason), then the motivation, literature comparison and experimental comparison should be much more targeted at methods that perform well in this regime. While I agree that the main idea is neat and it is nice to see it performing reasonably well, I currently do not see how the method provides strong and convincing advantages over the existing body of methods. I am very happy to be convinced of the opposite during rebuttal by the authors or other reviewers of course. Currently however, I would suggest to take a bit more time and think about particular use-cases that the method is targeted at and flesh this out much stronger - I am not sure whether the rebuttal phase is sufficient for this and therefore (currently) vote for rejection. Improvements a) The paper and the method needs a clear focus. I personally would roughly group methods into >=8-bit or <2-bit. The first one is becoming the new out-of-the-box standard and the latter allows for very efficient multiplier-free implementations (even on standard hardware to some degree, particularly for ternary/binary networks). Anything in-between might be interesting but would also probably require non-standard hardware, or variable precision hardware to be practically useful. If the target regime is between 2- and 8-bit, please expand the literature review accordingly and compare against relevant methods previously reported that operate well in this regime. b) If the emphasis is on training-free methods, also adjust the literature and comparisons accordingly, but also have a strong point when and why training-free methods are important (especially when they come with an overhead for each forward-pass during test-time). This does not mean to shorten the literature review or remove comparisons, but add the most relevant ones given that there is such a large body of network compression methods published in the last three years. c) I currently have a hard time judging to which degree the benefits reported could actually be exploited by (somewhat realistic future) hardware. Is it possible to sketch an efficient way of implementing the activation-quantization (together with quantized weights of course) on some low-bit-width hardware? While the results show that the method can work in theory with <8-bit activations, I am not sure how the quantization scheme could be efficiently implemented on actual hardware. Is it possible to do some back-of-the-envelope calculations on how much computational overhead the quantizations would add to each forward-pass (e.g. for one of the CIFAR-10/ImageNet networks). For instance, how many 32-/16-bit floating point operations would the sampling of activations roughly require, and how many would it then correspondingly save by performing 4- to 8-bit operations? Or could the whole scheme be implemented in low bit-width (<16bits)? Comments ii) Related to c) above: at which point would the computational overhead for sampling quantized activations in each forward-pass exceed the cost of retraining a network that can natively deal with low-bit activations. I don*t expect a quantitatively precise answer here, but just something that gives a rough idea of the orders of magnitude. Currently I find it very hard to judge how expensive the sampling would be after deployment. iv) In the results-tables the fractional numbers for the bit-widths (e.g. 5.6bit) are a bit misleading. While it is ok to report them, the comparison against other methods should probably use the bit-widths rounded up to the next integer. This should be mentioned at least in the text."}
{"id": "iclr2020_918", "title": "MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning | OpenReview", "abstract": "Abstract:###In this paper, we propose a framework, called MaskConvNet, for ConvNets filter pruning. MaskConvNet provides elegant support for training budget-aware pruned networks from scratch, by adding a simple mask module to a ConvNet architecture. MaskConvNet enjoys several advantages - (1) Flexible, the mask module can be integrated with any ConvNets in a plug-and-play manner. (2) Simple, the mask module is implemented by a hard Sigmoid function with a small number of trainable mask variables, adding negligible memory and computational overheads to the networks during training. (3) Effective, it is able to achieve competitive pruning rate while maintaining comparable accuracy with the baseline ConvNets without pruning, regardless of the datasets and ConvNet architectures used. (4) Fast, it is observed that the number of training epochs required by MaskConvNet is close to training a baseline without pruning. (5) Budget-aware, with a sparsity budget on target metric (e.g. model size and FLOP), MaskConvNet is able to train in a way that the optimizer can adaptively sparsify the network and automatically maintain sparsity level, till the pruned network produces good accuracy and fulfill the budget constraint simultaneously. Results on CIFAR-10 and ImageNet with several ConvNet architectures show that MaskConvNet works competitively well compared to previous pruning methods, with budget-constraint well respected. Code is available at https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0. We hope MaskConvNet, as a simple and general pruning framework, can address the gaps in existing literate and advance future studies to push the boundaries of neural network pruning.", "review": " This paper proposes a masking process to improve the pruning of DNN. In addition, the algorithm proposes to automatically allocate the pruning rates over layers. On the positive side: - I do believe the main contribution is automatically allocating the pruning rates over the network. Related works: - How does this relate to methods using gating to prune in the presence of residual layers or BN? - Paper claims multiple times that related works need a train-prune-retrain process which is only valid for post-processing works. - I missed sparsity promoting works related to training from scratch where similar masks are implicitly used during training (even not explicitly stated) to maintain the zeros in the network. At least in [1,2] the training time is the same as the time used for training a network from scratch (same claim as in this paper). Method: - The mask is trained using a sigmoid function and claims this will be representative of the relevance of the *neuron* within the layer. How is this really related? - Why the initialization of the mask is to the mean? I think I am confused there. For initialization, I would argue all the neurons/parameters are relevant, right? - The claim that this method is *much simpler* is a bit subjective. I do not see why. Please elaborate. - I am not sure if the claim of pruning filters and not considering the bn layer is correct. I would tend to think that, if pruning a neuron, the corresponding BN module should be modified (that is, propagating the zero to subsequent layers). - I do like the extension to residual connections. It would be great to have more and clearer details on how is this done. Would this also apply to the UNET type of architecture? - What is the intuition behind Eq 4 and how it is related to the relevance of a parameter? - The extension to multiple metrics is of interest, however, there is little detailed there. How is the automatic allocation done? This is repeated in section 3.3 but details are missing. - My understanding is that the regularization multiplier is affected by the learning rate, therefore their effect is lower as the training progresses. In this case, seems the opposite, right? (page 6 before 3.4). - The part with the sparsity budget is interesting. What guarantees are that the newly enabled neurons are actually useful? Could it be possible that the budget suggested is not the right for the task at hand and, therefore, the additional parameters are not really relevant / needed? Experiments: - There is loss with little sparsity when it comes to Imagenet (15 and 17% pruning) does not seem very promising even the training time is similar. - Seems like the experiments and comparisons to L1-pruning are not very surprising. It would be nice to have more comprehensive numbers. For imagenet, if using Resnet-50, would be easier to compare to other numbers. - In the imagenet comparison, L1-pruning is the version-A of the paper. What about the others or why that particular model? - How are the actual groups made? Minor details: - Please improve figure 1. It is not easy to understand. Same with Figure 3. What is seen in Figure 4. - I do believe the WideResNet-28-10 number of parameters for BAR is not correct. - Section 4.2 is a bit overselling. I do not see *much-higher* parameter sparsity. The claims are mostly valid for VGG type of networks in this particular setting. [1] Learning the Number of Neurons in Deep Networks, NeurIps 2016 [2] Compression-aware training of DNN, NeurIps 2017"}
{"id": "iclr2020_919", "title": "Multi-Precision Policy Enforced Training (MuPPET) : A precision-switching strategy for quantised fixed-point training of CNNs | OpenReview", "abstract": "Abstract:###Large-scale convolutional neural networks (CNNs) suffer from very long training times, spanning from hours to weeks, limiting the productivity and experimentation of deep learning practitioners. As networks grow in size and complexity one approach of reducing training time is the use of low-precision data representation and computations during the training stage. However, in doing so the final accuracy suffers due to the problem of vanishing gradients. Existing state-of-the-art methods combat this issue by means of a mixed-precision approach employing two different precision levels, FP32 (32-bit floating-point precision) and FP16/FP8 (16-/8-bit floating-point precision), leveraging the hardware support of recent GPU architectures for FP16 operations to obtaining performance gains. This work pushes the boundary of quantised training by employing a multilevel optimisation approach that utilises multiple precisions including low-precision fixed-point representations. The training strategy, named MuPPET, combines the use of multiple number representation regimes together with a precision-switching mechanism that decides at run time the transition between different precisions. Overall, the proposed strategy tailors the training process to the hardware-level capabilities of the utilised hardware architecture and yields improvements in training time and energy efficiency compared to state-of-the-art approaches. Applying MuPPET on the training of AlexNet, ResNet18 and GoogLeNet on ImageNet (ILSVRC12) and targeting an NVIDIA Turing GPU, the proposed method achieves the same accuracy as the standard full-precision training with an average training-time speedup of 1.28× across the networks.", "review": "Review:###The article presents an approach to reduce the precision of weights, activations and gradients to speed up the training of deep neural networks. The precision of these values is increased according to a dynamic schedule such that the original classification accuracy is reached after training. The manuscript is in most parts well written and the addressed topic is of general interest for the research community represented at ICRL. Still, I recommend a weak reject, since the core idea of the manuscript, i.e. the dynamic switching between precision levels, is not shown to be a necessary condition for good classification results. Major points: • The introduction does not give a clear statement about the novel contribution of the paper. Only the very last paragraph is specific about the paper. • Your results support that step-wise increasing the resolution speeds up training without significant losses in accuracy. However, the impact of the gradient diversity, choice of p and threshold parameters on the performance of the trained networks are unclear. What is the isolated impact of every of these choices? According to Figure 2, pre-defined switching points between precision levels may also generalize between networks and datasets. • The description of the quantization scheme is not clear enough in order to reproduce the results: o Please give details about every step from FP32 to FPx values or cite appropriate literature. o Equation 4 and 5: How are the scaling factors SC determined? o Please clarify the difference/relation between n and WL. Minor points: • Equation 3: What does “represent. range(q^i)” mean? • Text in Figure 1 and 2 is far too small and barely readable • Step 5 in Algorithm in Section 3.3: What does “p violates y more than gamma times” mean? What is y? • Please clarify “distribution approach”. Distribution of what? • Table 1: For the baseline experiments, the precision is switched from 8 to 32 bits, for MuPPET from 8 to 12 bits (see main text). What is the motivation behind these different choices? • Do you use any type of data augmentation? • Table 3: Please clarify “theoretical limit”. Does this limit include 12 and 14 bit quantisation. What do you mean by “optimized quantization implementation” in main text?"}
{"id": "iclr2020_920", "title": "Independence-aware Advantage Estimation | OpenReview", "abstract": "Abstract:###Most of existing advantage function estimation methods in reinforcement learning suffer from the problem of high variance, which scales unfavorably with the time horizon. To address this challenge, we propose to identify the independence property between current action and future states in environments, which can be further leveraged to effectively reduce the variance of the advantage estimation. In particular, the recognized independence property can be naturally utilized to construct a novel importance sampling advantage estimator with close-to-zero variance even when the Monte-Carlo return signal yields a large variance. To further remove the risk of the high variance introduced by the new estimator, we combine it with existing Monte-Carlo estimator via a reward decomposition model learned by minimizing the estimation variance. Experiments demonstrate that our method achieves higher sample efficiency compared with existing advantage estimation methods in complex environments.", "review": "Review:###This paper proposes a new advantage estimator in reinforcement learning based on importance sampling. This form allows for a significantly lower-variance estimator for situations where the current action *stops mattering* to the future state. A control variate, as in Grathwohl et al., is used to combine the importance sampling estimator with the *standard* estimator in a way that is always unbiased and attempts to minimize the overall variance. The overall setting makes sense. I found your example (in the second paragraph of the introduction) initially somewhat misleading, though: in the setting where a game is composed of fully independent rounds, surely these would simply be modeled as completely separate MDPs. Even if not, settings where the rounds are reset after a variable length of time (e.g. the round ends when one player achieves some objective) would *not* fit the exact independence structure you assume at the start of Section 3, if your current action affects when the game will reset. But of course your estimator does not rely on actual *independence* (C = 0); it can take advantage of only *weak dependence* (and moreover this dependence need not be pre-specified). You might think about emphasizing this a little more in the introduction to emphasize that the estimator is general, and you*re looking for one that can take advantage of these kinds of situations. It might be worth noting after (2) that is upper-bounded by , so that the importance sampler is always well-defined and unbiased when action probabilities are nonzero. This does raise an issue: a policy which *ever* deterministically avoids an action, i.e. in (13) is 0, will break the method. This is worth explicitly stating somewhere. Something worth thinking about a bit: any choice of weights for your control variate provably doesn*t affect your estimator in expectation (and you try to decrease its variance), so that bad estimation of e.g. the quantities in (7) won*t lead you to being *incorrect,* just higher-variance. But a bad choice of parameters in your estimator *would* bias your estimates. This is in some ways the same as the effect of using a value function or -function approximator, but can we say anything about the ways in which a bad estimator would likely affect the overall optimization process, perhaps in some very simple case? Would an unbiased estimator lead to an unbiased advantage estimator? (Not that it*s clear how to get an unbiased estimator of the ratio in anyway.) Some minor points on notation: Using for the control variate was initially confusing to me, because elsewhere you*ve used e.g. for the random variable of a state and as the value of that state -- it made me think that was somehow supposed to be the value of a reward . Another letter might be better. Similarly, of (7) isn*t really a value function; it*s the difference between the value function and the sum of discounted control variates. Also, doesn*t estimate : it estimates , so it might make more sense notationally to just subtract one from the definition of . Overall: I think the idea in this paper is sensible, the derivations fairly clear, and it seems to help empirically. It does add a lot of *moving parts* to the already-complicated RL setup, though, and I*m not well-versed enough in the RL literature to have much of a sense of how convincing these experiments are; hopefully another reviewer is."}
{"id": "iclr2020_921", "title": "Deep Variational Semi-Supervised Novelty Detection | OpenReview", "abstract": "Abstract:###In anomaly detection (AD), one seeks to identify whether a test sample is abnormal, given a data set of normal samples. A recent and promising approach to AD relies on deep generative models, such as variational autoencoders (VAEs),for unsupervised learning of the normal data distribution. In semi-supervised AD (SSAD), the data also includes a small sample of labeled anomalies. In this work,we propose two variational methods for training VAEs for SSAD. The intuitive idea in both methods is to train the encoder to ‘separate’ between latent vectors for normal and outlier data. We show that this idea can be derived from principled probabilistic formulations of the problem, and propose simple and effective algorithms. Our methods can be applied to various data types, as we demonstrate on SSAD datasets ranging from natural images to astronomy and medicine, and can be combined with any VAE model architecture. When comparing to state-of-the-art SSAD methods that are not specific to particular data types, we obtain marked improvement in outlier detection.", "review": "Review:###This paper proposes two variational methods for training VAEs for SSAD (Semi-supervised Anomaly Detection). Experiments on benchmarking datasets show improvements over state-of-the-art SSAD methods. In generally, the paper is well written. But I have some concerns. 1. Some of the results have not yet been obtained. 2. Missing some relevant references. In addition to VAEs, there is another class of deep generative models - random fields (a.k.a. energy-based models, EBMs), which have been applied to anomaly detection (AD) recently. Particularly, the unsupervised AD results on MNIST and CIFAR-10 from [2] are much better than the proposed methods (MML-VAE, DP-VAE). Though semi-supervised AD is interesting, good performances on unsupervised AD can be a baseline indicator of the effectiveness of the AD models. The authors should add comments and comparisons. [1] S. Zhai, Y. Cheng, W. Lu, and Z. Zhang, “Deep structured energy based models for anomaly detection,” ICML, 2016. [2] Y. Song, Z. Ou. *Learning Neural Random Fields with Inclusive Auxiliary Generators,* arxiv 1806.00271, 2018. 3. “For all of the experiments, our methods use an ensemble of size K = 5.” Are other methods also tested by using an ensemble? --------update after reading the response----------- The updated paper has been improved to address my concerns. I partly agree with the authors that their results demonstrate the importance of the semi-supervised AD setting (a 1% fraction of labelled anomalies can improve over the state-of-the-art AD scores of deep energy based models). However, I think, the proposed methods in this paper will not be as competitive as semi-supervised deep energy based models."}
{"id": "iclr2020_922", "title": "Curvature-based Robustness Certificates against Adversarial Examples | OpenReview", "abstract": "Abstract:###A robustness certificate against adversarial examples is the minimum distance of a given input to the decision boundary of the classifier (or its lower bound). For {it any} perturbation of the input with a magnitude smaller than the certificate value, the classification output will provably remain unchanged. Computing exact robustness certificates for deep classifiers is difficult in general since it requires solving a non-convex optimization. In this paper, we provide computationally-efficient robustness certificates for deep classifiers with differentiable activation functions in two steps. First, we show that if the eigenvalues of the Hessian of the network (curvatures of the network) are bounded, we can compute a robustness certificate in the norm efficiently using convex optimization. Second, we derive a computationally-efficient differentiable upper bound on the curvature of a deep network. We also use the curvature bound as a regularization term during the training of the network to boost its certified robustness against adversarial examples. Putting these results together leads to our proposed {_x0008_f C}urvature-based {_x0008_f R}obustness {_x0008_f C}ertificate (CRC) and {_x0008_f C}urvature-based {_x0008_f R}obust {_x0008_f T}raining (CRT). Our numerical results show that CRC outperforms CROWN*s certificate by an order of magnitude while CRT leads to higher certified accuracy compared to standard adversarial training and TRADES.", "review": "Review:###Summary ======== This paper proposes the Curvature-based Robustness Certificate (CRC) and Curvature-based Robust Training (CRT) for robustness certificate against adversarial examples. The proposed techniques are theoretically formulated and empirically justified. The authors showed that, when the curvature (Hessian) of the network is bounded, improved certificate can be achieved by convex optimization. Explicit curvature regularization via CRT seems further improve both the certified robustness accuracy and the certificate, at a cost of 2% decrease in empirical robust accuracy. While the proposed approaches are theoretically sound, I have several concerns, mostly on the experiments. ================= 1. Existing certification methods should be compared in more details, in terms of different assumptions, activation functions, etc. A summary table can help. 2. Comprehensive comparisons to more existing works such as *Certified adversarial robustness via randomized smoothing* (more formal comparison than in the appendix). 3. Experiments on CIFAR-10 and different activation functions in the main text. 4. Not sure why compare to uncertified defenses PGD and TRADES, though it doesn*t seem to hurt the conclusions. =============== After rebuttal: Thanks for the response, my concerns have been addressed, rating upgraded to 6: weak accept."}
{"id": "iclr2020_923", "title": "Strong Baseline Defenses Against Clean-Label Poisoning Attacks | OpenReview", "abstract": "Abstract:###Targeted clean-label poisoning is a type of adversarial attack on machine learning systems where the adversary injects a few correctly-labeled, minimally-perturbed samples into the training data thus causing the deployed model to misclassify a particular test sample during inference. Although defenses have been proposed for general poisoning attacks (those which aim to reduce overall test accuracy), no reliable defense for clean-label attacks has been demonstrated, despite the attacks* effectiveness and their realistic use cases. In this work, we propose a set of simple, yet highly-effective defenses against these attacks. We test our proposed approach against two recently published clean-label poisoning attacks, both of which use the CIFAR-10 dataset. After reproducing their experiments, we demonstrate that our defenses are able to detect over 99% of poisoning examples in both attacks and remove them without any compromise on model performance. Our simple defenses show that current clean-label poisoning attack strategies can be annulled, and serve as strong but simple-to-implement baseline defense for which to test future clean-label poisoning attacks.", "review": " In this paper, the authors propose a simple and effective method to address the problem of targeted clean-label poisoning where the adversary injects a few samples into the training data thus causing the deployed model to misclassify a particular test sample during inference. Experiments demonstrate that the proposed defenses are able to detect over 99% of poisoning examples. The paper is clearly written by addressing an important problem but I still have several concerns: 1. In general, the Knn is sensitive to the parameter k, which is not robust in some cases. The authors are expected to clarify how to set the proper parameters especially for the complex tasks. 2. The authors are expected to make more comprehensive evaluations to demonstrate their advantages compared with alternative methods, and the effectiveness to other poisoning attacks. 3. It is supposed that the performance highly relies on the feature extraction and the authors are expected to further justify how the performance would be if the features are not reliable due to the adversarial attacks."}
{"id": "iclr2020_924", "title": "EDUCE: Explaining model Decision through Unsupervised Concepts Extraction | OpenReview", "abstract": "Abstract:###Providing explanations along with predictions is crucial in some text processing tasks. Therefore, we propose a new self-interpretable model that performs output prediction and simultaneously provides an explanation in terms of the presence of particular concepts in the input. To do so, our model*s prediction relies solely on a low-dimensional binary representation of the input, where each feature denotes the presence or absence of concepts. The presence of a concept is decided from an excerpt i.e. a small sequence of consecutive words in the text. Relevant concepts for the prediction task at hand are automatically defined by our model, avoiding the need for concept-level annotations. To ease interpretability, we enforce that for each concept, the corresponding excerpts share similar semantics and are differentiable from each others. We experimentally demonstrate the relevance of our approach on text classification and multi-sentiment analysis tasks.", "review": "Review:###The authors proposed a self-explainable deep net architecture that could be used for text categorization. The main idea is to force the network to extract *excerpts*, from the input text, each corresponds to a concept, which are also learned for interpretation. The classification is finally made based off of the learned concept, which is a binary vector. All three steps are learned in an end-to-end manner. The learning of concepts is regularized to make sure the concepts are consistent and non-overlapping. The idea sounds interesting and the experimental results support the usefulness of the proposed method on a variety of datasets. My sole concern is about the sensitivity analysis of the explanation, i.e. how robust is the explanation with respect to the perturbations that do not change the classifier prediction. It has been discussed in the literature that many explanation methods suffer from this sensitivity issue."}
{"id": "iclr2020_925", "title": "Connecting the Dots Between MLE and RL for Sequence Prediction | OpenReview", "abstract": "Abstract:###Sequence prediction models can be learned from example sequences with a variety of training algorithms. Maximum likelihood learning is simple and efficient, yet can suffer from compounding error at test time. Reinforcement learning such as policy gradient addresses the issue but can have prohibitively poor exploration efficiency. A rich set of other algorithms, such as data noising, RAML, and softmax policy gradient, have also been developed from different perspectives. In this paper, we present a formalism of entropy regularized policy optimization, and show that the apparently distinct algorithms, including MLE, can be reformulated as special instances of the formulation. The difference between them is characterized by the reward function and two weight hyperparameters. The unifying interpretation enables us to systematically compare the algorithms side-by-side, and gain new insights into the trade-offs of the algorithm design. The new perspective also leads to an improved approach that dynamically interpolates among the family of algorithms, and learns the model in a scheduled way. Experiments on machine translation, text summarization, and game imitation learning demonstrate superiority of the proposed approach.", "review": "Review:###This paper claims to propose a general entropy regularized policy optimization paradigm. MLE and RL are special cases of this training paradigm. Paper is well written, and the experimental results are convincing enough. However, there are still some minor problems in the paper. For the optimization framework ERPO (shown in Equation 1), it consists of three parts, a cross-entropy term (Shannon entropy), a KL divergence term, and a reinforcement learning reward loss item. From the framework point of view, it is not like the author claim that is supposed to present a general optimization framework, including various optimization algorithms. Instead, it is just a combined loss through weight control and the selection of corresponding functions. It may not really theoretically work to unify various types of optimization algorithms for general cases, let alone claiming that this is a general optimization algorithm framework. For the interpolation algorithm (I regard this is the true technical contribution of this paper), the authors used an annealing mechanism to use different weights and functions at different stages of training. The essence is that after MLE pre-training, different optimization algorithms are used in different stages, and this should be the focus of the article. The annealing settings used is only introduced in the appendix simply. Without more comparison experiments, we cannot clearly get the conditions for the annealing algorithm to be effective and ineffective. For the title of connecting the dots between MLE and RL, this paper did not do so, MLE and RL are only used collaboratively, and this has also been mentioned in previous work. typo Page 6 Paragraph “Other Algorithms & Discussions”: We We show in the appendix… -> We show in the appendix…"}
{"id": "iclr2020_926", "title": "Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning | OpenReview", "abstract": "Abstract:###The posteriors over neural network weights are high dimensional and multimodal. Each mode typically characterizes a meaningfully different representation of the data. We develop Cyclical Stochastic Gradient MCMC (SG-MCMC) to automatically explore such distributions. In particular, we propose a cyclical stepsize schedule, where larger steps discover new modes, and smaller steps characterize each mode. We prove non-asymptotic convergence theory of our proposed algorithm. Moreover, we provide extensive experimental results, including ImageNet, to demonstrate the effectiveness of cyclical SG-MCMC in learning complex multimodal distributions, especially for fully Bayesian inference with modern deep neural networks.", "review": " The paper develops a cyclical stepsize schedule for choosing stepsize for Langevin dynamics. The authors prove the non-asymptotic convergence theory of the proposed algorithm. Many experimental results, including ImageNet, are given to demonstrate the effectiveness of the proposed method. Here I suggest that authors also need to point out that the continuous-time MCMC is the Wasserstein gradient flow of KL divergence. The bound derived in this paper focus on the step size choice of gradient flows. This could be a good direction for combining gradient flows studies in optimal transport and MCMC convergence bound for the choice of step size. Overall, I think that the paper is well written with clear derivations. I strongly suggest the publication of this paper."}
{"id": "iclr2020_927", "title": "B-Spline CNNs on Lie groups | OpenReview", "abstract": "Abstract:###Group convolutional neural networks (G-CNNs) can be used to improve classical CNNs by equipping them with the geometric structure of groups. Central in the success of G-CNNs is the lifting of feature maps to higher dimensional disentangled representations, in which data characteristics are effectively learned, geometric data-augmentations are made obsolete, and predictable behavior under geometric transformations (equivariance) is guaranteed via group theory. Currently, however, the practical implementations of G-CNNs are limited to either discrete groups (that leave the grid intact) or continuous compact groups such as rotations (that enable the use of Fourier theory). In this paper we lift these limitations and propose a modular framework for the design and implementation of G-CNNs for arbitrary Lie groups. In our approach the differential structure of Lie groups is used to expand convolution kernels in a generic basis of B-splines that is defined on the Lie algebra. This leads to a flexible framework that enables localized, atrous, and deformable convolutions in G-CNNs by means of respectively localized, sparse and non-uniform B-spline expansions. The impact and potential of our approach is studied on two benchmark datasets: cancer detection in histopathology slides (PCam dataset) in which rotation equivariance plays a key role and facial landmark localization (CelebA dataset) in which scale equivariance is important. In both cases, G-CNN architectures outperform their classical 2D counterparts and the added value of atrous and localized group convolutions is studied in detail.", "review": "Review:###This paper proposes a neural network architecture which that enables the implementation of group convolutional neural networks for arbitrary Lie groups. This lifts a significant limitation of such models which were previously confined to discrete or continuous compact groups due to tractability issues. I*m afraid that this paper is over my head. It relies heavily on field-specific terminology and as such is likely to be accessible to a relatively small subset of researchers. This looks to me like a solid contribution, however I*m really not qualified to judge."}
{"id": "iclr2020_928", "title": "Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving | OpenReview", "abstract": "Abstract:###Detecting objects such as cars and pedestrians in 3D plays an indispensable role in autonomous driving. Existing approaches largely rely on expensive LiDAR sensors for accurate depth information. While recently pseudo-LiDAR has been introduced as a promising alternative, at a much lower cost based solely on stereo images, there is still a notable performance gap. In this paper, we provide substantial advances to the pseudo-LiDAR framework through improvements in stereo depth estimation. Concretely, we adapt the stereo network architecture and loss function to be more aligned with accurate depth estimation of faraway objects --- currently the primary weakness of pseudo-LiDAR. Further, we explore the idea to leverage cheaper but extremely sparse LiDAR sensors, which alone provide insufficient information for 3D detection, to de-bias our depth estimation. We propose a depth-propagation algorithm, guided by the initial depth estimates, to diffuse these few exact measurements across the entire depth map. We show on the KITTI object detection benchmark that our combined approach yields substantial improvements in depth estimation and stereo-based 3D object detection --- outperforming the previous state-of-the-art detection accuracy for faraway objects by 40%.", "review": "Review:###The paper proposes two extensions to the recent work of (Wang et al., 2019) on 3D object detection with pseudo LiDAR data. Wang et al. showed that 3D object detection using stereo images as inputs can be significantly improved if the depth map is projected to 3D and treated like a LiDAR point cloud (i.e., using methods that utilize the LiDAR point cloud). This paper shows that one shortcoming of this approach is given by the fact that the depth uncertainty increases the farther the objects are away. To remedy this, the authors propose to train the stereo estimation network (based on Chang & Chen, 2018) directly with depth outputs, instead of disparity values (inverse depth), by rewriting the loss and converting the cost volume. This already boosts the performance for far away objects. The authors demonstrate that the usage of a (simulated) low-cost 4-beam LiDAR can further facilitate the detection. For this purpose a graph diffusion algorithm is listed that aligns the pseudo LiDAR point cloud from the stereo set-up with the depth estimates from the low-cost LiDAR. Simulating the low-cost LiDAR on the Kitti benchmarks shows that this approach further increases the performance of the object detection methods. In general, I am in favour of accepting the paper as it shows two orthogonal and interesting additions to the pseudo LiDAR paper of Wang et al. that improve its performance. However, I would like to see some clarifications in the rebuttal. The proposed stereo network converts a disparity cost volume to a depth cost volume using bilinear interpolation. I agree, that the 3D convolutions are more meaningful (given the spacing of the grid cells) on the latter, but why the detour over the disparity cost volume? It should be possible to build the depth cost volume directly, which would lead to decreased memory consumption and speed up the method without any loss in accuracy? One assumption of the second contribution (GDC) is that at least one beam of the LiDAR will hit the k-connected local point cloud. Can you give some bounds on the likelihood that this happens, especially for far away objects it could be unlikely, although it is most beneficial for those objects. Further, I am missing a details on the optimization of (7) and (8). What is meant with slight L2 regularization? In the appendix it is also stated that a slightly different objective is optimized? Finally, the notation could also be improved. The authors are using L and G for the LiDAR point cloud and PL and Z for the pseudo LiDAR point cloud and then in the Z* is used for both. Fig. 4 shows the median error in meters for the different variants of the stereo network. Why has the median been used? Are there severe outliers? If yes, it would also be interesting to quantify those and compare them (e.g., box plots). In the abstract and in the discussion the authors oversell their results a bit. At the one hand they state that PL++ with GDC performs significantly better than PL++ w/o GDC, on the other hand they also claim that PL++ achieves comparable results to models that have access to the full 64-beam LiDAR data. However, if you compare the differences, then the gaps are for several cases almost as big, or bigger as in the former claim. Things to improve the paper that did not impact the score: - In equation (2) you could replace the x with a . (cdot), or completely remove it - On page 5: KNN neighbors -> k-nearest neighbors - Also on page 5: write out W.l.o.g. - In Tab. 1 it would help to highlight (bold) the best entries per column"}
{"id": "iclr2020_929", "title": "Predictive Coding for Boosting Deep Reinforcement Learning with Sparse Rewards | OpenReview", "abstract": "Abstract:###While recent progress in deep reinforcement learning has enabled robots to learn complex behaviors, tasks with long horizons and sparse rewards remain an ongoing challenge. In this work, we propose an effective reward shaping method through predictive coding to tackle sparse reward problems. By learning predictive representations offline and using these representations for reward shaping, we gain access to reward signals that understand the structure and dynamics of the environment. In particular, our method achieves better learning by providing reward signals that 1) understand environment dynamics 2) emphasize on features most useful for learning 3) resist noise in learned representations through reward accumulation. We demonstrate the usefulness of this approach in different domains ranging from robotic manipulation to navigation, and we show that reward signals produced through predictive coding are as effective for learning as hand-crafted rewards.", "review": "Review:###*Synopsis*: This paper proposes using the features learned through Contrastive Predictive Coding as a means for reward shaping. Specifically, they propose to cluster the embedding using the clusters to provide feedback to the agent by applying a positive reward when the agent enters the goal cluster. In more complex domains they add another negative distance term of the embedding of the current state and goal state. Finally, they provide empirical evidence of their algorithm working in toy domains (such as four rooms and U-maze) as well as a set of control environments including AntMaze and Pendulum. Main Contributions: - Using the embedding learned through Contrastive Predictive Coding for reward shaping. - A reward shaping scheme that seems generally applicable to any embedding. *Review*: I think the paper provides a compelling motivation, and is well written. I think using the embedding learned through CPC could provide meaningful semantics for representation learning and for reward shaping (as done in the current paper), and encourage the authors to continue down this line of inquiry. Unfortunately, I have several concerns over the method as currently implemented and the empirical comparisons (specifically with the chosen competitors) which I detail below. Given these concerns I am unwilling to recommend accepting this paper, unless several of these concerns are addressed. 1. This algorithm, by nature, is purely offline as the CPC and clustering all are currently done offline. Furthermore, the clustering portion of this approach requires states to be randomly sampled from the environment to create a nice set of clusters which are representative of the environment*s full state space. These two requirements significantly limit this approach, especially when looking at domains where simulation is not possible, or the underlying state distribution is unknown. By and all, I don*t think this means we should completely discount this method entirely and the authors do mention this as a detriment to their algorithm in section 6.3. I*m wondering if this paper should look at implementing an online version before publication, but think this is less prescient to the other concerns. 2. I am concerned about the current policy learning scheme (i.e. tiered policy (1) go to the correct cluster (2) go to the goal) which seems to only be used by your approach. This invalidates the comparisons made with the other reward shaping schemes as this goes beyond reward shaping (you are learning two separate policies). 3. The current competitors are unsatisfactory as they don*t include other reward shaping techniques from the literature. Also the related works section seems to completely disregard this part of the literature. I would recommend comparing your approach to (methods you even cite!): - *Reward Shaping via Meta Learning* by Zou et. al https://arxiv.org/pdf/1901.09330.pdf - *Potential based reward shaping* by Gao et. al. https://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/viewPaper/10930 (I*m sure there are others beyond these) 4. I*m also concerned with the current significance of the results, particularly AntMaze, Half Cheetah, and Reacher. I*m especially concerned because I don*t feel your competitors are not relevant/representative of the current state of reward shaping. 5. It would be worthwhile to try this method on several RL algorithms (i.e. Q-learning, TRPO, SAC, DDPG, etc...). This will help readers understand if this approach is a general method, or only applicable to PPO. 6. I quite like the idea of predictive coding (albeit the original scheme presented by Rao and Ballard 1999) as an unsupervised representation learning scheme, but am unsure this is critical for your method and the current approach is not really predictive coding in a sense (or at least the ideas I*m familiar with from cognitive computational neuroscience). I am concerned with the predictive coding idea being highlighted here as a key ingredient, but none of the papers containing the originating idea of predictive coding are mentioned. Rao and Ballard is one, but there is a rich literature following from this work into the free energy formulation (Friston) and active learning. These ideas should appear in your introduction, as you heavily rely on them. Also there are other predictive coding schemes for unsupervised representation learning (such as PredNets from David Cox https://coxlab.github.io/prednet/), which I believe should be mentioned. In light of these other methods, I*m not sure your discussion on only using predictive features for reward shaping is accurate, and instead these claims should be softened for only features learned through CPC. Missing experimental settings: - Number of runs tested - What are the error bars in your plots? I would also like to recommend *Deep Reinforcement Learning that Matters* by Henderson for a reference on how to conduct meaningful Deep RL experiments using policy gradient methods (https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16669). I think this paper could benefit from the recommendations made there. More questions/clarifications: Q1: What constitutes success in the grid world domains for table 1? Q2: If you were to train a single policy using your reward shaping scheme (i.e. not the tiered approach currently employed) how would the new policy perform? Are there problems with the current scheme where you have to have the tiered policy? Q3: Why CPC and not say an autoencoder or VAE? A comparison over many types of unsupervised embedding learning algorithms could be interesting and make your method more general than currently presented. It could also strengthen your argument for using CPC. Q4: How long were the trajectories for training CPC? *Other minor comments not taken into account in the review* - I think your labels are backwards in table 1."}
{"id": "iclr2020_930", "title": "Blockwise Adaptivity: Faster Training and Better Generalization in Deep Learning | OpenReview", "abstract": "Abstract:###Stochastic methods with coordinate-wise adaptive stepsize (such as RMSprop and Adam) have been widely used in training deep neural networks. Despite their fast convergence, they can generalize worse than stochastic gradient descent. In this paper, by revisiting the design of Adagrad, we propose to split the network parameters into blocks, and use a blockwise adaptive stepsize. Intuitively, blockwise adaptivity is less aggressive than adaptivity to individual coordinates, and can have a better balance between adaptivity and generalization. We show theoretically that the proposed blockwise adaptive gradient descent has comparable regret in online convex learning and convergence rate for optimizing nonconvex objective as its counterpart with coordinate-wise adaptive stepsize, but is better up to some constant. We also study its uniform stability and show that blockwise adaptivity can lead to lower generalization error than coordinate-wise adaptivity. Experimental results show that blockwise adaptive gradient descent converges faster and improves generalization performance over Nesterov*s accelerated gradient and Adam.", "review": " The paper proposes adaptive gradient approaches where the step-size is not determined on the per-coordinate basis but rather for blocks of coordinates. Theoretical results are presented in terms of regret in online convex optimization, regarding convergence in non-convex optimization, and with respect to uniform stability and generalization. These indicate that under certain conditions adaptivity at the block level could outperform coordinate-wise adaptivity. The approach is evaluated against alternatives on simulated and real-world problems. The paper considers an important topic, which has been the object of many related studies. Though the proposed approach is interesting, there are several issues with the present manuscript that would warrant significant revision. - Though the discussion in section 3 aims at motivating the use of block-wise adaptivity, it is quite confusing. Indeed the problem considered in that section is different from the setup eventually considered. Moreover the paper claims that it is more general than the many previous work on layer-wise adaptation. However, the example considered here does consider layers. - The paper organization could be improved significantly. BAG is presented followed by a regret analysis in convex optimization. Then BAGM is presented followed by the theory on non-convex optimization. It might be more effective to first present the algorithms BAG and BAGM and then have a theory section. It would also be good to emphasize that the regret analysis is solely for convex optimization, which makes it much less relevant in the context of deep learning. - The claimed superiority of block-wise adaptivity in the theory relies heavily on an assumption relying on tightness and closeness of upper bounds on gradient magnitude / gradient second moment in each block. As this assumption is crucial to the results, its validity in practice should be more thoroughly investigated, beyond the small study relegated in the appendix B. It would also be important to assess what happens when the assumption breaks down. - The aforementioned issue is somewhat also related to the choice of design for the blocks. Some choices might help, some might hurt based on the unknown structure of the data. This is barely touched upon in section 5.1. as it is noted that the more sever the mismatch the worse the results. - The empirical evaluation needs to be more comprehensive in terms of comparison methods. Among others it would be important to assess performance against previously proposed layer wise stepwise approaches mentioned in the introduction. - These related approaches should also be discussed more deeply and contrasted against qualitatively. - The empirical results are not substantially superior. It is also quite disappointing to see that the training, testing and generalization curves on Figure 2 are quite similar with NAG and Adam and do not exhibit less instability etc."}
{"id": "iclr2020_931", "title": "A Graph Neural Network Assisted Monte Carlo Tree Search Approach to Traveling Salesman Problem | OpenReview", "abstract": "Abstract:###We present a graph neural network assisted Monte Carlo Tree Search approach for the classical traveling salesman problem (TSP). We adopt a greedy algorithm framework to construct the optimal solution to TSP by adding the nodes successively. A graph neural network (GNN) is trained to capture the local and global graph structure and give the prior probability of selecting each vertex every step. The prior probability provides a heuristics for MCTS, and the MCTS output is an improved probability for selecting the successive vertex, as it is the feedback information by fusing the prior with the scouting procedure. Experimental results on TSP up to 100 nodes demonstrate that the proposed method obtains shorter tours than other learning-based methods.", "review": "Review:###The paper proposes learning a TSP solver that incrementally constructs a tour by adding one city at a time to it using a graph neural network and MCTS. The problem is posed as a reinforcement learning problem, and the graph neural network parameters are trained to minimize the tour length on a training set of TSP instances. A graph neural network architecture called Static Edge Graph Neural Networks is introduced which takes into account the graph of all cities in a given problem instance as well as the partial tour constructed so far in an episode. The network predicts probabilities for the remaining cities to be selected as the next city in the tour, which is then used to compute a value function that guides MCTS. Results on synthetic TSP instances with 20, 50, and 100 cities show that the approach is able to achieve better objective values than prior learning-based approaches. Applying AlphaZero-like approaches to TSP is an interesting test case for understanding how well they can work on hard optimization problems. The paper has several drawbacks: - The evaluation seems to be flawed as there is no mention of running time of the various algorithms being compared anywhere in the text. It’s not possible to make a fair comparison without controlling for running time. As an extreme example, even random search will eventually find the global optimum if given sufficient time. So the results are not very meaningful without the running times. - Novelty is fairly low. The changes in SEGNN compared to previous works are incremental or not novel, and the overall idea is the same as AlphaGo/Zero. While I don’t think novelty is a strict requirement, if it is absent, then it should be compensated with strong empirical results, but the paper lacks that as well. - A discussion on whether the approach can plausibly scale to much larger TSP instances is missing. First, there is the question of whether learning can succeed on much larger instances. Second, even if good policies can indeed be learned, can they provide competitive running times compared to the state-of-the-art TSP solvers? Graph net inference’s compute cost scales linearly with graph size (number of cities), and since multiple inference passes need to be performed per step (to pick the next city to add to the current partial tour), the overall cost scales quadratically. This is worse than the empirical scaling of solvers like LKH and POPMUSIC. One has to consider approaches with cost that scales roughly linearly to be able to compete with state-of-the-art solvers. It should be noted that TSP instances with <= 100 cities are really trivial for the best solvers, and outperforming them with a learning-based approach may not be plausible until much larger instances are considered (e.g., > 10K cities). The ML community needs to move away from evaluating on small instances if the long term goal is to beat state-of-the-art solvers with learning. Additional comments: - There are a lot of typos. A few that I caught: Tables 1 and 7 say “Rondom”, “approximation ration”, “ReLu”, “provides a heuristics”, “Similar to the implement”. - Table 6 gives the highest test accuracy during training, but this could be misleading (e.g., there could be random spikes in test performance during training). A smoother metric should be used. - Table 3 title is confusing."}
{"id": "iclr2020_932", "title": "Learning to Infer User Interface Attributes from Images | OpenReview", "abstract": "Abstract:###We present a new approach that helps developers automate the process of user interface implementation. Concretely, given an input image created by a designer (e.g, using a vector graphics editor), we learn to infer its implementation which when rendered (e.g., on the Android platform), looks visually the same as the input image. To achieve this, we take a black box rendering engine and a set of attributes it supports (e.g., colors, border radius, shadow or text properties), use it to generate a suitable synthetic training dataset, and then train specialized neural models to predict each of the attribute values. To improve pixel-level accuracy, we also use imitation learning to train a neural policy that refines the predicted attribute values by learning to compute the similarity of the original and rendered images in their attribute space, rather than based on the difference of pixel values.", "review": "Review:###This paper proposes an approach for reverse-engineering webpages using Siamese networks and imitation learning. While the idea of using synthetic data (which can be easily procedurally generated) to do this reverse-engineer training is very clever, prior work has exploited it also. Novel elements include the attribute refinement using imitation learning, and the authors show the effect of this step, but the improvement is small. Thus, the limited novelty and not very convincing results make the question the potential impact of this paper. Some questions: a) The authors mention they cannot use a GAN-style method because all generated images are by definition true/real; how about learning whether a *pair* is real or fake? (where the pair consists of the design specification and the rendered version). b) Are the baselines strong enough? None of them seem to be from recent prior work. How about a direct comparison to some of the work listed in the second para on page 2?"}
{"id": "iclr2020_933", "title": "Entropy Minimization In Emergent Languages | OpenReview", "abstract": "Abstract:###There is a growing interest in studying the languages emerging when neural agents are jointly trained to solve tasks requiring communication through a discrete channel. We investigate here the information-theoretic complexity of such languages, focusing on the basic two-agent, one-exchange setup. We find that, under common training procedures, the emergent languages are subject to an entropy minimization pressure that has also been detected in human language, whereby the mutual information between the communicating agent*s inputs and the messages is minimized, within the range afforded by the need for successful communication. This pressure is amplified as we increase communication channel discreteness. Further, we observe that stronger discrete-channel-driven entropy minimization leads to representations with increased robustness to overfitting and adversarial attacks. We conclude by discussing the implications of our findings for the study of natural and artificial communication systems.", "review": " This paper sets up a couple discrete communication games in which agents must communicate a discrete message in order to solve a classification task. The paper claims that networks trained to perform this case show a tendency to use as little information as necessary to solve the task. I vote to reject this paper. The experiments are not very interesting and I don*t at all agree with the assertion of the paper. The paper claims the networks use only the entropy necessary to solve the task, but there are two main problems with this assertion. (1) their own experiments don*t support this all that strongly, as in the limit of few hidden bits (left half of the x axis in Figure 1), the networks all had noticeable excess information, and (2) and perhaps most damning the paper applies entropy regularization on the sender during training? Could it perhaps instead be the fact that the entropy of the sender was penalized as an explicit regularization term that the entropy of the senders messages tended to be small? I also find the experimental design puzzling. Why both reinforce and the *stochastic computation graph* approach? Treating the receiver*s output as binary and stochastic without using the log loss of the bernoulli observation model is just giving up on a good gradient as far as the receiver is concerned. The experiments done are much to simple and the protocol flawed. The second set of experiments in Figure 3 were not left to converge, so I*m not sure how we can derive a great deal of insight. Additionally, relaxing the gumbel softmax channel to being continuous rather than discrete technically ruins any argument that there is an entropy bottleneck present anymore, as theoretically even a single dimensional continuous signal could store an arbitrary amount of information. If the paper wanted to, it could have upper bounded the mutual information between the input and the message using a variational upper bound. --------------- Response to Response --------------------------------- I*m editing here in light of continuing to look at the paper and the responses from the author below. I have to still argue for a rejection of this paper. I thank the authors for addressing my comments and I admit that at first I thought the paper was minimizing the entropy during training which would have been particularly bad. While I was mistaken on that point, I still believe the paper is deeply flawed. In particular, the paper makes a very bold claim, namely that *We find that, under common training procedures, the emergent languages are subject to an entropy minimization pressure that has also been detected in human language, whereby the mutual information between the communicating agent’s inputs and the messages is minimized, within the range afforded by the need for successful communication.* But if we are being honest here, the experiments are very lacking to support such a bold claim. In particular there was one thing I was worried about upon reading the paper again, and is similar to the point raised by the other reviewers. In Figure 1, we are shown the entropy only of those networks that have succeeded. Naturally to succeed, the entropy i the message must be large enough to accomodate the size of the remaining bits we are trying to reconstruct. That is why Figure 1 includes the dotted line, since the networks must be above that line to have good performance. And the main evidence for the main claim of the paper is that the trained networks are above that line and arguably close to it. Now, we know that there are clearly solutions to these tasks (in particular the Guess Number task) which could achieve good performance at noticeably higher entropy. For instance we could take any minimal solution and simply split up each message into 8x different buckets, each of which had exactly the same behavior from the decoder. This would give us a +3 in the entropy of our message space while having no effect whatsoever on the loss. The claim of the paper is that under normal training procedures it seems like we don*t find those solutions and instead seem to find minimal ones. But after implementing a simplified version of the experiment in the paper (Notebook available here: https://nbviewer.jupyter.org/urls/pastebin.com/raw/ZF7g34GN ) I suspect something much simpler is going on. The reason the solutions look minimal in Figure 1 is probably because the initialization chosen for the encoder they used in the paper tended to start at low entropies. Imagine if all of the networks started out with an initial message entropy of around 3 bits. Then Figure 1 could be explained by the problems with hidden bits ~< 3 bits simply preserved their entropy, which in order to solve the task with higher numbers of digits hidden we know requires some minimal budget, so they get sort of pushed up. This could explain the figure, but we wouldn*t claim this explains why we observe small entropy for the high number of hidden digits case. In particular, if we initialized the encoders with higher entropy, we might expect that we fail to see this phenomenon. That is exactly what I was able to show for myself in that notebook. If you simply initialize the encoder to have high entropy, all of the solutions have high entropy and the observed effect goes away. Overall, the paper as I said is low quality. Several choices were made that don*t make a lot of sense. With the experiments being as small scale as they were, why not explicitly marginalize out the message (as I did in the notebook)? Why use single layer neural networks to predict 256 x 1024 parameters? Why not just learn them directly? If the paper aimed to mimic more standard setups and show that under those setups we observe this kind of minimal message entropy, then it would have to much better tease out the effects of all of these choices. Why does the decoder use a mean field sigmoid bernoulli observation model to try to predict something is in one of ~32 states? The missing digits are not independent given the message, why model them as so? Is that part of the purported reason why these models show minimal entropy (cause it isn*t discussed). For such a simple problem, you could presumably analytically compute the gradient with respect to the loss and study whether that correlates with the gradient of the entropy. There are several things I could imagine checking, none of which are checked in the paper. The primary question the paper addresses is an interesting one. But this paper does very little to carefully investigate that question. I maintain my vote to reject."}
{"id": "iclr2020_934", "title": "Conditional Invertible Neural Networks for Guided Image Generation | OpenReview", "abstract": "Abstract:###In this work, we address the task of natural image generation guided by a conditioning input. We introduce a new architecture called conditional invertible neural network (cINN). It combines the purely generative INN model with an unconstrained feed-forward network, which efficiently pre-processes the conditioning input into useful features. All parameters of a cINN are jointly optimized with a stable, maximum likelihood-based training procedure. Even though INNs and other normalizing flow models have received very little attention in the literature in contrast to GANs, we find that cINNs can achieve comparable quality, with some remarkable properties absent in cGANs, e.g. apparent immunity to mode collapse. We demonstrate these properties for the tasks of MNIST digit generation and image colorization. Furthermore, we take advantage of our bidirectional cINN architecture to explore and manipulate emergent properties of the latent space, such as changing the image style in an intuitive way.", "review": " The authors propose to use a normalizing flow architecture to tackle the structured output problem of generalization. They propose: - a conditioning architecture: they use a convolutional feature extractor (similar to a U-Net architecture), and (on top of the common architectural details of models like Glow - Kingma and Dhariwal, 2018) uses Haar wavelets for downsampling; - they train their architecture stably using the maximum likelihood principle; - they demonstrate interesting properties of their model coming from the bijectivity. This is an interesting application of the architecture to colorization. The diverse and consistent colorization results are compelling (with comparison with previous methods), while clearly showing the failure cases where the model should be improved. Ablation studies are done to show the importance of different components (e.g. the conditioning network). The paper is clearly written. A few remarks: - arctan soft-clamping seems very similar to the scalar times tanh soft-clamping of Real NVP (Dinh et al., 2016). Why was arctan adopted? - the choice of the car image (the biggest one in Figure 10) for the colorization transfer is questionable. I*m not able to tell from this figure if there was any segmentation happening in the model. The pose of the cars are similar, the car in the back is mostly black. The colorization transfer result gives me the impression that the segmentation is not done properly, e.g. the red color from the red car image seems to spill outside of the confine of the car in the colorization transfer."}
{"id": "iclr2020_935", "title": "Dynamics-Aware Unsupervised Skill Discovery | OpenReview", "abstract": "Abstract:###Conventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can potentially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynamical systems is difficult, and even then, the model might not generalize well outside the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that make model-based planning easy. To that end, we aim to answer the question: how can we discover skills whose outcomes are easy to predict? We propose an unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which simultaneously discovers predictable behaviors and learns their dynamics. Our method can leverage continuous skill spaces, theoretically, allowing us to learn infinitely many behaviors even for high-dimensional state-spaces. We demonstrate that zero-shot planning in the learned latent space significantly outperforms standard MBRL and model-free goal-conditioned RL, can handle sparse-reward tasks, and substantially improves over prior hierarchical RL methods for unsupervised skill discovery.", "review": "Review:###The authors try to incorporate intermediate-level skills into model-based RL, which is an essential problem in the RL field. The algorithm works well even in the case of high-dimensional state and action spaces. Their contributions are in four aspects: (1)they propose an unsupervised RL framework for learning intermediate-level representations, i.e. skills, based on maximizing the mutual information between the future state and current skill given the current state. This procedure is well-motivated and the mathematics is easy to follow. (2)they reformulate model predictive control (MPC) in the latent skill space. (3)their method is compatible with the idea of continuous skill spaces, which seems to give rise to more diverse trajectories and hence offers greater utility. (4)their method yields low-variance behavior while maintaining enough diversity. It’s an accept for me. On one hand, using model-free unsupervised RL methods to learn intermediate-level skills is not a new idea. on the other, approaching this problem via mutual information is, as far as I know, new to this field. Although, the novelty of this approach remains undecided, this method seems to work well enough compared to model-based, model-free and hierarchical RL methods. Their analysis from the perspectives of continuous skill space and skill variance also seem to hold. Nevertheless, the study would benefit from more comparison with other methods using intermediate-level primitives (apart from DIAYN). Moreover it would be interesting to show this method works in scenarios apart from locomotion. I wonder how well the approximation p(z|s) approx p(z) works in non-locomotion tasks. Otherwise, the authors should mention this method is somewhat limited to locomotion tasks in the main text. Others: Typo: 1.Page 3: “maximally informative about about …” remove the redundant “about”; 2.Page 8, first line in section 6.3 “is to be enable use of planning algorithms…” may be changed to “is to take advantage of planning algorithms”."}
{"id": "iclr2020_936", "title": "Restricting the Flow: Information Bottlenecks for Attribution | OpenReview", "abstract": "Abstract:###Attribution methods provide insights into the decision-making of machine learning models like artificial neural networks. For a given input sample, they assign a relevance score to each individual input variable, such as the pixels of an image. In this work we adapt the information bottleneck concept for attribution. By adding noise to intermediate feature maps we restrict the flow of information and can quantify (in bits) how much information image regions provide. We compare our method against ten baselines using three different metrics on VGG-16 and ResNet-50, and find that our methods outperform all baselines in five out of six settings. The method’s information-theoretic foundation provides an absolute frame of reference for attribution values (bits) and a guarantee that regions scored close to zero are not required for the network*s decision.", "review": "Review:###Summary The paper proposes a novel perturbation-based method for computing attribution/saliency maps for deep neural network based image classifiers. In contrast to most previous work on perturbation-based attribution, the paper proposes to inject carefully crafted noise into an early layer of the network. Importantly, the noise is chosen such that it optimizes an information-theoretically motivated objective (rate-distortion/info bottleneck) that ensures that decision-relevant signal is flowing while constraining the overall channel-capacity, such that decision-irrelevant signal is blocked from flowing. The flow of signal is controlled by the amount of noise injected, which translates into a certain amount of mutual information between input image regions and noisy activations/features. This mutual information can be visualized in the input image, but it also has a clear, quantitative meaning that is readily interpretable. The paper introduces two ways to construct the injected noise, based on the information bottleneck. Resulting attribution maps are computed and evaluated on VGG-16 and ResNet-50 (on ImageNet), and are compared against an impressive number of previously proposed attribution methods. Importantly, the paper uses three different quantitative measures to compare the quality of attribution maps. The proposed method performs well on all three measures. Contributions i) Derivation of a novel method for constructing attribution maps. Importantly, the method is grounded on solid theoretical footing for extracting minimal relevant information (rate-distortion theory / information bottleneck method). ii) Proposal of a novel quantitative measure to compare quality of pixel-level attribution maps in image classification, and extension of a previously reported method. iii) Evaluation and comparison against a large body of state-of-the-art attribution methods. Quality, Clarity, Novelty, Impact The paper is clear and well written, with a nice introduction to the information bottleneck method. Experiments are well described and hyper-parameter settings are given in the appendix. To the best of my knowledge, the proposed method is sufficiently novel and the application of the information bottleneck framework to pixel-level attribution has not been reported before. Some of the design- and implementation-choices needed to render the intractable info bottleneck objective tractable could perhaps be discussed and potentially even improved in light of recent results in other fields (Bayesian DL, deep latent-variable generative models, and variational methods for deep neural network compression), but I currently don’t consider this a major issue. To me personally the work in convincing and mature enough to vote for acceptance - perhaps most importantly it lays important groundwork for important connections to the theory of relevant information and puts a lot of much needed emphasis on objective evaluation of attribution methods (i.e. without subjective visual judgement of saliency maps). My suggestions below are aimed at helping improve the paper even further. Improvements I) A short section of current shortcomings/limitations could be added to the discussion. II) Perturbation-based approaches that inject noise (into the input image directly) have been proposed previously. Most notably: Visualizing and Understanding Atari Agents, Greydanus et al. 2018 and potentially follow-up citations. It would be interesting to compare both works empirically, but perhaps also theoretically/conceptually. Could the Greydanus work be related to applying the noise directly to the input image along with some additional constraints? Minor Comments a) Is there a particular reason for this choice of colormap? While it seems to be roughly perceptually uniform (which is of course good), why not choose a simple sequential colormap (instead of a rainbow-like one)? At least the use of red and green at the same time should rather be avoided to maximize colormap readability under the most common forms of color vision deficiencies. b) Just a pointer - no need to act on this for the current paper. Large parts of the field of neural network compression are concerned with a similar kind of attribution - the question is which weights/neurons/filters are relevant and which ones are not and can thus be removed from the network without loss in accuracy. Information-bottleneck style objectives (or the closely related ELBO / variational free energy) in conjunction with sparsity inducing priors have been proven to be quite fruitful. See e.g. Variational Dropout Sparsifies Deep Neural Networks, Molchanov et al. 2017 for interesting work, that aims at learning the variance of Gaussian noise that is injected into neural network weights using a similar construction and variational objective as shown in this paper. Perhaps some ideas can be borrowed/translated for future, improved versions of the method from that body of literature (Molchanov 2017, but also more sophisticated follow-up work)."}
{"id": "iclr2020_937", "title": "Query2box: Reasoning over Knowledge Graphs in Vector Space Using Box Embeddings | OpenReview", "abstract": "Abstract:###Answering complex logical queries on large-scale incomplete knowledge graphs (KGs) is a fundamental yet challenging task. Recently, a promising approach to this problem has been to embed KG entities as well as the query into a vector space such that entities that answer the query are embedded close to the query. However, prior work models queries as single points in the vector space, which is problematic because a complex query represents a potentially large set of its answer entities, but it is unclear how such a set can be represented as a single point. Furthermore, prior work can only handle queries that use conjunctions ( ) and existential quantifiers ( ). Handling queries with logical disjunctions ( ) remains an open problem. Here we propose query2box, an embedding-based framework for reasoning over arbitrary queries with , , and operators in massive and incomplete KGs. Our main insight is that queries can be embedded as boxes (i.e., hyper-rectangles), where a set of points inside the box corresponds to a set of answer entities of the query. We show that conjunctions can be naturally represented as intersections of boxes and also prove a negative result that handling disjunctions would require embedding with dimension proportional to the number of KG entities. However, we show that by transforming queries into a Disjunctive Normal Form, query2box is capable of handling arbitrary logical queries with , , in a scalable manner. We demonstrate the effectiveness of query2box on two large KGs and show that query2box achieves up to 25% relative improvement over the state of the art.", "review": "Review:###This paper proposes a method to answer complex logical queries in large incomplete knowledge bases (KB). Specifically it considers the class of existential first-order logical queries (EPFO) which includes the logical and, or and existential operator. The key contribution of this paper is to represent sets of entities via regions, more specifically as boxes or hyper-rectangles. This is well motivated because such logical queries often involves working over sets of entities at once and involves applying set based operators. Previous work which represented queries as a point in vector space are not well suited for these queries. Instead, this paper models sets of entities as boxes or axis-aligned hyper-rectangles which is parameterized by two vectors in mathbb{R}^{n] denoting the center and the offset respectively. Boxes can also be understood to represent all the points in it (measure by element-wise comparison with the min and max coordinate). Handling the queries require projection and intersection operation. They are defined by simple addition operation (which guarantees the boxes grow in size, due to positive offset values) and a shrinkage function to denote intersection that guarantees the output area is smaller and is inside the set of boxes. For handling disjunctive queries, they make the clever trick of converting queries to DNF form so that the union operation is at the end of the computation graph which effectively reduces to taking the union of sets at the end. Experiments are run on standard datasets (FB15k and FB15k-237) — however, they generate their own query patterns. Specifically, they train on 5 patterns involving projection and intersection operation and test on 4 unseen ones. For baselines, they only compare to previous work of Hamilton et al., 2018 that maps queries to vectors. Strengths: 1. Most knowledge base comprehension benchmark tests on link prediction problems which are queries of kind (e1, r, ?). However, semantic parsers of natural language produce queries that are much richer in shape. This paper (and Hamiltion et al., 2018 before) considers answering complex logical queries (although the shape of query is pre-defined and not arbitrarily complex). 2. Modeling logical queries into regions in vector space is an interesting idea, and it would be nice to see followup work in this direction. 3. The paper is nicely written and ablation experiments were helpful. 4. Compared to the baseline they used, the paper does a better job of modeling complex logical queries. Weaknesses / Questions: 1. I understand that the papers have considered various pre-determined shapes of queries, but the simple 1p query is similar to the usual benchmarks, I don’t understand why results for 1p were not compared with existing benchmark results. Without that comparison, I don’t have a good sense if region-based method actually are effective for “1p” kind of queries. 2. Even though the model was tested on two variants of freebase datasets, FB15k is well-known to have a lot of issues (Toutanova and Chen, 2015). Why weren*t other standard datasets such as Nell-995, WN18RR and so many other biomedical KBs not considered for experiments, especially because the query generation process is very simple and it*s easy to run experiments 3. It was not clear to me how the intersection operator would give zero offset for a set of non-overlapping boxes as input. Is the zero value coming from the deep-set model?, If so, how do you ensure that? Minor: Please include the deep-set network here instead of in Sec 4.3. I was confused about what the deepest model is until this point. 4. Regarding the results, is there any particular reason the MRR metric was pushed to the appendix and only results of Hits@3 was shown in the main section of the paper. I believe MRR is a better metric for your case because you are modeling sets of entities as answers and hence a ranking metric that ranks all entities is better, Hits@k is 1 if any of the answers in the set is present in top-k and hence quite a loose metric. 5. Why is the result of 3i is better than 2i. I am not sure why the model would do a better job in handling 3 intersections better than it does 2 intersections. 6. How many answers are there on an average for each question. This will better help me understand how hard the dataset actually is. 7. It is nice to see, that the model prefers boxes of different width. Do you have a sense of which type of entities (or relations) have higher offsets. This analysis would be nice to have for readers in the appendix section"}
{"id": "iclr2020_938", "title": "Conservative Uncertainty Estimation By Fitting Prior Networks | OpenReview", "abstract": "Abstract:###Obtaining high-quality uncertainty estimates is essential for many applications of deep neural networks. In this paper, we theoretically justify a scheme for estimating uncertainties, based on sampling from a prior distribution. Crucially, the uncertainty estimates are shown to be conservative in the sense that they never underestimate a posterior uncertainty obtained by a hypothetical Bayesian algorithm. We also show concentration, implying that the uncertainty estimates converge to zero as we get more data. Uncertainty estimates obtained from random priors can be adapted to any deep network architecture and trained using standard supervised learning pipelines. We provide experimental evaluation of random priors on calibration and out-of-distribution detection on typical computer vision tasks, demonstrating that they outperform deep ensembles in practice.", "review": " This work introduces a simple technique to obtain uncertainty estimates for deep neural networks. This is achieved by having a set of random networks (i.e. neural networks where their parameters are randomly initialized) and then computing an uncertainty value based on the difference in the predictions between those random networks and networks that are trained to mimic them on a finite collection of points. The authors further show that this method results into uncertainties that are conservative, meaning that they are higher than the uncertainty of a hypothetical posterior, and concentrate, i.e. they converge towards zero when we get more and more data. The authors further draw connections to ensemble methods and discuss how such a method can be effectively realized in practice. They then evaluate their approach on an out-of-distribution detection task, they measure the calibration of their uncertainty estimates and finally perform a small ablation study for their concentration result. This work is interesting as it seems to provide a simple way to obtain reasonable uncertainty estimates. For this reason it can potentially serve as a strong baseline for this field. The theoretical considerations also help in providing some guarantees about such an approach. Having said that, in my opinion the writing could use some more work in order to make things more clear as some critical experimental details and baselines are missing and thus do not make the method as convincing. Furthermore, I also believe that some clarifications on the theoretical aspects of this work, will help in boosting its quality. More specifically: - How exactly do you apply your method on the classification scenario? Do you select an arbitrary hidden layer of the classification model for the prior and predictor network architectures or the output logits / softmax probabilities? In appendix A you mention the architecture but not precisely how it is employed. I believe this can be an important piece of information in order to decipher the importance of e.g. the output dimensionality on the uncertainty quality, as higher dimensional outputs might be harder to approximate thus could induce a larger squared error and hence uncertainty. - What is the average training error of the predictor networks for the out-of-distribution task and subsampling ablation task, i.e. how far away from concentration were the priors? - An effect that I found weird is the following: what happens for the out-of-distribution examples when the predictor networks can perfectly predict the prior network outputs? Wouldn’t that then imply that the uncertainty would be zero for any input (even an out-of-distribution one), as the prior network and predictor network always agree? One could imagine that for e.g. simple priors and with sufficiently dense sampling of the domain of the function this can happen in practice. - For the conservatism you show that your uncertainty estimate is higher, on average, than the posterior variance when you sample points from the model itself. In a sense this guarantee translates to the actual data when the prior is “correct”. How do those conservatism guarantees translate to the case when there is model misspecification, i.e. when the prior is not correct? Perhaps a small toy example would be informative. - For the predictor networks as described in figure 2; do you train both the green and red parts of the network or only the red parts and keep the green part fixed to the values you used for the prior f? (This helps in understanding how easy / difficult is the task of the predictor network). - What is the accuracy on the actual in-distribution prediction task for the RP and baselines? What did “B” correspond to for the dropout networks? Was it the number of dropout samples you averaged over to get the final predictive? - How sensitive are the results on the actual initialization strategy of the prior network? It would be good to see e.g. some form of performance / init variance curve in order to decipher the sensitivity. Other comments - It is worth pointing out that [1] showed that Monte-Carlo dropout performs approximate MAP inference, which seems more plausible than the approximate Bayesian inference perspective of [2]. - In the introduction you argue that Bayesian neural networks rely on procedures different from standard supervised learning and thus most ML pipelines are not optimized for them in practice. Could you elaborate a bit about this statement? Variationally trained BNNs with e.g. the reparametrization trick [3, 4] are straightforward since you can just use backpropagation to update their (variational) parameters. - What is the x-axis for Figure 3 for the baselines? (I take it that for RP it is the hat{sigma}^2(x)). - I believe that a comparison against a simple variationally trained BNN would make the results more convincing. Misc - Second page, “Figure 1, top two plota” -> “Figure 1, top two plots” - Third page, “[…] introduced in equation 2 denotes the posterior covariance [….]” -> “[…] introduced in equation 2 denotes the posterior variance […]“ - Fifth page, “[…] this makes it is reasonable for W large enough […]” -> “[…] this makes it reasonable for W large enough […]” - Sixth page, “Corollary 1 and proposition 2”; where is corollary 1? Do you mean Proposition 1? - Seventh page, “[…] inspired by, an builds on, […]” -> “inspired by, and builds on, […]” - Ninth page “montonicity” -> “monotonicity” Overall, I tend to accept this work, although, depending on the author rebuttal and other discussions, I am willing to change my rating accordingly. [1] Eric Nalisnick, José Miguel Hernández-Lobato, Padhraic Smyth, Dropout as a Structured Shrinkage Prior, 2019 [2] Yarin Gal, Zoubin Ghahramani, Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, 2016 [3] Diederik P. Kingma, Max Welling, Auto-Encoding Variational Bayes, 2014 [4] Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra, Stochastic Backpropagation and Approximate Inference in Deep Generative Models, 2014"}
{"id": "iclr2020_939", "title": "Adversarial Imitation Attack | OpenReview", "abstract": "Abstract:###Deep learning models are known to be vulnerable to adversarial examples. A practical adversarial attack should require as little as possible knowledge of attacked models T. Current substitute attacks need pre-trained models to generate adversarial examples and their attack success rates heavily rely on the transferability of adversarial examples. Current score-based and decision-based attacks require lots of queries for the T. In this study, we propose a novel adversarial imitation attack. First, it produces a replica of the T by a two-player game like the generative adversarial networks (GANs). The objective of the generative model G is to generate examples which lead D returning different outputs with T. The objective of the discriminative model D is to output the same labels with T under the same inputs. Then, the adversarial examples generated by D are utilized to fool the T. Compared with the current substitute attacks, imitation attack can use less training data to produce a replica of T and improve the transferability of adversarial examples. Experiments demonstrate that our imitation attack requires less training data than the black-box substitute attacks, but achieves an attack success rate close to the white-box attack on unseen data with no query.", "review": " Authors propose a GAN-based adv imitation attack that can use less training data to produce a replica of the model. The idea of using a GAN in producing adv examples in quite interesting. But the proposed approach is closely related to the following paper: https://arxiv.org/pdf/1801.02610.pdf Therefore, I am not sure about the novelty of the proposed approach. Also can authors comment on the stability of GAN*s training? Are any stabilizing methods integrated in GANs being used? How does the proposed approach relate to the adversarial distillation literature?"}
{"id": "iclr2020_940", "title": "The problem with DDPG: understanding failures in deterministic environments with sparse rewards | OpenReview", "abstract": "Abstract:###In environments with continuous state and action spaces, state-of-the-art actor-critic reinforcement learning algorithms can solve very complex problems, yet can also fail in environments that seem trivial, but the reason for such failures is still poorly understood. In this paper, we contribute a formal explanation of these failures in the particular case of sparse reward and deterministic environments. First, using a very elementary control problem, we illustrate that the learning process can get stuck into a fixed point corresponding to a poor solution. Then, generalizing from the studied example, we provide a detailed analysis of the underlying mechanisms which results in a new understanding of one of the convergence regimes of these algorithms. The resulting perspective casts a new light on already existing solutions to the issues we have highlighted, and suggests other potential approaches.", "review": "Review:###Summary: This work studies the instability problem of DDPG in the setting of a deterministic environment and sparse reward. This work designed a toy environment to showcase the potential issues of leading to the instability of DDPG. The observations, such as the correlation between the early access of good trajectory leads to more stable performance later, the deadlock of training could be beneficial. It is essential to analyze and understand the intrinsic properties of the classic algorithms, and it would benefit the research community a lot if the empirical study is appropriately designed and conducted. Overall, this paper studied an essential problem of the vulnerability of the classic DRL algorithm (DDPG), which should attract more attention and efforts from the research community. Detailed comments: Methodology: The experiments conducted cannot support the conclusions in this paper. I can not fully understand the conclusion from the subsection *Residual failure to converge using different noise processes*. The DDPG agent is finding the reward regularly while it couldn*t converge to the 100% optimal performance. In my opinion, this is an observation, instead of giving any useful conclusion. The convergent issues when using the combination of off-policy learning, function approximation, and bootstrapping are known (Sutton and Barto, Chap 11, 2018). The increasing of Q value seems natural to me due to the overestimation of Q learning, even with the zero reward setting, which is one of the motivations of Double DQN. Several potential solutions are discussed while no empirical evidence or theoretical justification is provided, even in the designed 1D-Toy example. It would be more convincing that the conclusions can be validated on more challenging tasks such as regular continuous action benchmarks (mujoco, etc.) Writing: The presentation of this work is not ready for publication, given its current form. What is the definition of reward function? The formula as shown in Eq 4e is not clear. What is the optimal performance of 1D-TOY example?"}
{"id": "iclr2020_941", "title": "Real or Not Real, that is the Question | OpenReview", "abstract": "Abstract:###While generative adversarial networks (GAN) have been widely adopted in various topics, in this paper we generalize the standard GAN to a new perspective by treating realness as a random variable that can be estimated from multiple angles. In this generalized framework, referred to as RealnessGAN, the discriminator outputs a distribution as the measure of realness. While RealnessGAN shares similar theoretical guarantees with the standard GAN, it provides more insights on adversarial learning. More importantly, compared to multiple baselines, RealnessGAN provides stronger guidance for the generator, achieving improvements on both synthetic and real-world datasets. Moreover, it enables the basic DCGAN architecture to generate realistic images at 1024*1024 resolution when trained from scratch.", "review": "Review:###Post rebuttal: The authors* responses have addressed most of my concerns, and I*ve raised my rating from 3 to 6. ---------------------------------------- Summary: This paper extends the discriminator of GAN to use a distributional output (multiple scalars) instead of a single scalar. As a result, the trained GAN becomes robust to the mode collapse. Pros: - The proposed method is clearly written and well-justified (e.g., Theorem 2). - Extension of the relativistic GAN [1] to the proposed setting is interesting. - The authors demonstrate that vanilla DCGAN architecture can generate high-fidelity (1024x1024) images. Cons: 1. An ensemble of discriminators? The authors use multiple scalars to consider diverse factors of the realness. However, it is simply an ensemble of discriminators [2] in a spirit. As each discriminator focus on different factors, it is not surprising that the generator becomes robust to the mode collapse. Also, recent work on mode collapse (e.g., [3]) shows better results on the mixture of gaussian experiments even using a single discriminator. At least, the authors should compare their method with the ensemble methods and claim the advantage over them. 2. Choice of the anchor distributions. The choice of anchor distributions A_0 and A_1 are not specified. While the authors provide some partial results in Table 2, it would be worthwhile to clarify the experimental details and justify them. 3. Role of each outcome u_i? The authors claim that each outcome u_i corresponds to the different factors of realness. However, the role of learned u_i is not investigated. Also, one may enforce u_i to learn different factors by promoting diversity of them, e.g., decrease their cosine similarity [4]. Minor comments: - The word *support* [5] is misused. The support itself means the set of non-zero elements, hence the authors should use the word *outcome* (or *sample*) instead of *support*. - The notation is not consistent. For example, the authors may use *x sim p_data(x)* (specify variable) or *z sim p_z* (omit variable), but not both. - Numbering is not consistent. For example, *Tab.4.2.* should be changed to *Tab.2.* for consistency. [1] Jolicoeur-Martineau. The relativistic discriminator: a key element missing from standard GAN. ICLR 2019. [2] Durugkar et al. Generative Multi-Adversarial Networks. ICLR 2017. [3] Xiao et al. BourGAN: Generative Networks with Metric Embeddings. NeurIPS 2018. [4] Elfeki et al. GDPP: Learning Diverse Generations Using Determinantal Point Process. ICML 2019. [5] https://en.wikipedia.org/wiki/Support_(mathematics)"}
{"id": "iclr2020_942", "title": "AutoGrow: Automatic Layer Growing in Deep Convolutional Networks | OpenReview", "abstract": "Abstract:###Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN.", "review": "Review:###Contributions: This paper best fits in the literature that explores growing network depth. The main framework here is to interleave training a shallower network and adding new layers. This paper (their final algorithm) differs from existing methods in that they: 1) initialize the new layers using standard initialization as opposed to the commonly used zero-init in this literature, 2) grows at a fixed interval , and this interval is short (to avoid the shallower nets being overly-trained)., 3) uses a large and constant learning rate during the growing phase. Empirically, they show competitive results on standard image benchmarks. More interestingly (to me), they provide interesting insights to this paradigm of ‘growing networks’. Comments/Questions: Section 2 of the paper describes the proposed method is good details. Section 3 of the paper describes the experiments. Since for now I see the contribution of this paper is mostly empirical, I will give my detailed feedback here. 3.1 (Suboptimum of Network Morphism (NM) ) Table 2 shows NM is worse than training from scratch, and this isn’t fixed by AdamInit. Table 3 shows c-AutoGrow (in between p-AutoGrow and NM) still does worse than from scratch, pinpoint the problem to converged subnetworks. 3.2 (p-AutoGrow) Table 3 shows +Constant LR helps, then +RandomInit helps. Table 4, 5 shows +Periodic gets the best performance. *Suggestion* The found net is Table 4,5 are significantly deeper than those in Table 2,3, also there are no Delta. Also, although within this write-up those are the highest numbers, in the broader literature of NAS this doesn’t seem to be that good. From a quick search, many methods in the Table 1 of [1] seems to give >96% accuracy on CIFAR10, some even close to 98%. It might be good to at least discuss why this method is limited from achieving that. I do like the finding that ZeroInit is unnecessary, as reported in the rest of this subsection. However, it is unsatisfying to me that many past works (as cited by the authors) required this ZeroInit without ever trying RandomInit. *Suggestion* I would love to see a more thorough discussion on why GauInit is better than ZeroInit, not just more numbers. For example, even just text description on why past works found ZeroInit useful, and countering some of those claims would be interesting. A more controlled experiment rather than training 2 networks by swapping this would be interesting. ZeroInit is used in more context than just NM. For example, good flow models like Glow also uses such initialization, for likely a different reason, but I wonder if findings here have any implication for ZeroInit more generally. 3.3 (Many datasets) Table 6 is a strong result. One odd thing is how deep the found-net has to be for MNIST. This actually suggest to me that AutoGrow does not have the ability to stop early when it can. And in the discussion, the authors argue that by using a better sub-module like in NAS they can do better. This raises the question why the authors did not choose to use it. I would believe it if the proposed method has obvious reasons that it can transfer to different architecture, but for now I cannot jump to the conclusion that, say, p-AutoGrow with GauInit will necessarily work when using a different sub-module. Perhaps, the reason past NM works didn’t use a GauInit was also due to the fact that past sub-modules didn’t work with GauInit. 3.4 (Scale to ImageNet) It’d be good to add reference results from other papers. Minor details: There are some good contents in this work, but for it to be a strong *empirical* contribution, perhaps it would be more useful to include experiments on other data modality where things are not so well tuned, and show state-of-the-art results. For it to be a strong *analysis* paper, it should expanded, at least addressing some of the *suggestions* mentioned above. Unrelated to my evaluation of this work, reading this makes me think we should (and can) develop theoretical understanding to this paradigm of growing networks. References: [1] https://arxiv.org/pdf/1905.13360.pdf"}
{"id": "iclr2020_943", "title": "Learning Boolean Circuits with Neural Networks | OpenReview", "abstract": "Abstract:###Training neural-networks is computationally hard. However, in practice they are trained efficiently using gradient-based algorithms, achieving remarkable performance on natural data. To bridge this gap, we observe the property of local correlation: correlation between small patterns of the input and the target label. We focus on learning deep neural-networks with a variant of gradient-descent, when the target function is a tree-structured Boolean circuit. We show that in this case, the existence of correlation between the gates of the circuit and the target label determines whether the optimization succeeds or fails. Using this result, we show that neural-networks can learn the (log n)-parity problem for most product distributions. These results hint that local correlation may play an important role in differentiating between distributions that are hard or easy to learn.", "review": "Review:###Thank the authors for their rebuttal. It resolves all my previous concerns. ############################## This paper proposes to use neural networks for learning binary tree structured boolean circuits. For the boolean circuits problem, the authors notice two importance factors influencing why the target circuit is easy to learn or not. The two factors include *local correlation* and *label bias*. On the one hand, *local correlation* requires every influential node in the circuit have strong correlations with the target label, which makes the network trainable to exploit this correlation for minimizing losses. On the other hand, *label bias* requires that there are not the same amount of positive and negative examples. The paper proves that the proposed algorithm can faithfully learn the target circuit and presents multiple examples for the scenario of learning binary tree structured boolean circuits. Strengths, 1, This paper points out the two key factors *local correlation* and *label bias* for the learnability of a boolean circuit. Empirically in Figure1, they also validates the finding by demonstrating problems with balanced labels are more difficult to train. 2, The paper puts their theoretical findings to the setup of k-parity problem, proving that their proposed algorithm can faithfully tackle the k-parity problem. Weakness, 1, The main theorem proves that the target networks can be approximated using O(n^logn) examples. However, it it apparent that binary tree boolean circuits cannot represent all 2^n n-ary boolean functions. Actually, I GUESS all functions a binary tree can represent MIGHT also be in the scale of O(n^logn). Then the result is not surprising, as the training set probably covers all training examples. I think it is necessary that the authors give some estimates on the total number of representable functions and compare it with their |S|. 2, It is not clear to me why the the *label bias* is important for learning a boolean circuit. Could the authors clarify to me ? 3, It is also helpful to empirically compare the cases when there are local correlations and there aren*t. 4, Sec 5.1 shows that the proposed algorithm can solve the k-parity problem, however they assumed that all parity nodes locates together. In practice, these parity nodes might scatter everywhere, which could incur big problems for a binary-tree to approximate."}
{"id": "iclr2020_944", "title": "Preventing Imitation Learning with Adversarial Policy Ensembles | OpenReview", "abstract": "Abstract:###Imitation learning can reproduce policies by observing experts, which poses a problem regarding policy propriety. Policies, such as human, or policies on deployed robots, can all be cloned without consent from the owners. How can we protect our proprietary policies from cloning by an external observer? To answer this question we introduce a new reinforcement learning framework, where we train an ensemble of optimal policies, whose demonstrations are guaranteed to be useless for an external observer. We formulate this idea by a constrained optimization problem, where the objective is to improve proprietary policies, and at the same time deteriorate the virtual policy of an eventual external observer. We design a tractable algorithm to solve this new optimization problem by modifying the standard policy gradient algorithm. It appears such problem formulation admits plausible interpretations of confidentiality, adversarial behaviour, which enables a broader perspective of this work. We demonstrate explicitly the existence of such *non-clonable* ensembles, providing a solution to the above optimization problem, which is calculated by our modified policy gradient algorithm. To our knowledge, this is the first work regarding the protection and privacy of policies in Reinforcement Learning.", "review": "Review:###Summary: The paper introduces a method for generating trajectories which prevent behavioral cloning in a policy gradient setting by learning varying experts which try to minimize the ability of a cloned policy. It runs experiments on a grid world to validate empirically that cloning is unsuccessful. Recommendation: While this is a novel concept and interesting, I cannot recommend acceptance in its current state. The paper was a bit hard to follow and I found the experiments not robust enough to fully characterize the method at this point. It is unclear whether this method really would prevent cloning given an apples-to-apples comparison. My understanding from the paper -- which was a bit hard to follow -- is that cloned policies were tabular while the APE policies were NNs. I would be more confident in results if more environment variations were tested, the cloned policies used more current and apples-to-apples comparisons, and overall if there were more clear details about the methodology. Comments: + It might be worth perusing the differential privacy and adversarial attack literature to think about whether demonstrations can simply be noised to retain information while crashing performance. This work seems relevant for example (it was put online in June which is sufficiently before the September deadline to mention it I believe): Behzadan, Vahid, and William Hsu. *Adversarial Exploitation of Policy Imitation.* arXiv preprint arXiv:1906.01121 (2019). + In the discussion: *We found in our preliminary results that using an RNN classifier which outputs p(c|?1:t) simply ended up in with either optimal policies or crippled policies. In both cases, there was a relatively minor difference in performance between the policy ensemble and the cloned policy.* --> There are no quantitative results for this so either results should be included and discussed or this should be future work. + The algorithm box doesn*t really add a whole lot of information other than saying that trajectories are collected and then gradients are updated. It would be really nice to have a very clear picture of what*s happening at each point in the algorithm. In its current state the paper is hard to follow and decipher this sequence. See for example Algorithm one in: https://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf . + Notation-wise, R(t) is a bit unusual notation for the RL literature, the advantage is usually r + gamma V(s*) - V(s), where r+gamma V(s*) is the action value Q(s,a). Given that the advantage is denoted as A(s,a), it would be clearer I think to use the Q(s,a) notation. Also the notation changes from section 2.2 to section 3.2 from A(s,a) to A(t). Keeping consistent notation would make this paper a lot easier to read. + The related work section is in the middle of the paper. it*d be nice to have it earlier to set the context of the work. + In the multiple policies section, a recent work has shown how to learn multiple policies from multiple experts using a mixture of experts framework -- though they frame it as options: Henderson, Peter, Wei-Di Chang, Pierre-Luc Bacon, David Meger, Joelle Pineau, and Doina Precup. *Optiongan: Learning joint reward-policy options using generative adversarial inverse reinforcement learning.* In Thirty-Second AAAI Conference on Artificial Intelligence. 2018. + Part of the way this defeats behaviour cloning is through the assumption that there are multiple trajectories to be learned from. It would be interesting to see if methods like the one above or any of the others mentioned can recover optimal performance from noisy trajectories by similarly learning multiple policies. In fact, + *Policy Gradient (PG) (Sutton et al., 2000) and its variants (Schulman et al., 2015) aim to directly learn the optimal policy ?, parameterized by ?.* --> I think some other citations of variants should be added for the final version instead of only referencing Schulman 2015. There are a lot now, so maybe adding PPO, DDPG, and a few others might be nice. Otherwise you could also just cut out the variants bit since it*s not necessary. + All first quotation marks are backwards in the document + I think the experiments ran were a bit lacking in robustness and details. Since this is an adversarial method, I would expect more variance across seeds and 3 seeds may not be enough to characterize this. Table 1 has +/- but does not state what this represents. Standard Deviation or Standard error? Does Table 1 represent returns for rolled out policies after learning or across all episode returns during learning? For the behavioural cloning method, it says a *tabular policy* was trained. Does this mean that the experts were trained using policy gradients and neural networks while the behavioural cloning method used a tabular policy? If so, I think this would be at a detriment to the method being tricked. I think it is a necessary condition to validate this method across several gridworld environment variations, seeds, and with more robust cloning methods (if in fact the behavioural policy was underpowered (tabular vs. nn). Overall, it would be great to have more details. While the visualizations of the gridworld itself were nice, I think they took up a lot of space which could be replaced with more detailed explanations and robust quantitative results."}
{"id": "iclr2020_945", "title": "The Dynamics of Signal Propagation in Gated Recurrent Neural Networks | OpenReview", "abstract": "Abstract:###Training recurrent neural networks (RNNs) on long sequence tasks is plagued with difficulties arising from the exponential explosion or vanishing of signals as they propagate forward or backward through the network. Many techniques have been proposed to ameliorate these issues, including various algorithmic and architectural modifications. Two of the most successful RNN architectures, the LSTM and the GRU, do exhibit modest improvements over vanilla RNN cells, but they still suffer from instabilities when trained on very long sequences. In this work, we develop a mean field theory of signal propagation in LSTMs and GRUs that enables us to calculate the time scales for signal propagation as well as the spectral properties of the state-to-state Jacobians. By optimizing these quantities in terms of the initialization hyperparameters, we derive a novel initialization scheme that eliminates or reduces training instabilities. We demonstrate the efficacy of our initialization scheme on multiple sequence tasks, on which it enables successful training while a standard initialization either fails completely or is orders of magnitude slower. We also observe a beneficial effect on generalization performance using this new initialization.", "review": " The aim of this paper is to suggest randomized initializations for the various weights of a recurrent neural network (GRUs and various LSTMs are covered), such that training these networks gets to a successful start, when the model is trained on long sequences. Instead of being heuristic, their approach follows first principles of analyzing signal propagation through time, using ideas from statistical thermodynamics (mean field approximations). Some experiments, on toy datasets, validate their approach. I am quite intrigued by this paper. It is using interesting theory, shapes it to a practically highly relevant and difficult applied problem, and in the end comes up with a computable criterion of how to choose hyperparameters (means and variances of Gaussians to sample initial weights from). While the results in practice are still not too convincing, I am strongly in favour of giving this approach the benefit of doubt, as it could lead to practically very useful downstream work. The main direction of improvement for this paper (given that experiments are what they are -- somewhat limited to toy situations right now) is to better explain the methodology to researchers not familiar with mean field methods. Most importantly, it is not explained in the main text how hyperparameters are really chosen in the end. Looking at Appendix E, I find some pretty basic choices, and no other alternatives considered. It is not explained why these choices satisfy the theory, why they*d be the only ones, etc. This creates a disconnect between the very nice (and seemingly useful) theory and its implications (they are not really well spelled out). Here is what I understood (and I am not specifically an expert on stat mech). The authors assume that the dimension of latent states (N) grows large. They assume that weights are sampled independently, and identical distributed in groups k (different cell types, weight vs bias), and that inputs are correlated with each other in each dimension. Based on these assumption, they follow Gaussians statistics through a number of time steps. In the limit, one gets a deterministic dynamical system, and as t -> infty, this may converge to a fixed point. In a very nice argument (which they could explain better), they state that such rapid convergence is bad news, because then information cannot spread across long time scales, so one has to find hyperparameters for which the system behaves *critically*. A second arguments tries to keep gradient sizes (under MF assumptions) of O(1), so neither -> 0, not -> infty, which is again some *critical* range. Under their assumption, these critical conditions can be computed depending on the hyperparameters. Unfortunately, this is where the paper somewhat stops, it does not give specific methods for finding hyperpars that satisfy the criteria, at least not in the sense of characterising the whole space of such hyperpars (instead, in Appendix E, they just state some few settings that do). As a direction for future work, this would be very important. Another side question is whether for what the authors call *trainability*, the only point that matters is whether for the initial weights, signals can spread and gradients are O(1). It is certainly necessary, I see that. Detailed comments: - Please fix Table 1, the expressions seem broken. What does *r2* mean in the GRU column? - At least for me, (1a) to (1c) really was too short. At least in the Appendix, please do explain how this gives GRU and LSTM - Please explain the untied weight assumption somewhere. s^t is a map of s^(t-1) and W_k, so how can W_k be independent of all s^t? What are you really assuming here? - It took some repeated reading until I understood why the expressions in (2a) to (4b) do not depend on i, j, a, b (except whether a = b or a != b). Explain that properly - The core of the whole approach seems to be first half of page 5. This seems like a very nice argument, but hard to understand. Try making it more crisp. I kind of get the rough idea why fast convergence over t would be bad, but would total divergence over t not also be bad? - In (12a-c), do you mean *equal* or *approximately equal*? - In 4.4: *This motivates the general form of the initializations used in the experiments*: You have to make this more explicit. Why are your choices the only ones? Could there not be other choices satisfying (12a-c) approx, and be better? - Value of Sigma_z = 1: This seems odd to me, then your covariances are degenerate (rank 1 instead of 2). Please explain - Standard LSTM harder than GRU or peephole LSTM: Again, this sounds real interesting, but I did not get it from the explanation - I did not understand Figure 2. How are Theta_0, Theta_1 chosen? - As said above, the experiments are interesting, but somewhat artificial. Please do at least comment on real-world applications, and whether (and how) the ideas here would apply - Discussion: *there is no clear principled way...*: Well, but practitioners need something. I*d disagree, at least one could attempt to navigate this space by global optimization techniques... ADDITIONAL COMMENT: I tried to append the following as comment, but the (pretty broken) system would not let me, insisting that *reader is not valid* (???). Anyway, here it is. I hope I am allowed to add to my own review. I*ve seen the argument in reviews that assumptions made by this paper about independencies between weights and inner states are wrong, and therefore conclusions are not valid. First, such assumptions are indeed pretty common in such statistical mech analyses of learning methods. Second, you have to distinguish between weights after (random) initialization and after training. Of course, LSTM represents long term dependencies after training, but initialization is a different story. If I was the AC for this paper, I*d ask somebody with at least some background in statistical mech to provide some additional opinions, as the reviewers (including myself) are not fully qualified."}
{"id": "iclr2020_946", "title": "A Kolmogorov Complexity Approach to Generalization in Deep Learning | OpenReview", "abstract": "Abstract:###Deep artificial neural networks can achieve an extremely small difference between training and test accuracies on identically distributed training and test sets, which is a standard measure of generalization. However, the training and test sets may not be sufficiently representative of the empirical sample set, which consists of real-world input samples. When samples are drawn from an underrepresented or unrepresented subset during inference, the gap between the training and inference accuracies can be significant. To address this problem, we first reformulate a classification algorithm as a procedure for searching for a source code that maps input features to classes. We then derive a necessary and sufficient condition for generalization using a universal cognitive similarity metric, namely information distance, based on Kolmogorov complexity. Using this condition, we formulate an optimization problem to learn a more general classification function. To achieve this end, we extend the input features by concatenating encodings of them, and then train the classifier on the extended features. As an illustration of this idea, we focus on image classification, where we use channel codes on the input features as a systematic way to improve the degree to which the training and test sets are representative of the empirical sample set. To showcase our theoretical findings, considering that corrupted or perturbed input features belong to the empirical sample set, but typically not to the training and test sets, we demonstrate through extensive systematic experiments that, as a result of learning a more general classification function, a model trained on encoded input features is significantly more robust to common corruptions, e.g., Gaussian and shot noise, as well as adversarial perturbations, e.g., those found via projected gradient descent, than the model trained on uncoded input features.", "review": "Review:###The paper proposes an approach to generalization for deep networks based on kolmogorov complexity. The normalized information distance and its approximation via a compression algorithm were developed in earlier work, as noted by the authors. So the main contribution seems to be framing the deep learning classifier as a source code and developing a method to minimize the proposed information distance to improve generalization. I found a few gaps that I think the authors should clarify for me. Why is framing the deep learning classifier as a source code important? It seems to me that the K(C) is the same as K(f) where the f is the function mapping X -> y, whether it is learned or not. - Moreover, unless I*m missing something, the source code is a way to encode the values of a random variable such that communication is minimized. If the C=f is a map from X_i -> y, X_i is an image, and y is a scalar, the source code as defined is not encoding X_i, it is simply encoding a part of X_i that is relevant to the classification task. 1. The claim * Because a sufficiently high-capacity neural network can memorize its input samples the Kolmogorov complexity of the true source code is larger than that of the learned source code: i.e., K(C) > K(C~).* needs proof in itself. This stands opposed to the intuition that an over-fit model has larger kolmogorov complexity because it is not *simple*. - Further, how does this square with the claim that the K(C~) is increased by adding encodings? If K(C~) is increased, then I think one needs to show that K(C~)< K(C) even with the encodings added? - Finally, by adding the encodings, the task has been fundamentally changed to the classification on the dataset [x, E1(x), E2(x) .. ] . So, the insight about minimizing information distance computed between C and C~ applies when the learned and the true *source code* correspond to the new task. This does not seem to be addressed in the theory. 2. For the noise robustness experiments, there are no other baselines for robustness provided. 3. For the adversarial robustness claims, I don*t believe that the justification provided in the paper is the necessarily only one. The adversarial attacks are only done on the uncoded image. So, if *encodings* are robust to these attacks, the network could also be robust. Unless this hypothesis is rejected, the provided theoretical justification and experiments do not exactly match-up. (writing comments): I felt that the paper could use a better structure with terms like source code defined in a consolidated section."}
{"id": "iclr2020_947", "title": "Fuzzing-Based Hard-Label Black-Box Attacks Against Machine Learning Models | OpenReview", "abstract": "Abstract:###Machine learning models are known to be vulnerable to adversarial examples. Based on different levels of knowledge that attackers have about the models, adversarial example generation methods can be categorized into white-box and black-box attacks. We study the most realistic attacks, hard-label black-box attacks, where attackers only have the query access of a model and only the final predicted labels are available. The main limitation of the existing hard-label black-box attacks is that they need a large number of model queries, making them inefficient and even infeasible in practice. Inspired by the very successful fuzz testing approach in traditional software testing and computer security domains, we propose fuzzing-based hard-label black-box attacks against machine learning models. We design an AdvFuzzer to explore multiple paths between a source image and a guidance image, and design a LocalFuzzer to explore the nearby space around a given input for identifying potential adversarial examples. We demonstrate that our fuzzing attacks are feasible and effective in generating successful adversarial examples with significantly reduced number of model queries and L0 distance. More interestingly, supplied with a successful adversarial example as a seed, LocalFuzzer can immediately generate more successful adversarial examples even with smaller L2 distance from the source example, indicating that LocalFuzzer itself can be an independent and useful tool to augment many adversarial example generation algorithms.", "review": " This paper employed fuzz testing approach to generate black-box adversarial examples. In general, the fuzz testing is new to the adversarial attack area to the best of reviewer*s knowledge. One difference with the existing black-box attack generation methods is that AdvFuzzer walks from a source image to a guidance image rather than starting from a targeted image. However, I still have concerns about this submission. 1) Figure 1 is not clear. Please highlight AdvFuzzer and LocalFuzzer and clarify the meaning of the color and symbols. 2) The algorithm presentation is poor. It makes the current technical contributions unclear. What is the challenge when applying fuzz testing to adversarial example generation? And what are technical contributions induced by this new challenge? 3) Some experiments are unclear to me. 3.1) The lack of ImageNet results is a weak point. 3.2) What is the computational complexity of the proposed fuzzing-based approach? Having a comparison on computation time might be helpful. 3.3) The second row of Figure 2 seemingly implies that fuzzing attacks lead to much higher L_inf distortion. Right? Is there any comparison under L_inf metric? This observation also holds in Table 1 and 2 (small L_0 but large L_2). Thus, I wonder if the reduced query number is at the cost of a significant increase of L_inf norm. For a fair comparison, it might need to add L_inf constraint such that all methods have similar pixel-level perturbation tolerance. E.g., one related work on black-box soft-label/hard-label attack https://arxiv.org/pdf/1907.11684.pdf Thus, my initial rating is weak reject. ############### Post-feedback ############ Thanks for the detailed response. My concerns on 1) and 2) have been addressed. However, my concern on ImageNet results remains. It is difficult to get an idea on how well the proposed algorithm performs in the absence of baseline in Table 7 * Average, Median, and Standard Deviation Results for Untargeted Attacks on ImageNet using Fuzzing attacks*. Thus, I keep my score intact."}
{"id": "iclr2020_948", "title": "The Dual Information Bottleneck | OpenReview", "abstract": "Abstract:###The Information-Bottleneck (IB) framework suggests a general characterization of optimal representations in learning, and deep learning in particular. It is based on the optimal trade off between the representation complexity and accuracy, both of which are quantified by mutual information. The problem is solved by alternating projections between the encoder and decoder of the representation, which can be performed locally at each representation level. The framework, however, has practical drawbacks, in that mutual information is notoriously difficult to handle at high dimension, and only has closed form solutions in special cases. Further, because it aims to extract representations which are minimal sufficient statistics of the data with respect to the desired label, it does not necessarily optimize the actual prediction of unseen labels. Here we present a formal dual problem to the IB which has several interesting properties. By switching the order in the KL-divergence between the representation decoder and data, the optimal decoder becomes the geometric rather than the arithmetic mean of the input points. While providing a good approximation to the original IB, it also preserves the form of exponential families, and optimizes the mutual information on the predicted label rather than the desired one. We also analyze the critical points of the dualIB and discuss their importance for the quality of this approach.", "review": "Review:###This paper proposes a new *dual* variant of the Information Bottleneck framework. The IB framework has been the subject of many papers in past years, with a focus on understanding the inner workings of deep learning. The framework poses machine learning as optimizing an internal representation to tradeoff between retaining less information about the input features and retaining more information about the output label (prediction). The existing framework measures the retained information about the prediction via mutual information, which can be expressed as a KL divergence. The new dual framework reverses the arguments of this divergence. The paper shows that this dual IB closely mirrors the original IB while having several additional nice mathematical properties. In particular, it can be shown that for exponential families the dual IB representation retains the exponential form. Overall, I think this paper adds a meaningful new perspective to the IB framework and the analysis appears to be thorough. As such, I support acceptance. The paper could be improved by giving more interpretation of the formal results and experiments - i.e. tying this framework back to the higher-level questions and explaining what the quantities mean. Figures 1, 2 part a: The meaning of both axes is unclear. The horizontal axis beta is a lagrangian parameter with no meaning outside the framework. The vertical axis Pr[y=0|hat x] has no semantics since the problem has not been defined. What is the reader meant to take away from these figures? Equation 3, part i: The numerator should have a beta subscript. The meaning of the denominator is not clear (it is a normalizing constant)."}
{"id": "iclr2020_949", "title": "Adversarially Robust Representations with Smooth Encoders | OpenReview", "abstract": "Abstract:###This paper studies the undesired phenomena of over-sensitivity of representations learned by deep networks to semantically-irrelevant changes in data. We identify a cause for this shortcoming in the classical Variational Auto-encoder (VAE) objective, the evidence lower bound (ELBO). We show that the ELBO fails to control the behaviour of the encoder out of the support of the empirical data distribution and this behaviour of the VAE can lead to extreme errors in the learned representation. This is a key hurdle in the effective use of representations for data-efficient learning and transfer. To address this problem, we propose to augment the data with specifications that enforce insensitivity of the representation with respect to families of transformations. To incorporate these specifications, we propose a regularization method that is based on a selection mechanism that creates a fictive data point by explicitly perturbing an observed true data point. For certain choices of parameters, our formulation naturally leads to the minimization of the entropy regularized Wasserstein distance between representations. We illustrate our approach on standard datasets and experimentally show that significant improvements in the downstream adversarial accuracy can be achieved by learning robust representations completely in an unsupervised manner, without a reference to a particular downstream task and without a costly supervised adversarial training procedure.", "review": "Review:###This paper analyzes the shortcoming of VAE objective, and propose a regularization method based on a selection mechanism that creates a fictive data point by explicitly perturbing an observed true data point. It is lead to Wasserstein distance between representations. Experiments are made on three datasets; ColorMNIST, MNIST, and CelebA, which shows superior performance on adversarial accuracy while similar accuracy to VAE on nominal accuracy. The paper is well-organized and well-written. The point is clear and the proposed algorithm is valid. The only problem of the paper is the improvement on the experiment is marginal. Although adversarial accuracy is far better (like 0% vs 50%), it is apparent that the vanilla VAE is fragile to the adversarial examples because the added noise is intended so. Thus I can not say this is a fair comparison and because the superiority of the proposed algorithm is shown in only this point, I am not sure the proposed algorithm is surely useful."}
{"id": "iclr2020_950", "title": "NEURAL EXECUTION ENGINES | OpenReview", "abstract": "Abstract:###Turing complete computation and reasoning are often regarded as necessary pre- cursors to general intelligence. There has been a significant body of work studying neural networks that mimic general computation, but these networks fail to generalize to data distributions that are outside of their training set. We study this problem through the lens of fundamental computer science problems: sorting and graph processing. We modify the masking mechanism of a transformer in order to allow them to implement rudimentary functions with strong generalization. We call this model the Neural Execution Engine, and show that it learns, through supervision, to numerically compute the basic subroutines comprising these algorithms with near perfect accuracy. Moreover, it retains this level of accuracy while generalizing to unseen data and long sequences outside of the training distribution.", "review": "Review:#### Summary This paper trains a network to mimic simple known algorithms in a way that guarantees that they generalize to out-of-distribution test instances. The network mimics the algorithms by running repeatedly in a loop where each iteration of the loop runs a Transformer and outputs a mask that tells the next iteration the inputs to process. The setup is tested on sorting, adding, and graph algorithms, and found to learn regular number representations that supposedly aid generalization. # Review This paper has an admirable and useful goal, but the way it is currently implemented and presented is not ready for publication at ICLR. My main issue is with the training/testing setup and its presentation. The authors assume a certain structure of an algorithm (for instance, the iterative structure of recursive selection sort), delegate one or more modules inside this structure to be implemented by neural networks, and train them only. Most of the *strong generalization* is coming from the fact that the iterative structure is fixed. The work abstracts out the most complex parts of each algorithm. In Figure 3, for instance, the NN must learn to find the smallest element among the non-masked-out ones on the input, return it, and mask it out. This is a much simpler task than the whole sorting algorithm. Training the network to solve *find_min* != claiming that the network solves and strongly generalizes on *sort*. Important training details are left unspecified. How is the data for training NEEs generated? For instance, for training the network in Figure 3, do you trace the whole selection sort on a randomly generated list, and collect the intermediate input/output pairs for *find_min*? If so, it*s absolutely unsurprising that the process also works for longer lists -- see above. Are composable NEEs, like the three networks in Figure 7, trained jointly or separately? Do they observe their own outputs that are fed into subsequent NEE networks, or are the previous outputs teacher-forced, or are they pre-trained? Many of these details need to be clarified precisely to make the experimental setup verifiable. Some important details are presented factually without any motivation. For example, why does Figure 5 use SHIFT and XOR? Why, in general, the next mask produced by a NEE is XORed with a previous one instead of replacing it? I liked the embedding visualizations, which clearly demonstrate structure in the latent space driven by (a) the binary number representations, and (b) the addition task objective. In addition to regular ordering structure (needed to implement addition), the latent space also clearly exhibits regularities inherent to the binary representation, such as the shift by 64 in Figure 8a. While interesting, this only confirms the findings of Shi et al., albeit in a more pure experimental setting. In summary, the scope of experiments and presentation of results would need to be significantly improved in order for this work to reach the quality bar of ICLR."}
{"id": "iclr2020_951", "title": "CEB Improves Model Robustness | OpenReview", "abstract": "Abstract:###We demonstrate that the Conditional Entropy Bottleneck (CEB) can improve model robustness. CEB is an easy strategy to implement and works in tandem with data augmentation procedures. We report results of a large scale adversarial robustness study on CIFAR-10, as well as the IMAGENET-C Common Corruptions Benchmark.", "review": "Review:###The paper modifies existing classifier architectures and training objective, in order to minimize *conditional entropy bottleneck* (CEB) objective, in attempts to force the representation to maximize the information bottleneck objective. Consequently, the paper claims that this CEB model improves general test accuracy and robustness against adversarial attacks and common corruptions, compared to the softmax + cross entropy counterpart. This claim is supported by experimental results on CIFAR-10 and ImageNet-C datasets. In overall, the manuscript is easy-to-follow with a clear motivation. I found the experimental results are also promising, at least for the improved test accuracy and corruption robustness. Regarding the results about adversarial robustness, however, it was really confusing for me to understand and validate the reported values. I would like to increase my score if the following questions could be addressed: - It is not clear whether adversarial training is used or not in CEB models for the adversarial robustness results. If the results were achieved *without* adversarial training, these would be somewhat surprising for me. At the same time, however, I would want to see more thorough evaluation than the current, e.g. PGD with more n, random restart, gradient-free attacks, or black-box attacks. - I wonder if the paper could provide a motivation on why the AutoAug policy is adopted when training robust model. Personally, this is one of the reasons that makes me hard to understand the values presented in the paper. - Figure 3, right: Does this plot indicates that *28x10 Det* is much more robust than *28x10 Madry*? If so, it feels so awkward for me, and I hope this results could be further justified in advance. - Figure 3: It is extremely difficult to understand the plots as the whole lines are interleaved on a single grid. I suggests to split the plots based on the main claims the paper want to demonstrate. - How was the issue of potential over-fitting on rho handled, e.g. using a validation set? - In general, I slightly feel a lack of justification on why the CEB model improves robustness. In Page 5: *... Every small model we have trained with Batch Normalization enabled has had substantially worse robustness, ...* - I think this line could be a critical point, and need further investigations in a manner of justifying the overall claim."}
{"id": "iclr2020_952", "title": "End-to-end named entity recognition and relation extraction using pre-trained language models | OpenReview", "abstract": "Abstract:###Named entity recognition (NER) and relation extraction (RE) are two important tasks in information extraction and retrieval (IE & IR). Recent work has demonstrated that it is beneficial to learn these tasks jointly, which avoids the propagation of error inherent in pipeline-based systems and improves performance. However, state-of-the-art joint models typically rely on external natural language processing (NLP) tools, such as dependency parsers, limiting their usefulness to domains (e.g. news) where those tools perform well. The few neural, end-to-end models that have been proposed are trained almost completely from scratch. In this paper, we propose a neural, end-to-end model for jointly extracting entities and their relations which does not rely on external NLP tools and which integrates a large, pre-trained language model. Because the bulk of our model*s parameters are pre-trained and we eschew recurrence for self-attention, our model is fast to train. On 5 datasets across 3 domains, our model matches or exceeds state-of-the-art performance, sometimes by a large margin.", "review": "Review:###The paper proposes a new joint learning algorithm that works for two tasks, NER and RE. The model is based on a pre-trained BERT model, which provides the word vectors of the input word sequence. Then it solves two tasks with two network branches: the first branch minimizes the loss for NER, and the second branch minimizes the loss for RE. The second branch uses entity labels predicted by the first branch, so joint learning may benefit both tasks. The design of the architecture is novel, but it is also not groundbreaking. Each network branch is from known structures, but the combination is not proposed before. The submission has evaluated the proposed algorithms on four datasets and improved SOTA performances. The ablation study justifies the design details. The writing is generally clear. Now critics: Ablation study: 1. As pointed by one public comment, the ablation study should show how much improvement is from BERT vectors. 2. I*d like to see another ablation study of whether RE helps NER. If you remove the RE component, does the NER performance suffer? Writing: 3. how are predicted labels embedded? Do you learn a vector of each tag of BIOES and then take a weighted sum of these vectors with predicted probabilities as weights?"}
{"id": "iclr2020_953", "title": "Bayesian Meta Sampling for Fast Uncertainty Adaptation | OpenReview", "abstract": "Abstract:###Meta learning has been making impressive progress for fast model adaptation. However, limited work has been done on learning fast uncertainty adaption for Bayesian modeling. In this paper, we propose to achieve the goal by placing meta learning on the space of probability measures, inducing the concept of meta sampling for fast uncertainty adaption. Specifically, we propose a Bayesian meta sampling framework consisting of two main components: a meta sampler and a sample adapter. The meta sampler is constructed by adopting a neural-inverse-autoregressive-flow (NIAF) structure, a variant of the recently proposed neural autoregressive flows, to efficiently generate meta samples to be adapted. The sample adapter moves meta samples to task-specific samples, based on a newly proposed and general Bayesian sampling technique, called optimal-transport Bayesian sampling. The combination of the two components allows a simple learning procedure for the meta sampler to be developed, which can be efficiently optimized via standard back-propagation. Extensive experimental results demonstrate the efficiency and effectiveness of the proposed framework, obtaining better sample quality and faster uncertainty adaption compared to related methods.", "review": " Summary of paper: The authors propose a neural sampler for probabilistic models in the meta-learning setting. Their main claim is that their model captures uncertainty in samples better than competing methods and does so at lower cost. In particular, they propose a scheme which separates sampling variables for a task tau into two components: a meta-sampler and a sample adapter. The meta-sampler intuitively plays the role of a learned conditional distribution over the target variables. The sample adapter is a sampler which is seeded with samples from the meta-sampler and moves them towards the desired data-distribution based on a technique called optimal-transport Bayesian sampling. Crucially, the meta-sampler is based on neural inverse autoregressive flows to have adequate representational capacity. The authors use the proposed algorithm (denoted DAMS for distribution agnostic meta sampling) for a variety of tasks: First, parameters for logistic regression models are generated and evaluated on various UCI tasks. Second, meta-learning over Gaussian mixture models (GMMs) is performed. Third, posterior adaption in a toy regression task where the frequency parameter of a sine wave is estimated. Last, the authors train neural networks with DAMS and test both classification accuracy as a function of test-time inference for test datasets of held out classes on cifar10, mnist and a few-shot training example on Mini-Imagenet, while also testing their method on meta-reinforcement learning Mujoco tasks. Main Comments to authors: Pros: -interesting combination of techniques (IAF flows and WGF/Stein inference) to do meta-sampling -empirically appealing results as pertaining to raw performance metrics like accuracy Weaknesses: - The evaluation is focused on #of steps during testing, but not on # of datapoints required for eval on new domains. -> this is only half of what we care about when saying a sampler is *fast*. The other half would be sample efficiency, which is typically the main motivation for Bayesian models and accurate uncertainty estimates, for instance when performing Bayesian Optimization. In fact, when making claims about uncertainty estimates as the goal of the paper, it would be most interesting to see how much test data the method needs to ingest before producing calibrated estimates. The method as currently presented only evaluates speed in terms of computation, but ignores sample efficiency entirely. As such, it is unclear from the given experiments to evaluate the main claim of the paper: that DAMS improves uncertainty adaptation. -> the uncertainty is barely evaluated except in the low-d and toy sine wave illustration, which probably can be done equally well or better with regular posterior inference on D = D_train union D_test, i.e. using HMC. My suggestion to the authors would be to consider comparing to regular Bayesian inference (i.e. Monte Carlo/HMC/SVI) based on train_data and test data to compare to a ground truth estimate they might want. -In Sec. 3.3 Eq. 6 and 7 a kernel is used and then not discussed much further. Kernels on high dimensional data (such as the weights of a NN) are problematic to be used due to the curse of dimensionality. For the meta-sampler, even in the case of the multiplicative parametrization which lowers dimensionality, this would indicate that the kernel part of the objective might not be doing much work at all as in high enough dimensions all distances become even. If that were to be the case, the model might just look for the mode in any high-d example instead of actually sampling from a posterior and producing uncertainty estimates. Any empirical analysis and discussion on this is entirely missing here, unfortunately. As presented, the reader just has to accept that the objective functions make sense because the final product of putting all of the components together produces high accuracies. I would appreciate more details and careful analysis. -Please also show the performance of meta-sampler with and without sample adapter in this case to clarify the effects of performing sample adaptation versus just using the meta-sampler, as this also is never compared in the paper. I.e. how good of a conditional model is the autoregressive flow? How much work does the sampler have to do? Would another conditional model do as well or worse? Why this choice of conditional model in particular if in the end sampling is put on top of it? -The ELBO in Sec.3.4 involves an inference network over NN parameters (or potentially latent Zs per feature when using multiplicative parametrization). This object is highly nontrivial and not analyzed in terms of performance at all here. Inference networks over neural networks are hard to get right and worthy of entire publications. -Please clarify the prior used for BNN models. -Please consider using HMC as a baseline for BNN models per task in terms of LLK to compare to DAMS. Baselines: - A lot of this paper relies on comparison to baselines, which are chosen to be mostly from the meta-learning field. However, in practice the goal of the paper is Bayesian Inference in a particular class of models. Hence: - please consider adding conditional MNF as a baseline. This would clarify if a simple conditional version of MNF would suffice here compared to the involved scheme proposed in this paper and might show the advantage of DAMS over MNF (effectively the main driver for most of the experiments here). -Similarly, please consider using conditional NAI flow as a baseline to see how far that gets the reader. Presentation Suggestions: -You might want to consider establishing a formal relationship to a hierarchical probabilistic model with plates and show that this is just a way to perform sampling the posterior in a model like: P(y|x, tau) = integral_w P(y|x, w_t) P(w_t|tau) d_w -This might help the flow of the paper by setting the stage early, as currently I had to read through it halfway to really understand the task before starting from the beginning to absorb the details of the proposed techniques. -figures only readable in pdf, not in printout. Please enlarge fonts to give *old school* readers a chance -page 6 *..could be calculated effectively..* What does effectively mean here? Efficiently? -page 6 under Theorem 1 typo: *via the Eulaer scheme* -> Euler scheme Related Work Suggestions: - Consider citing *Predictive Uncertainty Quantification with Compound Density Networks* by Kristiadi et al, as it uses a conditional model with multiplicative parametrization successfully. I understand this is per data-point and the meta-learning scenario is focused on the per-dataset setting, but I find them related enough to consider a discussion. - With regards to inference networks on BNNs, please cite *Latent Projection BNNs: Avoiding weight-space pathologies by learning latent representations of neural network weights* by Pradier et al, which attempts to do this and also discusses related work in more detail than this paper here. It is a hard task to be done well. -The general form of the ELBO shown here is an instance of *Hierarchical Variational Models* by Ranganath et al, which should also be cited. -Last but not least a recent paper in an ICML workshop on automatic machine learning (https://sites.google.com/view/automl2019icml/accepted-papers) had a paper on *Improving Automated Variational Inference with Normalizing Flows* by Webb et al. This method looks a lot like a baseline method for this paper before task adaptation and WGF is considered and would effectively subsume the first batch of experiments entirely. I would propose the authors cite and discuss differences in detail. Decision: The paper uses a variety of *puzzle pieces* that are quite involved on their own right. Putting them together and making it work is nontrivial and the authors demonstrate in their experiments that they get strong performance metrics. However, unfortunately, systematic ablation experiments and detailed analysis for the individual components used here and systematic comparisons to simple baselines are not performed. In addition, the paper is presented as a method for adaptive uncertainty quantification, which as argued above is not demonstrated empirically or else. What the paper does achieve is build a pipeline that gets high predictive performance on a meta-learning setting with lower computational requirements during testing than competing methods. I would suggest the authors focus on that aspect and add the required baselines that would clarify what ablations would do to the system and how the components interact. As currently presented, I would argue for rejection since I am not sure of the scientific value of the interplay of components here as regarding uncertainty quantification. However, I think this paper is promising for a slightly different story with small experimental adjustments and would encourage the authors to consider that route. (Edit Post Rebuttal: revising score given the author response which addressed some of my concerns)"}
{"id": "iclr2020_954", "title": "Decoupling Adaptation from Modeling with Meta-Optimizers for Meta Learning | OpenReview", "abstract": "Abstract:###Meta-learning methods, most notably Model-Agnostic Meta-Learning (Finn et al, 2017) or MAML, have achieved great success in adapting to new tasks quickly, after having been trained on similar tasks. The mechanism behind their success, however, is poorly understood. We begin this work with an experimental analysis of MAML, finding that deep models are crucial for its success, even given sets of simple tasks where a linear model would suffice on any individual task. Furthermore, on image-recognition tasks, we find that the early layers of MAML-trained models learn task-invariant features, while later layers are used for adaptation, providing further evidence that these models require greater capacity than is strictly necessary for their individual tasks. Following our findings, we propose a method which enables better use of model capacity at inference time by separating the adaptation aspect of meta-learning into parameters that are only used for adaptation but are not part of the forward model. We find that our approach enables more effective meta-learning in smaller models, which are suitably sized for the individual tasks.", "review": "Review:###This paper presents an experimental study of gradient based meta learning models and most notably MAML. The results suggest that modeling and adaptation are happening on different parts of the network leading to an inefficient use of the model capacity which explains the poor performance of MAML on linear (or small networks) models. To tackle this issue they proposed a kronecker factorization of the meta optimizer. The paper is well motivated and well written in terms of clarity in the message and being easy to follow. One major issue is that the experimental study is not that comprehensive to support the claim of the paper. Especially, in analyzing the failure case of linear models.For example, one may try small (but nonlinear networks) and compare its performance with larger (possibly overparameterized) ones on at least 2 standard network architectures. But, it doesn*t mean that I don*t like the paper at its current state. The paper yet has a message and it*s delivered clearly. I wonder if the overparameterized is just related to depth or overparameterization in width would work too? If not then it might be the *nonlinearity* that is doing the work In section 3.2 (Figure 2, left) and (Figure2, mid) show that FC follows the pattern of C1-C3. t Then the authors proposed the experiment related to perturbing FC (Figure 2, right) to show that FC is actually not similar to C1-C3 and is important to adaptation. However, one can do similar experiments for C1-C3 and claim they are also important to adaptation. It seems that FC and C4 are really different. For a non-expert reader it*s not readily clear that how the kronecker factorization of A leads to equation 5. An explanation can help. Also, a few sentences or schematic demonstration of kronecker product makes the paper self-contained. There are a few typos in the paper that can be removed after a thorough proofreading."}
{"id": "iclr2020_955", "title": "AMUSED: A Multi-Stream Vector Representation Method for Use In Natural Dialogue | OpenReview", "abstract": "Abstract:###The problem of building a coherent and non-monotonous conversational agent with proper discourse and coverage is still an area of open research. Current architectures only take care of semantic and contextual information for a given query and fail to completely account for syntactic and external knowledge which are crucial for generating responses in a chit-chat system. To overcome this problem, we propose an end to end multi-stream deep learning architecture which learns unified embeddings for query-response pairs by leveraging contextual information from memory networks and syntactic information by incorporating Graph Convolution Networks (GCN) over their dependency parse. A stream of this network also utilizes transfer learning by pre-training a bidirectional transformer to extract semantic representation for each input sentence and incorporates external knowledge through the neighbourhood of the entities from a Knowledge Base (KB). We benchmark these embeddings on next sentence prediction task and significantly improve upon the existing techniques. Furthermore, we use AMUSED to represent query and responses along with its context to develop a retrieval based conversational agent which has been validated by expert linguists to have comprehensive engagement with humans.", "review": " This paper presents a unified representation of query-response pairs for response selection in chit-chat dialogues. The representation includes the following four core components: syntactic embedding by graph convolution network, sentence representation from BERT, knowledge embedding based on entity linking to Wikipedia, and context representation with memory networks. The experimental results show the effectiveness of each proposed representation in both automatic metrics and human evaluations. The main contribution of this work is to combine various different representations into a single end-to-end model architecture for dialogues response selection. However, each of the proposed ideas is not novel enough. And many parts of the paper are not very clearly written. The experiments also need to be further improved to support the contributions of this work. Please find below my major concerns: - The motivation of each proposed component is not very clear. I suggest to highlight this more in the Introduction. - It would be better to emphasize the contributions of this work in Related Work section. Now the section just lists the related studies with no explicit comparison to this work. - I*m not convinced that the proposed methods are novel enough to be presented at the conference. I don*t find any significant contribution of this work from the original work for each core component. - I*m curious how much the long range dependency problem affects to the dialogue response selection task in general. The experimental results show that the model with *Bi-GRU & GCN* helped to improve the performances. But there should be an additional configuration only with *Bi-GRU* with no *GCN* to see the impact of the dependency parsing. - It*s a bit confusing why the BERT embedding is considered as a knowledge module along with the actual KB representation with Wikipedia. - I*m wondering what if the Bi-GRU in the syntactic module is replaced with BERT. In the experiment, the BERT only model already achieved higher performances than the Bi-GRU+GCN only. It would be also interesting to see the performances of BERT+GCN. - It*s not clear how to get the KB embedding in Section 4.2.2. Did you take the title of Wikipedia article or also with body texts for each entity? More details are needed. - This model uses Stanford CoreNLP for dependency parsing and entity linking. I guess there might have been some errors from CoreNLP models due to the characteristics of chit-chat conversations. I suggest to add some analysis to show the impact of the pre-processing errors to the overall performances. - It*s not clear why DSTC2 dataset is used in the experiment. I don*t think this dataset is appropriate for response selection. I suggest to use DSTC7 Track 1 dataset at https://github.com/IBM/dstc-noesis instead. - It would be great to compare the model performances with other stronger baselines on Persona Chat."}
{"id": "iclr2020_956", "title": "Energy-Aware Neural Architecture Optimization with Fast Splitting Steepest Descent | OpenReview", "abstract": "Abstract:###Designing energy-efficient networks is of critical importance for enabling state-of-the-art deep learning in mobile and edge settings where the computation and energy budgets are highly limited. Recently, Wu et al. (2019) framed the search of efficient neural architectures into a continuous splitting process: it iteratively splits existing neurons into multiple off-springs to achieve progressive loss minimization, thus finding novel architectures by gradually growing the neural network. However, this method was not specifically tailored for designing energy-efficient networks, and is computationally expensive on large-scale benchmarks. In this work, we substantially improve Wu et al. (2019) in two significant ways: 1) we incorporate the energy cost of splitting different neurons to better guide the splitting process, thereby discovering more energy-efficient network architectures; 2) we substantially speed up the splitting process of Wu et al. (2019), which requires expensive eigen-decomposition, by proposing a highly scalable Rayleigh-quotient stochastic gradient algorithm. Our fast algorithm allows us to reduce the computational cost of splitting to the same level of typical back-propagation updates and enables efficient implementation on GPU. Extensive empirical results show that our method can train highly accurate and energy-efficient networks on challenging datasets such as ImageNet, improving a variety of baselines, including the pruning-based methods and expert-designed architectures.", "review": "Review:###This paper is based on a prior work which proposed Splitting Steepest Descent to search better network structures via splitting existing neurons to multiple off-springs. As an improvement, the authors (1) incorporate the energy cost to better guide the splitting process and (2) reduce the time and space complexity by approximating the original computation process with Rayleigh-Quotient Gradient Descent. They conduct experiments on public image classification datasets using lightweight networks as backbones to show that their algorithm outperforms existing methods. The paper is well written and easy to follow. The experiments are comprehensive and the evaluation results shows good properties of proposed method. In brief, this paper is an improvement to a splitting algorithm in a previous work, achieving good efficiency and enabling application on large datasets. However, the theory needs more justification and the experiments results are not sufficient to show the significance of their contribution. Therefore, this paper may not be accepted unless more experiment results are given. For the theory & modeling, the following should be addressed. 1. The mathematical justification of the optimization on energy cost is not very sound, and the definition of optimal splitting set seems arbitrary. 2. Given that the experiments are conducted on convolutional networks, it would be more illustrative if the paper describe the process of applying the algorithm on common convolutional operators. 3. The novelty is limited by the prior work. For the experiment, the following should be addressed. 1. The experiments are mainly conducted on MobileNet network. It would be more convincing if more experiment is done on other lightweight or normal convolutional networks. 2. This paper reduces time and space complexity of the algorithm in a previous work, but there is no running time or memory footprint statistics to support this argument. 3. The paper only lists one pruning-based method as comparison in the experiments. It would be more convincing if more pruning and splitting methods are presented."}
{"id": "iclr2020_957", "title": "ICNN: INPUT-CONDITIONED FEATURE REPRESENTATION LEARNING FOR TRANSFORMATION-INVARIANT NEURAL NETWORK | OpenReview", "abstract": "Abstract:###We propose a novel framework, ICNN, which combines the input-conditioned filter generation module and a decoder based network to incorporate contextual information present in images into Convolutional Neural Networks (CNNs). In contrast to traditional CNNs, we do not employ the same set of learned convolution filters for all input image instances. And our proposed decoder network serves the purpose of reducing the transformation present in the input image by learning to construct a representative image of the input image class. Our proposed joint supervision of input-aware framework when combined with techniques inspired by Multi-instance learning and max-pooling, results in a transformation-invariant neural network. We investigated the performance of our proposed framework on three MNIST variations, which covers both rotation and scaling variance, and achieved 0.98% error on MNIST-rot-12k, 1.12% error on Half-rotated MNIST and 0.68% error on Scaling MNIST, which is significantly better than the state-of-the-art results. Our proposed model also showcased consistent improvement on the CIFAR dataset. We make use of visualization to further prove the effectiveness of our input-aware convolution filters. Our proposed convolution filter generation framework can also serve as a plugin for any CNN based architecture and enhance its modeling capacity.", "review": " This paper proposed an Input-conditioned Convolutional Neural Network (ICNN) to automatically impose transformation-invariance. The contribution of the manuscript is two-folds (a) After transforming the input using a pre-determined set of transformations, a set of input-conditioned filter generators are used (and trained) to cater to different input contents. (b) A decoder is used after the max-pooling layer (of the Siamese network.) And an L-2 reconstruction loss (with respect to a chosen class representative) is added to the cross-entropy loss for classification. Overall the paper is well written, and it is fairly easy to read. However, I am not totally convinced that the two contributions of the paper are significant to transformation-invariant representations, and my reasonings are follows 1. Why is a decoder needed in the architecture? If the objective is to achieve transformation-invariance, one can easily compare the L-2 distance between the max-pooled feature maps of a given input to that of the class representative. Why bother using a decoding architecture? 2. Choosing a *class representative* in the CNN seems very restrictive. Why if the underlying task is not image classification? Besides, I am very curious about the experiment on the CIFAR-10 dataset: do the constructed images of all test samples look like the one chosen class representative in the training data? (i.e., compared to figure 3) 3. The input-conditioned filter generation seems a little confusing. Is this what you want to achieve? Say if the pre-determined transformations are rotation (scaling), then the input-conditioned filter should be generated as rotated (scaled) version of the same filters? If so, why not just rotate (rescale) the filters? There are lots of group-equivariant CNNs that have been proposed before for such effect. Besides, I am confused why fractionally-strided convolutions are used for filter generation? Other comments: 1. The reference for fractionally-strided convolutions should be fixed. 2. Why there is no bias term in convolutional modules (page 4, second paragraph?) 3. What does ICNN short for? The first appearance of the abbreviation in the abstract needs more explanation."}
{"id": "iclr2020_958", "title": "INVOCMAP: MAPPING METHOD NAMES TO METHOD INVOCATIONS VIA MACHINE LEARNING | OpenReview", "abstract": "Abstract:###Implementing correct method invocation is an important task for software developers. However, this is challenging work, since the structure of method invocation can be complicated. In this paper, we propose InvocMap, a code completion tool allows developers to obtain an implementation of multiple method invocations from a list of method names inside code context. InvocMap is able to predict the nested method invocations which their names didn’t appear in the list of input method names given by developers. To achieve this, we analyze the Method Invocations by four levels of abstraction. We build a Machine Translation engine to learn the mapping from the first level to the third level of abstraction of multiple method invocations, which only requires developers to manually add local variables from generated expression to get the final code. We evaluate our proposed approach on six popular libraries: JDK, Android, GWT, Joda-Time, Hibernate, and Xstream. With the training corpus of 2.86 million method invocations extracted from 1000 Java Github projects and the testing corpus extracted from 120 online forums code snippets, InvocMap achieves the accuracy rate up to 84 in F1- score depending on how much information of context provided along with method names, that shows its potential for auto code completion.", "review": "Review:###The paper proposes a code completion tool InvocMap, to predict nested method invocations in programming language code. I do not entirely buy the premise that *developers need to remember the structure and the combination of invocations depending on their purpose.* which is stated as a reason for why the task is challenging. The order of nesting in method invocation can be resolved often by examining the return type and argument type(s) of each method. Weaknesses 1. Writing can be improved significantly. It is hard to understand the difficulty of the problem that is being addressed. 2. Little novelty. A standard SMT approach is adopted with little modification. 3. How is it ensured that the output is syntactically correct? 4. Little analysis in the experiments, no strong baselines. Why not try a neural seq2seq model? Typos/Grammar 1. There are two reasons *cause to* this challenge. 2. First, large scale code corpus contains *noise* data I am not entirely certain that this paper is appropriate for this venue."}
{"id": "iclr2020_959", "title": "Discovering Topics With Neural Topic Models Built From PLSA Loss | OpenReview", "abstract": "Abstract:###In this paper we present a model for unsupervised topic discovery in texts corpora. The proposed model uses documents, words, and topics lookup table embedding as neural network model parameters to build probabilities of words given topics, and probabilities of topics given documents. These probabilities are used to recover by marginalization probabilities of words given documents. For very large corpora where the number of documents can be in the order of billions, using a neural auto-encoder based document embedding is more scalable then using a lookup table embedding as classically done. We thus extended the lookup based document embedding model to continuous auto-encoder based model. Our models are trained using probabilistic latent semantic analysis (PLSA) assumptions. We evaluated our models on six datasets with a rich variety of contents. Conducted experiments demonstrate that the proposed neural topic models are very effective in capturing relevant topics. Furthermore, considering perplexity metric, conducted evaluation benchmarks show that our topic models outperform latent Dirichlet allocation (LDA) model which is classically used to address topic discovery tasks.", "review": "Review:###This paper proposes a neural topic model that aim to discover topics by minimizing a version of the PLSA loss. According to PLSA, a document is presented as a mixture of topics, while a topic is a probability distribution over words, with documents and words assumed independent given topics. Thanks to this assumption, each of these probability distributions (word|topic, topic|document, and word|document) can essentially be expressed as a matrix multiplication of the other two, and EM is usually adopted for the optimization. This paper proposes to embed these relationships in a neural network and then optimize the model using SGD. I believe the paper should be rejected because: 1) most aspects of this paper are a little dated 2) novelty is little 3) experimental section is very limited and unconvincing. To elaborate on the experimental section: - Only LDA has been presented as baseline. There*s plenty of neural topic models to compare against (you mentioned some in your related work section) but no comparison with any of those is presented. If the concern is their training time on large datasets, they should be at least presented as comparison for the smaller datasets. For the large datasets there*s other approaches that would scale and should be presented as baselines: 1) train on a sample of the dataset 2) co-occurrence based topic methods on sliding windows of text are extremely fast (eg see *A Biterm Topic Model*, *A Practical Algorithm for Topic Modeling with Provable Guarantees*, and *A Reduction for Efficient LDA Topic Reconstruction* which could fit your scenario with large datasets where topics most likely have small overlap with each other and are almost separable by anchor words.) - Even regarding just LDA: what hyper-parameters alpha and _x0008_eta did you set for LDA? Tuning _x0008_eta to a small value might have an impact for large datasets. - Metrics: only perplexity is presented and metrics but it*s well known that perplexity on its own is quite limited and often is not correlated to human judgment. Consider adding topic coherence measures as well. - The section on continuous document embeddings is confusing and the explanation should be improved and the formalism tightened. Other (did not impact the score): - Biases: you*re adding biases to your probability estimation equations. This is not in line with the PLSA assumption. What happens if no biases are used? The paper has several typos and grammatical errors, e.g.: - page 2, L#1: networks -> network - page 4, sec 3.2: set unobserved -> set of unobserved - page 5, sec 5: pratise -> practice - several places: it*s -> its"}
{"id": "iclr2020_960", "title": "Few-Shot Few-Shot Learning and the role of Spatial Attention | OpenReview", "abstract": "Abstract:###Few-shot learning is often motivated by the ability of humans to learn new tasks from few examples. However, standard few-shot classification benchmarks assume that the representation is learned on a limited amount of base class data, ignoring the amount of prior knowledge that a human may have accumulated before learning new tasks. At the same time, even if a powerful representation is available, it may happen in some domain that base class data are limited or non-existent. This motivates us to study a problem where the representation is obtained from a classifier pre-trained on a large-scale dataset of a different domain, assuming no access to its training process, while the base class data are limited to few examples per class and their role is to adapt the representation to the domain at hand rather than learn from scratch. We adapt the representation in two stages, namely on the few base class data if available and on the even fewer data of new tasks. In doing so, we obtain from the pre-trained classifier a spatial attention map that allows focusing on objects and suppressing background clutter. This is important in the new problem, because when base class data are few, the network cannot learn where to focus implicitly. We also show that a pre-trained network may be easily adapted to novel classes, without meta-learning.", "review": "Review:###This paper proposed a new realistic setting for few-shot learning that we can obtain representations from a pre-trained model trained on a large-scale dataset, but cannot access its training details. Also, there may be a large domain shift between the dataset of the pre-trained model and our dataset. For the pre-trained model, they will not only use its weights but also use it to generate a spatial attention map and help the model focuses on objects of images. Back to the standard few-shot classification problem, they will first adapt the model with base class samples and then adapt to novel classes. The proposed new setting is very meaningful since we already have many powerful pre-trained models and why not exploit its usage for few-shot learning problems. However, I doubt the novelty and effectiveness of the attention way used in the paper. The attention module helps the model focuses on the objects not the background, which is absolutely correct. But there are already some relevant studies in the missing reference Large-Scale Long-Tailed Recognition in an Open World, CVPR2019. Also, from the results, the significant improvements come from the weights of the pre-trained model but not the attention used. Is the attention way used in the paper a good way to exploit the pre-trained model for few-shot classification problems? Also, I am curious about the dense classification used in the adaptation phase. Will it achieve similar performance with finetuning using just standard loss? Btw, according to the formatting instructions, the abstract should be limited in one paragraph. ========================================================= After Rebuttal: I thank the author for the response. I do see there are differences in the way of generating attention masks between the proposed work and (Liu et al.). But the improvements from the attention module is not significant, especially when using all base data. I keep my original scores."}
{"id": "iclr2020_961", "title": "CAPACITY-LIMITED REINFORCEMENT LEARNING: APPLICATIONS IN DEEP ACTOR-CRITIC METHODS FOR CONTINUOUS CONTROL | OpenReview", "abstract": "Abstract:###Biological and artificial agents must learn to act optimally in spite of a limited capacity for processing, storing, and attending to information. We formalize this type of bounded rationality in terms of an information-theoretic constraint on the complexity of policies that agents seek to learn. We present the Capacity-Limited Reinforcement Learning (CLRL) objective which defines an optimal policy subject to an information capacity constraint. This objective is optimized by drawing from methods used in rate distortion theory and information theory, and applied to the reinforcement learning setting. Using this objective we implement a novel Capacity-Limited Actor-Critic (CLAC) algorithm and situate it within a broader family of RL algorithms such as the Soft Actor Critic (SAC) and discuss their similarities and differences. Our experiments show that compared to alternative approaches, CLAC offers improvements in generalization between training and modified test environments. This is achieved in the CLAC model while displaying high sample efficiency and minimal requirements for hyper-parameter tuning.", "review": "Review:###The authors propose capacity-limited reinforcement learning and apply an actor-critic method (CLAC) in some continuous control domains. The authors claim that CLAC gives improvements in generalization from training to modified test environments, and that it shows high sample efficiency and requires minimal hyper-parameter tuning. The introduction started off making me think about this area in a new way, but as the paper continued I started to find some issues. To begin with, I think the motivation in the introduction could be improved. Why would I choose to limit capacity? This is not sufficiently motivated. I suspect that the author(s) want to argue that it *should* give better generalization, but this argument is not made very clearly in the introduction. Perhaps this is because it would be difficult to make this argument formally, and so it is merely suggested at? Are there connections between this and things like variational intrinsic control (VIC, Gregor et al. 2016) and diversity is all you need (DIAYN, Eysenbach et al., 2019)? These works aim to maximize the mutual information between latent variable policies and states/trajectories, whereas this work is really doing the opposite. I would be interested in understanding the author’s take on how the two are related conceptually. Moving to the connections with past work, this paper seriously abuses notation in a way that actually hinders comprehension. Some of the parts that really bothered me, and should be fixed to be correct: Mutual information is a function of two random variables, whereas it is repeatedly expressed as a function of the policy. Being explicit about the random variables / distribution here is pretty important. In Equation 2 (and subsequent paragraph) the marginal distributions p_a(a) and p_s(s) are not well defined, marginalizing over what, what are these distributions? I might guess that p_s(s) is the steady state distribution under a policy pi, and that p_a(a) is marginalizing over the same distribution, essentially capturing the prior probability of each action under the policy. But these sort of things need to be said explicitly. In KL-RL section there is a sentence with “This allows us to define KL-RL to be the case where p_0(a, s) = pi_0(a_t | s_t).” What does this actually mean? One of these is a joint probability for state and action, and one is an action probability conditional on a state. What does pi_mu(a_t) sim mathcal{D} mean? In the block just before Algorithm 1, many of these symbols are never defined. This needs a significant amount of care (by the authors) and right now relies on the reader to simply make a best guess at what the authors probably intend. Overall in the first three sections the message I would like the authors to understand is that, in striving for a concise explanation they have significantly overshot. These sections require some significant work to be considered publishable. The experiment in section 4.1 is intended to give a clean intuitive understanding of the method, but falls a bit short here. It is clean, but I needed more explanation to really drive the intuition home. I see that CLAC finds a solution more sensitive to the beta distribution, but help me understand why this is the right solution in this particular case. I really disagree with the conclusions around the experiments in section 4.2. I do not think these results show that for the CLAC model increasing the mutual information coefficient increases performance on the perturbed environments. First, the obvious, how many seeds and where are the standard deviations? Second, the trend is extremely small and the gap between CLAC and SAC is just as minor. Finally, CLAC has better performance on the training distribution which means that it actually lost *more* performance than SAC when transferring to the testing and extreme testing distributions. The results for section 4.3 are just not significant enough to draw any real conclusions. The massive temporal variability makes me very suspicious of those super tight error bands, but even without that question, the gap is just not very large. Finally, in section 4.4 we see the first somewhat convincing experimental results. These look reasonable, but even here I have a fairly pointed question: compared with the results in Packer et al (2018) the amount of regression from training to testing is extremely large (whereas they found vanilla algorithms transfer surprisingly well). Can you explain why there is such a big discrepancy between those results and these? But again, this section’s results are in my opinion the most convincing that something interesting is happening here. Lastly, in section 8.1 the range of hyper-parameters for the mutual information coefficient is very broad, which really makes it hard to buy the claim of requiring minimal hyper-parameter tuning. All in all there is something truly interesting in this work, but in the present state I am unable to recommend acceptance, and the amount of work required along with questions raised lead me to be fairly confident in this assessment."}
{"id": "iclr2020_962", "title": "Time2Vec: Learning a Vector Representation of Time | OpenReview", "abstract": "Abstract:###Time is an important feature in many applications involving events that occur synchronously and/or asynchronously. To effectively consume time information, recent studies have focused on designing new architectures. In this paper, we take an orthogonal but complementary approach by providing a model-agnostic vector representation for time, called Time2Vec, that can be easily imported into many existing and future architectures and improve their performances. We show on a range of models and problems that replacing the notion of time with its Time2Vec representation improves the performance of the final model.", "review": "Review:#### Summary This paper proposes a simple representation of time (Time2Vec) for modelling sequential data. The idea is to apply multiple sine functions to the time with trainable period and offset and concatenate them together, which is similar to positional encoding [Vaswani et al.] except that the periods and offsets are learned. The results on several sequential modelling datasets show that Time2Vec performs better than naive representations and alternative baselines. # Originality - Although the proposed representation looks simple and similar to positional encoding [Vaswani et al.], the idea of parameterizing sine functions is novel and interesting. # Quality - The proposed idea is very simple but seems very effective in practice as shown by the empirical results. - The paper also provides in-depth analysis and ablation studies showing that each of the proposed component is helpful. - Although the results presented in this paper look very promising, it would be much stronger if the paper presented results on other sequential modelling tasks such as machine translation and language modelling that the research community cares about much more. For example, it would be great if the paper showed that replacing the fixed positional encoding with Time2Vec improves the performance on a machine translation dataset. - It would be good to show the effect of the number of sine units (k) in Time2Vec. - (Minor) Clockwork RNN [Koutn´?k et al.] introduces a nice toy periodic sequential prediction problem, where the model has to recover a mixture of sine/cosine function. It looks like a natural task to show in this paper as well (which can potentially replace the synthesized data experiment). # Clarity - The paper is overall well-written. - Figure 4 is not mentioned in the main text. - Figure 5 is mentioned earlier than Figure 3. It would be good to swap them. # Significance - This paper proposes a simple but effective idea that can be potentially widely used by the research community. The paper would be stronger if it included more results on high-impact sequential modelling tasks and datasets such as machine translation."}
{"id": "iclr2020_963", "title": "Curriculum Learning for Deep Generative Models with Clustering | OpenReview", "abstract": "Abstract:###Training generative models like Generative Adversarial Network (GAN) is challenging for noisy data. A novel curriculum learning algorithm pertaining to clustering is proposed to address this issue in this paper. The curriculum construction is based on the centrality of underlying clusters in data points. The data points of high centrality takes priority of being fed into generative models during training. To make our algorithm scalable to large-scale data, the active set is devised, in the sense that every round of training proceeds only on an active subset containing a small fraction of already trained data and the incremental data of lower centrality. Moreover, the geometric analysis is presented to interpret the necessity of cluster curriculum for generative models. The experiments on cat and human-face data validate that our algorithm is able to learn the optimal generative models (e.g. ProGAN) with respect to specified quality metrics for noisy data. An interesting finding is that the optimal cluster curriculum is closely related to the critical point of the geometric percolation process formulated in the paper.", "review": "Review:###The paper states that training generative models is challenging when there are noisy data points in your training set. To address this, the authors propose to training methodology (or curricula) where *easier* or more *relevant* points are presented first followed to the model followed by the less relevant ones. The relevance is determined used by calculating centrality on a graph constructed out of the data points. I lean towards rejecting the paper, primarily because 1) The authors have not provided evidence to claim the noisy data points makes training challenging or characterized anything about under how much noise the training breaks down. 2) Experimental evidence is not convincing 3) There are not of imprecise statements in the paper. The authors claim (without citation) that training generative models in the presence of noisy data is challenging. How much noise are we talking about? If the entire training set is very noisy, maybe we don*t have any hope to learn to generate clean samples, but if it*s just a little bit of noise, maybe it*s fine. I also understand that when the authors use the word *noise* they don*t really mean noise, but changes in view point etc. Some convincing demonstration that such characteristics of dataset adversely affects the training of generative models will be helpful (in addition to one passing sentence in the experiment section about it). I am not convinced by any of the experiments. More importantly, it seems like the proposed training curricula is in general valid for a lot of generative models. The experimental evidence is really not convincing. Very limited experiments is done on two datasets only using one particular GAN. To convincingly demonstrate that your training methodology is doing something non-trivial, you should show that this works on multiple generative models on multiple datasets and compare your performance (and visualize some generated samples) against just training blindly on the entire dataset. In addition to this, I don*t understand the relevance of some of the results in the paper. For ex, what does Corollary 3 signify? Some other points: 1. First para. *non-trial* -> non-trivial 2. First para: *model collapse* -> mode collapse 3. 3.2 para 2. *base set that guarantees a proper converge of generative models*. What do you mean by *proper convergence*? 4. 3.2 para 2. *moderate compared to m*. Again, what is moderate? 5. Why do you use ResNet features (5.1) for distance? Why is this a reasonable or the best metric while computing your graph? 6. *We determine the parameter alpha of the edge weight by enforcing geometric mean of the weights to be 0.8*. It seems very arbitrary to me. Can you justify this choice?"}
{"id": "iclr2020_964", "title": "FINBERT: FINANCIAL SENTIMENT ANALYSIS WITH PRE-TRAINED LANGUAGE MODELS | OpenReview", "abstract": "Abstract:###While many sentiment classification solutions report high accuracy scores in product or movie review datasets, the performance of the methods in niche domains such as finance still largely falls behind. The reason of this gap is the domain-specific language, which decreases the applicability of existing models, and lack of quality labeled data to learn the new context of positive and negative in the specific domain. Transfer learning has been shown to be successful in adapting to new domains without large training data sets. In this paper, we explore the effectiveness of NLP transfer learning in financial sentiment classification. We introduce FinBERT, a language model based on BERT, which improved the state-of-the-art performance by 14 percentage points for a financial sentiment classification task in FinancialPhrasebank dataset.", "review": " This paper proposes a domain adaptation type of task via proposing fine-tuning of pre-trained models such as BERT on data from financial domains. The paper starts off with a good motivation about requiring some kind of domain adaptation particularly when performing tasks such as sentiment analysis on data sets from the financial domain. However, there is not much novelty in this paper. 1)The authors do not propose any new model architectures. Even if we were to argue the novelty is in terms of their empirical work, there are some flaws/missing details in the experiments. 2)In table 1 authors present agreement amongst annotators, it would be nice if in addition to mentioning the source of the data, the authors included what metric was used to attain agreement. I had to read the original paper releasing the data set to figure this out. 3)Table 4 presents results that do not seem significant. It is hard to conclude if a certain pre-training strategy worked for sure. On the whole I am very lukewarm on this paper. I find this paper lacking in novelty. Seems like an ambitious class project turned into an ICLR submission."}
{"id": "iclr2020_965", "title": "Programmable Neural Network Trojan for Pre-trained Feature Extractor | OpenReview", "abstract": "Abstract:###Neural network (NN) trojaning attack is an emerging and important attack that can broadly damage the system deployed with NN models. Different from adversarial attack, it hides malicious functionality in the weight parameters of NN models. Existing studies have explored NN trojaning attacks in some small datasets for specific domains, with limited numbers of fixed target classes. In this paper, we propose a more powerful trojaning attack method for large models, which outperforms existing studies in capability, generality, and stealthiness. First, the attack is programmable that the malicious misclassification target is not fixed and can be generated on demand even after the victim*s deployment. Second, our trojaning attack is not limited in a small domain; one trojaned model on a large-scale dataset can affect applications of different domains that reuses its general features. Third, our trojan shows no biased behavior for different target classes, which makes it more difficult to defend.", "review": " This paper proposes an adaption of existing backdoor attacks, with the main goal of enabling backdoor attacks in the transfer learning setting. Specifically, instead of pre-defining the trigger pattern and the target label, they train a neural network to generate different trigger patterns for different target images, so that after the source image is blended with the generated trigger pattern, it will be classified in the same way as the target image. This formulation makes it possible to generate backdoor attacks that stay effective in the transfer learning setting, when the label set of the fine-tuning task is different from the original task. They evaluate their approach using pre-trained models on ImageNet, and also show transfer learning results using two smaller datasets. I think studying the effectiveness of backdoor attacks under the transfer learning setting is a good topic. However, I am not convinced that the proposed approach is necessary a good way to do so, and have the following questions: 1. To train the trigger generator, do the authors only train it on the pre-trained dataset, or the images of the downstream task is also used? If training does not use the images from the downstream task at all, then it is interesting that the generator can generalize well, which may suggest that although the downstream task has a different prediction goal, the input images themselves share some similarity to the pre-trained task. Could the authors provide some explanation on it? 2. For the attack success rate, I would like to see more analysis on how the choice of the source and target images affect the attack performance. Specifically, if the source and target images have the same label on the pre-trained task, but different on the downstream task, is it easier or harder to generate successful backdoors for the downstream task? Similarly, what if the source and target images have different labels on the pre-trained task, but the same label on the downstream task? 3. Is it necessary to generate different trigger patterns for every different target image? Is it possible to use the same trigger pattern for multiple target images, at least in the case when they look similar? In general, backdoor attacks would expect that the same trigger pattern can be re-used among different input instances. ------------- Post-rebuttal comments Thanks for your response! However, I still think the evaluation is weak, given that the attack success rate is low, and the current version of the paper does not provide a good justification for both the design choices and the empirical results. Therefore, I keep my original assessment. ------------"}
{"id": "iclr2020_966", "title": "ODE Analysis of Stochastic Gradient Methods with Optimism and Anchoring for Minimax Problems and GANs | OpenReview", "abstract": "Abstract:###Despite remarkable empirical success, the training dynamics of generative adversarial networks (GAN), which involves solving a minimax game using stochastic gradients, is still poorly understood. In this work, we analyze last-iterate convergence of simultaneous gradient descent (simGD) and its variants under the assumption of convex-concavity, guided by a continuous-time analysis with differential equations. First, we show that simGD, as is, converges with stochastic sub-gradients under strict convexity in the primal variable. Second, we generalize optimistic simGD to accommodate an optimism rate separate from the learning rate and show its convergence with full gradients. Finally, we present anchored simGD, a new method, and show convergence with stochastic subgradients.", "review": " *Summary* This paper provides the analysis of three algorithms in the context of minmax convex concave games: Simultaneous stochastic subgradient method, Simultaneous gradient with optimism and Simultaneous gradient with anchoring. These three algorithm are first analysed in continuous time with an ODE perspective and then leverage these intuitions and techniques to analyze the discrete time versions. I think that these contributions are of interest of ICLR community, however, I have some concerns regarding the presentations of the results. *Decision* I vote for a weak accept that could move to an accept if the authors improve the presentation of the paper: The paragraph “Regularized dynamics and convergence’ in Section 3.1 in quite hard to follow. This subsection is basically the proof of the convergence of the continuous version of GD-O. Stating the result before the proof would help the reader to understand where the authors want to go. Very same point for The subsection 4.1 For the stochastic version of SimGD-A a new parameter is introduced without any comment or description on why it is necessary. A FID above 40 for MNIST is very far from standard results (that are below 1). Thus I am not convinced by the practical advantage of Anchored Adam on these models that have performances results very far from the standard ones. (for instance between 20 and 25 for CIFAR10 is very reasonable). It is maybe because you do not use convolutional layers in your architecture. I think that using a DGAN architecture (for instance the one from the pytorch tutorial https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) would give the expected results (i.e. FID close to 0). *Questions* - When you cite Lassale Principle you mention that if is a limit point of then starting at , you stay at a constant distance to . But in the bilinear example you give any point in the circle is not a limit point (size no dynamics converge to it). I guess when you said limit point you meant adherent point ? - Is the condition only necessary for the proof ? Does work in practice ? If no, what is the best value for ?"}
{"id": "iclr2020_967", "title": "FoveaBox: Beyound Anchor-based Object Detection | OpenReview", "abstract": "Abstract:###We present FoveaBox, an accurate, flexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis. Without bells and whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance. We believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection.", "review": " This paper introduces an anchor-free object detection framework that aims at simultaneously predicting the object position and the corresponding boundary. To achieve this, the proposed FoveaBox detector predicts category-sensitive semantic maps for the object existing possibility, and produces category-agnostic bounding box for each position that is likely to contain an object. The scales of target boxes are associated with feature pyramid representations. Experiments are performed on MS COCO detection benchmark. Pros: The proposed approach is simple and is shown to avoid most computation and hyper-parameters related to anchor boxes. The paper is well written and easy to follow. Cons: The main issue with the paper is the main idea is similar to [1,2, 3, 4, 5]. For instance, CenterNet also represents each object instance by its features at the center point and achieves similar detection performance compared to the proposed detector. Further, no speed comparison with these approaches is provided in the paper. Without a fair speed comparison and with similar detection performance, it is difficult to fully assess the merits of the proposed approach. Though inference speed comparison is reported with RetineNet. However, a proper and detailed comparison with [1, 2, 3, 4, 5] is missing. 1: Xingyi Zhou, Dequan Wang, Philipp Krähenbühl: Objects as Points. CoRR abs/1904.07850 (2019). 2: Xingyi Zhou, Jiacheng Zhuo, Philipp Krähenbühl: Bottom-Up Object Detection by Grouping Extreme and Center Points. CVPR 2019. 3: Zhi Tian, Chunhua Shen, Hao Chen, Tong He: FCOS: Fully Convolutional One-Stage Object Detection. CoRR abs/1904.01355 (2019). 4: Hei Law and Jia Deng: Cornernet: Detecting objects as paired keypoints. ECCV 2018. 5: Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, Qi Tian: CenterNet: Keypoint Triplets for Object Detection. CoRR abs/1904.08189 (2019)."}
{"id": "iclr2020_968", "title": "Gating Revisited: Deep Multi-layer RNNs That Can Be Trained | OpenReview", "abstract": "Abstract:###Recurrent Neural Networks (RNNs) are widely used models for sequence data. Just like for feedforward networks, it has become common to build *deep* RNNs, i.e., stack multiple recurrent layers to obtain higher-level abstractions of the data. However, this works only for a handful of layers. Unlike feedforward networks, stacking more than a few recurrent units (e.g., LSTM cells) usually hurts model performance, the reason being vanishing or exploding gradients during training. We investigate the training of multi-layer RNNs and examine the magnitude of the gradients as they propagate through the network. We show that, depending on the structure of the basic recurrent unit, the gradients are systematically attenuated or amplified, so that with an increasing depth they tend to vanish, respectively explode. Based on our analysis we design a new type of gated cell that better preserves gradient magnitude, and therefore makes it possible to train deeper RNNs. We experimentally validate our design with five different sequence modelling tasks on three different datasets. The proposed stackable recurrent (STAR) cell allows for substantially deeper recurrent architectures, with improved performance.", "review": "Review:###This paper introduces a new structure to help gradients propagate through the multilayer recurrent neural network. The authors demonstrate the problems of backpropagation in RNN and LSTM analytically and numerically. Finally, the authors showcase the power of the their new structures on three datasets: pixel MNIST, TUM and Jester. The paper should be rejected because: (1) The novelty of the paper is limited. The final recurrent unit looks very similar to GRU but the authors do not compare it with GRU in any of the experiments. (2) The calculation of the gradient norm is ambiguous. The weights are shared in different timesteps in recurrent neural network. But it is unclear that the authors show the gradient of loss function with respect to parameters on only one timestep or the sum of all time steps. (3) It is unclear why the depth is an important problem in recurrent neural network. Backpropagating through the layers of the unit should be similar to backpropagating through the timesteps. Main arguments: 1. The authors start from the LSTM cell and analysis each part of it. Finally, the authors come up with a new structure (equations (10) (11) (12)). This new structure looks extremely similar to GRU (Gated recurrent units). The only difference is that the authors use another tanh outside the gated update in equation (12). However, the authors never compare it with GRU in all the experiments or even mention it in the main text. The only place to mention GRU is the appendix. From figure 6, GRU performed similarly as STAR in norm of gradients. But the authors never mention it in the main text. 2. RNN parameters are shared across all the timesteps. If you calculate the gradient, g_w (equation 3), it should be a sum of all the timesteps sum_t(dh_t^l/dw)g_{h_t^l}. Even though the vanishing gradient problem only cares about one term in the sum above, it is hard to tell if the authors notice the difference. Moreover, it is hard to tell if the authors plot the norm of one term in the above sum or the whole sum. This should be clarified. 3. The vanishing gradient problem is that gradient decreases exponentially with the timestep but the authors seem to misinterpret it as the gradient is small, which is incorrect. If the authors still want to show the vanishing gradient/exploding gradient, the authors would better show the log scale norms of the gradients in figure 2. From the current plot, I cannot see *The gradient decreases exponentially with the timestep/number of layers*. They are all roughly in the same log scale. 4. It is unclear why the depth is, in particular, a problem in recurrent neural network. Backpropagating through the layers of the unit should be similar to backpropagating through the timesteps. The maximum backpropagating length will be depth+length, which is just a longer sequence. 5. Some parts of the analysis are unclear. Like: a. In the last paragraph of page 4, *sigmoid function causes...drop to 0.25*. I do not know why this sentence is important since there are multiple terms in the sum in equation (8). *0.25* may not make the whole gradient smaller. b. Not sure why the two Jacobians now has singular values equal to 0.5 in the sentence below equation (14). Minors: 1. The last paragraph *We note that...* in the related work seems to be unrelated to the paper. I would remove it."}
{"id": "iclr2020_969", "title": "Improved Training Speed, Accuracy, and Data Utilization via Loss Function Optimization | OpenReview", "abstract": "Abstract:###As the complexity of neural network models has grown, it has become increasingly important to optimize their design automatically through metalearning. Methods for discovering hyperparameters, topologies, and learning rate schedules have lead to significant increases in performance. This paper shows that loss functions can be optimized with metalearning as well, and result in similar improvements. The method, Genetic Loss-function Optimization (GLO), discovers loss functions de novo, and optimizes them for a target task. Leveraging techniques from genetic programming, GLO builds loss functions hierarchically from a set of operators and leaf nodes. These functions are repeatedly recombined and mutated to find an optimal structure, and then a covariance-matrix adaptation evolutionary strategy (CMA-ES) is used to find optimal coefficients. Networks trained with GLO loss functions are found to outperform the standard cross-entropy loss on standard image classification tasks. Training with these new loss functions requires fewer steps, results in lower test error, and allows for smaller datasets to be used. Loss function optimization thus provides a new dimension of metalearning, and constitutes an important step towards AutoML.", "review": " *Summary* The authors propose using evolutionary computation (EC) to perform meta learning over the set of symbolic expressions for loss functions. It*s a compelling idea that is well-motivated. They find that applying their EC method to mnist yields an interesting loss function that they name the *Baikal loss.* Much of the paper is devoted to analyzing the properties and performance of the Baikal loss. *Overall Assessment* The paper*s idea is very interesting. However, there are some important drawbacks of this work. These should be fixed and the paper should be resubmitted to a different conference soon. 1) The experiments focus almost entirely on the Baikal loss (a particular loss function found once when running EC on mnist), and do not analyze the overall behavior of EC for loss functions. Does EC consistently converge to the same loss, or do different ones emerge different times you run it? What happens if you optimize convergence speed vs. generalization accuracy with EC? How do these loss functions differ? 2) The experiments are largely on mnist, with a small study showing that the Baikal loss can be applied to cifar-10. It would be good to show that loss functions meta-learned on mnist generalize to larger-scale problems than cifar. *Comments* I was surprised when you optimized in fig 3 for convergence speed, rather than final accuracy of something that runs for a while. Why should our goal be to find loss functions that lead to fast optimization, instead of loss functions that lead to models that generalize best? If these are two different goals, then you should have two sets of experiments analyzing how GLO can find interesting (and perhaps different) loss functions for each. Mnist is possible to get basically 100% accuracy. This means that the loss will only be evaluated in certain regimes of its inputs. What happens when you transfer this to problems where the best achievable accuracy is something like 60% for binary classification? You should cite the Focal loss as another alternative to the cross entropy loss. Is the focal loss achievable in your particular grammar over loss functions? You should also cite label smoothing as an additional way to achieve a very similar implicit regularization effect as the Baikal loss. You only analyze one loss function that came from your EC. What if you run it multiple times? Do you find different formulas? How do these perform? The beginning of the paper is very focused on EC, but then you transition suddenly to only discussing the Baikal loss. Can you present experiments demonstrating, for example, how the EC performance varies with the number of steps, with different ways to define the search space, etc?"}
{"id": "iclr2020_970", "title": "Meta Decision Trees for Explainable Recommendation Systems | OpenReview", "abstract": "Abstract:###We tackle the problem of building explainable recommendation systems that are based on a per-user decision tree, with decision rules that are based on single attribute values. We build the trees by applying learned regression functions to obtain the decision rules as well as the values at the leaf nodes. The regression functions receive as input the embedding of the user’s training set, as well as the embedding of the samples that arrive at the current node. The embedding and the regressors are learned end-to-end with a loss that encourages the decision rules to be sparse. By applying our method, we obtain a collaborative filtering solution that provides a direct explanation to every rating it provides. With regards to accuracy, it is competitive with other algorithms. However, as expected, explainability comes at a cost and the accuracy is typically slightly lower than the state of the art result reported in the literature. Our code is attached as supplementary.", "review": "Review:###Very well written paper. A very easy read. Usually authors leave the references riddled with minor formatting errors, which you didn*t. Very nice method. Well constructed cost function (Sct 3.2), clear, simple approach (Alg. 1), and a technique for converting regression trees to decision trees (with tests on single variables) that surprised me with its simplicity and effectiveness. Note in a sense you are using amortised inference methods (the functions g() and h()), which makes the local trees work. Suggest you make this connection in the Related Work. Its definitely an important characteristic of recent algorithms. Experimental work is convincing enough. Thanks for doing the ablation studies. I think this should be an essential part of all machine learning algorithm papers. The most important part of the paper is the discussion on white-box introspection. This is what makes the algorithm worth something. Without this discussion, you just have yet another reasonable RS method. I wonder how this sort of evaluation can be made more *scientific*, more *objected* and *measured*. Please think about this and read papers from your colleagues doing explainable systems to see what they have done. I*d think a user study would help, though this adds a level of complexity machine learning folks have not been trained to do. Not much to say because, well, its surprisingly simple but effective, and well written."}
{"id": "iclr2020_971", "title": "GUIDEGAN: ATTENTION BASED SPATIAL GUIDANCE FOR IMAGE-TO-IMAGE TRANSLATION | OpenReview", "abstract": "Abstract:###Recently, Generative Adversarial Network (GAN) and numbers of its variants have been widely used to solve the image-to-image translation problem and achieved extraordinary results in both a supervised and unsupervised manner. However, most GAN-based methods suffer from the imbalance problem between the generator and discriminator in practice. Namely, the relative model capacities of the generator and discriminator do not match, leading to mode collapse and/or diminished gradients. To tackle this problem, we propose a GuideGAN based on attention mechanism. More specifically, we arm the discriminator with an attention mechanism so not only it estimates the probability that its input is real, but also does it create an attention map that highlights the critical features for such prediction. This attention map then assists the generator to produce more plausible and realistic images. We extensively evaluate the proposed GuideGAN framework on a number of image transfer tasks. Both qualitative results and quantitative comparison demonstrate the superiority of our proposed approach.", "review": "Review:###This paper proposes an extension of the conditional GAN objective, where the generator conditions on an attention map produced by the discriminator in addition to the input image. The motivation is that the discriminator is usually too powerful, and so the gradient the generator receives is often too small in magnitude. By conditioning on the attention map, the generator could leverage information about the regions in the image that the discriminator attends to and use it to generate a new image that better fools the discriminator. My main concern is about whether the proposed extension achieves the desired goal. The intuitive motivation provided in the paper aims to add a cooperative component to the two-player game, but the min-max objective corresponds to a zero-sum adversarial game. As a result, when training the discriminator, the discriminator is encouraged to reveal as little information as possible via the attention map, so that the loss maximized. This appears to be the opposite of the desired behavior, so the objective needs to be reformulated. Also, it is unclear how inference is performed: at test time, the attention map is unknown and so some placeholder must be used in its place. The paper should clarify what is done at test time, and clearly state the shortcomings as a result of this, i.e. different procedures are used for training and testing, which is not principled. I imagine the generator could rely too much on the attention map as a result - how this is alleviated/prevented should be explained. Figure 2: Only the qualitative results for unsupervised image-to-image translation are available; qualitative results for supervised image-to-image translation should also be provided. While the quantitative improvement over existing methods is somewhat insignificant, I appreciate the authors discussing their hypotheses why this might be the case. It would be more useful to empirically validate these hypotheses as well. For example, for the claim that *maybe the attention map only focuses on a few domain specific classes so the generator works too hard on those classes and ignores others*, it might be good to compute the average per-class attention map intensity to show that some classes appear rarely in the attention map. The evaluation protocol should be explained in greater detail (perhaps in the appendix); the segmentation model (which I assume is FCN) should be described and each of the evaluation metrics (per-pixel acc., per-class acc. and IoU) should be described for the benefit of researchers outside the area. pg. 4: *in their implementation contains several Resblock (He et al., 2016), which makes it infeasible in our framework*. Why is it infeasible? pg. 6: What are the architectures used by the baselines? Are they comparable to the architecture the proposed method used? Minor Issues: pg. 3: *differences between P_x and G_Y cdot P_y, P_y and G_X cdot P_x are minimized* - confusing; should rephrase as *the difference between P_x and G_Y cdot P_y and the difference between P_y and G_X cdot P_x are minimized*. Also should replace cdot with circ. pg. 4: *like random noisy* -> *like random noise* pg. 4, last paragraph: *Our trainable attention module follows the same structure of the attention block in RAM (Wang et al., 2017). They built a very deep network with several such blocks, each containing two branches: mask branch and trunk branch. Mask branch cascades the input features through a bottom-up top- down architecture that mimics human attention. Trunk branch is applied as feature processing.* - this is very confusing; it would be easier to refer readers to the appendix. pg. 5 - *Attention mask can potentially break good property of the raw input.* - what does this mean? pg. 6 - *as showed in Table 4.1* -> *as shown in Table 4.1*"}
{"id": "iclr2020_972", "title": "Automated Relational Meta-learning | OpenReview", "abstract": "Abstract:###In order to efficiently learn with small amount of data on new tasks, meta-learning transfers knowledge learned from previous tasks to the new ones. However, a critical challenge in meta-learning is the task heterogeneity which cannot be well handled by traditional globally shared meta-learning methods. In addition, current task-specific meta-learning methods may either suffer from hand-crafted structure design or lack the capability to capture complex relations between tasks. In this paper, motivated by the way of knowledge organization in knowledge bases, we propose an automated relational meta-learning (ARML) framework that automatically extracts the cross-task relations and constructs the meta-knowledge graph. When a new task arrives, it can quickly find the most relevant structure and tailor the learned structure knowledge to the meta-learner. As a result, the proposed framework not only addresses the challenge of task heterogeneity by a learned meta-knowledge graph, but also increases the model interpretability. We conduct extensive experiments on 2D toy regression and few-shot image classification and the results demonstrate the superiority of ARML over state-of-the-art baselines.", "review": "Review:###This paper mainly tackles the problem of heterogeneous tasks in meta-learning by proposing a new meta-learning framework ARML, which contains a module extracting relations across classes and a module representing meta-knowledge. When processing a new task, a graphical task representation is firstly constructed based on class prototypes, and then information propagation is conducted on a super-graph to find the most relevant meta-knowledge in the meta-knowledge graph. Ideally, the higher similarity between a prototype and a meta-knowledge node means the higher the correlation between a class and a specific type of meta-knowledge. In order to construct task-specific meta-learners, the authors utilize two auto-encoders to encode task representations with and without meta-knowledge graph. After that, a modulating function is applied to a set of shared parameters, which finishes the calculation of task-specific parameters. The authors empirically evaluated the proposed method on several datasets and it seems that ARML outperforms some compared methods. This paper should be rejected. Firstly, the proposed method is not well motivated. It’s true that tasks in meta-learning may be sampled from a complex (or multi-modal) task distribution, but why to represent a task as a graph? I think the relation between tasks can be simply obtained from instances (CNN embeddings). Secondly, it’s hard to say the meta-knowledge graph can really capture knowledge with ‘exact meanings’ even though in some situations, a subset of nodes is activated and others are not. Main arguments 1. The whole framework is too complex and it’s hard to say every module in the framework really works even ablation study is done. 2. The meta-knowledge graph lacks interpretability. From my perspective, it’s just a set of learnable parameters without any exact meanings. Authors tried to analyze the constructed meta-knowledge graph by some experiments, but these discussions are farfetched. Things to improve the paper 1. Simplify the proposed method. 2. Make it clear why should we represent a task as a graph. 3. Some most widely used benchmark datasets such as mini-imagenet and tiered-imagenet are not used. For a fair and convincing comparison, I suggest the authors test the proposed method on these benchmark datasets. Moreover, more methods should be compared."}
{"id": "iclr2020_973", "title": "Deep RL for Blood Glucose Control: Lessons, Challenges, and Opportunities | OpenReview", "abstract": "Abstract:###Individuals with type 1 diabetes (T1D) lack the ability to produce the insulin their bodies need. As a result, they must continually make decisions about how much insulin to self-administer in order to adequately control their blood glucose levels. Longitudinal data streams captured from wearables, like continuous glucose monitors, can help these individuals manage their health, but currently the majority of the decision burden remains on the user. To relieve this burden, researchers are working on closed-loop solutions that combine a continuous glucose monitor and an insulin pump with a control algorithm in an `artificial pancreas.* Such systems aim to estimate and deliver the appropriate amount of insulin. Here, we develop reinforcement learning (RL) techniques for automated blood glucose control. Through a series of experiments, we compare the performance of different deep RL approaches to non-RL approaches. We highlight the flexibility of RL approaches, demonstrating how they can adapt to new individuals with little additional data. On over 21k hours of simulated data across 30 patients, RL approaches outperform baseline control algorithms (increasing time spent in normal glucose range from 71% to 75%) without requiring meal announcements. Moreover, these approaches are adept at leveraging latent behavioral patterns (increasing time in range from 58% to 70%). This work demonstrates the potential of deep RL for controlling complex physiological systems with minimal expert knowledge.", "review": "Review:###Paper Summary This paper examines reinforcement learning in the context of blood glucose control to help individuals with type 1 diabetes. The authors show that their methods lead to strong algorithms that can improve artificial pancreas systems. Their results are promising, and, very importantly, do not require meal announcements. The importance of their application is self evident. Decision Should their claim to novelty hold up, then the authors have provided evidence that RL can be useful for this important application of glucose control. Overall, the paper is very well written with clear arguments, and the impact of their study for type 1 diabetes is high. However, there are novelty concerns with the proposed methods. The paper needs some work before acceptance. Additional Feedback One caveat is that a small search of previous RL methods in blood glucose control did yield some similarly titled papers (please discuss *Reinforcement Learning Algorithm for Blood Glucose Control in Diabetic Patients* by Javad et al), and they were not addressed or compared in this paper. To push this review over the edge, the authors should address these papers in the related work, and discuss how this paper*s method compares. Additionally, the novelty of the actual RL methods is not entirely clear. The authors should very clearly point out their contributions within the methods sections, differentiating between past methods and the proposed one. Most importantly, the authors should write a paragraph-length section at the end of the introduction detailing their proposed methods, with a bullet-point layout of every novel detail. This will help future readers get the gist of the paper more accurately. Lastly, though the authors addressed the limitations of their dataset in terms of it being a simulation, they should also discuss the sample size being only 10 patients in different age groups. It would be helpful for readers to know how the method will generalize to new patients."}
{"id": "iclr2020_974", "title": "Self-Imitation Learning via Trajectory-Conditioned Policy for Hard-Exploration Tasks | OpenReview", "abstract": "Abstract:###Imitation learning from human-expert demonstrations has been shown to be greatly helpful for challenging reinforcement learning problems with sparse environment rewards. However, it is very difficult to achieve similar success without relying on expert demonstrations. Recent works on self-imitation learning showed that imitating the agent*s own past good experience could indirectly drive exploration in some environments, but these methods often lead to sub-optimal and myopic behavior. To address this issue, we argue that exploration in diverse directions by imitating diverse trajectories, instead of focusing on limited good trajectories, is more desirable for the hard-exploration tasks. We propose a new method of learning a trajectory-conditioned policy to imitate diverse trajectories from the agent*s own past experiences and show that such self-imitation helps avoid myopic behavior and increases the chance of finding a globally optimal solution for hard-exploration tasks, especially when there are misleading rewards. Our method significantly outperforms existing self-imitation learning and count-based exploration methods on various hard-exploration tasks with local optima. In particular, we report a state-of-the-art score of more than 20,000 points on Montezumas Revenge without using expert demonstrations or resetting to arbitrary states.", "review": "Review:###Note: the style-formatting of this paper has been heavily tweaked, and so the evaluation should be calibrated for a 9-page paper. This paper proposes an approach for diverse self-imitation for hard exploration problems. The idea is leverage recently proposed self-imitation approaches for learning to imitate good trajectories generated by the policy itself. By encouraging diversity in the pool of trajectories for self-imitation, the idea is to encourage faster learner -- this basic concept is also used in approaches like prioritized experience replay, albeit at the entire trajectory level rather than individual state/action level. The authors view this approach as a generalization of Go-Explore, since it does not rely on having a reset mechanism. However, I think this discussion has a lot of subtle nuances pertaining to the stochasticity of the environment (which the authors acknowledge). For instance, if the environment is deterministic, then why not just do something like Go-Explore, since state-reset is just memorizing a deterministic action sequence? The empirical results are very strong, achieving state-of-the-art results for any approach not reliant on a reset mechanism. All the primary experiments appear to be for deterministic environments. The results on stochastic environments (in the Appendix) seem pretty weak (but please correct me if I*m mistaken here). So one major question is whether Go-Explore is a scientifically appropriate benchmark to compare with for this setting. In summary, I*m willing to be convinced that this is an interesting and scientifically novel result. I have some concerns as expressed above. **** After Author Response **** Thanks for the response. I*m willing to raise my score to weak accept. I think the authors did a reasonable job addressing my specific questions. Some further reflection revealed to me that there is a huge opportunity to scientifically investigate how stochasticity impacts the proposed algorithm. For instance, one could conduct a systematic study (say of the Apple domain) where one varies the degree of stochasticity and measures how the performance the proposed algorithm changes, perhaps relative to Go-Explore on the purely deterministic version of the environment. It seems a bit of a cop-out to say that Go-Explore is not applicable, and misses out a huge opportunity for real scientific understanding."}
{"id": "iclr2020_975", "title": "Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation | OpenReview", "abstract": "Abstract:###Natural question generation (QG) aims to generate questions from a passage and an answer. Previous works on QG either (i) ignore the rich structure information hidden in text, (ii) solely rely on cross-entropy loss that leads to issues like exposure bias and inconsistency between train/test measurement, or (iii) fail to fully exploit the answer information. To address these limitations, in this paper, we propose a reinforcement learning (RL) based graph-to sequence (Graph2Seq) model for QG. Our model consists of a Graph2Seq generator with a novel Bidirectional Gated Graph Neural Network based encoder to embed the passage, and a hybrid evaluator with a mixed objective function that combines both the cross-entropy and RL loss to ensure the generation of syntactically and semantically valid text. We also introduce an effective Deep Alignment Network for incorporating the answer information into the passage at both the word and contextual level. Our model is end-to-end trainable and achieves new state-of-the-art scores, outperforming existing methods by a significant margin on the standard SQuAD benchmark for QG.", "review": "Review:###The authors propose a Graph-to-Sequence Reinforcement Learning Model for Natural Question Generation, evaluated on SQuAD benchmark in for Question Generation. An interesting aspect of the work is related to the Graph2Seq model, and the use of the Reinforcement Learning to fine-tune the model. The latter stage seems to improve the structure of the answers considerably. An interesting use of RL algorithm and apparently a good choice of reward functions. Questions: in the combined loss used in the RL run: 1. Have you managed to have a successful run with gamma = 1? 2. I understand that the L_rl factor is computed based on the sampling, and the L_lm is computed based on the top variant from the nbest list?"}
{"id": "iclr2020_976", "title": "Self-Educated Language Agent with Hindsight Experience Replay for Instruction Following | OpenReview", "abstract": "Abstract:###Language creates a compact representation of the world and allows the description of unlimited situations and objectives through compositionality. These properties make it a natural fit to guide the training of interactive agents as it could ease recurrent challenges in Reinforcement Learning such as sample complexity, generalization, or multi-tasking. Yet, it remains an open-problem to relate language and RL in even simple instruction following scenarios. Current methods rely on expert demonstrations, auxiliary losses, or inductive biases in neural architectures. In this paper, we propose an orthogonal approach called Textual Hindsight Experience Replay (THER) that extends the Hindsight Experience Replay approach to the language setting. Whenever the agent does not fulfill its instruction, THER learn to output a new directive that matches the agent trajectory, and it relabels the episode with a positive reward. To do so, THER learns to map a state into an instruction by using past successful trajectories, which removes the need to have external expert interventions to relabel episodes as in vanilla HER. We observe that this simple idea also initiates a learning synergy between language acquisition and policy learning on instruction following tasks in the BabyAI environment.", "review": " What is the specific question/problem tackled by the paper? This paper tackles the problem of learning language-conditioned policies from reinforcement learning. Unlike most language-conditioned navigation work which relies on human demonstrations (e.g. in the room2room environment), this work only learns from the agent’s experience using a generalization of hindsight experience replay. Method overview: THER (textual HER) generalizes HER to cases where goals are not in the same space as states. To deal with this gap, THER learns a mapping from state space to goal space using successful trajectories. This mapping is then used to relabel unsuccessful trajectories with a guess of what goal was reached. This intuitive approach allows the text-conditioned agent to reach 40% at a 2D navigation task when conditioned on text such as “Pick the large red circle”. Strengths: The method is well motivated and would be useful. The ablations of showing how many successful trajectories are needed to learn the mapping (~1000-5000), how many time steps are needed to reach 1000 successes (~400k steps), and how accurate the mapping needs to be for HER to work (~80%) and thorough and easy to understand. This experimental completeness is itself a contribution. Additionally, although the authors do not discuss this, this method is actually agnostic to the particular modality (e.g. text) of the goal space and could be used anytime the goal space differs from the state space. Weaknesses: The primary weakness of the paper is that the testbed environment and the textual goals are very simple. The “language” is just a list of up to 4 attributes describing the different objects and the control is simple navigation without any walls of visual variation. Additionally, the method requires accidentally getting successful trajectories early in training in order to train the mapping, and in this environment it is very easy to get successful trajectories. I would interested in seeing how this method would work in the room2room environment (or some other more complex task). While it is unlikely to outperform the prior methods that use the human demonstrations, it would be useful to see how close THER can get to that performance and with how many environment steps. The advantage of this environment is that it has real human knowledge, and the textual goals are limited in number, making the experiment much more realistic (as humans are unlikely to sit next to an agent and generate infinitely many diverse textual goals). A missing ablation in Figure 4 left is THER without waiting 400k steps before relabeling. In realistic scenarios, we would not be able to evaluate the mapper ahead of time to know when to start relabeling. How is performance affected is this knowledge is not available? Overall, I lean to reject the paper in it’s current form, I believe this paper would be more impactful with experiments involving more language complexity or more policy complexity."}
{"id": "iclr2020_977", "title": "Geometry-Aware Visual Predictive Models of Intuitive Physics | OpenReview", "abstract": "Abstract:###Learning object dynamics for model-based control usually involves choosing among two alternatives: i) engineered 3D state representations comprised of 3D object locations and poses, or, ii) learnt 2D image representations trained end-to-end for the dynamics prediction task. The former requires laborious human annotations to extract the 3D information from 2D images, and does not permit end-to-end learning. The latter has not shown until today to generalize across camera viewpoints or to handle camera motion and cross-object occlusions. We propose neural architectures that learn to disentangle an RGB-D video steam into camera motion and 3D scene appearance, and capture the latter into 3D feature representations that can be trained end-to-end with 3D object detection and object motion forecasting. We feed object-centric 3D feature maps and actions of the agent into differentiable neural modules and learn to forecast object 3D motion. We empirically demonstrate the proposed 3D representations learn object dynamics that generalize across camera viewpoints and can handle object occlusions. They do not suffer from error accumulation when unrolled over time thanks to the permanence of object appearance in 3D. They outperform by a margin both 2D learned image representations as well as engineered 3D ones in forecasting object dynamics.", "review": "Review:###The paper proposes a model to extract 3D object-centric representations of scenes from multiple RGB(-D) scene observations. These scene representations can be used with an action-conditional forward model to predict the state of the future state scene. The full model is comprised of multiple modules (each individual module is a well-known differentiable module) which can be tuned end-to-end by backpropagating the errors from the forward model. The authors emphasize that this is the first model that computes object centric representations in 3D and reasons about scene dynamics in 3D, providing advantages over previous systems that reasoned in 2D spaces or didn*t have an object-centric representation. The authors then test the model to perform single or multi step forward prediction and for model-based control, obtaining an advantage over models that use representations of only object position and rotation or 2D object-centric models. Overall the paper proposes a highly engineered system that seems i) highly specific to a particular setting/task and ii) very difficult to reproduce. The model works by inferring 3D object representations from RGB(D) images through neural networks that predict representations which are then manipulated through geometric operations to rotate/transform the scene. The authors reason that this does not require access to ground-truth (GT) 3D information as other previous models do, and can be trained end-to-end with a forward prediction task. While this is true, it also assumes that we can easily infer 3D object centric representations from scene views and that we can perfectly infer the future scene representation through geometrical transforms such as those performed with a 3D Spatial Transformer. This might be true in synthethic environments with 1 or 2 objects, but it seems quite impossible to infer and manipulate these representations from real world scenes with tens of objects, with extreme variations of relative locations, shapes, sizes, materials across objects. Small errors in inferring the scene representation could lead to large prediction errors, or interactions in complex scenes might not be properly captured by spatial transformers. Therefore, while the prior knowledge added to the system is more general than models that use GT information, it seems that the model could only be trained when it is easy to infer the scene state. In fact, the experiments are conducted on a synthetic environment with at most two objects, which perfectly fits the inductive bias of the model. It is not surprising that this model would surpass the baselines if properly optimized. I would like to see experiments on real data or way more complicated synthetic scenes with tens of objects to be convinced that this model could be used in a real world scenario. My guess is that such a complex model with many different modules can only be properly trained end-to-end when each of the individual tasks are relatively simple (inferring the 3D scene state, manipulating the scene). Furthermore, the model is not described with enough detail to reproduce it. The details of the neural architectures used are not given, how the Mask R-CNN module is repurposed to operate in 3D is not explained and the details of the GRNN used are missing. While most of the individual components are already existing models, knowing the specific details would help understand better the model. In summary, the paper proposes a highly complex system to learn 3D object centric scene representations with a forward model that doesn*t seem that it would generalize past simple environments and that is not described in sufficient detail to be reproduced. I would change my score if the authors showed that such model can perform predictions in simple yet real world scenarios that go beyond the synthetic environment used. For example, the authors should be able to directly use one of the BAIR Robotic Interaction datasets (https://sites.google.com/berkeley.edu/robotic-interaction-datasets) with their model, which replicates fairly well the synthetic environment used to test the model but has real data, and compare the accuracy of the predictions to a 2D video prediction model such as SVG-LP (Denton and Fergus, ICML 2018) that does not make any assumptions about the 3D world nor has an object-centric representation. Minor note: I*d recommend not including extraneous or non-supported claims such as *humans excel in manipulation despite that less than 1% of us know what inertia is* or *human brain effortlessly disentangles camera motion from object motion and appearance*"}
{"id": "iclr2020_978", "title": "Scale-Equivariant Neural Networks with Decomposed Convolutional Filters | OpenReview", "abstract": "Abstract:###Encoding the input scale information explicitly into the representation learned by a convolutional neural network (CNN) is beneficial for many vision tasks especially when dealing with multiscale input signals. We study, in this paper, a scale-equivariant CNN architecture with joint convolutions across the space and the scaling group, which is shown to be both sufficient and necessary to achieve scale-equivariant representations. To reduce the model complexity and computational burden, we decompose the convolutional filters under two pre-fixed separable bases and truncate the expansion to low-frequency components. A further benefit of the truncated filter expansion is the improved deformation robustness of the equivariant representation. Numerical experiments demonstrate that the proposed scale-equivariant neural network with decomposed convolutional filters (ScDCFNet) achieves significantly improved performance in multiscale image classification and better interpretability than regular CNNs at a reduced model size.", "review": " *Paper summary* The authors propose a CNN architecture, that is theoretically equivariant to isotropic scalings and translations. For this they add an extra scale-dimension to activation tensors, along with the existing two spatial dimensions. In practice they implement this with scale-steerable filters, which are discretised and truncated in both the spatial and scale-dimensions. They also provide a deformation robustness analysis. *Paper decision* Thank you for writing a very interesting paper indeed. I have to admit I am somewhat on the fence about this paper. I think it contains many nice ideas, but the experimental section is somewhat lacking in terms of comparisons or insights which I can gain, and the theory has some missing elements too (which I shall discuss below). For that reason, I am recommending a weak reject, but would very easily upgrade this if the authors provide strong rebuttal to my comments below. *Supporting arguments* -Experiments: The experiments are quite light, although I must admit that many other works in the area of equivariance are also light on experimentation and if there is enough theory, that is not such a great issue. The main issues I have are 1) the choice of experiments, 2) the comparisons against an insufficient number of baselines, and 3) the ablation studies. 1) The choice of experiments: I think looking at scaled-MNIST is not particularly useful as an experiment nowadays, unless used as a toy experiment. A larger dataset with real-life scale variations would have been better. Furthermore, I’m not sure what the image reconstruction task is supposed to tell the reader, that the ScDCFNet is able to generalise to new scales? 2) There is a lot of concurrent work on multiscale architecture. To name a few: Multigrid Neural Architectures, Ke et al., 2017 Feature Pyramid Networks for Object Detection, Lin et al., 2017 Multi-Scale Dense Networks for Resource Efficient Image Classification, Huang et al., 2018 Deep Scale-spaces, Worrall and Welling, 2019 I believe these works should at least be cited, but ideally compared against. 3) I would have liked to have seen some numerical results for the verification of scale equivariance. In the first layer the equivariance improbably going to be close to perfect because there is no truncation of the scale-dimension in the network, but after this layer I predict the equivariance error increases due to truncations effects. This would be a similar effect to other works, such as “Deep Scale-spaces” (Worrall and Welling, 2019). -Theory: I think the theory is very interesting and a meaningful contribution in its own right. The authors treat scale-translation in continuous space as a group action on signals. This motivates the convolution presented in Theorem 1. This is a group convolution as per Cohen and Welling, (2016) modified to continuous space for a non-compact group. (Actually, this should be mentioned in the text as a matter of good scholarship). This is nice, since the group convolution has not been used in for a regular representation of a non-compact group (other than translation) as far as I can tell. What is perhaps not clear for me is how the theory breaks down in practice, since the implementation requires discretisation in space AND scale, which is not discussed much and furthermore, filters are restricted in spatial AND scale dimensions, leading to truncation errors in the equivariance. This last perspective was not discussed, and I feel it rather should be. The theory goes further into deformation stability, which is a fresh perspective in the equivariance literature, so I am happy for its inclusion. Perhaps more motivation for why you think this is necessary would be warmly welcomed. *Smaller questions/notes for the authors* - Technically this is scale-translation equivariance, you even write this in the method section of your paper, why is it not in the title? The reason I mention this Is because there are scale equivariant networks, which are not translation equivariant in the literature, see “Warped Convolutions: Efficient Invariance to Spatial Transformations” (Henriques and Vedaldi, 2019). - Please make the link between Theorem 1 and the group convolution of Cohen and Welling (2016) - Last paragraph of page 1: A steerable-in-scale filter does exist, see “Deformable kernels for early vision” (Perona, 1991) - Please use numbering for all display-mode equations. - This scheme works perfectly in the continuous-image setting, but how about for discretized images? In that case it cannot be scale-translation equivariant because the scaling-action is no longer part of a group. - In equations 6 and 7, what is the specific motivation for using the laplacian eigen-decompositions as a basis? Is it for steerability with respect to the scale-translation action? Otherwise, surely any basis will do? - Remark 2: If you are considering a truncation of the scale-axis, surely you can still use an L2 norm when quantifying the robustness of your representation? - Bandlimiting of the filters: I would consider citing “Structured Receptive Fields in CNNs”, (Jacobsen et al., 2016) - Pooling: A useful citation here would be “Making Convolutional Networks Shift-Invariant Again” (Zhang, 2019). They precise low pass filter before pooling. - Experiments: —I’m not clear on the reason to include reasons with and without batch normalization. This is quite unconventional — Did you ever use scale augmentation? What is the effect of training on one scale and then testing on another?"}
{"id": "iclr2020_979", "title": "Combining graph and sequence information to learn protein representations | OpenReview", "abstract": "Abstract:###Computational methods that infer the function of proteins are key to understanding life at the molecular level. In recent years, representation learning has emerged as a powerful paradigm to discover new patterns among entities as varied as images, words, speech, molecules. In typical representation learning, there is only one source of data or one level of abstraction at which the learned representation occurs. However, proteins can be described by their primary, secondary, tertiary, and quaternary structure or even as nodes in protein-protein interaction networks. Given that protein function is an emergent property of all these levels of interactions in this work, we learn joint representations from both amino acid sequence and multilayer networks representing tissue-specific protein-protein interactions. Using these representations, we train machine learning models that outperform existing methods on the task of tissue-specific protein function prediction on 10 out of 13 tissues. Furthermore, we outperform existing methods by 19% on average.", "review": "Review:###This work tries to predict the protein functional activation on a tissue by combining the information from amino acid sequence, and tissue-specific protein-protein interaction network. The authors claim that with this joint representation, their model outperforms current methods (Omhnet) on 10 out of 13 tissues by a larger margin(19% on average). Notations: The notations in experiment is a little bit confusing. In Table 1, the authors refer to different representations with Ohmnet128, Ohmnet64, Ohmnet-Unirep, etc. However, these are not consistent to the ones introduced in Section 4.1: Ohmnet, Ohmnet64-Unirep64, etc. And *0-pad* is introduced in section 3.3 while they denote one method as *Ohmnet64-0Padded* in section 4.1. It would be difficult for the reader to infer the meaning of these abbreviations. Method: --amino acid sequence representation: It would be better to report the explained variance when using Principle Component Analysis (PCA) to project the 1024-dimensional output vector of SeqVec to 64 dimensional space. And the authors can show us more results of different projected dimensions (with different explained variance of the PCA). Experiments: --model: Maybe the authors can provide us more information about the model they use. For classification, what exactly the linear model is? For learning representation, is there any modification of the structure and hyperparameter of UniRef, SeqVec and OhmNet? And is there any regularization? Showing training details like batch size, epochs would be helpful, too. --data: It would be better to show the details of the data this paper uses, like what the data looks like, what is the size, the distribution, and the pre-processing. What*s more, since validation set is used for tuning, it would be better to report the results on test set. --result: In the second paragraph of Section 4.1, it would be more clear to use a table instead of words to show the results. What*s more, what*s exactly the 13 tissues this paper is using? Why they are chosen? Exactly what is the AUROC of each protein in each tissue? What the learning curves look like? Another big issue is, what *current methods* is this paper comparing its result with? It seems like the authors are comparing their implementation of Ohmnet-SeqVec + linear model with Ohmnet + linear model, and report that the former one is of 19% higher AUROC than the latter. But how about the results of other models/methods on the same task in the literature. Is there anyone using similar joint representation and what is their results? --conclusion: Since the proposed methods only achieve best results in 10 out of 13 tissues, it is improper to claim *… we make consistently better tissue-specific function predictions in 13 complex tissues …*. In conclusion, I find this is an interesting paper, that the authors tries to combine amino acid sequence representation and tissue information to predict the activation of protein on specific tissue. However, the authors should perform more rigorous experiment, and show us more implementation details. What*s more, comparing results with the start-of-art methods on the same task setting is important, too."}
{"id": "iclr2020_980", "title": "InfoCNF: Efficient Conditional Continuous Normalizing Flow Using Adaptive Solvers | OpenReview", "abstract": "Abstract:###Continuous Normalizing Flows (CNFs) have emerged as promising deep generative models for a wide range of tasks thanks to their invertibility and exact likelihood estimation. However, conditioning CNFs on signals of interest for conditional image generation and downstream predictive tasks is inefficient due to the high-dimensional latent code generated by the model, which needs to be of the same size as the input data. In this paper, we propose InfoCNF, an efficient conditional CNF that partitions the latent space into a class-specific supervised code and an unsupervised code that shared among all classes for efficient use of labeled information. Since the partitioning strategy (slightly) increases the number of function evaluations (NFEs), InfoCNF also employs gating networks to learn the error tolerances of its ordinary differential equation (ODE) solvers for better speed and performance. We show empirically that InfoCNF improves the test accuracy over the baseline while yielding comparable likelihood scores and reducing the NFEs on CIFAR10. Furthermore, applying the same partitioning strategy in InfoCNF on time-series data helps improve extrapolation performance.", "review": "Review:###This paper proposed a conditional CNF based on a similar intuition of the InfoGAN that partitions the latent space into a class-specific supervised code and an unsupervised code shared among all classes. To improve speed, the paper further proposed to employ gating networks to learn the error tolerance of its ODE solver. The experiments are performed on the CIFAR-10 dataset and synthetic time-series data. The paper has addressed an important issue of investigating efficient conditional CNF. The general idea of the paper is clear, but I found certain parts can be improved, such as the formulation of InfoCNF. It seems the authors assume readers know InfoGAN well enough, which might not be the case. My main concern is the limited evaluation as all the experiments are performed on the CIFAR-10 and synthetic data. Since the paper address efficient conditional CNF, it would make the claim much stronger if more experiments could be performed on larger images: if not the original imagenet, maybe imagenet-64 or imagenet-128 or image benchmarks with higher resolutions. Why does InfoCNF achieve slightly worse NLL in small batch training, while it outperforms CCNF in all the other metrics? Do you have any explanations?"}
{"id": "iclr2020_981", "title": "Self-Supervised Speech Recognition via Local Prior Matching | OpenReview", "abstract": "Abstract:###We propose local prior matching (LPM), a self-supervised objective for speech recognition. The LPM objective leverages a strong language model to provide learning signal given unlabeled speech. Since LPM uses a language model, it can take advantage of vast quantities of both unpaired text and speech. The loss is theoretically well-motivated and simple to implement. More importantly, LPM is effective. Starting from a model trained on 100 hours of labeled speech, with an additional 360 hours of unlabeled data LPM reduces the WER by 26% and 31% relative on a clean and noisy test set, respectively. This bridges the gap by 54% and 73% WER on the two test sets relative to a fully supervised model on the same 360 hours with labels. By augmenting LPM with an additional 500 hours of noisy data, we further improve the WER on the noisy test set by 15% relative. Furthermore, we perform extensive ablative studies to show the importance of various configurations of our self-supervised approach.", "review": " This work proposed a distillation approach which use ASRs to generate hypotheses for unsupervised data, run a LM to get probability for the hypothesis, and perform distillation with the resulting probability. The ASRs being used for generating hypotheses can be either a model trained with the supervised data or the student model, and can switch between the two during training. In the experiments, ASR models are pre-trained with the subset of Librispeech data and use the rest of Librispeech data as unsupervised data, and the LM is trained with Librispeech LM data. The experiments shown the proposed approach improve baseline model trained with the Librispeech subset significantly. The use of LM to provide soft target is a good idea as LMs can utilize unsupervised text data as opposed to the requirement of training a strong teacher model with paired data, and can be easily integrated with existing distillation approaches for ASRs. The switching to the student model for generating hypotheses when it outperforms the pre-trained ASR also makes a good sense. The overall novelty however is a bit limited compared to the existing work, as the major contribution is to propose to use LMs as teacher rather than ASRs, with the rest of the design to be similar to existing works. The paper relates their method to self-supervised learning, yet I find it having stronger correlation with existing distillation approaches, and can be better understood through the distillation perspective."}
{"id": "iclr2020_982", "title": "On the Global Convergence of Training Deep Linear ResNets | OpenReview", "abstract": "Abstract:###We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training -hidden-layer linear residual networks (ResNets). We prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden weights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. Compared with the global convergence result of GD for training standard deep linear networks citep{du2019width}, our condition on the neural network width is sharper by a factor of , where denotes the condition number of the covariance matrix of the training data. In addition, for the first time we establish the global convergence of SGD for training deep linear ResNets and prove a linear convergence rate when the global minimum is .", "review": "Review:###*Summary* This paper deals with the global convergence of deep linear ResNets. The author show that under some initialization conditions for the first and the last layer (that are not optimized !) GD and SGD does converge to a global minimum of the min squared error. The closed related work seems to be Bartlett et al. 2019 that study the convergence of GD in the case of linear networks. *Decision* On issue for Bartlett et al. 2019 was that they required a condition on the initial suboptimality to be small in order to insure convergence. This work shows that in the case of linear ResNets, with a well chosen initialization, a similar condition holds with high probability. I think this paper is interesting for the ICLR community and seems to provide good contributions (like for instance the analysis for SGD). However I have some question that I would like the authors to answer. *Questions* - In Proposition 3.3 you show an upperbound on in order to show in Corollary 3.4 that the condition to apply Theorem 3.1 is true. However, it seems to me that you need a lower bound on to prove that (A.3) is true. - To what extent the proof of Theorem 3.1 uses the proof technique of Bartlett et al. 2019 ? - With you small enough conditions, are you in the lazy regime described by Chizat, Lenaic, Edouard Oyallon, and Francis Bach. *On Lazy Training in Differentiable Programming.* (2019). NeurIPS - In Theorem 3.1 What is ? - Could you prove the same result as Theorem 3.1 and 3.6 but with an inequality constraint on the step size? It seems very restrictive to me to ask a stepsize to be exactly equal to a quantity. If you cannot relax you equality constraint into an inequality constraint, can you at least show that your result hold for step size in an interval? === After rebuttal === Thank you for this detailed answer. It confirms that this paper is of interest to the ICLR community."}
{"id": "iclr2020_983", "title": "Anomalous Pattern Detection in Activations and Reconstruction Error of Autoencoders | OpenReview", "abstract": "Abstract:###In real-world machine learning applications, large outliers and pervasive noise are commonplace, and access to clean training data as required by standard deep autoencoders is unlikely. Reliably detecting anomalies in a given set of images is a task of high practical relevance for visual quality inspection, surveillance, or medical image analysis. Autoencoder neural networks learn to reconstruct normal images, and hence can classify those images as anomalous if the reconstruction error exceeds some threshold. In this paper, we proposed an unsupervised method based on subset scanning over autoencoder activations. The contributions of our work are threefold. First, we propose a novel method combining detection with reconstruction error and subset scanning scores to improve the anomaly score of current autoencoders without requiring any retraining. Second, we provide the ability to inspect and visualize the set of anomalous nodes in the reconstruction error space that make a sample noised. Third, we show that subset scanning can be used for anomaly detection in the inner layers of the autoencoder. We provide detection power results for several untargeted adversarial noise models under standard datasets.", "review": "Review:###* Summary * The paper proposes a new unsupervised anomaly detection method using the discrepancy between the activations of a deep convolutional autoencoder (AE) for clean (in-distribution) samples and noisy (out-distribution) samples. The authors propose a Subset scanning method which searches for a subset of node activations that are higher than expected, in order to classify new inputs are anomalous or clean. Furthermore, the authors propose the ability to visualize the set of anomalous nodes in the AE output error space to provide an explanation for the decision of the anomaly detector. They also extend the idea of AE reconstruction error based anomaly detection to incorporate hidden layers of the AE. The focus is on detecting anomalies generated via adversarial attacks, rather than on out-of-distribution samples coming from different classes. Pros: 1. The paper is well written, and provides sufficient background on the topic. 2. The idea of using a subset scanning approach for activations in the hidden layer is interesting. 3. The motivation and background for the subset scanning approach are well explained. Cons: 1. The experimental section lacks in depth, and tests only a very restricted scenario of anomaly detection. The method is sold as working on *any pre-trained, off-the-shelf autoencoder network*, yet the evaluation is only on very simple datasets MNIST and Fashion-MNIST, which have no background, centered, single-scale images. Evaluations on harder datasets which require more sophisticated autoencoder architectures would have been interesting to showcase the strength of the method. If e.g. geometrical transformations are applied to images (translations, rotations, etc.), I strongly assume that the subset scanning method would have difficulty, because the set of highly activated neurons would most likely differ, especially in early layers. 2. The comparison is only done to two not exactly state-of-the- art methods. Again, if the method can be applied to pre-trained models, it would have been interesting to apply the method to existing state-of-the-art models, or at least multiple variations of autoencoder architectures. 3. The authors focus entirely on adversarially generated samples as anomalies. Although this is an interesting and difficult scenario, this is not properly motivated, and there should also be an evaluation of other anomaly cases, e.g. detection of out-of-distribution data coming from different classes or different datasets. 4. The focus is on detecting adversarial examples from a single attack model, then the performance of the proposed unsupervised method should be compared to supervised detectors of adversarial attacks, e.g. as described in Metzen et al. (2017) *On detecting adversarial attacks* and follow-up papers. Minor issues: 1. Many of the referenced figures are in the appendix, and figures are not presented in the order in which they are cited. 2. There are quite a few typos in the manuscript. 3. Fig. 5 is confusing to understand. Why should we expect the clean images to have anomalous nodes along the contours of the digit? Overall I think the authors propose an interesting idea but not enough convincing evidence that their method is actually improving unsupervised anomaly detection in its general setting. In its current form the paper is not fit for publication at ICLR, but after addressing the weak points and a more thorough experimental evaluation this could become a good paper."}
{"id": "iclr2020_984", "title": "Domain-invariant Learning using Adaptive Filter Decomposition | OpenReview", "abstract": "Abstract:###Domain shifts are frequently encountered in real-world scenarios. In this paper, we consider the problem of domain-invariant deep learning by explicitly modeling domain shifts with only a small amount of domain-specific parameters in a Convolutional Neural Network (CNN). By exploiting the observation that a convolutional filter can be well approximated as a linear combination of a small set of basis elements, we show for the first time, both empirically and theoretically, that domain shifts can be effectively handled by decomposing a regular convolutional layer into a domain-specific basis layer and a domain-shared basis coefficient layer, while both remain convolutional. An input channel will now first convolve spatially only with each respective domain-specific basis to ``absorb* domain variations, and then output channels are linearly combined using common basis coefficients trained to promote shared semantics across domains. We use toy examples, rigorous analysis, and real-world examples to show the framework*s effectiveness in cross-domain performance and domain adaptation. With the proposed architecture, we need only a small set of basis elements to model each additional domain, which brings a negligible amount of additional parameters, typically a few hundred.", "review": " The paper proposes an approach to learning domain invariant representations using the adaptive decomposition of the convolutional filters. The approach is similar to methods that use multi-stream networks (a stream for each domain), but using the filer decomposition scheme, the authors avoid the issue of excessive increase in the number of parameters typical in fully multi-stream architectures. This is achieved by learning a separate basis for convolutional filters for each domain while sharing the basis coefficients across domains. This encourages shared semantics across domains while maintaining a balance between the network expressiveness and the computational complexity. The authors argue that the basis learned for each domain can be understood as correction/alignment mappings to bring together the representations of each domain. A toy example presented is convincing and shows the correction basis learned in a simple synthetic case. Theoretical arguments are provided to show that the proposed correction scheme covers a large range of possible domain shifts. The authors also show that plugging the decomposition scheme into existing CNN based unsupervised domain adaptation algorithms results in consistent improvements across methods and datasets. How does the method compare to approaches that learn to adapt the representations using conditional/adaptive batch norm [1,2]? Overall, the paper was well motivated and easy to read. The methods appear to be a useful addition to tools available for domain invariant learning. [1] Chang, Woong-Gi, Tackgeun You, Seonguk Seo, Suha Kwak, and Bohyung Han. *Domain-Specific Batch Normalization for Unsupervised Domain Adaptation.* In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7354-7362. 2019. [2] Kumar, Abhishek, Prasanna Sattigeri, Kahini Wadhawan, Leonid Karlinsky, Rogerio Feris, Bill Freeman, and Gregory Wornell. *Co-regularized alignment for unsupervised domain adaptation.* In Advances in Neural Information Processing Systems, pp. 9345-9356. 2018."}
{"id": "iclr2020_985", "title": "Adversarial Filters of Dataset Biases | OpenReview", "abstract": "Abstract:###Large-scale benchmark datasets have been among the major driving forces in AI, supporting training of models and measuring their progress. The key assumption is that these benchmarks are realistic approximations of the target tasks in the real world. However, while machine performance on these benchmarks advances rapidly --- often surpassing human performance --- it still struggles on the target tasks in the wild. This raises an important question: whether the surreal high performance on existing benchmarks are inflated due to spurious biases in them, and if so, how we can effectively revise these benchmarks to better simulate more realistic problem distributions in the real world. In this paper, we posit that while the real world problems consist of a great deal of long-tail problems, existing benchmarks are overly populated with a great deal of similar (thus non-tail) problems, which in turn, leads to a major overestimation of true AI performance. To address this challenge, we present a novel framework of Adversarial Filters to investigate model-based reduction of dataset biases. We discuss that the optimum bias reduction via AFOptimum is intractable, thus propose AFLite, an iterative greedy algorithm that adversarially filters out data points to identify a reduced dataset with more realistic problem distributions and considerably less spurious biases. AFLite is lightweight and can in principle be applied to any task and dataset. We apply it to popular benchmarks that are practically solved --- ImageNet and Natural Language Inference (SNLI, MNLI, QNLI) --- and present filtered counterparts as new challenge datasets where the model performance drops considerably (e.g., from 84% to 24% for ImageNet and from 92% to 62% for SNLI), while human performance remains high. An extensive suite of analysis demonstrates that AFLite effectively reduces measurable dataset biases in both the synthetic and real datasets. Finally, we introduce new measures of dataset biases based on K-nearest-neighbors to help guide future research on dataset developments and bias reduction.", "review": "Review:###the paper proposes an algorithm that adversarially filters out examples to reduce dataset-specific spurious bias. the key intuition is that the datasets are curated in a way that easy to obtain samples have higher probability to be admitted to the dataset. however, not all real world samples are easy to obtain. in other words, real world samples may follow a completely different distribution than curated samples with easy-to-obtain ones. the proposed approach discounts the data-rich head of the datasets and emphasizes the data-low tail. they quantify data-rich / data-low by the best possible out-of-sample classification accuracy achievable by models when predicting. then adjust the dataset via the expected out-of-sample classification accuracy. the idea of the paper is interesting and the experiments show a substantial reduction in the performance of existing algorithms. this make the paper a promising proposal."}
{"id": "iclr2020_986", "title": "GUIDEGAN: ATTENTION BASED SPATIAL GUIDANCE FOR IMAGE-TO-IMAGE TRANSLATION | OpenReview", "abstract": "Abstract:###Recently, Generative Adversarial Network (GAN) and numbers of its variants have been widely used to solve the image-to-image translation problem and achieved extraordinary results in both a supervised and unsupervised manner. However, most GAN-based methods suffer from the imbalance problem between the generator and discriminator in practice. Namely, the relative model capacities of the generator and discriminator do not match, leading to mode collapse and/or diminished gradients. To tackle this problem, we propose a GuideGAN based on attention mechanism. More specifically, we arm the discriminator with an attention mechanism so not only it estimates the probability that its input is real, but also does it create an attention map that highlights the critical features for such prediction. This attention map then assists the generator to produce more plausible and realistic images. We extensively evaluate the proposed GuideGAN framework on a number of image transfer tasks. Both qualitative results and quantitative comparison demonstrate the superiority of our proposed approach.", "review": "Review:###This paper introduces a feedback mechanism in the GAN framework which improves the quality of generated images in the context of image-to-image translation. The key contribution is that the discriminator not only predicts the probability of an image being real or fake, but also outputs a map which indicates where the generator should focus in the next iteration in order to make its results more convincing. The paper explores ways of obtaining such a map 1) by summing feature activations of the discriminator on a specific or group of layers 2) by predicting it via augmenting the capacity of the discriminator. After such a map is obtained, it is concatenated with the input image and fed iteratively to the generator. The proposed setting have been tested on the setting of supervised and unsupervised image translation on 4 datasets. Quantitative experiments show that the proposed approach improves over other baselines. I think this paper introduces an interesting and important new GAN framework. However, I feel that the paper requires a major revision strengthening the experiments, before it can be reconsidered for ICLR: More qualitative results showing comparisons with other algorithms should be shown for Day-Night, Apple-Orange, Horse-Zebra. The only comparison available in the paper is on the well constrained problem of segmentation maps to images. More quantitative experiments should be provided for other datasets (perhaps using FID and KID). It is not entirely clear if the produced results are better than cycleGAN’s (my subjective analysis is that the results of cycleGAM look better on Fig. 2). The paper is closely related to [Huh et al: Feedback Adversarial Learning: Spatial Feedback for Improving Generative Adversarial Networks] in that they share the idea of a feedback mechanism from the discriminator. It hence seems reasonable to compare with this approach. I was struggling to understand precisely how the StarGAN results were obtained on CityScapes: As a multi-modal image-to-image translation model, StarGan takes as input, an image and a binary vector pointing to which modality to transform the image into. In the case of CityScape, there is no such multi-modality (at least none that is provided as ground truths, via for example, a binary vector). More details on this process would make the experimental section clearer. Table 4.1 is often used however such a Table does not exist (probably Table 1,2 was meant here)."}
{"id": "iclr2020_987", "title": "Granger Causal Structure Reconstruction from Heterogeneous Multivariate Time Series | OpenReview", "abstract": "Abstract:###Granger causal structure reconstruction is an emerging topic that can uncover causal relationship behind multivariate time series data. In many real-world systems, it is common to encounter a large amount of multivariate time series data collected from heterogeneous individuals with sharing commonalities, however there are ongoing concerns regarding its applicability in such large scale complex scenarios, presenting both challenges and opportunities for Granger causal reconstruction. To bridge this gap, we propose a Granger cAusal StructurE Reconstruction (GASER) framework for inductive Granger causality learning and common causal structure detection on heterogeneous multivariate time series. In particular, we address the problem through a novel attention mechanism, called prototypical Granger causal attention. Extensive experiments, as well as an online A/B test on an E-commercial advertising platform, demonstrate the superior performances of GASER.", "review": "Review:###The paper proposes a novel way of reconstructing Granger causal structures using a differentiable neural network architecture that contains attention modules that are proportional to the Granger causality of the input layers. Furthermore, the architecture blends individual-specific induced causal structures and cross-population prototypical causal structures. The paper has an extensive experimental section on which the proposed method shows impressive improvements in causal discovery performance and predictive performance on par with state-of-the-art. As main contributions the paper: * proposes a novel architecture * shows its values using extensive experiments Overall, I find the paper well-written with a clear description of the proposed architecture and clear experiments showing the importance of the architectural choices. A possible downside is the relative lack of novelty, since the method seems like a reasonable extension of the existing work. However, I think this counterbalanced by the excellent results on the causal discovery task and the extensive nature of the experiments. In terms of suggestions, I think an illustrative example of the granger attention on an artificial / toy example would help a lot to give an intuitive understanding on how the method works and how the causal structure is being built."}
{"id": "iclr2020_988", "title": "WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia | OpenReview", "abstract": "Abstract:###We present an approach based on multilingual sentence embeddings to automatically extract parallel sentences from the content of Wikipedia articles in 85 languages, including several dialects or low-resource languages. We do not limit the extraction process to alignments with English, but systematically consider all possible language pairs. In total, we are able to extract 135M parallel sentences for 1620 different language pairs, out of which only 34M are aligned with English. This corpus of parallel sentences is freely available (URL anonymized) To get an indication on the quality of the extracted bitexts, we train neural MT baseline systems on the mined data only for 1886 languages pairs, and evaluate them on the TED corpus, achieving strong BLEU scores for many language pairs. The WikiMatrix bitexts seem to be particularly interesting to train MT systems between distant languages without the need to pivot through English.", "review": "Review:###The paper creates a large dataset for machine translation, called WikiMatrix, that contains 135M parallel sentences in 1620 language pairs from Wikipedia. The paired sentences from different languages are mined based on the sentence embeddings. Training NMT systems based on the mined dataset, and comparing with those trained based on existing dataset, the authors claim that the quality of the dataset is good. The effect of thresholding values of similarity scores for selecting parallel sentences is studied. Since the data is huge, dimension reduction and data compression techniques are used for efficient mining. The study is the first one that systematically mine for parallel sentences of Wikipedia for a large number of languages. The results are solid and the dataset is valuable for research in multilinguality."}
{"id": "iclr2020_989", "title": "Hardware-aware One-Shot Neural Architecture Search in Coordinate Ascent Framework | OpenReview", "abstract": "Abstract:###Designing accurate and efficient convolutional neural architectures for vast amount of hardware is challenging because hardware designs are complex and diverse. This paper addresses the hardware diversity challenge in Neural Architecture Search (NAS). Unlike previous approaches that apply search algorithms on a small, human-designed search space without considering hardware diversity, we propose HURRICANE that explores the automatic hardware-aware search over a much larger search space and a multistep search scheme in coordinate ascent framework, to generate tailored models for different types of hardware. Extensive experiments on ImageNet show that our algorithm consistently achieves a much lower inference latency with a similar or better accuracy than state-of-the-art NAS methods on three types of hardware. Remarkably, HURRICANE achieves a 76.63% top-1 accuracy on ImageNet with a inference latency of only 16.5 ms for DSP, which is a 3.4% higher accuracy and a 6.35x inference speedup than FBNet-iPhoneX. For VPU, HURRICANE achieves a 0.53% higher top-1 accuracy than Proxyless-mobile with a 1.49x speedup. Even for well-studied mobile CPU, HURRICANE achieves a 1.63% higher top-1 accuracy than FBNet-iPhoneX with a comparable inference latency. HURRICANE also reduces the training time by 54.7% on average compared to SinglePath-Oneshot.", "review": "Review:###This paper proposes a hardware-aware neural architecture search (NAS) approach. The main idea is to customize the search space with hardware profiling information, and then iteratively search for the top half and bottom half network structure using coordinate ascent. Overall, the hardware-related analysis is quite inspiring and the results look promising. Pros: - Well-motivated and well-written paper. - Hardware profiling information in Figure 1 and Table 2 are quite interesting. - Results look promising. In particular, the DSP model speedup than FBNet/ProxylessNAS/Oneshot-NAS is quite impressive. Cons: My main concern is that it couples many things together (new search space, new coordinate ascent, and new search algorithm), and it is not clear where the performance gain comes from. More ablation studies would be helpful: - The new search space includes a lot more options than other NAS works presented in Table 2. For example, none of FBNet/ProxylessNAS/SinglePath uses SE. For fair comparison, the authors should consider comparing to other NAS works with SE included (such as MnasNet/MobileNetV3). - On top of the new search space, the authors propose a new search algorithms with two optimizations: op-level search space reduction and layer-level coordinate ascent. It is not clear how important is each optimization. I also have a few minor questions to the authors: - The latency numbers on Table 2 seem to be much lower than expected: for example, FBNet-S8 reports 19.84ms on iPhone CPU and 23.33ms on Samsung S8 CPU in the original paper, but in Table 2, this paper reports >300ms on CPU and >100ms on DSP. Do you have any idea what could be the potential reasons? - It is great to see HURRICANE(DSP) model is almost 10x faster than FBNet/ProxylessNAS/SinglePath models. Could you add more discussion about the reasons? This would be helpful to guide future DSP model design."}
{"id": "iclr2020_990", "title": "Three-Head Neural Network Architecture for AlphaZero Learning | OpenReview", "abstract": "Abstract:###The search-based reinforcement learning algorithm AlphaZero has been used as a general method for mastering two-player games Go, chess and Shogi. One crucial ingredient in AlphaZero (and its predecessor AlphaGo Zero) is the two-head network architecture that outputs two estimates --- policy and value --- for one input game state. The merit of such an architecture is that letting policy and value learning share the same representation substantially improved generalization of the neural net. A three-head network architecture has been recently proposed that can learn a third action-value head on a fixed dataset the same as for two-head net. Also, using the action-value head in Monte Carlo tree search (MCTS) improved the search efficiency. However, effectiveness of the three-head network has not been investigated in an AlphaZero style learning paradigm. In this paper, using the game of Hex as a test domain, we conduct an empirical study of the three-head network architecture in AlpahZero learning. We show that the architecture is also advantageous at the zero-style iterative learning. Specifically, we find that three-head network can induce the following benefits: (1) learning can become faster as search takes advantage of the additional action-value head; (2) better prediction results than two-head architecture can be achieved when using additional action-value learning as an auxiliary task.", "review": "Review:###This paper applies the three-head neural network architecture as well as the corresponding training loss proposed in (Gao et al., 2018b) to alphazero style learning of the Hex game. The paper is mainly an empirical study, and shows that the architecture leads to faster and better learning results for Hex. The evaluation is done on two datasets, one with examples from near-optimal players produced by MoHex 2.0, and the other from randomly sampled but perfectly labelled examples generated by benzene. Performance improvement is evaluated from several different perspectives, including state-value errors, action-value errors and policy prediction accuracies. Finally, the match performance is also reported for competing with MoHex 2.0, one of the state-of-the-art agent for Hex. Generally speaking, the paper does a good job in introducing and analyzing the structure of the alphazero learning scheme and the related alphago and alphago zero schemes, and the experiments within the scope of Hex is relatively thorough and the performance improvement is consistent and convincing. However, the description of the three-head neural network in Section 3 is too brief, and without looking at the original paper (Gao et al., 2018b), it is quite hard to understand the motivation of the objectives (especially the definitions and explanations of R1, R2 and R3). Additionally, the challenge of applying three-head neural network architecture in the alphazero learning setting is almost not mentioned. In particular, what are the modifications needed compared to the original work (Gao et al., 2018b)? The authors may want to explain clearly how the training scheme is different, and clearly state what the detailed neural network architecture (at least in the appendix) used is, and how they are different from the original alphazero paper and (Gao et al., 2018b). Without these explanations, the significance of the paper would be largely limited to coding and engineering efforts (which are also valuable but not that much in the research sense). Another related issue of this paper is that it is not clear (at least to me, who know little about the Hex game) how difficult it is to tackle Hex (compared to Go, Shogi and chess, etc.). The authors may want to elaborate more on this as well to further showcase the significance of the work. Finally, there are also some inconsistency in the hyper-parameter choices and architecture design. In particular, it is not clear why the authors choose the expansion threshold to 0 in the match performance part, whereas the authors use threshold 10 elsewhere. The turning on and off of the data augmentation in 3HNN in different experiments mentioned in the appendix are also not well explained. Nevertheless, I still value the paper*s effort and success in applying a newly proposed approach for a relatively challenging real-world game problem, despite the issues about experimental design and writing mentioned above. Some minor suggestions: the title of the rightmost plots should better be *perfectly labelled examples* instead of *perfect examples*, and the authors may want to make it clearer which plot corresponds to dataset T1 and which corresponds to T2."}
{"id": "iclr2020_991", "title": "MaskConvNet: Training Efficient ConvNets from Scratch via Budget-constrained Filter Pruning | OpenReview", "abstract": "Abstract:###In this paper, we propose a framework, called MaskConvNet, for ConvNets filter pruning. MaskConvNet provides elegant support for training budget-aware pruned networks from scratch, by adding a simple mask module to a ConvNet architecture. MaskConvNet enjoys several advantages - (1) Flexible, the mask module can be integrated with any ConvNets in a plug-and-play manner. (2) Simple, the mask module is implemented by a hard Sigmoid function with a small number of trainable mask variables, adding negligible memory and computational overheads to the networks during training. (3) Effective, it is able to achieve competitive pruning rate while maintaining comparable accuracy with the baseline ConvNets without pruning, regardless of the datasets and ConvNet architectures used. (4) Fast, it is observed that the number of training epochs required by MaskConvNet is close to training a baseline without pruning. (5) Budget-aware, with a sparsity budget on target metric (e.g. model size and FLOP), MaskConvNet is able to train in a way that the optimizer can adaptively sparsify the network and automatically maintain sparsity level, till the pruned network produces good accuracy and fulfill the budget constraint simultaneously. Results on CIFAR-10 and ImageNet with several ConvNet architectures show that MaskConvNet works competitively well compared to previous pruning methods, with budget-constraint well respected. Code is available at https://www.dropbox.com/s/c4zi3n7h1bexl12/maskconv-iclr-code.zip?dl=0. We hope MaskConvNet, as a simple and general pruning framework, can address the gaps in existing literate and advance future studies to push the boundaries of neural network pruning.", "review": "Review:###In this work, the authors propose a network pruning method to learn a pruned network during training. Specifically, they add a pruning mask for each layer and induce a sparisity loss on the mask variables during training. The pruned network is obtained by applying the learned mask to the networks. The paper seems to be well contained. However, my assessment of this paper is weak reject. I am mainly concerned with the novelty of this method. Also i think some more evaluation is needed to fully understand the effectiveness of this method. My questions are summarized as follows: Q1: In the methods part, the authors said that “Previous pruning approaches often prune Conv filters with their successive Batch Normalization layer unchanged.” Can the authors give some reference here as to which pruning approaches? Q2: Did the authors compare the proposed approach to training the pruned networks from scratch as done in [1]? Also can the authors analyze the sparsity patterns of the pruned networks as done in section G in the appendix of [1]? Q3: What is the difference of your approach to [2]? They seem to be very similar. I think it is necessary to add some discussion in the related work. Is there any experimental results for comparison with [2]? [1] Rethinking the Value of Network Pruning. Liu et al. ICLR 2019 [2] AutoPruner: An End-to-End Trainable Filter Pruning Method for Efficient Deep Model Inference. Luo et al. Arxiv, 2018."}
{"id": "iclr2020_992", "title": "Dynamically Pruned Message Passing Networks for Large-scale Knowledge Graph Reasoning | OpenReview", "abstract": "Abstract:###We propose Dynamically Pruned Message Passing Networks (DPMPN) for large-scale knowledge graph reasoning. In contrast to existing models, embedding-based or path-based, we learn an input-dependent subgraph to explicitly model a sequential reasoning process. Each subgraph is dynamically constructed, expanding itself selectively under a flow-style attention mechanism. In this way, we can not only construct graphical explanations to interpret prediction, but also prune message passing in Graph Neural Networks (GNNs) to scale with the size of graphs. We take the inspiration from the consciousness prior proposed by Bengio to design a two-GNN framework to encode global input-invariant graph-structured representation and learn local input-dependent one coordinated by an attention module. Experiments show the reasoning capability in our model that is providing a clear graphical explanation as well as predicting results accurately, outperforming most state-of-the-art methods in knowledge base completion tasks.", "review": "Review:###The authors propose to use sampling methods in order to apply graph neural networks to large scale knowledge graphs for semantic reasoning. To this end induced subgraphs are constructed in a data-dependent way using an attention mechanism. This improves the efficiency and leads to interpretable results. The experiments show some improvements over path- and embedding-based methods. The paper is partially difficult to read and not well structured (see minor comments below). Overall, I think that the proposed GNN architecture is an original and interesting approach for this specific application. The experimental evaluation presented in the Section 4 shows clear improvements. This, however, is not true for the results presented in the appendix (Table 4). I am missing a discussion of the limitations of the proposed approach. Moreover, a thorough discussion of the hyper-parameter selection and, if possible, theoretical justification would be highly desirable and could strengthen the paper. Minor comments: - Section 2 start with the paragraph *Notation*, but does not contain any other paragraph. - The sampling strategy should not be introduced as part of the section *problem formulation*. - using standard terms from graph theory for well-known concepts (such as *induced subgraph*) would improve the readability ---------------------------- Update after the rebuttal: The authors have addressed several of my concerns and improved the manuscript. I have raised my score from *3: Weak Reject* to *6: Weak Accept*."}
{"id": "iclr2020_993", "title": "Generative Restricted Kernel Machines | OpenReview", "abstract": "Abstract:###We introduce a novel framework for generative models based on Restricted Kernel Machines (RKMs) with multi-view generation and uncorrelated feature learning capabilities, called Gen-RKM. To incorporate multi-view generation, this mechanism uses a shared representation of data from various views. The mechanism is flexible to incorporate both kernel-based, (deep) neural network and convolutional based models within the same setting. To update the parameters of the network, we propose a novel training procedure which jointly learns the features and shared representation. Experiments demonstrate the potential of the framework through qualitative evaluation of generated samples.", "review": "Review:###This is a very good paper, building on the idea of Restricted Kernel Machines (drawing a nice parallel between Restricted Bolzman Machines and tools available in the Kernel modelling literature). In this manuscript, the author(s) extend the work to a generative model setting to achieve multi-view generation -- a generative model that can explain correlated variables from a common subspace. The manuscript is well-written and easy to follow and the algorithmic details are clear. Image generation is illustrated on standard datasets (MNIST / CIFAR / CelebA). While the framework and learning algorithm are good, and novel extensions to what appears to be previous work of the authors, I am less persuaded by the empirical work. Latent variable-based generative modes such as this (and this is motivated in the introduction to the paper) should be judged on if they can extract anything useful about the problem domain in the latent representations that we can interpret. This is not the case here -- the results presented are examples of images that the models can generate. No critical appraisal is given about when the models might fail or when one ought to resort to this approach and not a sample from the plethora of variants of VAE we read about. What have we learnt about images / hand-written characters / faces of popular people from a study like this? From the above empirical results point of view, I do not think this manuscript is ready for publication, despite what I see as the elegance of the framework."}
{"id": "iclr2020_994", "title": "Gradient Perturbation is Underrated for Differentially Private Convex Optimization | OpenReview", "abstract": "Abstract:###Gradient perturbation, widely used for differentially private optimization, injects noise at every iterative update to guarantee differential privacy. Previous work first determines the noise level that can satisfy the privacy requirement and then analyzes the utility of noisy gradient updates as in non-private case. In this paper, we explore how the privacy noise affects the optimization property. We show that for differentially private convex optimization, the utility guarantee of both DP-GD and DP-SGD is determined by an emph{expected curvature} rather than the minimum curvature. The emph{expected curvature} represents the average curvature over the optimization path, which is usually much larger than the minimum curvature and hence can help us achieve a significantly improved utility guarantee. By using the emph{expected curvature}, our theory justifies the advantage of gradient perturbation over other perturbation methods and closes the gap between theory and practice. Extensive experiments on real world datasets corroborate our theoretical findings.", "review": "Review:###Differential privacy (DP) can be achieved by perturbing the objective function, the output or the gradient. In this paper, the authors consider gradient perturbation and claim that it is more advantageous than other methods. To prove this claim, they present a novel utility analysis by taking the noise into account. The previous papers (like Bassily et. al) present utility guarantees in DP context, but their analysis follow the same steps used for non-noisy setting. In non-noisy setting, the analysis is based on strong convexity parameter mu which is the minimum curvature. However, in this study they present ”expected curvature” which is computed by considering the noise variance and based on averaging the curvatures along the number of iterations. The order of utility is given for both convex and strongly convex objectives and it has become smaller than the previous studies. Since other perturbation methods does not add noise at intermediate steps, expected curvature is the same with mu and the utility advantage is not valid. Comments (Positive) - The paper is well-written and easy to follow. I didn’t see typos or mistakes (I didn’t check the last proof). - They claim that they are the first study showing the advantage of gradient perturbation theoretically (I haven’t seen such a study either). - Since they remove the dependency to minimum curvature mu, they present utility order for both convex and strongly convex objective for DP-GD and DP-SGD. - With the help of privacy noise, they obtain a better utility which is an interesting contribution. Comments (Negative) - In the numerical experiments, number of iterations are taken as 20, 200 and 800. It might be checked for more iterations. The chosen privacy levels are tight enough (0.1 - 1). - The learning rate of DP-SGD is divided by 2 at the middle of training. The reason and whether it is applied to other SGD method is not clear. Overall, this type of utility analysis exists in the DP literature, but their novelty comes from the idea of averaging the curvature."}
{"id": "iclr2020_995", "title": "A Bayes-Optimal View on Adversarial Examples | OpenReview", "abstract": "Abstract:###Adversarial attacks on CNN classifiers can make an imperceptible change to an input image and alter the classification result. The source of these failures is still poorly understood, and many explanations invoke the *unreasonably linear extrapolation* used by CNNs along with the geometry of high dimensions. In this paper we show that similar attacks can be used against the Bayes-Optimal classifier for certain class distributions, while for others the optimal classifier is robust to such attacks. We present analytical results showing conditions on the data distribution under which all points can be made arbitrarily close to the optimal decision boundary and show that this can happen even when the classes are easy to separate, when the ideal classifier has a smooth decision surface and when the data lies in low dimensions. We introduce new datasets of realistic images of faces and digits where the Bayes-Optimal classifier can be calculated efficiently and show that for some of these datasets the optimal classifier is robust and for others it is vulnerable to adversarial examples. In systematic experiments with many such datasets, we find that standard CNN training consistently finds a vulnerable classifier even when the optimal classifier is robust while large-margin methods often find a robust classifier with the exact same training data. Our results suggest that adversarial vulnerability is not an unavoidable consequence of machine learning in high dimensions, and may often be a result of suboptimal training methods used in current practice.", "review": " The paper analyzed the adversarial examples from the Bayes-optimal view. Specifically, the authors analyzed the relationship between the symmetry of covariance of data distribution and the amount of data which are close to the decision boundary. The authors proved that when the covariance of data distribution is asymmetric, a large amount of data will be close to the decision boundary (easy to be attacked). The authors also provided the new datasets which is easy to compute for the bayes-optimal classifier so as to verify the effect of symmetry of covariance on vulnerability of classifier. Moreover, the paper indicated that the vulnerability of CNNs is due to asymmetric distributions or non-optimal learning. It is interesting that the paper investigated the adversarial examples from the Bayes-optimal view. However, there are some drawbacks: 1. The motivation of this paper is not clear to me. In other words, what is the benefit of analyzing the adversarial examples from the Bayes-optimal viewpoint, since Bayes model mentioned in this paper is easy to attack. I am not fully convinced by the presentation of the paper. 2. The theorem or the observation in the paper appears too straightforward. And the ‘observation 1’is not general. The authors may need to consider more general cases that when the standard deviation of eigen value of covariance matrix is large, the Bayes model will be easily attacked. (not just the case that one of eigen value is zero). 3. One minor point, it appears somewhat strange that “observations” were proved. It is better to change observations to theorems or lemmas. 4. The authors tried to explain directly the vulnerability of CNN in a same way. However, CNN is a totally different model compared with the Bayes model (one is a discriminative model and the other is a generative model). For generative models, the classification boundary is closely related to all training samples. Therefore, the variance of data distribution is important for attack. For discriminative models, the decision boundary is related to local information. It may not be proper to analyze CNN in the way same as the Bayes model. This should be further clarified and discussed."}
{"id": "iclr2020_996", "title": "Deep geometric matrix completion: Are we doing it right? | OpenReview", "abstract": "Abstract:###We address the problem of reconstructing a matrix from a subset of its entries. Current methods, branded as geometric matrix completion, augment classical rank regularization techniques by incorporating geometric information into the solution. This information is usually provided as graphs encoding relations between rows/columns. In this work we propose a simple spectral approach for solving the matrix completion problem, via the framework of functional maps. We introduce the zoomout loss, a multiresolution spectral geometric loss inspired by recent advances in shape correspondence, whose minimization leads to state-of-the-art results on various recommender systems datasets. Surprisingly, for some datasets we were able to achieve comparable results even without incorporating geometric information. This puts into question both the quality of such information and current methods* ability to use it in a meaningful and efficient way.", "review": "Review:###This paper proposes a novel approach for the loss function of matrix completion when geometric information is available. The proposed method consists of two ideas: (1) spectral regularization (i.e., Dirichlet energy) with a re-parameterizing basis and (2) multiresolution of spectral loss (i.e., zoomout loss). In addition, the zoomout loss is motivated by the approach for shape correspondence and can be a generalization of the recent matrix completion method (deep matrix factorization). Empirical results show the best performance compared to other recent methods under small-scale datasets. Moreover, the proposed method outperforms when the geometric model is accurate (verified on the synthetic setting) and this can reflect that the proposed method is a good choice when the graph structures are given. This work can be a significant contribution as it is a simple linear model but practically performs better than other deep nonlinear networks (e.g., RGCNN). Additionally, the proposed loss functions utilize only the spectral information of graph structure with novel approaches. However, there are some drawbacks to this work. First, it requires a good quality of geometric model which is hard to obtain in practical datasets. Second, the proposed method has a scalability issue since it requires eigendecompositions of graph Laplacians (as discussed in the paper). This can be a problem for real and large-scale datasets. Overall, this paper presents a novel approach utilizing graph spectral information with empirical improvements. But, I vote for weak acceptance due to its drawbacks as mentioned above. Main concerns: 1. It is not clear why minimizing Dirichlet energy can improve the performance of matrix completion. In the paper, the authors mention that it promotes smooth functions on the graph nodes, but not fully clear why smooth functions are good. And how much does the accuracy increase (or decrease) when using the Dirichlet regularization? 2. Authors argue that the re-parameterizing of the basis (emerging P and Q) can find a better geometric model (section 2). So, it is expected that the proposed method shows a better result when the given geometric model is not accurate. However, the empirical results are reported poor improvements for inaccurate geometric models. Does this make sense? For experiments: 1. What is the number of trainable parameters for each method? Since the proposed method is overparameterized, it is not clear that the empirical improvements come from the overparameterizing or the proposed loss function. It would be great to report the number of parameters of all other methods by setting similar numbers. 2. It is not clear how to generate the synthetic dataset, i.e., projecting a random matrix on te the first few eigenvectors of L_r and L_c. It would be better to give more details. 3. What are the training times of the proposed method and other competitors? 4. Why results of FM are not reported under other datasets? Minor comments: 1. In page 4, please edit “We explore The” -> “We explore the”. 2. In equation (15), writing “odot S” twice seems to be unnecessary."}
{"id": "iclr2020_997", "title": "Constant Curvature Graph Convolutional Networks | OpenReview", "abstract": "Abstract:###Interest has been rising lately towards methods representing data in non-Euclidean spaces, e.g. hyperbolic or spherical. These geometries provide specific inductive biases useful for certain real-world data properties, e.g. scale-free or hierarchical graphs are best embedded in a hyperbolic space. However, the very popular class of graph neural networks is currently limited to model data only via Euclidean node embeddings and associated vector space operations. In this work, we bridge this gap by proposing mathematically grounded generalizations of graph convolutional networks (GCN) to (products of) constant curvature spaces. We do this by i) extending the gyro-vector space theory from hyperbolic to spherical spaces, providing a unified and smooth view of the two geometries, ii) leveraging gyro-barycentric coordinates that generalize the classic Euclidean concept of the center of mass. Our class of models gives strict generalizations in the sense that they recover their Euclidean counterparts when the curvature goes to zero from either side. Empirically, our methods outperform different types of classic Euclidean GCNs in the tasks of node classification and minimizing distortion for symbolic data exhibiting non-Euclidean behavior, according to their discrete curvature.", "review": "Review:###In this paper, the authors address representation learning in non-Euclidean spaces. The authors are motivated by constant curvature geometries, that can provide a useful trade-off between Euclidean representations and Riemannian manifolds, i.e. arriving at more suitable representations than possible in the Euclidean space, while not sacrifising closed-form formulae for estimating distances, gradients and so on. The authors point out that an extension of the gyrovector space formalization to spaces of constant positive curvature (spherical) is required, and with the corresponding formalization for hyperbolic spaces, one can arrive at a unified formalism that can interpolate smoothly between all geometries of constant curvature. The authors propose to do so by replacing the curvature while flipping the sign in the standard Poincare model. This is a strong point reagarding this work, as it seems that no such unification has been attempted in the past (although simply replacing the curvature in the Poincare model seems a bit too straightforward to not have been attempted, it seems to be the case). The authors also provide extensive supplementary material (around 20 pages) with detailed derivations and descriptions of experiments. This also makes me wonder if this paper is more suitable for a journal - both in terms of the extensive supplementary material (e.g., curvature sampling algorithm and other details can be found only in supplementary), as well as the more rigorous review process that a journal paper goes through. In the main paper, only proof of concept experiments are provided (one experiment), that nevertheless show competitive performance under varying settings. However, it seems to me that such contributions in the rising field of geometric deep learning, where several challenges are yet to be overcome, can be beneficial for future research. Since in the supplementary experiments also, it seems that curvature does have small variance in the results, how would the authors assess the robustness of the curvature sampling method with respect to the results?"}
{"id": "iclr2020_998", "title": "Adapting Behaviour for Learning Progress | OpenReview", "abstract": "Abstract:###Determining what experience to generate to best facilitate learning (i.e. exploration) is one of the distinguishing features and open challenges in reinforcement learning. The advent of distributed agents that interact with parallel instances of the environment has enabled larger scale and greater flexibility, but has not removed the need to tune or tailor exploration to the task, because the ideal data for the learning algorithm necessarily depends on its process of learning. We propose to dynamically adapt the data generation by using a non-stationary multi-armed bandit to optimize a proxy of the learning progress. The data distribution is controlled via modulating multiple parameters of the policy (such as stochasticity, consistency or optimism) without significant overhead. The adaptation speed of the bandit can be increased by exploiting the factored modulation structure. We demonstrate on a suite of Atari 2600 games how this unified approach produces results comparable to per-task tuning at a fraction of the cost.", "review": "Review:###This paper develops a multi-arm bandit-based algorithm to dynamically adapt the exploration policy for reinforcement learning. The arms of the bandit are parameters of the policy such as exploration noise, per-action biases etc. A proxy fitness metric is defined that measures the return of the trajectories upon perturbations of the policy z; the bandit then samples perturbations z that are better than the average fitness of the past few perturbations. I think this paper is just below the acceptance threshold. My reservations and comments are as follows. 1. While I see the value in designing an automatic exploration mechanism, the complexity of the underlying approach makes the contribution of the bandit-based algorithm difficult to discern from the large number of other bells and whistles in the experiments. For instance, the authors use Rainbow as the base algorithm upon which they add on the exploration. Rainbow itself is an extremely complicated algorithm, how can one be certain that the improvements in performance are caused by the improved exploration and not a combination of the bandit’s actions with the specifics of Rainbow? 2. I don’t understand Figure 4. The score defined in Appendix is the average over games for which seed performs better. Why is the random seed being used to compare the performance of different arms? Do you instead mean that s and s’ are two values of the arm in Figure 4? If not, how should one interpret Figure 4, no fixed arm is always good because the performance varies across the seeds. The curated bandit does not seem to be doing any better than a fixed arm. I have a few more questions that I would like the authors to address in their rebuttal or the paper. 1. The proxy f(z) does not bear any resemblance to LP(z). Why discuss the LP(z) then. The way f(z) is defined, it is just the value function averaged over perturbations of the policy. If one were to consider z as an additional action space that is available to the agent during exploration, f(z) is the value function itself. The exploration policy is chosen not to maximize the E_z [f(z)] directly but to maximize the lower bound in Markov’s inequality (P(f(z) >= t) <= E_z [f(z)]/t) in Section 4. 2. Can you elaborate more on the metric for measuring the learning progress LP? Why does the myopic metric make sense in spite of the there being plateaus in the training curves? 3. The key contribution of the paper that the authors could highlight better is that they do not add new hyper-parameters. In this aspect, the auto-tuner for exploration is a plug-and-play procedure in other RL algorithms. 4. From Figure 6 and Figure 8-11, it looks like the bandit is more or less on par with fixed exploration policies. What is the benefit of the added complexity?"}
{"id": "iclr2020_999", "title": "Interpreting video features: a comparison of 3D convolutional networks and convolutional LSTM networks | OpenReview", "abstract": "Abstract:###A number of techniques for interpretability have been presented for deep learning in computer vision, typically with the goal of understanding what it is that the networks have actually learned underneath a given classification decision. However, when it comes to deep video architectures, interpretability is still in its infancy and we do not yet have a clear concept of how we should decode spatiotemporal features. In this paper, we present a study comparing how 3D convolutional networks and convolutional LSTM networks respectively learn features across temporally dependent frames. This is the first comparison of two video models that both convolve to learn spatial features but that have principally different methods of modeling time. Additionally, we extend the concept of meaningful perturbation introduced by Fong & Vedaldi (2017) to the temporal dimension to search for the most meaningful part of a sequence for a classification decision.", "review": "Review:###This paper presents a paradigm for generating saliency maps for video models, specifically, I3D (3D CNN) and C-LSTM. It extends Fong & Vedaldi, 2017 to generate a temporal mask and introduces two types of *meaningful perturbations* for videos: freezing and reversing frames; they use Grad-CAM (with no modifications) for generating spatial masks. The problem is well-motivated, as saliency maps have been extensively studied for image classification models, but rarely for video classification. Quantitatively, they demonstrate that frame-reversal is meaningful for the Something-something dataset but less for KTH because those actions rely more on spatial information than temporal (i.e., running, clapping). Qualitatively, they show their spatial and temporal masks on both datasets and suggest the a few insights: * I3D*s Grad-CAM visualizations show a center, default bias * I3D is less sensitive to the reverse perturbation * I3D temporal masks are typically shorter. I currently rate this paper as a weak reject (though closer to borderline) paper for the following reasons (all of which can be improved in rebuttal): 1. Quality of technique While the temporal masks are novel and qualitatively *make sense* (though this is subjective), the generation of the spatial masks is not novel, is unconnected to the temporal mask generation, and often doesn*t make sense because of lack of temporal smoothness / cohesion, particularly for C-LSTM, where the visualizations when the mask is on appear quite *jumpy* (see Fig 2, Seq 1). It would be great to see more innovation on the temporal mask generation to address some of these issues (one natural approach that comes to mind would be learn spatial masks as done in Fong & Vedaldi, 2017, possibly with a temporal smoothness term between spatial masks and possibly combining temporal + spatial masks for freezing operation, i.e., only freeze spatial pixels) -- that said, I realize that this may be out of scope for a rebuttal. 2. Lack of support for qualitative claims The paper makes 3 claims based on qualitative examples shown (see above bullets in summary); these claims could be easily substantiated qualitatively (by evaluating over the dataset or a subset of it). 3. Lack of discussion on limitations/benefits of technique + how to use/interpret technique * There are no baseline comparisons for the proposed temporal mask generation. natural ones would be visualizing saliency methods (i.e., gradient, SmoothGrad, occlusion [Zeiler & Fergus, 2014], RISE [BMVC 2018], for instance, as examples of a few easy-to-implement, representative methods for backprop and perturbation methods) w.r.t. to temporal dimensions and then thresholding and applying those baseline temporal masks and demonstrating that the proposed temporal mask generation is. * There*s no discussion (or experiments) on the benefits & limitations of their approach; this is important as we*ve seen from papers like Mahendran & Vedaldi, ECCV 2016, Adebayo et al., NeurIPS 2018 and Kindermans et al., arXiv 2017 that some saliency methods fail to meet basic desirata (i.e., specificity to output class and model weights, etc.). Ideally, the authors would show that their temporal mask generation meets some desired criteria (as well as compare with baseline methods) to justify their approach. * One limitation of Fong & Vedaldi 2017 is the difficulty of finding global optimum for the different hyper-parameter terms (Fong et al, ICCV 2019 addresses this, which might be of interest to the authors). There*s no discussion about whether this problem persists for this work. Also -- it*d be interesting to see whether the reverse loss function (i.e., maximizing class score) yields similar results. * More discussion can be added about how to interpret results / use the technique (i.e., what is this technique useful/not useful for? what do results mean?). For instance, the claim that I3D temporal mask is shorter suggests a complex phenomenon -- that the necessary temporal evidence is smaller. This is a bit surprising to me, as I3D performs better overall, so I would have expected it to encode more redundancy (this can be checked by exploring the classes in which I3D does perform better) -- however, this interpretation differs from that of the authors (*This is especially visible in the temporal mask of Sequence #3, where it is active specifically*). In addition to responding to the above with relevant text + preliminary experimental results, I also had the following questions/asks: * For misclassified classes in Fig 2, are you optimizing w.r.t. the top predicted class or the ground truth class? Do they differ substantially when optimizing for different output classes? * What were the lambda hyperparameters from Eq. 1 (and how were they chosen)? Are these relatively stable or are their instances of technique failure due to the difficulty in balancing these terms? * Show Table 1 on only the classes that were focused on (i.e., the ones w comparable performance between the two models); I*m wondering if the impact of reversal on I3D is less than that on C-LSTM, as claimed by the authors (it*s hard to tell when their baseline performance is different)"}
